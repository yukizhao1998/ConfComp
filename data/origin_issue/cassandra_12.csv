Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Wrong bloom_filter_fp_chance for newly created CFs with LeveledCompactionStrategy,CASSANDRA-5093,12625373,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,27/Dec/12 23:58,16/Apr/19 09:32,14/Jul/23 05:52,28/Dec/12 15:38,1.2.0,,,,,,0,,,,,,,"0.1 is supposed to be the default bloom_filter_fp_chance for LeveledCompactionStrategy (and 0.01 for all other strategies).
However, CFPropDefs#applyToCFMetadata() sets bloom_filter_fp_chance before setting compaction strategy class, so the default bloom_filter_fp_chance is always 0.01 no matter what the compaction strategy is.

The fix is to move cfm#bloomFilterFpChance() call below cfm#compressionParameters().

The attached patch also kills dead default consistency level-related code.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/12 00:00;aleksey;5093.txt;https://issues.apache.org/jira/secure/attachment/12562532/5093.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301915,,,Fri Dec 28 15:37:58 UTC 2012,,,,,,,,,,"0|i16xef:",248543,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"28/Dec/12 15:20;jbellis;+1;;;","28/Dec/12 15:37;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_range_slices does not validate end_token,CASSANDRA-5089,12625154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,apesternikov,apesternikov,23/Dec/12 21:04,16/Apr/19 09:32,14/Jul/23 05:52,09/Jan/13 22:29,1.1.9,1.2.0,,,,,0,,,,,,,"get_range_slices times out, java log has the following exception:
ERROR [Thrift:1] 2012-12-22 08:14:30,120 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [DecoratedKey(28555413689034504124051437792156504, 6434313866653035643631663962323635323937343337653666636265616162),max(0)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:45)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:38)
        at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:698)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)

We see it every time on the SECOND get_range_slices call when we clear start_token and set start_key in the key range.
We noticed this in 1.1.7 first, 1.1.8 still affected. 1.1.6 is fine.
Please contact me if you need more information.
 ",,apesternikov,dbrosius,dbrosius@apache.org,kzadorozhny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5106,,,,"27/Dec/12 16:26;jbellis;5089-v2.txt;https://issues.apache.org/jira/secure/attachment/12562485/5089-v2.txt","24/Dec/12 02:17;jbellis;5089.txt;https://issues.apache.org/jira/secure/attachment/12562302/5089.txt","07/Jan/13 16:52;apesternikov;5089unittest.diff;https://issues.apache.org/jira/secure/attachment/12563593/5089unittest.diff",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301694,,,Tue Jan 08 21:40:49 UTC 2013,,,,,,,,,,"0|i16vpr:",248270,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,"24/Dec/12 02:03;jbellis;It looks like you are specifying start_key and end_token, with an invalid end_token.  Don't do that.  (If you want to specify ""the rest of the token range"" then use end_key="""" and leave end_token null.)
;;;","24/Dec/12 02:17;jbellis;Prior to CASSANDRA-4804 we always ignored end_token which is why this is biting you now.

Patch attached to add extra validation so we don't rely on AssertionError to stop this.;;;","24/Dec/12 15:44;apesternikov;what is the proper format for token? I assumed it is decimal string. I think it is worked in 1.1.6
re using end_key="""": start_key + end_token range has its very valid use case, when the whole data set should be processed (think of map/reduce) on multiple distributed workers. We split the ring into N parts, every worker start with (start_token - end_token) range, when first batch is received it switches to (star_key - end_token). ColumnFamilyRecordReader is cheating here by using internals for calculating token:
startToken = partitioner.getTokenFactory().toString(partitioner.getToken(Iterables.getLast(rows).key));
unfortunately it is not an option for us, we are using C++
So, what is the proper format for token and how to do iteration over several ranges?
Thank you.;;;","27/Dec/12 16:26;jbellis;simplified v2 attached ;;;","27/Dec/12 16:37;dbrosius@apache.org;v2 lgtm, assuming we don't allow a crazy combination of range.start_key and range.end_token or such.

also note, pushing to trunk will not compile cleanly as on trunk throws of IRE needs to be fully qualified... (or an import needs to be added).;;;","27/Dec/12 17:10;apesternikov;Dave,
either your statement about craziness of start_key - end_token or first paragraph of https://issues.apache.org/jira/browse/CASSANDRA-4804 description is incorrect.
I would like to reiterate that start_key - end_token combination let us split ranges and iterate over segments using API only without reaching internals.
Ok, if you guys decide that you don't want to support this combination, can you give us another tool for iteration? IMHO something like 
3:  optional string token,
in KeySlice would be sufficient for proper iteration over token ranges. As a free benefit we would get a better token iterator semantics.;;;","27/Dec/12 17:13;jbellis;bq. assuming we don't allow a crazy combination of range.start_key and range.end_token or such

Heh, that's what I was trying to validate in v1.  I retract v2. :);;;","27/Dec/12 17:25;dbrosius@apache.org;Sorry Aleksey, ignore my craziness comment :)

V1, needs semi at RowPosition stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)

also, it seems there is ambiquity if end_key and end_token are specified. (or start_key and start_token for that matter).;;;","27/Dec/12 19:17;dbrosius@apache.org;ah, there's a check at the top for this ambiquity, so i was mistaken...

patch lgtm, except for semi.;;;","27/Dec/12 20:45;jbellis;committed, thanks;;;","31/Dec/12 05:54;apesternikov;I applied the 5089-v2.txt to 1.1.8. Unfortunately, I have to report that it does not fix the problem.
It is not really surprising, because the patch does not change anything for our case of (start_key, end_token) range.
I have a huge log with debug logging level, please contact me if interested.;;;","31/Dec/12 06:18;dbrosius;i would have thought this

+            RowPosition stop = p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)
+            if (range.start_key != null && RowPosition.forKey(range.start_key, p).compareTo(stop) > 0)
+                throw new InvalidRequestException(""Start key's token sorts after end token"");


would have addressed the issue. Can you produce a simple test case that shows the problem?;;;","07/Jan/13 16:52;apesternikov;test attached.
Please note that you need -ea JVM command line flag to reveal the problem, without -ea it works fine.
I think the problem is in Bounds.java:45 assert statement:

        assert left.compareTo(right) <= 0 || right.isMinimum(partitioner) : ""["" + left + "","" + right + ""]"";

 Why left is even compared to right? Why ""bounds may not wrap"" as it stated in the comment? Logically, ""less"" and ""more"" are irrelevant to iterator position as we are talking about RING.;;;","08/Jan/13 21:40;jbellis;Let's leave this closed since 1.2.0 is already released with the patch.  CASSANDRA-5106 is open to fix a regression it caused.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Major compaction IOException in 1.1.8,CASSANDRA-5088,12625118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kmueller,kmueller,22/Dec/12 18:38,16/Apr/19 09:32,14/Jul/23 05:52,07/Jan/13 22:55,1.1.9,1.2.1,,,,,0,compression,,,,,,"Upgraded 1.1.6 to 1.1.8.

Now I'm trying to do a major compaction, and seeing this:

ERROR [CompactionExecutor:129] 2012-12-22 10:33:44,217 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:129,1,RMI Runtime]
java.io.IOError: java.io.IOException: Bad file descriptor
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:195)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:298)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Bad file descriptor
        at sun.nio.ch.FileDispatcher.preClose0(Native Method)
        at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.FileInputStream.close(FileInputStream.java:258)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
        at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
        at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
        at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
        at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
        at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
        at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
        ... 9 more
",,aleksey,cdaw,christianmovi,kmueller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/12 21:18;jbellis;5088-v2.txt;https://issues.apache.org/jira/secure/attachment/12562407/5088-v2.txt","26/Dec/12 22:04;jbellis;5088-v3.txt;https://issues.apache.org/jira/secure/attachment/12562411/5088-v3.txt","26/Dec/12 16:01;jbellis;5088.txt;https://issues.apache.org/jira/secure/attachment/12562383/5088.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301654,,,Mon Jan 07 22:56:42 UTC 2013,,,,,,,,,,"0|i16vbb:",248205,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,"22/Dec/12 20:48;jbellis;Jason reported something similar in CASSANDRA-5059.  Looks like an environment problem though since we can't reproduce on the same sstables.;;;","22/Dec/12 20:54;kmueller;What would be helpful to debug?  The same environment worked fine for 1.1.6 - I didn't try 1.1.7

I'm running a bit old of a JDK - I could try upgrading it.;;;","26/Dec/12 04:41;cdaw;I am able to consistently reproduce this running upgrade scenarios for DataStax Enterprise (basically C* 1.1.6 to C* 1.1.8).
* I can't reproduce this going from vanilla C* 1.1.6 to C* 1.1.8 using cassandra-stress
* I can reproduce this on my mac using DSE.  Java version is: 1.6.0_24
* I can't reproduce this on ubuntu precise 64-bit using Java 1.6.0_31

*Pre-Upgrade: run on DSE 2.2.1 / Cassandra 1.1.6*
{code}
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o INSERT_PRICES
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o UPDATE_PORTFOLIOS
~/dse-2.2.1/demos/portfolio_manager/bin/pricer -o INSERT_HISTORICAL_PRICES -n 100
~/dse-2.2.1/bin/dse  hive -f ~/dse-2.2.1/demos/portfolio_manager/10_day_loss.q
~/dse-2.2.1/bin/nodetool drain
sudo pkill -9 java

# then restart using C* 1.1.8
{code}

+Below are the different related errors+


*Post-Upgrade: read CF created pre-upgrade*
{code}
ERROR [Thrift:3] 2012-12-25 18:53:22,139 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:3,5,main]
java.io.IOError: java.io.IOException: Bad file descriptor
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore$2.close(ColumnFamilyStore.java:1411)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1490)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1435)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:50)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:876)
	at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:705)
	at com.datastax.bdp.server.DseServer.get_range_slices(DseServer.java:1087)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at com.datastax.bdp.transport.server.ClientSocketAwareProcessor.process(ClientSocketAwareProcessor.java:43)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:192)
{code}

*Post-Upgrade: running upgradesstables*
{code}
Error occured while upgrading the sstables for keyspace HiveMetaStore
java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Bad file descriptor
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:226)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:242)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:983)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1789)
{code}

*Post-Upgrade: running nodetool scrub*
{code}
WARN [CompactionExecutor:23] 2012-12-25 14:42:50,024 FileUtils.java (line 116) Failed closing /var/lib/cassandra/data/cfs/inode/cfs-inode-hf-1-Data.db - chunk length 65536, data length 48193.
java.io.IOException: Bad file descriptor
	at sun.nio.ch.FileDispatcher.preClose0(Native Method)
	at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.FileInputStream.close(FileInputStream.java:258)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:121)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
	at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:130)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:112)
	at org.apache.cassandra.db.compaction.Scrubber.close(Scrubber.java:306)
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:500)
	at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:485)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:69)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:235)
	at org.apache.cassandra.db.compaction.CompactionManager$3.call(CompactionManager.java:205)
{code}
;;;","26/Dec/12 15:17;jbellis;Can you try with compression disabled?;;;","26/Dec/12 16:01;jbellis;... Testing w/o compression won't be necessary, this is clearly a problem with the compression code only.

This looks like the following JDK bug: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6322678

It is marked fixed in JDK7, but inspection of JDK6u38 source looks like the fix is there as well.  Not sure how early the fix was made in JDK6.

So upgrading to the latest jdk6 or jdk7 should fix the problem.  I've also attached a workaround for platforms (basically: Mac OS users running DSE) where that isn't an option.;;;","26/Dec/12 21:18;jbellis;v2 removes a redundant channel creation which might also help.;;;","26/Dec/12 21:49;cdaw;Results from testing v2:
* We originally reproduced this with a forked/modified version of C* 1.1.8 dropped in to DSE.  When we dropped in the C* 1.1.8 jar file from the apache download, we were also to reproduce this as well.
* Post-upgrade exception running:  list Stocks
* Post-upgrade exception running: nodetool upgradesstables
* Post-upgrade no errors running: nodetool compact or nodetool scrub

{code}
ERROR [Thrift:3] 2012-12-26 13:42:25,343 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[Thrift:3,5,main]
java.io.IOError: java.io.IOException: Bad file descriptor
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore$2.close(ColumnFamilyStore.java:1411)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1490)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1435)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:50)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:876)
	at org.apache.cassandra.thrift.CassandraServer.get_range_slices(CassandraServer.java:703)
	at com.datastax.bdp.server.DseServer.get_range_slices(DseServer.java:1087)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3083)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices.getResult(Cassandra.java:3071)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at com.datastax.bdp.transport.server.ClientSocketAwareProcessor.process(ClientSocketAwareProcessor.java:43)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:192)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Bad file descriptor
	at sun.nio.ch.FileDispatcher.preClose0(Native Method)
	at sun.nio.ch.FileDispatcher.preClose(FileDispatcher.java:59)
	at sun.nio.ch.FileChannelImpl.implCloseChannel(FileChannelImpl.java:96)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at java.io.RandomAccessFile.close(RandomAccessFile.java:541)
	at org.apache.cassandra.io.util.RandomAccessReader.close(RandomAccessReader.java:224)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.close(CompressedRandomAccessReader.java:131)
	at org.apache.cassandra.io.sstable.SSTableScanner.close(SSTableScanner.java:89)
	at org.apache.cassandra.utils.MergeIterator.close(MergeIterator.java:61)
{code}
;;;","26/Dec/12 22:04;jbellis;v3 removes CRAR.source entirely in favor of using the FileChannel handle we already have in RAR;;;","26/Dec/12 22:23;cdaw;[~jbellis]
three is the charm. the last patch worked.;;;","07/Jan/13 17:35;aleksey;+1;;;","07/Jan/13 22:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing from higher to lower compaction throughput causes long (multi hour) pause in large compactions,CASSANDRA-5087,12625016,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jblangston@datastax.com,jblangston@datastax.com,jblangston@datastax.com,21/Dec/12 16:17,16/Apr/19 09:32,14/Jul/23 05:52,27/Dec/12 15:15,1.1.9,1.2.1,,,,,0,,,,,,,"We're running a major compaction against a column family that is 2.1TB (yes, I know it's crazy huge, that's an entirely different discussion). During the evenings, we run a setcompactionthroughput 0 to unthrottle completely, and throttle again down to 20mb at the end of the maintenance window. 

Every morning we've come in to check progress, we find that the progress completely halts as soon as the compaction throttling command is issued. Eventually, compaction continues. I was looking at the throttling code, and I think I see the issue, but would like confirmation:

throttleDelta (org.apache.cassandra.utils.Throttle.throttleDelta) sets a sleep time based on the amount of data transferred since the last throttle time. Since we've gone from 20 MB to wide open, and back to 20MB, the wait that is calculated is based on an attempt to average the new throttling rate over the last 6.5 hours of running wide open.

I think this could be fixed by adding a reset of bytesAtLastDelay and timeAtLastDelay to the current values after the check at line 64:

Current:

        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) 
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
        targetBytesPerMS = newTargetBytesPerMS;

New:

 
        // if the target changed, log
        if (newTargetBytesPerMS != targetBytesPerMS) {
            logger.debug(""{} target throughput now {} bytes/ms."", this, newTargetBytesPerMS);
            if(newTargetBytesPerMS < targetBytesPerMS || targetBytesPerMS < 1) {
            	bytesAtLastDelay += bytesDelta;
            	timeAtLastDelay = System.currentTimeMillis();
                targetBytesPerMS = newTargetBytesPerMS;
            	return;
            }
            targetBytesPerMS = newTargetBytesPerMS;
        }

Some redundancies that can be removed there, but I wanted to keep the approach local to where I thought the problem was. ",,christianmovi,jblangston@datastax.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jblangston@datastax.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301552,,,Thu Dec 27 15:15:35 UTC 2012,,,,,,,,,,"0|i16umf:",248093,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Dec/12 15:15;jbellis;Added comments and committed.  (Note: to 1.1.9 and 1.2.1, but not 1.2.0.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow counters in collection (CQL3),CASSANDRA-5082,12624811,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Dec/12 13:10,16/Apr/19 09:32,14/Jul/23 05:52,20/Dec/12 15:13,1.2.0,,,,,,0,,,,,,,We don't support counters in collections but we don't throw an error when someone tries to create such a thing. Attaching patch to return a validation error.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/12 13:10;slebresne;5082.txt;https://issues.apache.org/jira/secure/attachment/12561885/5082.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301319,,,Thu Dec 20 15:13:29 UTC 2012,,,,,,,,,,"0|i16rnj:",247612,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Dec/12 14:39;jbellis;+1;;;","20/Dec/12 15:13;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't support JMX authentication.,CASSANDRA-5080,12624803,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,michalm,solf,solf,20/Dec/12 11:17,16/Apr/19 09:32,14/Jul/23 05:52,15/Feb/13 19:55,1.1.11,1.2.2,,Legacy/Tools,,,0,,,,,,,"It seems that cassandra-cli doesn't support JMX user authentication.

Specifically I went about securing our Cassandra cluster slightly -- I've added cassandra-level authentication (which cassandra-cli does support), but then I discovered that nodetool is still completely unprotected. So I went ahead and secured JMX (via -Dcom.sun.management.jmxremote.password.file and -Dcom.sun.management.jmxremote.access.file). Nodetool supports JMX authentication via -u and -pw options.

However it seems that cassandra-cli doesn't support JMX authentication, e.g.:
{quote}
apache-cassandra-1.1.6\bin>cassandra-cli -h hostname -u experiment -pw password
Starting Cassandra Client
Connected to: ""db"" on hostname/9160
Welcome to Cassandra CLI version 1.1.6

[experiment@unknown] show keyspaces;
WARNING: Could not connect to the JMX on hostname:7199, information won't be shown.

Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
  Durable Writes: true
    Options: [replication_factor:1]
... (rest of keyspace output snipped)
{quote}

help connect; and cassandra-cli --help do not seem to indicate that there's any way to specify JMX login information.",,aleksey,solf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/13 18:41;michalm;5080-v1.patch;https://issues.apache.org/jira/secure/attachment/12567615/5080-v1.patch","01/Feb/13 18:41;michalm;enable-jmx-authentication.patch;https://issues.apache.org/jira/secure/attachment/12567616/enable-jmx-authentication.patch",,,,,,,,,,,,,,,,2.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301311,,,Fri Feb 15 19:55:19 UTC 2013,,,,,,,,,,"0|i16rbr:",247559,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,"01/Feb/13 18:41;michalm;Attaching patch (5080-v1.patch) that adds JMX authentication support for cassandra-cli. 

For easier testing apply the second patch (enable-jmx-authentication.patch) - it enables JMX authentication for Cassandra. With this patch you will have two roles available for test (username / password / permissions):
adminRole / admin / readwrite
userRole / user / readonly

Then start Cassandra and launch:

{noformat}./bin/cassandra-cli --jmxusername adminRole --jmxpassword admin{noformat}

or 

{noformat}./bin/cassandra-cli --jmxusername userRole --jmxpassword user{noformat}
;;;","15/Feb/13 19:55;aleksey;Committed. Thanks, Michał.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction deletes ExpiringColumns in Secondary Indexes,CASSANDRA-5079,12624763,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,20/Dec/12 04:05,16/Apr/19 09:32,14/Jul/23 05:52,20/Dec/12 09:12,1.1.9,1.2.0 rc2,,Feature/2i Index,,,0,,,,,,,"From this discussion http://www.mail-archive.com/user@cassandra.apache.org/msg26599.html

CompactionManager.getDefaultGcBefore() set's the gc_before to be Integer.MAX_VALUE. 

In the example all entries in the secondary index have a TTL. In PreCompactedRow.removeDeletedAndOldShards() the CF is determined to have irrelevant data, the call to CFS.removeDeleted() results in the ExpiringColumns being removed and the row is treated as empty. CompactionTask.execute() exits at the {{if (!nni.hasNext())}} test, so the sstables are marked as compacted and soon deleted. 

In the example the localDeletionTime was Thu, 21 Mar 2013 08:25:22 GMT and should not have been purged. 

In the example when the first compaction on the secondary index runs the on disk data for the index is deleted. The logs show a compaction starting and no associated ""Compacted to"" message from that compaction thread. 

The impact is incorrect results from secondary indexes queries.",,christianmovi,colinkuo,mck,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5024,,CASSANDRA-4670,,,,,,,,"20/Dec/12 04:32;amorton;5079.txt;https://issues.apache.org/jira/secure/attachment/12561842/5079.txt",,,,,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,301269,,,Fri Dec 21 08:05:39 UTC 2012,,,,,,,,,,"0|i16r1b:",247512,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"20/Dec/12 04:32;amorton;first version uses the current time for gc_before for secondary indexes.;;;","20/Dec/12 05:09;jbellis;Thanks Aaron, that looks like good detective work.

(Setting affects-version to when 2I were added.  Please correct if this was a more recent regression.);;;","20/Dec/12 08:07;amorton;Yup, was a fun few hours.

bq. (Setting affects-version to when 2I were added. Please correct if this was a more recent regression.)
Yes it has always been there. 

Will need to patch 0.8 onwards. Do we still release 0.7?;;;","20/Dec/12 08:31;slebresne;That patch is reversed, right? (i.e. it adds what it is supposed to remove and remove what it is supposed to add)

bq. Will need to patch 0.8 onwards. Do we still release 0.7?

We don't release 0.7 and we don't release 0.8 either. I don't even want to release a 1.0 anymore. If you haven't complained about this, you probably don't use 2i and TTL. Even if you do, then that's a good excuse to upgrade to 1.1 at last.;;;","20/Dec/12 09:12;slebresne;Alright, I've committed assuming the patch was indeed reversed (didn't make any sense otherwise anyway). Thanks.;;;","21/Dec/12 02:27;amorton;bq. assuming the patch was indeed reversed

Not sure what you mean. Did i get the diff wrong?;;;","21/Dec/12 08:05;slebresne;Yeah. If you look at the patch, the '+' should be '-' and vice-versa. You probably did a {{git diff patched HEAD}} instead of {{git diff HEAD patch}}. But anyway, that's ok, no harm done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Murmur3Partitioner#describeOwnership calculates ownership% wrong,CASSANDRA-5076,12624529,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,18/Dec/12 20:37,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/12 18:44,1.2.0 rc2,,,,,,0,,,,,,,"When I issued 'nodetool status' on Murmur3-partitioned cluster I got the following output:

{code}
$ bin/nodetool -p 7100 status                                                                                                                                                                                                                                                                                       (git)-[5065]-[~/Developments/workspace/cassandra]
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Owns   Host ID                               Token                                    Rack
UN  127.0.0.1         24.78 KB   66.7%  ace7e54c-9fe1-4b23-83b0-949772b24c30  -9223372036854775808                     rack1
UN  127.0.0.2         29.22 KB   66.7%  67146442-dbfd-449c-82e1-26729b8ac89c  -3074457345618258603                     rack1
UN  127.0.0.3         6.19 KB    66.7%  3fab9f18-daf3-4452-8b9c-204ea0ee2e15  3074457345618258602                      rack1
{code}

Notice that 'Owns' percentages add up to 200%.

I think the problem is that Murmur3Partitioner#describeOwnership currently calculate ownership% based on [0, Long.MAX_VALUE] range, but we have to consider about negative tokens.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4598,,,,,,,,"19/Dec/12 18:16;yukim;5076.txt;https://issues.apache.org/jira/secure/attachment/12561755/5076.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,300350,,,Wed Dec 19 18:44:36 UTC 2012,,,,,,,,,,"0|i167dz:",244329,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"19/Dec/12 18:16;yukim;Patch attached to calculate ownership using BigInteger/BigDecimal for range (Long.MIN_VALUE, Long.MAX_VALUE].
This will also resolve problem described in CASSANDRA-4598.;;;","19/Dec/12 18:34;slebresne;For the last range, I think the {{ti.subtract(tim1)}} part should be changed to {{(BigIntegerToken)start).token.subtract(ti)}} as done in RandomPartitionner.describeOwnership. Cause at the end of the loop, {{ti == tim1}}.

But +1 with that fixed.;;;","19/Dec/12 18:44;yukim;Committed with fix for review. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an official way to disable compaction,CASSANDRA-5074,12624486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,jbellis,jbellis,18/Dec/12 16:13,16/Apr/19 09:32,14/Jul/23 05:52,09/Apr/13 16:38,2.0 beta 1,,,,,,0,,,,,,,"We've traditionally used ""min or max compaction threshold = 0"" to disable compaction, but this isn't exactly intuitive and it's inconsistently implemented -- allowed from jmx, not allowed from cli.",,a.gazzarini,liqusha,marcuse,mbulman,slebresne,vongocminh,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 13:49;marcuse;0001-CASSANDRA-5074-make-it-possible-to-disable-autocompa.patch;https://issues.apache.org/jira/secure/attachment/12577213/0001-CASSANDRA-5074-make-it-possible-to-disable-autocompa.patch","09/Apr/13 10:25;marcuse;0001-CASSANDRA-5074-v2.patch;https://issues.apache.org/jira/secure/attachment/12577774/0001-CASSANDRA-5074-v2.patch",,,,,,,,,,,,,,,,2.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,300035,,,Mon Dec 02 22:27:29 UTC 2013,,,,,,,,,,"0|i165fr:",244013,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"18/Dec/12 16:13;jbellis;Adding a NoCompactionStrategy may be the simplest approach.;;;","15/Feb/13 17:12;a.gazzarini;Hi, as far as I understood, that requires writing a NullObject implementation of AbstractCompactionStrategy. Is that enough? I tried in my local Cassandra installation 

> create column family with compaction_strategy=NoCompactionStrategy 

and effectively the compaction never run...but to be honest I'm a Cassandra newbie so I'm wondering if that is enough. In this case I could submit the patch, otherwise please point me to the right direction, i'd like to give a hand. ;;;","17/Feb/13 13:51;a.gazzarini;It seems things are not so easy as described in my previous post. At the moment there are two possible strategies: Leveled and SizeTiered compaction. Looking at their code it seems that a disabled compaction should affect only getNextBackgroundTask(...) and not getMaximalTask and getUserDefinedTask (although this latter is not allowed in LeveredCompaction). 
So the question is: what kind of compaction should run when I'm using a NoCompactionStrategy and those two methods are invoked? SizeTiered, Leveled or another that maybe will be implemented tomorrow and plugged in?  

The obvious thing that comes in my mind is to think about NoCompactionStrategy as a decorator; something like

> create column family X with compaction_strategy=NoCompactionStrategy AND compaction_strategy_options={delegate_strategy: LeveledCompactionStrategy, <other options>};
> create column family X with compaction_strategy=NoCompactionStrategy AND compaction_strategy_options={delegate_strategy: SizeTieredCompactionStrategy, <other options>};

So basically the getNextBackgroundTask will do nothing, effectively disabling (minor) compaction, and the getMaximalTask and getUserDefinedTask will delegate to a wrapped strategy that has been injected (the ""delegate_strategy"" option). Now, I have a problem with the design of the XXXCompactionStrategy: in general compaction strategies (this is valid both for SizeTiered and Leveled) 

- have a lot of initialization code in their constructor, this makes hard to use those strategies as an ""inner"" state of a decorator, I mean something like : decoratee.init() --> decoratee.getMaximalTask() --> decoratee.close();   
- receive a shared instance of ColumnFamilyStore and eventually (e.g. SizeTieredCompactionStrategy) modify it; the initialization code assumes no other compaction strategy are associated with the column family; so, based on this assumption, the concrete strategy instance is free to modify the state of its context (e.g. column family store)     

All above makes difficult to implement a decorator pattern because the decoratee instance will modify the same column family store of the decorator (for example the SizeTieredCompactionStrategy modifies the min and max threshod which are supposed to be 0 in case of compaction disabled).    

For example, the ColumnFamilyStore has a method isCompactionDisabled that basically checks if min or max threshols are set to 0. Now, when instantiating the NoCompactionStrategy, one of the first thing that shoujld be done is 

store.setCompactionThresholds(0,0);

after that, the wrapped instance should be initialized. Suppose for example that the decoratee instance is a SizeTieredCompactionStrategy; creating a new instance will immediately reset the previous thresholds to 4 and 32 because the STCS has this code in its constructor:

cfs.setCompactionThresholds(cfs.metadata.getMinCompactionThreshold(), cfs.metadata.getMaxCompactionThreshold());

Nothing, just to say that a simple NullObject is not enough, I'm trying to solve that; once did, I'll submit a patch.  ;;;","17/Feb/13 19:14;slebresne;To be honest, I'm not completely convinced about that NoCompactionStrategy idea. I agree with the ticket premises than using min/max thresholds is adhoc, especially since those threshold have no other meaning for leveled compaction.  However, as Andrea says, we only want to disable automatic (background) compaction, not the user triggered ones, so replacing the strategy entirely feels overkill to me. Overall, a simple boolean in ColumnFamilyStore would seem much simpler to me. And since we already have the disableAutoCompaction call in JMX (for some reason it doesn't seem the enableAutoCompaction is expose in JMX so we should add it), I'd suggest using that as the sole way to enable/disable auto compactions.
;;;","18/Feb/13 08:48;a.gazzarini;Right, I agree...the simplest thing that could possibly work; but in my opinion there's still something that needs to be changed: setting the min and / or max threshold, strictly speaking doesn't disable the compaction, because is up to the concrete strategy implementor to check that doing 

if (cfs.isCompactionDisabled()) {
...
}

I think it should be better to move this responsibility to the superlayer. 
So my suggestion is to change a little bit the AbstractCompactionStrategy in order to use a template method. Something like this:

(AbstractCompactionStrategy)

{noformat}
final synchronized AbstractCompactionTask getNextBackgroundTask(final int gcBefore)
{
    if (!cfs.isCompactionDisabled()) { 
       doGetNextBackgroundTask(gcBefore);
    }
}
...

abstract AbstractCompactionTask doGetNextBackgroundTask(final int gcBefore);
{noformat}
;;;","18/Feb/13 17:27;slebresne;In case that wasn't clear, I didn't said there is nothing to change. On the contrary, we should *stop* relying on setting the min/max compaction threshold to disable compaction (I'd be in favor of always considering that setting them to 0 is disallowed, but we'd have to leave that to 2.0 to not break it in a minor release). So instead I would add a per-cfs 'isAutoCompactionEnable' boolean. As for the call to isCompactionDisabled, it should indeed be moved, but I would move it to CompactionManager directly (where I would put 2 checks, one prior to submitting to the compaction executor, to avoid submitting useless task in the first place, and then again before the call to getNextBackgroundTask, just in case the runnable has been sitting for too long on the executor and has missed the first check).;;;","21/Feb/13 12:08;a.gazzarini;Probably that wasn't clear, but honestly I believe it's a problem of mine: I have just a little bit of experience on Cassandra source code and its internal architecture. 

Ok, now I think I got the point about your idea, so I will continue investigating the code in order to implement that. 

A question: in case the logic is moved on the CompactionMananger, what should be, from client point of view, a nice and good way to indicate that the (background) compaction must be disabled? I believe the command line of my previous post is no longer appropriate

> create column family XYZ with compaction_strategy=NoCompactionStrategy 

   ;;;","05/Apr/13 13:49;marcuse;* adds nodetool commands (disableautocompaction, enableautocompaction)
* makes it possible to set via schema, ""update cf x with auto_compaction_disabled=true""
* should be backwards compactible, if someone has disabled compactions with max_compaction_threshold, it will disable compactions using the new way (maybe should output a warning or similar though);;;","08/Apr/13 09:59;slebresne;The general idea lgtm. A few remarks though:
* There is no handling of CQL3 (which is in the package org.apache.cassandra.cql3). I'll not that for CQL3, this setting should probably be a 'compaction' option, not at top-level one. Thruth being told, maybe it would be cleaner to have that be a compaction option in the code too (handled by AbstractCompactionStrategy)?
* Currently, whether compaction is disabled or not is checked by the compaction strategy themselves. In particular, SizeTiered directly check the min and max thresholds (rather than calling isCompactionDisabled) in getNextBackgroundSSTables, so that needs to be fixed. Furthermore, those checks are redundant with the newly added checks in CompactionManager. And since there's the new 'isActive' flag that also mean ""don't create a compaction task"", maybe it would be simpler to create an AbstractCompactionStrategy.isEnabled() method that would return 'isActive && !isAutoCompactionDisabled()' and use that exclusively.
* We should probably now refuse setting the min/max thresholds to 0 everywhere (and add a mention in the NEWS file).
* In nodeCmd, the convention for methods that take a keyspace and column family argument is that if they are not given, the method applies to all keyspace/CF. Could be handy here too.
* Nit: For the enableAutoCompaction with an argument, we can add a @VisibleForTesting annotation.
;;;","09/Apr/13 10:25;marcuse;+1 on all the comments

now it is a compaction_strategy_option in cli, and a compaction = {..} option in CQL.

i also removed a call to cfs.setCompactionThresholds in the STCS constructor, couldn't find a reason it was there.

and, if there is a better way of disallowing min/max thresholds = 0 than checking in cql, cql3 and cli that does not break existing schemas, let me know;;;","09/Apr/13 14:45;slebresne;bq. i also removed a call to cfs.setCompactionThresholds in the STCS constructor, couldn't find a reason it was there

Back in the days (like 1.0 old, when dinosaurs where roaming the earth), we used to set the min/max thresholds to 0/Integer.MAX_VALUE in LCS. So STCS needed to restore the settings to sane levels in case we were switching from LCS to STCS. But since then we've fixed our ways (in CASSANDRA-4233 apparently, thanks git pickaxe) so this is just some leftover. It's fine removing it and you can even remove the comment the patch adds imo.

bq. if there is a better way of disallowing min/max thresholds = 0 than checking in cql, cql3 and cli

Not really.

Otherwise, I've just realized that following CASSANDRA-3430, we need to check the ACS.isActive flag *within* getNextBackgroundTask (with the synchronized block in particular) otherwise it could be racy. So I think we should move back the isAutoCompactionDisabled check from CompactionManager to STCS/LCS.getNextBackgroundTask (it's fine to keep the check at the beginning of CompactionManager.submitBackground however, no point is pushing tasks that will do nothing on the executor). My bad for suggesting otherwise.

But with the above fixed, +1.

Nit: could be nice to preserve the comment in CFS.disableAutoCompaction(), it's still useful.
;;;","09/Apr/13 16:38;marcuse;thanks, fixed the comments and pushed as ebefb77c6e8a5046a8c1b1bb0edd258aaf0ad8b7;;;","02/Dec/13 17:56;vongocminh;Hello,

Could you please confirm that KS and CF are ""optional"" parameter in nodetool disableautocompaction?

We are running C* v2.0.3 and the command nodetool does not seem working. We disable the auto compaction on all nodes via nodetool and the servers continue to compact CFs.
{quote}
INFO 18:45:03,594 Compacted 4 sstables to [D:\AtlasData\titan\pdl_identity\titan-pdl_identity.pdl_identity_portfolio_idx-jb-449,].  183á596 bytes to 183á289 (~99% of original) in 546ms = 0,320143MB
{quote}
Thanks for your help.
Best regards,
Minh;;;","02/Dec/13 18:46;marcuse;yes, without ks/cf it disables for all column families

note that it resets on restart though, if you want to disable it forever, you need to set the compaction strategy option ""enabled"" to false

if it does not disable all compactions, it is a bug

;;;","02/Dec/13 19:15;vongocminh;I'm kind of lost here: with nodetool in C* v2.0.1, *disableautocompaction* does no effect (servers continue compressing SSTables); with v2.0.3, the ""feature"" is no longer recorgnized by nodetool:
{quote}
C:\Workspace\apache-cassandra-2.0.3\bin>nodetool -h parw00146880 disableautocompaction
Starting NodeTool
Unrecognized command: disableautocompaction
usage: java org.apache.cassandra.tools.NodeCmd --host <arg> <command>
...
{quote};;;","02/Dec/13 19:33;marcuse;uh that is weird, just downloaded the 2.0.3 tarball and disableautocompaction exists

btw, nodetool disableautocompaction disables for all *existing* column families, so if you disableautocompaction and then start stress (for example), it will not be disabled for the cfs it creates;;;","02/Dec/13 22:27;vongocminh;Thanks a lot for your quick answer.
It is indeed very weird. I downloaded the binary for Windows from this address last week:
http://archive.apache.org/dist/cassandra/2.0.3/

I will recheck it tomorrow ...
(edit)
The cqlsh shows this:
{quote}
[cqlsh 4.1.0 | Cassandra 2.0.3 | CQL spec 3.1.1 | Thrift protocol 19.38.0]
{quote}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in creating EnumSet in SimpleAuthorizer example,CASSANDRA-5072,12624114,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jsanda,jsanda,15/Dec/12 13:24,16/Apr/19 09:32,14/Jul/23 05:52,17/Dec/12 15:45,1.2.0 rc2,,,,,,0,authentication,,,,,,"In SimpleAuthorizer around line 47 we have,

EnumSet<Permission> authorized = EnumSet.copyOf(Permission.NONE);

This results in an IllegalArgumentException since Permission.NONE is an empty set. I think it should be changed to,

EnumSet<Permission> authorized = EnumSet.noneOf(Permission.class);
",,aleksey,dbrosius@apache.org,jsanda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/12 16:07;aleksey;5072.txt;https://issues.apache.org/jira/secure/attachment/12561191/5072.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,298474,,,Mon Dec 17 15:44:57 UTC 2012,,,,,,,,,,"0|i15sen:",241902,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Dec/12 01:09;dbrosius@apache.org;it used to just return Permission.NONE (1.1), which is an immutable set. doing what this patch says seems fine to me.;;;","16/Dec/12 15:54;aleksey;It used to return Permission.NONE which used to be a (mutable) EnumSet. In 1.2 it's an ImmutableSet, as it should be. EnumsSet.copyOf is there to convert it back to EnumSet, since that's what the old interface requires.

I guess we should fix it, even though Simple* examples will be dropped in 1.2.1 or 1.2.2 entirely (were supposed to be dropped in 1.2.0 actually, which is why there wasn't much testing done with them).;;;","16/Dec/12 15:57;aleksey;Still, there is a reason why they are in 'examples' directory. They are not intended to be used in production, and now they don't even serve as good IAuthenticator/IAuthorizer examples - they only showcase LegacyAuthenticator/LegacyAuthorizer classes).

Maybe we should drop'em for 1.2.0 actually.;;;","16/Dec/12 16:11;aleksey;The attached patch updates SimpleAuthorizer. Not sure if the right solution is to fix it or to drop these examples entirely, but it should be resolved somehow.;;;","17/Dec/12 15:26;jbellis;Let's go ahead and fix it (and drop the examples for 2.0).

Patch LGTM.;;;","17/Dec/12 15:44;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_QUORUM consistency causes Tracing to fail,CASSANDRA-5070,12623989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,tjake,tjake,14/Dec/12 15:21,16/Apr/19 09:32,14/Jul/23 05:52,17/Dec/12 15:46,1.2.0,,,,,,0,,,,,,,"{code}
cqlsh:prod4> CONSISTENCY LOCAL_QUORUM;
Consistency level set to LOCAL_QUORUM.

cqlsh:prod4> TRACING ON;
Now tracing requests.
cqlsh:prod4> select * from table1 limit 10 

Bad Request: consistency level LOCAL_QUORUM not compatible with replication strategy (org.apache.cassandra.locator.SimpleStrategy)
{code}


Looks to be the issue with LocalStrategy",,aleksey,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/12 17:20;aleksey;5070.txt;https://issues.apache.org/jira/secure/attachment/12560998/5070.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297889,,,Mon Dec 17 15:45:50 UTC 2012,,,,,,,,,,"0|i14vv3:",236627,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Dec/12 16:26;jbellis;(a) this has nothing to do with tracing

(b) The IRE is deliberate; asking for LOCAL_QUORUM with a DC-oblivious replication strategy is nonsensical;;;","14/Dec/12 16:40;tjake;My replication strategy *is* NTS, the SimpleStrategy is being used in the tracing system space.;;;","14/Dec/12 16:57;jbellis;Ah, that makes sense.  Sounds like cqlsh needs to set CL back to ONE when selecting from the trace events.;;;","14/Dec/12 17:22;aleksey;The attached patch also forces cf-name autocompletion and DESCRIBE to use CL.ONE.;;;","17/Dec/12 15:27;jbellis;+1;;;","17/Dec/12 15:45;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLONE - Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered",CASSANDRA-5068,12623863,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,peter-librato,peter-librato,13/Dec/12 23:50,16/Apr/19 09:32,14/Jul/23 05:52,11/Feb/13 19:06,1.1.10,1.2.2,,,,,0,qa-resolved,,,,,,"We have ""0 row"" hinted handoffs every 10 minutes like clockwork. This impacts our ability to monitor the cluster by adding persistent noise in the handoff metric.

Previous mentions of this issue are here:
http://www.mail-archive.com/user@cassandra.apache.org/msg25982.html

The hinted handoffs can be scrubbed away with
nodetool -h 127.0.0.1 scrub system HintsColumnFamily
but they return after anywhere from a few minutes to multiple hours later.

These started to appear after an upgrade to 1.1.6 and haven't gone away despite rolling cleanups, rolling restarts, multiple rounds of scrubbing, etc.

A few things we've noticed about the handoffs:
1. The phantom handoff endpoint changes after a non-zero handoff comes through

2. Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before

3. The sstable2json output seems to include multiple sub-sections for each handoff with the same ""deletedAt"" information.



The phantom handoff endpoint changes after a non-zero handoff comes through:
 INFO [HintedHandoff:1] 2012-12-11 06:57:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:35,092 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.1
 INFO [HintedHandoff:1] 2012-12-11 07:07:37,915 HintedHandOffManager.java (line 392) Finished hinted handoff of 1058 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:17:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2
 INFO [HintedHandoff:1] 2012-12-11 07:27:35,093 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.2



Sometimes a non-zero handoff will be immediately followed by an ""off schedule"" phantom handoff to the endpoint the phantom had been using before:
 INFO [HintedHandoff:1] 2012-12-12 21:47:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 21:57:39,335 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,319 HintedHandOffManager.java (line 392) Finished hinted handoff of 1416 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:07:43,320 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.3
 INFO [HintedHandoff:1] 2012-12-12 22:17:39,357 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4
 INFO [HintedHandoff:1] 2012-12-12 22:27:39,337 HintedHandOffManager.java (line 392) Finished hinted handoff of 0 rows to endpoint /10.10.10.4



The first few entries from one of the json files:
{
    ""0aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"": {
        ""ccf5dc203a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
        ""ccf603303a2211e20000e154da71a9bb"": {
            ""deletedAt"": -9223372036854775808, 
            ""subColumns"": []
        }, 
","cassandra 1.1.6
java 1.6.0_30",christianmovi,kmueller,mheffner,mkjellman,peter-librato,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3733,,,,,,,,,,,,"05/Feb/13 21:43;brandon.williams;5068.txt;https://issues.apache.org/jira/secure/attachment/12568090/5068.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297583,,,Mon Feb 11 19:06:24 UTC 2013,,,,,,,,,,"0|i14rjb:",235926,,jbellis,,jbellis,Low,,,,,,,,,,,,,,enigmacurry,,,"13/Dec/12 23:53;peter-librato;Cloning CASSANDRA-3733 as it seems to be the same issue.;;;","14/Dec/12 00:11;peter-librato;When there are zero-row hinted handoffs the output of ""list HintsColumnFamily""
might show that 9 of 12 nodes in a ring have a row key like this:
9 of 12 nodes in a ring might have a row key like this:
RowKey: 75555555555555555555555555555554

1 of the 12 nodes will have a different row key than all the rest:
RowKey: 15555555555555555555555555555554

another 1-2 nodes might not have any RowKeys at all
;;;","14/Dec/12 02:18;jbellis;Somehow you've got an empty hints row there that hasn't gotten compacted away.  Not sure how that's possible since we only ever do one handoff at a time per target, and each handoff does a full compaction after it's delivered hints successfully.;;;","11/Jan/13 06:23;mkjellman;reproduced this in 1.2.0 after a rolling restart of the cluster;;;","11/Jan/13 16:07;brandon.williams;[~mkjellman] can you post logs?;;;","11/Jan/13 16:35;brandon.williams;I'm not sure how we're getting into this situation with an empty hint row (machine restarted before compaction finished?) but one thing we can do to mitigate it is remove the check that we replayed > 0 rows before compacting.  It shouldn't really be necessary since the isEmpty check on hintStore should prevent it, unless something like this has happened.;;;","11/Jan/13 16:46;mkjellman;a bit messy due to the repair log lines

{code}
tem-local-ib-671-Data.db'), SSTableReader(path='/data2/cassandra/system/local/system-local-ib-672-Data.db'), SSTableReader(path='/data2/cassandra/system/local/system-local-ib-670-Data.db')]
 INFO [CompactionExecutor:45] 2013-01-10 21:57:11,166 CompactionTask.java (line 267) Compacted 4 sstables to [/data/cassandra/system/local/system-local-ib-673,].  975 bytes to 590 (~60% of original) in 214ms = 0.002629MB/s.  4 tot
al rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
 INFO [GossipStage:1] 2013-01-10 21:57:16,342 Gossiper.java (line 772) InetAddress /10.8.30.102 is now dead.
 INFO [GossipStage:1] 2013-01-10 21:59:01,958 Gossiper.java (line 790) Node /10.8.30.102 has restarted, now UP
 INFO [GossipStage:1] 2013-01-10 21:59:01,959 Gossiper.java (line 758) InetAddress /10.8.30.102 is now UP
 INFO [HintedHandoff:2] 2013-01-10 21:59:01,960 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,000 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-hints@479784922(38/69 serialized/live bytes, 46 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:02,001 Memtable.java (line 424) Writing Memtable-hints@479784922(38/69 serialized/live bytes, 46 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:02,195 Memtable.java (line 458) Completed flushing /data2/cassandra/system/hints/system-hints-ib-187-Data.db (85 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position
=806355)
 INFO [CompactionExecutor:60] 2013-01-10 21:59:02,200 CompactionTask.java (line 120) Compacting [SSTableReader(path='/data2/cassandra/system/hints/system-hints-ib-187-Data.db'), SSTableReader(path='/data2/cassandra/system/hints/sy
stem-hints-ib-186-Data.db')]
 INFO [CompactionExecutor:60] 2013-01-10 21:59:02,431 CompactionTask.java (line 267) Compacted 2 sstables to [/data2/cassandra/system/hints/system-hints-ib-188,].  32,814 bytes to 32,729 (~99% of original) in 230ms = 0.135708MB/s.
  8 total rows, 7 unique.  Row merge counts were {1:8, 2:0, }
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,432 HintedHandOffManager.java (line 408) Finished hinted handoff of 47 rows to endpoint /10.8.30.102
 INFO [GossipStage:1] 2013-01-10 21:59:11,999 StorageService.java (line 1288) Node /10.8.30.102 state jump to normal
 INFO [GossipStage:1] 2013-01-10 21:59:12,003 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-peers@1233529943(306/5247 serialized/live bytes, 21 ops)
 INFO [FlushWriter:10] 2013-01-10 21:59:12,004 Memtable.java (line 424) Writing Memtable-peers@1233529943(306/5247 serialized/live bytes, 21 ops)
 INFO [FlushWriter:10] 2013-01-10 21:59:12,265 Memtable.java (line 458) Completed flushing /data2/cassandra/system/peers/system-peers-ib-589-Data.db (351 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=806482)
 INFO [GossipStage:1] 2013-01-10 21:59:12,272 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-local@1657301357(69/69 serialized/live bytes, 2 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:12,273 Memtable.java (line 424) Writing Memtable-local@1657301357(69/69 serialized/live bytes, 2 ops)
 INFO [FlushWriter:9] 2013-01-10 21:59:12,455 Memtable.java (line 458) Completed flushing /data2/cassandra/system/local/system-local-ib-674-Data.db (129 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=806675)
 WARN [MemoryMeter:1] 2013-01-10 21:59:30,213 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.09066707435830113
 INFO [MemoryMeter:1] 2013-01-10 21:59:30,214 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 7ms for 55 columns
 INFO [HintedHandoff:1] 2013-01-10 22:00:20,287 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:00:20,288 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [Thread-50] 2013-01-10 22:02:39,618 StorageService.java (line 2304) Starting repair command #1, repairing 1 ranges for keyspace evidence
 INFO [AntiEntropySessions:1] 2013-01-10 22:02:39,637 AntiEntropyService.java (line 652) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] new session: will sync /10.8.25.101, /10.8.30.14 on range (28356863910078205288614550619314017620,42535295865117307932921825928971026436] for evidence.[fingerprints, messages]
 INFO [AntiEntropySessions:1] 2013-01-10 22:02:39,646 AntiEntropyService.java (line 857) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] requesting merkle trees for fingerprints (to [/10.8.30.14, /10.8.25.101])
 INFO [ValidationExecutor:1] 2013-01-10 22:02:39,665 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-fingerprints@1756165009(409626/409626 serialized/live bytes, 53 ops)
 INFO [FlushWriter:11] 2013-01-10 22:02:39,666 Memtable.java (line 424) Writing Memtable-fingerprints@1756165009(409626/409626 serialized/live bytes, 53 ops)
 INFO [FlushWriter:11] 2013-01-10 22:02:39,871 Memtable.java (line 458) Completed flushing /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-195-Data.db (349405 bytes) for commitlog position ReplayPosition(segmentId=1357883369951, position=3000340)
 WARN [MemoryMeter:1] 2013-01-10 22:02:39,917 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.03491183672633014
 INFO [MemoryMeter:1] 2013-01-10 22:02:39,917 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 19ms for 106 columns
 INFO [AntiEntropyStage:1] 2013-01-10 22:05:31,251 AntiEntropyService.java (line 214) [repair #815023d0-5bb4-11e2-906d-dd50a26832ff] Received merkle tree for fingerprints from /10.8.25.101
 WARN [MemoryMeter:1] 2013-01-10 22:05:53,141 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.0038005359709140643
 INFO [MemoryMeter:1] 2013-01-10 22:05:53,142 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='fingerprints') liveRatio is 1.0 (just-counted was 1.0).  calculation took 6ms for 10 columns
 INFO [MemoryMeter:1] 2013-01-10 22:08:53,699 Memtable.java (line 207) CFS(Keyspace='brts', ColumnFamily='evidence_index') liveRatio is 3.018170276918194 (just-counted was 3.018170276918194).  calculation took 24ms for 235 columns
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,290 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,291 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 WARN [MemoryMeter:1] 2013-01-10 22:12:57,094 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.033105659834837216
 INFO [MemoryMeter:1] 2013-01-10 22:12:57,095 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 18ms for 213 columns
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,293 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,294 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,296 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,297 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,299 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,300 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 WARN [MemoryMeter:1] 2013-01-10 22:44:25,510 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.03129749755193589
 INFO [MemoryMeter:1] 2013-01-10 22:44:25,510 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='messages') liveRatio is 1.0 (just-counted was 1.0).  calculation took 20ms for 387 columns
 WARN [MemoryMeter:1] 2013-01-10 22:47:55,174 Memtable.java (line 191) setting live ratio to minimum of 1.0 instead of 0.013355315914417125
 INFO [MemoryMeter:1] 2013-01-10 22:47:55,175 Memtable.java (line 207) CFS(Keyspace='evidence', ColumnFamily='fingerprints') liveRatio is 1.0 (just-counted was 1.0).  calculation took 11ms for 63 columns
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,302 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,307 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,304 HintedHandOffManager.java (line 293) Started hinted handoff for host: a1429d88-a084-46b2-a92d-81bb43b7ccc4 with IP: /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,305 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
{code}

digest version
{code}
INFO [HintedHandoff:1] 2013-01-10 22:00:20,288 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:10:20,291 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:20:20,294 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:30:20,297 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 22:40:20,300 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 22:50:20,307 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:00:20,305 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 23:10:20,308 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:20:20,311 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:2] 2013-01-10 23:30:20,314 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
 INFO [HintedHandoff:1] 2013-01-10 23:40:20,317 HintedHandOffManager.java (line 408) Finished hinted handoff of 0 rows to endpoint /10.8.30.102
{code};;;","11/Jan/13 17:16;brandon.williams;Hmm, so it did correctly compact just before logging:

{noformat}
 INFO [HintedHandoff:2] 2013-01-10 21:59:02,432 HintedHandOffManager.java (line 408) Finished hinted handoff of 47 rows to endpoint /10.8.30.102
{noformat}

I'm not sure why anything would be left after that.;;;","18/Jan/13 20:34;kmueller;I'm also seeing this, running 1.1.8 :);;;","05/Feb/13 21:43;brandon.williams;I ran into this in 1.2, and the problem is definitely old sstables full of tombstones being left behind:

{noformat}
 activity                                                                                        | timestamp    | source        | source_elapsed
-------------------------------------------------------------------------------------------------+--------------+---------------+----------------
                                                                              execute_cql3_query | 21:10:25,847 | 10.179.64.227 |              0
                                                                               Parsing statement | 21:10:25,847 | 10.179.64.227 |             39
                                                                              Peparing statement | 21:10:25,847 | 10.179.64.227 |            208
                                                                   Determining replicas to query | 21:10:25,847 | 10.179.64.227 |            316
 Executing seq scan across 2 sstables for [min(-9223372036854775808), min(-9223372036854775808)] | 21:10:25,870 | 10.179.64.227 |          23223
                                                          Read 0 live cells and 13800 tombstoned | 21:10:25,928 | 10.179.64.227 |          81432
                                                          Read 0 live cells and 13068 tombstoned | 21:10:26,015 | 10.179.64.227 |         168213
                                                                    Scanned 2 rows and matched 2 | 21:10:26,016 | 10.179.64.227 |         169206
                                                                                Request complete | 21:10:26,016 | 10.179.64.227 |         169585
{noformat}


The problem appears to occur here:

{noformat}
 INFO [GossipStage:1] 2013-02-04 22:54:34,828 Gossiper.java (line 754) InetAddress /10.179.111.137 is now UP
 INFO [GossipStage:1] 2013-02-04 22:54:34,830 Gossiper.java (line 754) InetAddress /10.179.65.102 is now UP
 INFO [HintedHandoff:1] 2013-02-04 22:54:34,830 HintedHandOffManager.java (line 297) Started hinted handoff for host: 5b49c861-0cf6-48dc-872a-7fcb89429dae with IP: /10.179.111.137
 INFO [HintedHandoff:2] 2013-02-04 22:54:34,830 HintedHandOffManager.java (line 297) Started hinted handoff for host: 0fd1d3b1-0f73-40fd-ab36-9f4b9636a205 with IP: /10.179.65.102
 INFO [HintedHandoff:2] 2013-02-04 22:54:41,039 ColumnFamilyStore.java (line 678) Enqueuing flush of Memtable-hints@164677298(1020984/1020984 serialized/live bytes, 39151 ops)
 INFO [FlushWriter:2] 2013-02-04 22:54:41,041 Memtable.java (line 453) Writing Memtable-hints@164677298(1020984/1020984 serialized/live bytes, 39151 ops)
 INFO [CompactionExecutor:8] 2013-02-04 22:54:41,043 CompactionTask.java (line 112) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ib-1-Data.db')]
 INFO [FlushWriter:2] 2013-02-04 22:54:41,200 Memtable.java (line 487) Completed flushing /var/lib/cassandra/data/system/hints/system-hints-ib-2-Data.db (166760 bytes) for commitlog position ReplayPosition(segmentId=1360018215672, position=11404848)
 INFO [CompactionExecutor:9] 2013-02-04 22:54:41,215 CompactionManager.java (line 452) SSTables for user defined compaction are already being compacted.
 INFO [HintedHandoff:2] 2013-02-04 22:54:41,216 HintedHandOffManager.java (line 412) Finished hinted handoff of 13909 rows to endpoint /10.179.65.102
 INFO [CompactionExecutor:8] 2013-02-04 22:54:41,444 CompactionTask.java (line 272) Compacted 1 sstables to [/var/lib/cassandra/data/system/hints/system-hints-ib-3,].  625,567 bytes to 625,567 (~100% of original) in 401ms = 1.487749MB/s.  2 total rows, 2 unique.  Row merge counts were {1:2, }
 INFO [HintedHandoff:1] 2013-02-04 22:54:41,445 HintedHandOffManager.java (line 412) Finished hinted handoff of 13230 rows to endpoint /10.179.111.137
{noformat}

The problem is that with concurrent delivery we end up in a state where we have two sstables that contain tombstones, each containing some portion of them for the endpoints that were delivered to.  Since we never replay any rows again, we never compact again, and thus never evict the tombstones.

I think I'm back to what I proposed before of just removing the replayed > 0 check and letting isEmpty check handle the common case.
 ;;;","05/Feb/13 21:51;jbellis;Shouldn't the ""I have > X % tombstones in this sstable"" code kick in?;;;","05/Feb/13 21:56;peter-librato;We see this in 1.1.9 as well.;;;","05/Feb/13 22:04;brandon.williams;bq. Shouldn't the ""I have > X % tombstones in this sstable"" code kick in?

As in the tombstone-aggressive code in STS?  I suspect we're ending up under the min compaction threshold all the time.;;;","11/Feb/13 18:59;slebresne;I believe this is a consequence of CASSANDRA-5241. Now while we should probably fix CASSANDRA-5241, relying on tombstone being always fully collected feels a bit fragile and not very useful since we have the isEmpty check at the beginning of the delivery. So +1 on the patch.;;;","11/Feb/13 19:06;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression params validation assumes SnappyCompressor,CASSANDRA-5066,12623832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,aleksey,aleksey,13/Dec/12 21:24,16/Apr/19 09:32,14/Jul/23 05:52,14/Dec/12 15:54,1.1.8,1.2.0,,,,,0,,,,,,,"This hasn't caused any issues yet since DeflateCompressor and SnappyCompressor have the same empty set for supportedOptions, but is a potential issue.

Combined with CASSANDRA-4996 this also brings back CASSANDRA-4266.",,aleksey,christianmovi,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/12 01:33;aleksey;5066-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560898/5066-1.1.txt","14/Dec/12 10:53;slebresne;5066-v2.txt;https://issues.apache.org/jira/secure/attachment/12560949/5066-v2.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297552,,,Fri Dec 14 15:54:59 UTC 2012,,,,,,,,,,"0|i14rcf:",235895,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,"14/Dec/12 10:53;slebresne;I prefer the simpler/less hackish solution attached as v2. I.e. if no compressor class has been provided, you should have no options, period.;;;","14/Dec/12 15:33;aleksey;+1;;;","14/Dec/12 15:54;slebresne;Alright, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool ownership is incorrect with vnodes,CASSANDRA-5065,12623814,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Dec/12 20:16,16/Apr/19 09:32,14/Jul/23 05:52,18/Dec/12 20:32,1.2.0 rc2,,,,,,0,,,,,,,"Example:

{noformat}
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns   Host ID                               Rack
UN  10.179.65.102     197.96 MB  256     0.1%   6ac56251-08ff-46be-be06-5b8dd929b937  rack1
UN  10.179.111.137    209.3 MB   256     0.0%   aade8ef6-c907-427c-87be-a5fe05a27fa4  rack1
UN  10.179.64.227     205.86 MB  256     0.1%   4634cc80-0832-4ea1-b4a6-39ae54985206  rack1
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/12 21:14;jbellis;5065.txt;https://issues.apache.org/jira/secure/attachment/12561353/5065.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297534,,,Tue Dec 18 20:32:54 UTC 2012,,,,,,,,,,"0|i14r87:",235876,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"17/Dec/12 21:14;jbellis;patch attached to fix getOwnership's assumption that each ip address only has one token associated with it.;;;","18/Dec/12 19:38;yukim;Patch lgtm, +1.;;;","18/Dec/12 20:32;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Alter table' when it includes collections makes cqlsh hang,CASSANDRA-5064,12623780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,enigmacurry,enigmacurry,13/Dec/12 16:48,16/Apr/19 09:32,14/Jul/23 05:52,17/Dec/12 11:05,1.2.0 rc2,,,,,,0,,,,,,,"Having just installed 1.2.0-beta3 issue the following CQL into cqlsh:
{code}
drop keyspace test;

create keyspace test with replication = {
          'class': 'SimpleStrategy',
          'replication_factor': '1'
        };

use test;

create table users (
            user_id text PRIMARY KEY,
            first_name text,
            last_name text,
            email_addresses set<text>
        );

alter table users add mailing_address_lines list<text>;
{code}

As soon as you issue the alter table statement cqlsh hangs, and the java process hosting Cassandra consumes 100% of a single core's CPU.

If the alter table doesn't include a collection, it runs fine.","Ubuntu 12.04 LTS
3.2.0-23-virtual
",enigmacurry,jbellis,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/12 09:51;slebresne;5064-v2.txt;https://issues.apache.org/jira/secure/attachment/12560939/5064-v2.txt","13/Dec/12 19:45;slebresne;5064.txt;https://issues.apache.org/jira/secure/attachment/12560838/5064.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297480,,,Mon Dec 17 15:59:00 UTC 2012,,,,,,,,,,"0|i14ozj:",235513,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"13/Dec/12 17:34;jbellis;Can you verify against git 1.2.0 branch?;;;","13/Dec/12 18:04;enigmacurry;Yes, it's affected in git 1.2.0 as well.;;;","13/Dec/12 18:11;brandon.williams;Repros on trunk as well.  Jstack indicates the commit log writer and memtable post flusher are consuming most of the cpu. Affects the entire cluster.;;;","13/Dec/12 19:48;slebresne;Patch attached.

This is due to the fix of CASSANDRA-4786 (and it happens that this example modifies the comparator of the CFS). The code from CASSANDRA-4786 is relying on the fact that maybeSwitchMemtable was either returning null or was really switching the memtable. That's not the case however if the memtable is clean (I could have swear I had checked for CASSANDRA-4786 but maybe I missed it, or the code has changed since then). In any case, the attached patch add a 'forceSwitch' flag to maybeSwitchMemtable that fixes the issue.;;;","13/Dec/12 22:39;jbellis;This is starting to feel fragile, and I've never been a fan of Table.switchLock/maybeSwitchMemtable in the first place (even though I wrote it :).

Pushed a different approach to http://github.com/jbellis/cassandra/branches/5064.  The first commit is the important one: we replace Table.switchLock with reference-counted memtables.  We can then merge DataTracker.[renew|switch]Memtable, change maybeSwitchMemtable to unconditionally switch (but only flush if there were updates made), and this problem goes away along with a bunch of other switchLock-related complexity.

Also suspect that an atomic increment is a performance win over ReentrantReadWriteLock, which is one of the most heavyweight concurrency classes.;;;","13/Dec/12 22:41;jbellis;NB: there may be a problem with flushing during CL replay since writeCommitLog is never passed as False since 1.0.  I've just removed it here for now.;;;","14/Dec/12 09:48;slebresne;I'm slightly confused by your branch. Do we agree that it doesn't do the ""change maybeSwitchMemtable to unconditionally switch""? Cause I don't see it.

Now about the memtable reference counting, I see at least a few problems:
# I see nothing that prevents flushing the same memtable multiple times.
# getting the commit log context and switching the memtable is not done atomically with respect to writes. So a write can be pushed in the commit log after the context we're getting but still reach the memtable we're about to flush. For normal update, this is mostly inefficient in that we'll kept commit logs around long than necessary and potentially replay some update unnecessarily, but for counter this is a bug.
# it's also possible that for postFlush tasks to not be scheduled in the order the commit log context were acquired. So we could discard a commitlog for which the data is not yet fully flushed.

Tbh, it may be possible to fix those problems (though for the 2nd one I don't have much idea), but I doubt we'll end up with something simpler than the current implementation. That might still be worth it ultimately for the performance improvement (though I'm not sure it's one of our bottleneck, so keeping the lock that is easier to reason about may be better), but I'm really doubtful that changing such an important piece of code just before a release is a good idea (again, supposing we even have a solution for the problems above).

I also don't think using reference counting really makes this issue simpler.  But I do agree that the looping in CFS.reload is a bit retarded. And more generally, the ""maybe"" part in maybeSwitchMemtable is probably not justified anymore. It was useful when this was called on the with path, where all we wanted was to avoid OOM and if some other thread was already flushing it was fine. But now that we don't do that, it probably does more bad than good. What I mean is that if you call forceFlush, you expect that any write that happens-before your call has been flushed. But currently, if some other thread flushes the memtable, you'll return immediately while some data may be currently under flush (but so not flushed yet). So attaching a v2 that remove the freeze business (thus getting rid of the CFS.reload loop). I've kept the forceSwitch flag of the first patch to avoid recreating a memtable object if not needed in the normal flush path, but we can also remove it and make forceSwitch the default if we prefer.
;;;","14/Dec/12 14:31;jbellis;bq. Do we agree that it doesn't do the ""change maybeSwitchMemtable to unconditionally switch""?

Here's the new core of switchMemtable:

{code}
.       for (ColumnFamilyStore cfs : concatWithIndexes())
        {
            Memtable mt = cfs.data.switchMemtable();
            if ((!mt.isClean()))
                memtables.add(mt);
        }
{code}

bq. I see nothing that prevents flushing the same memtable multiple times.

What prevents it is that DataTracker.switchMemtable is atomic and will only return a given memtable once.  (So, if we have a whole bunch of threads calling forceFlush at once, we'll probably switch out a few empty ones, but that is the end of the extra work we do.)

bq. for counter this is a bug

Ugh, counters again.  Have to think about this.

bq. I also don't think using reference counting really makes this issue simpler.

I do. :)  We're net -120 lines of fairly tricky code.  We've had multiple bugs from not dealing with switchlock correctly (CASSANDRA-3712 and the flushing code from CASSANDRA-3411, off the top of my head).

bq. I'm really doubtful that changing such an important piece of code just before a release is a good idea

I'll buy that.;;;","14/Dec/12 15:13;jbellis;v2 LGTM.;;;","14/Dec/12 15:46;jbellis;bq. getting the commit log context and switching the memtable is not done atomically with respect to writes

Isn't this easily fixed by moving the context request to after the wait-for-writes-to-finish loop?

bq. it's also possible that for postFlush tasks to not be scheduled in the order the commit log context were acquired

Bunch of ways we could fix this, including moving switchMemtable to its own single-threaded executor.  Or just synchronizing it, which is no worse performance-wise than the old writeLock.lock but much more localized.;;;","17/Dec/12 11:05;slebresne;bq. v2 LGTM

Alright. For now I've committed v2 to 1.2.0 and closing this ticket.

bq. Ugh, counters again.

I agree about the counters being annoying once more. That being said, even counters aside, I do like the fact that we don't ever replay useless things. It gives me the confidence that ""we're doing it right"". Just to say that adding back the possibility to potentially hold commit logs longer than necessary doesn't feel like progress to me (it's probably a relatively minor downside compare to the performance benefits, a downside nonetheless).

bq. We're net -120 lines of fairly tricky code.

I do think it's just flushing that is tricky, because we have to somehow coordinate on-going writes, switching the memtable and the commit log and all that must be thread-safe. I'm abolutely convinced that our current stop-the-world does make it easier to reason about (granted it's not efficient and granted it's possible to forgot taking the read-lock at time, but that's still easy to understand errors). I do have a harder time convincing myself of the correctness of switchMemtable with the reference counting, because it allows for much more interleaving of events (that's what make concurrent programming hard after all). So I do buy ""more efficient"", I don't buy ""much less tricky"" so much, and what I really mean by that is that I would be mildly enthousiastic at committing such change before 2.0 (but I do think it would be a good change for performance overall).

bq. Isn't this easily fixed by moving the context request to after the wait-for-writes-to-finish loop?

If you do that, the context you'll get will include writes that went in the new memtable, so you'll discard commit log that have write not flushed yet, that's even worst.

bq. Bunch of ways we could fix this.

Sure. Just saying it ain't no child's play :)
;;;","17/Dec/12 15:59;enigmacurry;Verified this is fixed on the 1.2.0 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgraded cassandra loses all cfs on restart,CASSANDRA-5061,12623752,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,13/Dec/12 14:02,16/Apr/19 09:32,14/Jul/23 05:52,13/Dec/12 16:53,1.1.8,,,,,,0,,,,,,,"A bit dramatic summary, but hey;

If you upgrade cassandra and then restart it, you lose all your CFs, but they come back if you restart again.

This is due to fixSchemaNanoTimestamp not flushing the new data after truncating the CF and re-doing the mutations.",,christianmovi,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/12 14:03;marcuse;0001-flush-data-after-fixing-nano-timestamps.patch;https://issues.apache.org/jira/secure/attachment/12560779/0001-flush-data-after-fixing-nano-timestamps.patch",,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297451,,,Thu Dec 13 16:53:32 UTC 2012,,,,,,,,,,"0|i14osf:",235481,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Dec/12 14:09;marcuse;ah! it comes back due to the commitlog being replayed after reading up the schema;;;","13/Dec/12 16:53;jbellis;added comment + committed.  thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian packaging should include shuffle,CASSANDRA-5058,12623485,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,12/Dec/12 04:49,16/Apr/19 09:32,14/Jul/23 05:52,18/Dec/12 17:19,1.2.0 rc2,,,Packaging,,,0,,,,,,,"Our debian packaging doesn't currently include shuffle, but we should add it so people have a way of upgrading to vnodes.  This might also be a good time to consider a different name for shuffle, though I don't believe it currently conflicts with anything else.",,slebresne,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/12 16:32;slebresne;5058.txt;https://issues.apache.org/jira/secure/attachment/12561509/5058.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297193,,,Tue Dec 18 17:19:56 UTC 2012,,,,,,,,,,"0|i14mzb:",235187,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"17/Dec/12 14:09;slebresne;bq. I don't believe it currently conflicts with anything else

Nothing come to mind either, but I'd be fine with something like shuffle-tokens just for the sake of avoiding a too general name.;;;","17/Dec/12 15:39;urandom;bq. Nothing come to mind either, but I'd be fine with something like shuffle-tokens just for the sake of avoiding a too general name.

Or {{cassandra-shuffle}}.  Somewhat long for a command name, but considering how seldom it will be used...;;;","17/Dec/12 21:15;jbellis;+1 for cassandra-shuffle (following precedent for cassandra-cli);;;","18/Dec/12 16:32;slebresne;Rename to cassandra-shuffle and add it to the debian packaging.;;;","18/Dec/12 17:16;urandom;bq. Rename to cassandra-shuffle and add it to the debian packaging.

+1;;;","18/Dec/12 17:19;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
not possible to change crc_check_chance,CASSANDRA-5053,12623365,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,11/Dec/12 12:02,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/12 13:11,1.1.9,1.2.0 rc2,,,,,0,,,,,,,"It is not possible to change crc_check_chance using a schema modification after CASSANDRA-4266

This patch fixes that and moves the setting out into a configuration parameter instead, you dont want to upgrade/scrub/.. all your sstables to change the crc_check_chance.

",,aleksey,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/12 12:55;marcuse;0001-CASSANDRA-5053-make-it-possible-to-change-crc_check_.patch;https://issues.apache.org/jira/secure/attachment/12561480/0001-CASSANDRA-5053-make-it-possible-to-change-crc_check_.patch","11/Dec/12 12:06;marcuse;0001-fix-CASSANDRA-5053-not-possible-to-change-crc_check_.patch;https://issues.apache.org/jira/secure/attachment/12560380/0001-fix-CASSANDRA-5053-not-possible-to-change-crc_check_.patch","19/Dec/12 05:08;aleksey;5053-v2.txt;https://issues.apache.org/jira/secure/attachment/12561642/5053-v2.txt",,,,,,,,,,,,,,,3.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297062,,,Wed Dec 19 13:10:50 UTC 2012,,,,,,,,,,"0|i14l47:",234884,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,"11/Dec/12 12:05;marcuse;* Move crc_check_chance from schema definition to config file
* Support if someone has actually managed to set the crc_check_chance in a CompressionMetadata file - pre 1.1.1 maybe?;;;","11/Dec/12 16:29;jbellis;Moving it from per-CF to global seems kind of backwards, don't you think?  The overwhelming tendency has been for people to push for more fine-grained control over things.;;;","11/Dec/12 17:13;marcuse;for this i actually prefer a config, then i can tweak it on one machine at a time

and since it has been broken since 1.1.1 i doubt anyone is actually using it;;;","17/Dec/12 15:51;jbellis;Let's fix the schema code then, and we can add a JMX interface for testing it machine-at-a-time, which is our usual convention for these things.;;;","17/Dec/12 16:11;marcuse;ok, sure, that works

i'll get it fixed;;;","18/Dec/12 12:55;marcuse;let me know if the double brace initialization in CompressionParameters is not allowed, then i'll remove it

patch against 1.1;;;","19/Dec/12 05:07;aleksey;LGTM and works.

Attaching v2 with minor tweaks:
- renamed CFS#crcCheckChance() to CFS#setCrcCheckChance()
- renamed CompressionParameters.globalOptions to CompressionParameters.GLOBAL_OPTIONS and made it an ImmutableSet instead of a HashSet
- CompressionParameters#parseCrcCheckChance() throws CE now - just logging the error and returning default is not enough (Cassandra will save invalid crc_check_chance and you'll get that NumberFormatException every time you start Cassandra)
- added crc_check_chance range validation to parseCrcCheckChance, otherwise we get the same issue as above with valid doubles, but out of range values
- very minor formatting tweaks for consistency

Will commit tomorrow.;;;","19/Dec/12 08:20;marcuse;uh my mbean/jmx skills fail me here, but seems when naming the method *set*CrcCheckChance it is not exposed via JMX, naming it crcCheckChance does, any clue why? naming convention rules i guess;;;","19/Dec/12 08:24;aleksey;No idea. But all other setters in that mbean start with 'set'. Are they not exposed either?;;;","19/Dec/12 08:26;marcuse;ah!

they end up under the attributes tab in jconsole;;;","19/Dec/12 13:10;aleksey;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli should escape keyspace name,CASSANDRA-5052,12623347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,shalupov,shalupov,11/Dec/12 08:39,16/Apr/19 09:32,14/Jul/23 05:52,24/Mar/13 15:07,1.1.11,,,Legacy/Tools,,,0,,,,,,,"show schema yields ""use __someKeyspace"", expecting ""use '__someKeyspace'"".
",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/13 13:52;aleksey;5052.txt;https://issues.apache.org/jira/secure/attachment/12575228/5052.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,297043,,,Sun Mar 24 16:44:42 UTC 2013,,,,,,,,,,"0|i14kzz:",234865,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Mar/13 14:21;jbellis;Why does underscore cause trouble?;;;","24/Mar/13 14:33;aleksey;bq. Why does underscore cause trouble?
Because names starting with _ cannot be identifiers (identifiers can contain _, but not in the beginning) and must be quoted.;;;","24/Mar/13 14:50;jbellis;+1;;;","24/Mar/13 15:07;aleksey;Committed, thanks.;;;","24/Mar/13 16:44;aleksey;Followed up with 0e03790059e390fb58f45b8dc5e44e76f5e913d3 to give names starting with digits the same treatment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cql3 token queries broken,CASSANDRA-5050,12623218,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,10/Dec/12 21:34,16/Apr/19 09:32,14/Jul/23 05:52,11/Dec/12 07:55,1.2.0 rc1,,,,,,0,,,,,,,"Currently any select statement that uses a token() predicate breaks with ""Bad Input""

After tracing the logic this error is caused in getTokenBounds because it assumes the token term is an actual token string what will pass the tokenizer

",,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/12 21:35;tjake;5050.txt;https://issues.apache.org/jira/secure/attachment/12560285/5050.txt",,,,,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296831,,,Tue Dec 11 07:55:31 UTC 2012,,,,,,,,,,"0|i14hkv:",234311,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"11/Dec/12 07:55;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auth.setup() is called too early,CASSANDRA-5049,12623189,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Dec/12 18:26,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 19:41,1.2.0 rc1,,,,,,0,,,,,,,"Auth.setup() triggers a request against the system_auth keyspace, request that is not an internal one, so it at least require TokenMetadata to be set up. However, Auth.setup() is call much too early, even before the commit log is replayed. The only reason this doesn't trigger an assertionError everytime is because Auth.setup() actually only schedul it's request after RING_DELAY, but still, replaying the commit log can take much more than that, and even without that I suspect this would be racy with normal bootstrap.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/12 19:17;aleksey;5049-v2.txt;https://issues.apache.org/jira/secure/attachment/12560242/5049-v2.txt","10/Dec/12 18:41;slebresne;5049.txt;https://issues.apache.org/jira/secure/attachment/12560234/5049.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296800,,,Mon Dec 10 19:40:50 UTC 2012,,,,,,,,,,"0|i14hdj:",234278,,,,,Low,,,,,,,,,,,,,,,,,"10/Dec/12 18:41;slebresne;Attaching a naive patch that call Auth.setup() only once we know it will work. I however suspect that the authenticator and authorizer should be setup much sooner than that. So the right approach is probably to split that setup() into two methods, but I'll wait for someone knowledgeable of the Auth thingy to confirm out of laziness.;;;","10/Dec/12 18:53;aleksey;Right, IAuthenticator#setup and IAuthorizer#setup should be called before that (where Auth#setup used to be), but superuser part should be scheduled after commit-log replay.;;;","10/Dec/12 19:31;slebresne;Auth.setupSuperuser needs to be called in the 'isSurveyMode' of joinRing too, but otherwise +1.;;;","10/Dec/12 19:40;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't show correct timezone when SELECTing a column of type TIMESTAMP,CASSANDRA-5046,12623040,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,btoddb,btoddb,09/Dec/12 03:24,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 15:28,1.1.8,1.2.0 rc1,,,,,0,,,,,,,"trying to figure out if i'm doing something wrong or a bug.  i am
creating a simple schema, inserting a timestamp using ISO8601 format,
but when retrieving the timestamp, the timezone is displayed
incorrectly.  i'm inserting using GMT, the result is shown with
""+0000"", but the time is for my local timezone (-0800)

tried with 1.1.6 (DSE 2.2.1), and 1.2.0-rc1-SNAPSHOT

here's the trace:

bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift
protocol 19.35.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE btoddb WITH replication =
{'class':'SimpleStrategy', 'replication_factor':1};
cqlsh>
cqlsh> USE btoddb;
cqlsh:btoddb> CREATE TABLE test (
          ...   id uuid PRIMARY KEY,
          ...   ts TIMESTAMP
          ... );
cqlsh:btoddb>
cqlsh:btoddb> INSERT INTO test
          ...   (id, ts)
          ...   values (
          ...     '89d09c88-40ac-11e2-a1e2-6067201fae78',
          ...     '2012-12-07T10:00:00-0000'
          ...   );
cqlsh:btoddb>
cqlsh:btoddb> SELECT * FROM test;

 id                                   | ts
--------------------------------------+--------------------------
 89d09c88-40ac-11e2-a1e2-6067201fae78 | 2012-12-07 02:00:00+0000

cqlsh:btoddb>",cassandra 1.1.6 (DSE 2.2.1) or cassandra RC1 (from tip),aleksey,btoddb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/12 16:47;aleksey;5046-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560096/5046-1.1.txt","09/Dec/12 16:46;aleksey;5046-1.2.txt;https://issues.apache.org/jira/secure/attachment/12560095/5046-1.2.txt",,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296632,,,Mon Dec 10 15:27:47 UTC 2012,,,,,,,,,,"0|i14b2f:",233256,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Dec/12 16:51;aleksey;Weird. I remember correcting this before attaching the original patches for CASSANDRA-4746, but apparently I attached the patches with this mistake back then.;;;","10/Dec/12 14:56;jbellis;+1;;;","10/Dec/12 15:27;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't insert only a key in CQL3,CASSANDRA-5040,12622913,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Dec/12 15:18,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 08:49,1.2.0 rc1,,,,,,0,,,,,,,"The following should work but well, doesnt:
{noformat}
cqlsh:k> CREATE TABLE t (k int PRIMARY KEY, v int);
cqlsh:k> INSERT INTO t (k) VALUES (0);
Bad Request: line 1:27 required (...)+ loop did not match anything at input ')'
{noformat}

The reason is just that the parser for INSERT has never been updated from the time where providing only a key was illegal.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/12 15:19;slebresne;5040.patch;https://issues.apache.org/jira/secure/attachment/12559873/5040.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296491,,,Mon Dec 10 08:49:25 UTC 2012,,,,,,,,,,"0|i149qv:",233042,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Dec/12 15:19;slebresne;Trivial patch attached;;;","07/Dec/12 16:46;jbellis;+1;;;","10/Dec/12 08:49;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong description of 'setstreamthroughput' option,CASSANDRA-5036,12619033,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,06/Dec/12 11:48,16/Apr/19 09:32,14/Jul/23 05:52,24/Mar/13 06:43,1.2.4,,,Legacy/Documentation and Website,Local/Config,,0,,,,,,,"There is a typo in description of 'setstreamthroughput' option. It is measured in megabits per second. Page with wrong description:
http://www.datastax.com/docs/1.1/references/nodetool#nodetool-setstreamthroughput
Page with right description:
http://www.datastax.com/docs/1.1/configuration/node_configuration#stream-throughput-outbound-megabits-per-sec



Also I want to discuss possibility to reduce default value for this option. I think that 400 Mbs is too high for common cases. 

Preface:
This option is used only in case streams. There are two cases when streams are actual. They are rebuilding of a node and repair process. Let's skip first case and will talk only about the second. Let's imagine that we have replication factor 3.

Cross-datacenter connectivity case: 
When we start repair process it will borrow all network channel. Let's do some calculations. You start repair on an one node, e.g. 5 node (3 remote and 2 local) should send us some data. Note that 3 of them are from remote datacenter. So 400 * 3 = 1,2 Gbs we should receive through WAN. I'm sure that it's too high.

I suggest to make it 2 times less.
",Cassandra 1.1.6 (DataStax distribution),azotcsit,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296311,,,Sun Mar 24 06:43:03 UTC 2013,,,,,,,,,,"0|i148ef:",232824,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,"24/Mar/13 05:13;jbellis;What do you think about the suggested default, [~krummas]?;;;","24/Mar/13 06:28;marcuse;I agree it might be a bit too high

defaulting to 200 sounds good;;;","24/Mar/13 06:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Downed node loses its host-id,CASSANDRA-5032,12618938,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,tjake,tjake,05/Dec/12 20:22,16/Apr/19 09:32,14/Jul/23 05:52,06/Dec/12 14:26,1.2.0 rc1,,,,,,0,vnodes,,,,,,"We took down one of our nodes for maintenance and during that time it seems the other nodes have lost the downed nodes node id

We also see lots of hint assertion exceptions ""Missing host ID for 10.6.27.98""

{code}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address           Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.6.27.96        129.37 GB  256     140.0%            59f3df94-e551-45ce-a3b0-51462f3ea868  27
UN  10.6.27.97        125.24 GB  256     133.7%            f5bb146c-db51-475c-a44f-9facf2f1ad6e  27
DN  10.6.27.98        ?          256     126.3%            null                                  27
{code}

We restarted c* on the two other nodes that are up, my guess is the host id was lost on restart of those.
",,carlyeks,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/12 23:59;brandon.williams;0001-rename-ring_id-to-host_id-for-clarity.txt;https://issues.apache.org/jira/secure/attachment/12556174/0001-rename-ring_id-to-host_id-for-clarity.txt","05/Dec/12 23:59;brandon.williams;0002-Load-host_ids-when-adding-saved-endpoints.txt;https://issues.apache.org/jira/secure/attachment/12556175/0002-Load-host_ids-when-adding-saved-endpoints.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296204,,,Thu Dec 06 14:26:21 UTC 2012,,,,,,,,,,"0|i146xj:",232586,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"05/Dec/12 23:59;brandon.williams;First patch renames 'ring_id' in peers to 'host_id' because that's what we call it everywhere else including nodetool, so we should be consistent.

Second patch updates the hostid in TMD when it updates the tokens.;;;","06/Dec/12 00:53;tjake;Looks straight fwd. I can test but this looks like I'll need to recreate the cluster/system table?;;;","06/Dec/12 00:57;brandon.williams;I tested it and it works, but yeah, you'll have to wipe out the system table, or I think starting once with -Dcassandra.load_ring=false will work.;;;","06/Dec/12 13:01;tjake;+1;;;","06/Dec/12 14:26;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexHelper.IndexFor call throws AOB exception when passing multiple slices,CASSANDRA-5030,12618912,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,05/Dec/12 17:26,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/12 19:49,1.2.0 rc1,,,,,,0,,,,,,,"While testing multiple slices I'm seeing some exceptions when a slice hits the end of an index.

{code}
ERROR [ReadStage:138179] 2012-12-04 18:04:28,796 CassandraDaemon.java (line 132) Exception in thread Thread[ReadStage:138179,5,main]
java.lang.IndexOutOfBoundsException: toIndex = 6
        at java.util.SubList.<init>(AbstractList.java:602)
        at java.util.RandomAccessSubList.<init>(AbstractList.java:758)
        at java.util.AbstractList.subList(AbstractList.java:468)
        at org.apache.cassandra.io.sstable.IndexHelper.indexFor(IndexHelper.java:182)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.setNextSlice(IndexedSliceReader.java:253)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.<init>(IndexedSliceReader.java:246)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:91)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:267)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1387)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1247)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1159)
        at org.apache.cassandra.db.Table.getRow(Table.java:348)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:48)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}


I can reproduce this in a test, attached",,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/12 18:38;tjake;5030-1.txt;https://issues.apache.org/jira/secure/attachment/12559923/5030-1.txt","05/Dec/12 18:26;tjake;5030.txt;https://issues.apache.org/jira/secure/attachment/12556129/5030.txt",,,,,,,,,,,,,,,,2.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296178,,,Fri Dec 07 19:49:41 UTC 2012,,,,,,,,,,"0|i146jb:",232522,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Dec/12 06:03;jbellis;SSTableNamesIterator doesn't check for index < 0, which could be problematic.

Nit: prefer static import for assertEquals.;;;","07/Dec/12 18:38;tjake;New version with recommended changes;;;","07/Dec/12 19:10;jbellis;+1;;;","07/Dec/12 19:49;tjake;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema push/pull race,CASSANDRA-5025,12618758,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,04/Dec/12 17:52,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/12 21:45,1.1.8,,,,,,0,,,,,,,"When a schema change is made, the coordinator pushes the delta to the other nodes in the cluster.  This is more efficient than sending the entire schema.  But the coordinator also announces the new schema version, so the other nodes' reception of the new version races with processing the delta, and usually seeing the new schema wins.  So the other nodes also issue a pull to the coordinator for the entire schema.

Thus, schema changes tend to become O(n) in the number of KS and CF present.",,cherro,christianmovi,ghinkle,jay.zhuang,pshirshov,webpoacher,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8387,,CASSANDRA-13061,,,,,,"06/Dec/12 19:10;jbellis;5025-v2.txt;https://issues.apache.org/jira/secure/attachment/12556480/5025-v2.txt","07/Dec/12 20:22;cherro;5025-v3.txt;https://issues.apache.org/jira/secure/attachment/12559943/5025-v3.txt","07/Dec/12 21:18;brandon.williams;5025-v4.txt;https://issues.apache.org/jira/secure/attachment/12559952/5025-v4.txt","10/Dec/12 00:49;cherro;5025-v5.txt;https://issues.apache.org/jira/secure/attachment/12560131/5025-v5.txt","04/Dec/12 21:01;jbellis;5025.txt;https://issues.apache.org/jira/secure/attachment/12555986/5025.txt",,,,,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,296006,,,Fri Jan 06 10:14:09 UTC 2017,,,,,,,,,,"0|i144m7:",232211,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"04/Dec/12 21:01;jbellis;patch attached to add a delay to rectifySchema to give concurrent changes a chance to propagate first;;;","04/Dec/12 22:48;cherro;For patch 5025.txt:

A single schema migration will result in N (num nodes) gossips of the new schema version (as before). Through MigrationManager.onChange()->rectifySchema(), those will each result in a delayed comparison of value 'theirVersion', but that value is now one minute old.

Further, if some new schema migration happens to be underway, the same effect of redundant repeat RowMutations will occur.

Schema migrations tend to happen in bursts - so this patch seems like it might reduce the problem but not eliminate it.

Would it not be better to have DefsTable.mergeSchema call Schema.instance.updateVersion instead of Schema.instance.updateVersionAndAnnounce and then deal with temporarily unavailable nodes by doing a MigrationManager.passiveAnnounce(version) if/when we see them come back online?;;;","04/Dec/12 22:52;jbellis;I really don't want to reinvent HH poorly for schema migrations.  Maybe Pavel has a better suggestion.;;;","05/Dec/12 16:47;cherro;Clarifying for anyone else who encounters this issue:
* This problem was introduced in CASSANDRA-3931
* For use cases that involve creation/update/deletion of multiple keyspaces or column families, the symptom will be increasingly slow schema migrations as the KS/CF population grows. Depending on client RPC timeout config, schema change requests may fail. 
* In a test environment running stock C* 1.1.7, for a test that creates new CFs in sequence, we see the following CF creation times:
** Empty cluster: sub-second
** 200+ CFs: 15s ave.
** 400+ CFs: 30s+ with eventual failure due to 30s client side (Hector) RPC timeout.
* In the same test environment running 1.1.7 patched with 5025.txt:
** For the first 60s duration of the test, CF creation times are sub-second
** At 60s, the delayed rectifySchema migration calls kick in and creation times drop to 50s+ (including waits for schema agreement) with eventual failure due to 30s client side RPC timeout.

;;;","05/Dec/12 23:34;xedin;[~cherro] I don't think that we every want to make such a frequent schema changes, it's not considered a good practice for a number of reasons, unless you are trying to do something like temp-tables which has it's own implications... Agree on [~jbellis] that it doesn't seem like a good idea to re-invent HH for schema until we have a good reason to do so.;;;","06/Dec/12 19:10;jbellis;bq. Through MigrationManager.onChange()->rectifySchema(), those will each result in a delayed comparison of value 'theirVersion', but that value is now one minute old.

v2 attached that compares the current version after the delay.;;;","06/Dec/12 20:28;cherro;[~jbellis]: patch 5025-v2.txt works better. For the same test, after 60s, the CF creation time drops from sub-second to 5 seconds average. Delayed rectifySchema work will still interfere with coincident schema migrations, but I think this is the right compromise. Thank you!

Minor: import for {{Callable}} was dropped, but is still referenced at line 229.

[~xedin]: This test was not endorsing a high rate of CF creation for real world use, the goal was to investigate if/why CF creation time was {{O(N)}}.;;;","06/Dec/12 20:32;xedin;+1 with [~cherro] nit.;;;","06/Dec/12 22:16;jbellis;committed, thanks guys!;;;","07/Dec/12 13:27;brandon.williams;This broke bootstrapping, the node thinks it has the schema when it does not, then of course streams nothing and joins the ring.

;;;","07/Dec/12 15:50;cherro;Could StorageServer.joinTokenRing wait max(RING_DELAY, 1min) (the 1 min being the delay in MigrationManager.maybeScheduleSchemaPull? Or could MigrationManager.maybeScheduleSchemaPull use some multiple of RING_DELAY?

Related: is it correct that StorageServer.joinTokenRing calls Schema.instance.updateVersionAndAnnounce and MigrationManager.passiveAnnounce(Schema.instance.getVersion()) in quick succession?;;;","07/Dec/12 16:50;cherro;From discussion on #cassandra-dev with [~brandon.williams], StorageServer.joinTokenRing could use Schema.emptyVersion as Schema UUID in order to allow the maybeScheduleSchemaPull delay to be skipped. Patch to follow...;;;","07/Dec/12 20:22;cherro;Attached patch 3 proposing the use of Schema.emptyVersion to differentiate StorageServer.joinTokenRing from other scenarios so that migration delay can be skipped for bootstrapping.;;;","07/Dec/12 21:18;brandon.williams;Close, but maybeScheduleSchemaPull is actually called by the bootstrapping node, so the check needs to see if the current schema version is empty and if so pull.  v4 attached.;;;","07/Dec/12 21:20;jbellis;+1;;;","07/Dec/12 21:45;brandon.williams;Committed.;;;","10/Dec/12 00:49;cherro;(Following up on IRC discussion)
* My patch 3 incorrectly hardcoded Schema.emptyVersion for the announcement in SS.joinTokenRing. For actual bootstrap scenario, the schema version should be Schema.emptyVersion anyway. 
* Since Schema.updateVersion actually reads rows, I wondered if this will be equivalent to    Schema.emptyVersion (perhaps Schema tables themselves are represented already by this point in time?) Brandon said that he would check this.
* I had asked in a previous comment in this jira, and Brandon also noticed that SS.joinTokenRing had been calling Schema.updateVersionAndAnnounce and Schema.passiveAnnounce in quick succession. Brandon said that it should be removed.

I'm attaching patch 5 with these changes:
* Reverted my hardcoded Schema.emptyVersion in SS.joinTokenRing (back to original Schema.updateVersionAndAnnounce).
* Removed apparently redundant call to Schema.passiveAnnounce.

Brandon, could you please confirm whether it is safe to assume that Schema.updateVersionAndAnnounce would emit Schema.emptyVersion in a bootstrap scenario?
 ;;;","10/Dec/12 00:56;xedin;bq. Since Schema.updateVersion actually reads rows, I wondered if this will be equivalent to Schema.emptyVersion (perhaps Schema tables themselves are represented already by this point in time?) Brandon said that he would check this.

If that is bootstrap and node is completely empty thus has newly created system tables Schema.updateVersion would emit ""emptyVersion"" as there is no data to be read and empty digest has a constant value.;;;","10/Dec/12 02:07;brandon.williams;I fixed this up in 36389f7d8d59881cad309b078fc89df5864fd6d1 and allowed pulling immediately after a restart.;;;","10/Dec/12 02:54;cherro;Thanks [~brandon.williams], [~xedin].;;;","20/Dec/16 15:59;pshirshov;Hi,

Looks like that the issue has not been resolved. 

I've just got this:

{code}
15:52:40.194 [cluster1-nio-worker-1] WARN  c.d.driver.core.RequestHandler - /127.0.0.1:9042 replied with server error (java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family ID mismatch (found 5230d9a0-c6cc-11e6-9ece-6d2c86545d91; expected 51d06a20-c6cc-11e6-9ece-6d2c86545d91)), defuncting connection.
{code}

On C* 3.9:

{code}
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh>
{code}

with latest driver: {{""com.datastax.cassandra"" % ""cassandra-driver-core"" % ""3.1.2""}}

It's one-node setup and everything I'm doing is just running {{CREATE TABLE IF NOT EXISTS}} from several threads at the same time.

The only difference with previous version is the fact that applications starts hanging forever with stack like

{code}
""pool-4-thread-5-ScalaTest-running-CassandraMwsRestTest"" #37 prio=5 os_prio=31 tid=0x00007f7f7048a800 nid=0x5113 waiting on condition [0x0000700006010000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000796682630> (a com.datastax.driver.core.DefaultResultSetFuture)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:445)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:143)
	at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:243)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:68)
	at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:43)
{code};;;","06/Jan/17 10:14;webpoacher;I experience similar problems in a one node cluster. When I start 3 applications at the same time, they all try to migrate the cassandra schema (but will wait for each other using a locking table). However, they will check for this lock table using a CREATE TABLE IF NOT EXISTS

Cassandra driver 2.1.10.1

{noformat}
2017-01-03 09:57:22,372 · WARN · cluster2-nio-worker-1 · com.datastax.driver.core.RequestHandler · /127.0.0.1:9042 replied with server error (java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family ID mismatch (found a1c87c40-d192-11e6-a126-1d2c09c16740; expected a1c56f00-d192-11e6-a126-1d2c09c16740)), defuncting connection. ·  ·  ·  ·
2017-01-03 09:57:22,394 · ERROR · main · com.contrastsecurity.cassandra.migration.action.Migrate · Migration of keyspace ces2 to version 0.1.0.1 failed! Please restore backups and roll back database and code! ·  ·  ·  ·
2017-01-03 09:57:24,652 · ERROR · main · nl.mypackage.CassandraMigrationService · Error during migration ·  ·  ·  ·
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.DriverException: Timeout while trying to acquire available connection (you may want to increase the driver number of per-host connections)))
        at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
        at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
        at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:217)
       at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:54)
        at com.contrastsecurity.cassandra.migration.dao.SchemaVersionDAO.tablesExist(SchemaVersionDAO.java:88)
{noformat}

What is the recommended way to perform schema migrations in a Cassandra cluster?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't prepare an UPDATE query with a counter column (CQL3),CASSANDRA-5022,12618606,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,aleksey,aleksey,03/Dec/12 18:24,16/Apr/19 09:32,14/Jul/23 05:52,05/Dec/12 08:03,1.2.0 rc1,,,,,,0,,,,,,,"CQL3, binary protocol:

demo(id int primary key, counter counter)

Preparing ""UPDATE test.counters SET counter = counter + ? WHERE id = ?"" yields 8704, ""Invalid operation for commutative columnfamily counters"" error.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/12 18:06;slebresne;5002.txt;https://issues.apache.org/jira/secure/attachment/12555958/5002.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,295764,,,Wed Dec 05 08:03:39 UTC 2012,,,,,,,,,,"0|i13xtr:",231110,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,"04/Dec/12 18:06;slebresne;Attaching patch to fix that. The patch also move some small bit of validation that were done at execution time to preparation time (since there is no good reason not to do it there).;;;","04/Dec/12 23:42;aleksey;+1;;;","05/Dec/12 08:03;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preparing UPDATE queries with collections returns suboptimal metadata,CASSANDRA-5017,12618587,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,aleksey,aleksey,03/Dec/12 16:35,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 16:34,1.2.0 rc1,,,,,,0,,,,,,,"CQL3, binary protocol.

collections (id int primary key, amap map<int, varchar>);

preparing ""UPDATE test.collections SET amap[?] = ? WHERE id = ?"" returns the following metadata:
[{column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""amap"">>,
                       {map,int,varchar}},
               {column,<<""test"">>,<<""collections"">>,<<""id"">>,int}]

Ideally it should return [int, varchar, int] types. Less ideally [{map, int, varchar}, int] and expect an encoded map with a single key-value pair. But certainly not what it currently returns.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5018,,,,,,,,,,,"04/Dec/12 13:54;slebresne;0001-Return-correct-metadata-when-preparing-map.txt;https://issues.apache.org/jira/secure/attachment/12555935/0001-Return-correct-metadata-when-preparing-map.txt","04/Dec/12 13:54;slebresne;0002-Fix-prepared-list-index-and-using-integer-for-map-keys.txt;https://issues.apache.org/jira/secure/attachment/12555936/0002-Fix-prepared-list-index-and-using-integer-for-map-keys.txt","04/Dec/12 13:54;slebresne;0003-Fix-handling-of-prepared-marker-for-deletes.txt;https://issues.apache.org/jira/secure/attachment/12555937/0003-Fix-handling-of-prepared-marker-for-deletes.txt","05/Dec/12 08:22;slebresne;0004-Update-validation-of-ListOperation.txt;https://issues.apache.org/jira/secure/attachment/12556075/0004-Update-validation-of-ListOperation.txt",,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293424,,,Mon Dec 10 16:34:56 UTC 2012,,,,,,,,,,"0|i0szov:",167259,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,"04/Dec/12 13:54;slebresne;Attaching 3 small patches that fixes a number of problems with prepared queries and collections, including this one, the fact that you weren't allowed to prepare a list index and the fact that delete wasn't working correctly when either a list index or a map key was prepared.

I note we have a small ""problem"" for the metadata return in all those cases, in that if you do
{noformat}
UPDATE foo SET amap[?] = ? WHERE ...
{noformat}
then it's unclear what name to return for the prepared variables in the metadata. The choice made by the patch is to return 'key(amap)' and 'value(amap)' respectively (and when preparing a list index, we return 'index(alist)').
;;;","05/Dec/12 00:08;aleksey;CASSANDRA-5018 in indeed no longer an issue. Also I do get the right columns in my metadata now:

{ok, R} = seestar_client:prepare(Pid, ""update test.collections set amap[?] = ? WHERE id = ?"").
{ok,{prepared,<<21,71,235,65,143,167,200,103,53,75,239,64,
                92,235,243,145>>,
              [{column,<<""test"">>,<<""collections"">>,<<""key(amap)"">>,int},
               {column,<<""test"">>,<<""collections"">>,<<""value(amap)"">>,
                       varchar},
               {column,<<""test"">>,<<""collections"">>,<<""id"">>,int}]}}

However, when I try to execute the query (or just run it in cqlsh), I get ""Bad Request: List operations are only supported on List typed columns, but org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.UTF8Type) given"" error.;;;","05/Dec/12 08:22;slebresne;Right, forgot to update the validation of ListType. Fourth patch attached to fix that.;;;","05/Dec/12 17:08;aleksey;Everything works now (prepared and unprepared). +1;;;","10/Dec/12 16:34;slebresne;My bad, forgot to resolve, but that has been committed a few days ago.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't prepare an INSERT query,CASSANDRA-5016,12618585,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,aleksey,aleksey,03/Dec/12 16:29,16/Apr/19 09:32,14/Jul/23 05:52,05/Dec/12 08:04,1.2.0 rc1,,,,,,0,,,,,,,"Preparing an INSERT query fails with CQL3+binary protocol (maybe thrift as well, haven't checked). Preparing an equivalent UPDATE query works just fine.

demo (id int primary key, value text)
preparing ""INSERT INTO test.demo (id, value) VALUES (?, ?)"" -> 8704, Invalid definition for id, not a collection type
prparing ""UPDATE test.demo SET value = ? WHERE id = ?"" -> ok",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5028,,,,,,,,,,,"03/Dec/12 18:22;slebresne;5016.patch;https://issues.apache.org/jira/secure/attachment/12555796/5016.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293422,,,Wed Dec 05 08:04:37 UTC 2012,,,,,,,,,,"0|i0szof:",167257,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,"04/Dec/12 23:33;aleksey;+1;;;","05/Dec/12 08:04;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow bloom filter false positive chance of 0,CASSANDRA-5013,12618512,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,02/Dec/12 21:43,16/Apr/19 09:32,14/Jul/23 05:52,07/Jan/13 18:11,1.2.1,,,Local/Config,,,0,,,,,,,"{pre}
ERROR [CompactionExecutor:16] 2012-11-30 08:44:32,546 SSTableWriter.java (line 414) Bloom filter FP chance of zero isn't supposed to happen
{pre}

when attempting to set it to zero, C* should either disallow the change or should just interpret 0 as ""make it the default"" and not continually log the above error message
",,andras.szerdahelyi@ignitionone.com,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/12 20:00;jbellis;5013-v2.txt;https://issues.apache.org/jira/secure/attachment/12561343/5013-v2.txt","07/Dec/12 14:16;brandon.williams;5013.txt;https://issues.apache.org/jira/secure/attachment/12559863/5013.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293330,,,Mon Jan 07 18:11:20 UTC 2013,,,,,,,,,,"0|i0swrr:",166786,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"07/Dec/12 14:12;brandon.williams;Since we're already converting zero to the default, it makes sense to maintain that behavior and simply remove the error.  Patch to do so.;;;","07/Dec/12 16:44;jbellis;Where are we converting zero to default?  ISTM that setting to zero is likely to either mean

# user is asking for no false positives at all, which is impossible.  should give an error instead of changing to default.
# user is asking for no bloom filter at all, in which changing to default is also not what we want to do.;;;","07/Dec/12 16:47;brandon.williams;If we detect zero we set fpChance to null, which then results in 15 target buckets.  I agree with both your points but for a point release in stable changing the behavior doesn't seem appropriate.;;;","07/Dec/12 17:14;jbellis;then let's fix it in 1.2 :);;;","07/Dec/12 19:42;brandon.williams;Sure, but let's apply this patch for 1.1.8 :);;;","07/Dec/12 19:47;jbellis;I'm not sure at all this is going to work as designed.  The comments there say ""this isn't supposed to happen"" which makes me think that there is some code somewhere else that tries to get rid of zeros.

That complexity is gone in 1.2 (since we gave up on grandfathering in the pre-1.0? code that calls the 15 bucket path) so I'd rather stick with changes there than risk regressions.;;;","17/Dec/12 20:00;jbellis;attached as v2;;;","17/Dec/12 20:04;andras.szerdahelyi@ignitionone.com;Thank you for your e-mail. I'm out of the office on vacation until Tuesday, the 8th of January 2013 with very little access to e-mail.
For technical questions or concerns, please write to tech-team@netmining.com.

happy holidays!
Andras Szerdahelyi
;;;","17/Dec/12 21:11;andras;well, that wasn't entirely intentional, but happy holidays nevertheless! :-)

;;;","04/Jan/13 12:59;brandon.williams;You need to check that bloomFilterFpChance isn't null before seeing if it's zero at the end to avoid NPE, but otherwise +1.;;;","04/Jan/13 17:35;jbellis;Not sure what you mean, isn't that what I'm doing here?

{code}
.       // we disallow bFFPC==null starting in 1.2.1 but tolerated it before that
        return (bloomFilterFpChance == null || bloomFilterFpChance == 0)
               ? compactionStrategyClass == LeveledCompactionStrategy.class ? 0.1 : 0.01
               : bloomFilterFpChance;
{code};;;","04/Jan/13 17:39;brandon.williams;The problem is that if the compaction class is not LCS, it's still null, and then further down:

{noformat}
 if (bloomFilterFpChance == 0)
{noformat}

Throws an NPE.;;;","07/Jan/13 18:11;jbellis;fixed + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter table alter causes TSocket read 0 bytes,CASSANDRA-5012,12618499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,khahn,khahn,02/Dec/12 18:12,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/12 09:14,1.2.0 rc1,,,,,,0,,,,,,,"Altering the type of a clustering key column causes TSocket error. 

To reproduce the problem:
1. CREATE SCHEMA ""Excalibur"" WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

2. CREATE TABLE ""Excalibur"".Test ( id int, species text, color text, PRIMARY KEY ((id, species), color)) WITH compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_compaction_threshold' : 6 };

3. Alter table ""Excalibur"".test ALTER color type int;

Expected result: Error message saying something like, ""Changing the type of a clustering key is not allowed.""

Actual result: TSocket read 0 bytes","On Mac OSX ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]",dbrosius,khahn,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/12 15:32;slebresne;5012.patch;https://issues.apache.org/jira/secure/attachment/12555770/5012.patch","03/Dec/12 20:04;khahn;log.txt;https://issues.apache.org/jira/secure/attachment/12555817/log.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293317,,,Fri Dec 07 09:14:39 UTC 2012,,,,,,,,,,"0|i0swon:",166772,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Dec/12 19:27;dbrosius;Some exception occurred on the server, that caused the TSocket error. Finding and posting that error in the server logs, will help what went wrong.;;;","03/Dec/12 15:32;slebresne;Patch attached to move the validation at the time of the application of the alter statement.;;;","03/Dec/12 20:04;khahn;Sorry for not including the log. Here it is if you're still interested in it.;;;","07/Dec/12 03:20;jbellis;+1;;;","07/Dec/12 09:14;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RangeStreamer has no way to report failures, allowing bootstrap/move etc to complete without data",CASSANDRA-5009,12618363,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,30/Nov/12 18:59,16/Apr/19 09:32,14/Jul/23 05:52,03/Dec/12 20:33,1.1.8,1.2.0 beta 3,,,,,1,,,,,,,"It looks like we fixed this for 1.2 by having RS.fetch() throw, but in 1.1 it does not and there doesn't appear to be a way to detect an RS failure, which among other things will cause bootstrap to succeed even though it failed to fetch any data.",,cherro,soboleiv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4651,,,,,,,,,,,"03/Dec/12 19:01;brandon.williams;5009-v2.txt;https://issues.apache.org/jira/secure/attachment/12555808/5009-v2.txt","30/Nov/12 22:32;brandon.williams;5009.txt;https://issues.apache.org/jira/secure/attachment/12555577/5009.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293028,,,Mon Dec 03 20:33:24 UTC 2012,,,,,,,,,,"0|i0srn3:",165955,,yukim,,yukim,Critical,,,,,,,,,,,,,,,,,"30/Nov/12 19:25;brandon.williams;Actually we haven't completely fixed this in 1.2, because we rely on the FD notification to fail but use phi * 2 to avoid a premature costly repair abortion, but the streaming can fail out before that and thus fetch() will never raise an exception.;;;","30/Nov/12 22:32;brandon.williams;Patch to maintain a set of completed ranges, then check at the end that every range was successful or raise an exception.

This patch is against 1.1, but I'll note that when merged RangeStreamer should look exactly the same.  That is to say, all the FD business will be removed, as I can't figure out how it makes any sense to check the FD at all since we're going to block on the streaming latch first.  If any ranges (not necessarily streams) failed, then it's time to bail regardless of the FD, and if the FD does convict before that it won't break us out of the latch wait.
;;;","03/Dec/12 18:50;yukim;two comments on the patch:

* looks like exceptionMessage is not used anywhere
* I think _completed_ needs to be synchronized and _completed.addAll(ranges)_ should be placed before _latch.countDown()_, otherwise we have chance to get incomplete completed set.;;;","03/Dec/12 19:01;brandon.williams;Oops, exceptionMessage was a vestige from my initial attempt at backporting the 1.2 version before I discovered it was flawed too.

Updated patch removes that, moves the addAll call before the latch countdown, and initiates the set with a concurrent hash map.;;;","03/Dec/12 19:52;yukim;+1;;;","03/Dec/12 20:33;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""show schema"" command in cassandra-cli generates wrong ""index_options"" values.",CASSANDRA-5008,12618350,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,thboileau,thboileau,30/Nov/12 17:42,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/12 17:47,1.1.8,1.2.0 rc1,,,,,0,,,,,,,"Using cassandra-cli, launch the ""show schema"" command and save the output to a file.
Try to import it in order to recreate the schema, it fails with error message :
""Syntax error at position 626: no viable alternative at input '}'""

",ubuntu 12.04,thboileau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,293011,,,Fri Dec 07 17:47:10 UTC 2012,,,,,,,,,,"0|i0srcf:",165907,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"30/Nov/12 17:45;thboileau;After some investigations, it appears that the ""index_options"" attributes are badly generated when there is actually no index options :

create column family Test
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'email',
    validation_class : UTF8Type,
    index_name : 'test_email',
    index_type : 0,
    index_options : {

}},
    {column_name : 'enabled',
    validation_class : UTF8Type,
    index_name : 'test_enabled',
    index_type : 0,
    index_options : {

}},
    {column_name : 'roles',
    validation_class : UTF8Type,
    index_name : 'test_roles',
    index_type : 0,
    index_options : {

}}]

When removing these empty ""index_options"" attributes, the script is correctly handled.;;;","30/Nov/12 17:52;thboileau;I guess the fix is located in method ""showColumnMeta"" of class ""org.apache.cassandra.cli.CliClient"". It could be something like this:

if (colDef.index_options != null && !colDef.index_options.entrySet().isEmpty())
            {
                sb.append(TAB + TAB + ""index_options : {""+NEWLINE);        
                for (Map.Entry<String, String> entry : colDef.index_options.entrySet())
                {
                    sb.append(TAB + TAB + TAB + CliUtils.escapeSQLString(entry.getKey()) + "": '"" + CliUtils.escapeSQLString(entry.getValue()) + ""',"" + NEWLINE);
                }
                sb.append(""}"");
            };;;","07/Dec/12 17:47;yukim;Thanks Thierry, committed your !isEmpty check in 1ac90589ead4b95a922d783ead75412dd46c0f28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUIGen should never use another host IP for its node part,CASSANDRA-5002,12618145,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Nov/12 12:32,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/12 09:15,1.2.0 rc1,,,,,,0,,,,,,,"UUIDGen allows to specify the inet address that we use to generate the node part of the created UUID. This is wrong however. More precisely, the node part is what make sure UUID generated on two different hosts are different, because we can't guarantee that the timestamp and clock parts will be different. In other words, generating on a host a UUID with the node part of another host is dangerous is clearly contrary to the spec.

And as it turns out, making sure we always use the local address means that the full lsb part of the UUID becomes constant (as it should) and we can remove the nodeCache from UUIDGen and simplify/speedup UUID generation, which is all the more reason to fix it.

I note that we were almost always using the local address to generate UUID anyway. The only place where we weren't is in Stream{In/Out}Session, and there is virtually no chance that this has ever broke anything (but we should still fix it).
",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/12 13:04;slebresne;5002.txt;https://issues.apache.org/jira/secure/attachment/12555348/5002.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292767,,,Fri Dec 07 09:16:33 UTC 2012,,,,,,,,,,"0|i0se2n:",163756,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Dec/12 06:15;jbellis;I'm pretty sure the first part of getAllLocalAddresses (the getAllByName list) will be a subset of the ones seen by getNetworkInterfaces' addresses.  (We do this in CFRR too -- would make sense to extract it somewhere.)

Otherwise +1.;;;","07/Dec/12 09:16;slebresne;You're right, the getAllByName part is probably useless. I've move the getNetworkInterfaces part in FBUtilities (and used it in CFRR and UUIDGen) and committed. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generated time-based UUID don't conform to the spec,CASSANDRA-5001,12618021,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,28/Nov/12 17:41,16/Apr/19 09:32,14/Jul/23 05:52,29/Nov/12 09:24,1.2.0 beta 3,,,,,,0,,,,,,,"When UUIDGen layout the clock sequence and and node part of version 1 UUID, it does so with
{noformat}
private long getClockSeqAndNode(InetAddress addr)
{
    long lsb = 0;
    lsb |= (clock & 0x3f00000000000000L) >>> 56; // was 58?
    lsb |= 0x0000000000000080;
    lsb |= (clock & 0x00ff000000000000L) >>> 48;
    lsb |= makeNode(addr);
    return lsb;
}
{noformat}
This is not correct however, as this layout the clock seq (and variant) on the right-most part of the lsb while it should be on the left-most one.

At a minimum, the generated UUID don't fully respect the spec since the variant is not set correctly. But it also means that the clock seq bits end up being all 0's (as can be trivially seen in the string representation of the generated UUIDs).

Note that none of those is a huge huge deal as there is still largely enough random bytes to ensure that two different nodes won't end up with the same lsb. And having the variant wrong has probably no practical implementation. There is no reason not to fix those though.

One other small details is that the getAdjustedTimestamp as a sign error so that it returns completely broken timestamps. That being said the method is currently unused so that's not a big deal. I'm attaching a fix for that part too because that method might be useful someday but I won't shed a tear if we prefer just removing it.

I'm marking this for 1.2 because I'm not sure it's worth bothering with 1.1.",,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/12 17:44;slebresne;5001.txt;https://issues.apache.org/jira/secure/attachment/12555199/5001.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292623,,,Thu Nov 29 09:24:37 UTC 2012,,,,,,,,,,"0|i0s9r3:",163056,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"29/Nov/12 01:56;vijay2win@yahoo.com;+1

Nit: it might be nice to move the calculation in getClockSeqAndNode into makeNode so we can cache it in nodeCache.;;;","29/Nov/12 09:24;slebresne;Alright, committed, thanks

bq. Nit: it might be nice to move the calculation in getClockSeqAndNode into makeNode so we can cache it in nodeCache.

That true, but truth is, you're not supposed to generate uuid for another node (and in practice we never do) so that whole nodeCache is useless imho and the whole UUID lsb could be a static. So anyway, I'll open a separate ticket in a gif to fix those inefficiencies.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After changing the compaction strategy, compression_strategy  always returning back to the ""SnappyCompressor"" through CQL 2.2.0",CASSANDRA-4996,12617744,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,shamim_ru,shamim_ru,27/Nov/12 08:06,16/Apr/19 09:32,14/Jul/23 05:52,13/Dec/12 19:50,1.1.8,1.2.0,,,,,0,,,,,,,"faced very strange behaviour when changing compression_parameters of exisiting CF. After changing the compaction strategy, compression_strategy returning back to the ""SnappyCompressor"".

Using cassandra version 1.1.5.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
I have one column family with following paramters:

cqlsh > describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
comment='' AND
comparator=text AND
read_repair_chance=0.100000 AND
gc_grace_seconds=864000 AND
default_validation=text AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
replicate_on_write='true' AND
compaction_strategy_class='SizeTieredCompactionStrategy' AND
compaction_strategy_options:sstable_size_in_mb='5' AND
compression_parameters:sstable_compression='SnappyCompressor';

Changing compression strategy to 'DeflateCompressor

cqlsh> ALTER TABLE auditlog_01 WITH compression_parameters:sstabl
e_compression = 'DeflateCompressor' AND compression_parameters:chunk_length_kb =
 64;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='DeflateCompressor';

it's sucessfuly changed the compression strategy to 'DeflateCompressor, after that when i am trying to change the compaction strategy, compression strategy returing back to ""SnappyCompressor"".
cqlsh> alter table auditlog_01 with compaction_strategy_class='Le
veledCompactionStrategy' AND compaction_strategy_options:sstable_size_in_mb=5;
cqlsh> describe columnfamily auditlog_01;

CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
dscn text,
asid text,
soapa text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';",,aleksey,derek.bromenshenkel,shamim_ru,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/12 18:30;aleksey;4996-1.1.txt;https://issues.apache.org/jira/secure/attachment/12560232/4996-1.1.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292260,,,Fri Dec 14 09:03:46 UTC 2012,,,,,,,,,,"0|i0rsvz:",160324,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"28/Nov/12 05:45;derek.bromenshenkel;I've run into this in the past, also.

It was introduced in 1.1 when compression was turned on by default.  The problem is that CFPropDefs class is shared by both CREATE and ALTER statements' code path and to provide the ""default on"" functionality, the sstable_compression value is statically set inside this class.  Then, because the ALTER statement provided does not define the value (and thus override the default), it ends up switching it back to the default.  This is also present in CQL 3 as far as I know.  I think the fix will be to remove the defaulting from CFPropDefs and add it over in the CREATE code path, since that is where the ""default on"" behavior is needed.

Also, notice that on your ALTER command, it did not actually change the compaction_strategy_class as you requested, but that is probably another issue.;;;","10/Dec/12 18:33;aleksey;Patch for 1.2 is similar - not sure whether it should go into 1.2.0 or rc1.;;;","11/Dec/12 22:32;jbellis;Can you summarize the fix?;;;","11/Dec/12 22:37;aleksey;Basically what [~derek.bromenshenkel] suggested:
1) Remove the default strategy from CFPropDefs
2) Set default strategy in CreateColumnFamilyStatement
3) Leave AlterTableStatement as is (mostly, cql3 version of it needed a slight fix);;;","13/Dec/12 16:55;jbellis;+1;;;","13/Dec/12 19:50;slebresne;If I trust the changelog this has been committed, so closing.;;;","13/Dec/12 20:05;aleksey;Thanks, committed.;;;","14/Dec/12 09:03;shamim_ru;thanks a lot - will make a try;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionSerializerTest fails to find jemalloc,CASSANDRA-4995,12617734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,27/Nov/12 04:10,16/Apr/19 09:32,14/Jul/23 05:52,27/Nov/12 08:45,2.0 beta 1,,,Legacy/CQL,,,0,,,,,,,"{noformat}
    [junit] Testcase: org.apache.cassandra.io.CompactSerializerTest:	Caused an ERROR
    [junit] Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] java.lang.UnsatisfiedLinkError: Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
    [junit] 	at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:163)
    [junit] 	at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:236)
    [junit] 	at com.sun.jna.Library$Handler.<init>(Library.java:140)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:379)
    [junit] 	at com.sun.jna.Native.loadLibrary(Native.java:364)
    [junit] 	at org.apache.cassandra.io.util.JEMallocAllocator.<clinit>(JEMallocAllocator.java:32)
    [junit] 	at java.lang.Class.forName0(Native Method)
    [junit] 	at java.lang.Class.forName(Class.java:169)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:109)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:100)
    [junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:142)
{noformat}

If jemalloc is now the preferred allocator, we should add it to the debian packaging.  However, I did install the lib and it still didn't work. ",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/12 05:37;vijay2win@yahoo.com;0001-CASSANDRA-4995.patch;https://issues.apache.org/jira/secure/attachment/12554970/0001-CASSANDRA-4995.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292249,,,Tue Nov 27 08:45:02 UTC 2012,,,,,,,,,,"0|i0rstj:",160313,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Nov/12 05:24;vijay2win@yahoo.com;JEMalloc is not default, looks like this is because the Directory scanner scans for all the files in the class path and initializes it which will cause the static variable to get intialized.;;;","27/Nov/12 07:34;jbellis;Constructor initializing a static field makes my OCD go crazy. :)

Should we just make the field non-static?  Or use a inner class static initializer + static instance() method like in MessagingService.;;;","27/Nov/12 07:43;vijay2win@yahoo.com;Hi Jonathan,

{quote}
Constructor initializing a static field 
{quote}
attached patch doesn't initialize the static field.... it was moved from static to non-static... 
{code}
-    private static final JEMLibrary instance = (JEMLibrary) Native.loadLibrary(""jemalloc"", JEMLibrary.class);
+    private final JEMLibrary instance;
{code}
not sure if we are talking about the same thing :) (May be the name caused the confusion?);;;","27/Nov/12 08:15;jbellis;I must be more jet lagged than I thought.  LGTM.;;;","27/Nov/12 08:45;vijay2win@yahoo.com;Committed, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTL/WRITETIME function against collection column returns invalid value,CASSANDRA-4992,12617624,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,26/Nov/12 15:02,16/Apr/19 09:32,14/Jul/23 05:52,26/Nov/12 18:23,1.2.0 beta 3,,,,,,0,,,,,,,"Since we cannot query individual content of collection in 1.2, TTL/WRITETIME function on collection column does not make sense. But currently we can perform those function on collection and get deserialization error like:

{code}
value '\x00\x03\x00\x01c\x00\x01b\x00\x01a' (in col 'writetime(l)') can't be deserialized as bigint: unpack requires a string argument of length 8
{code}

Looks like it tries to deserialize whole list/set/map content as bigint for WRITETIME and int for TTL.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/12 16:27;slebresne;4992.txt;https://issues.apache.org/jira/secure/attachment/12554873/4992.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292127,,,Mon Nov 26 18:23:37 UTC 2012,,,,,,,,,,"0|i0rryn:",160174,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"26/Nov/12 16:27;slebresne;Simple patch attached to just refuse said function on collection columns.;;;","26/Nov/12 16:50;yukim;+1;;;","26/Nov/12 18:23;slebresne;Commited, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow use of collection with compact storage,CASSANDRA-4990,12617621,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,26/Nov/12 14:57,16/Apr/19 09:32,14/Jul/23 05:52,26/Nov/12 18:21,1.2.0 beta 3,,,,,,0,,,,,,,"You can define ColumnFamily with collection type and compact storage as follows:

{code}
CREATE TABLE test (
  user ascii PRIMARY KEY,
  mails list<text>
) WITH COMPACT STORAGE;
{code}

This does not make sense and end up error when inserting data to collection.

{code}
INSERT INTO test (user, mails) VALUES ('foo', ['foo@foo.org']);
{code}

I think it is better not to allow defining such ColumnFamily.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/12 16:12;slebresne;4990.txt;https://issues.apache.org/jira/secure/attachment/12554872/4990.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292124,,,Mon Nov 26 18:21:15 UTC 2012,,,,,,,,,,"0|i0rrxz:",160171,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"26/Nov/12 16:13;slebresne;Trivial patch attached.;;;","26/Nov/12 16:44;yukim;+1;;;","26/Nov/12 18:21;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error opening data file at startup,CASSANDRA-4984,12617298,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,carlyeks,zenek_kraweznik0,zenek_kraweznik0,22/Nov/12 09:17,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 15:00,1.1.8,,,,,,0,,,,,,,"I've found this in logfile, this happens at cassandra startup:

INFO 10:06:13,670 Opening /var/lib/cassandra/data/MYKSPC/MYCF/MYKSPC-MYCF-hf-5547 (1073761823 bytes)
ERROR 10:06:13,670 Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:166)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:242)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)


Every CF in this Keyspace has cashing set to 'NONE'",Oracle Java 1.6u37,carlyeks,zenek_kraweznik0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/12 23:03;carlyeks;4984.patch;https://issues.apache.org/jira/secure/attachment/12560050/4984.patch",,,,,,,,,,,,,,,,,1.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,259511,,,Mon Dec 10 15:00:44 UTC 2012,,,,,,,,,,"0|i0lq53:",124886,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"23/Nov/12 11:00;zenek_kraweznik0;It happens after offline scrub too.;;;","08/Dec/12 23:05;carlyeks;The assertion is being raised because the primary index of the file is missing. Attaching trivial patch to make the assertion error more descriptive.;;;","10/Dec/12 15:00;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: alter table add column with table that has collection fails,CASSANDRA-4982,12617225,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dbrosius,dbrosius,21/Nov/12 22:03,16/Apr/19 09:32,14/Jul/23 05:52,26/Nov/12 09:25,1.2.0 beta 3,,,,,,0,,,,,,,"create keyspace collections with replication = {'class':'SimpleStrategy', 'replication_factor':1};
use collections;
create table collections (key int primary key, aset set<text>);
insert into collections (key, aset) values (1, {'fee', 'fi'});
alter table collections add aaa text;
 
 
ERROR 16:52:33,792 Error occurred during processing of message.
java.lang.UnsupportedOperationException: ColumnToCollectionType should only be used in composite types, never alone
        at org.apache.cassandra.db.marshal.ColumnToCollectionType.validate(ColumnToCollectionType.java:103)
        at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1094)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:202)
        at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:217)
        at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:73)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1706)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",,dbrosius,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/12 10:42;slebresne;4982.txt;https://issues.apache.org/jira/secure/attachment/12554721/4982.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,259429,,,Mon Nov 26 09:25:32 UTC 2012,,,,,,,,,,"0|i0lp33:",124714,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,"23/Nov/12 10:42;slebresne;Trivial patch attached.;;;","24/Nov/12 04:50;dbrosius;fixed... LGTM.;;;","26/Nov/12 09:25;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageServiceClientTest/RecoveryManager2Test fail on 1.2.0 and above branch,CASSANDRA-4980,12617135,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,yukim,yukim,21/Nov/12 16:50,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/12 19:18,,,,,,,0,,,,,,,"Looks like change in c4cca2d8bba20a7651b956e1893727391bf5f10a (store schema_version to system.local) broke both StorageServiceClientTest and RecoveryManager2Test.
StorageServiceClientTest assert data directories are not created in client mode but this change actually creates data directories. RecoveryManager2Test fails with ""junit.framework.AssertionFailedError: Expecting only 1 replayed mutation, got 10"" error and I think extra commit log also comes from this insert to system.local.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/12 17:26;slebresne;0001-fix-StorageServiceClientTest.txt;https://issues.apache.org/jira/secure/attachment/12554535/0001-fix-StorageServiceClientTest.txt","21/Nov/12 17:26;slebresne;0002-Clean-up-the-commit-log-before-RecoveryManager2Test.txt;https://issues.apache.org/jira/secure/attachment/12554536/0002-Clean-up-the-commit-log-before-RecoveryManager2Test.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,259336,,,Wed Nov 21 19:18:31 UTC 2012,,,,,,,,,,"0|i0lofr:",124609,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"21/Nov/12 16:54;jbellis;My vote would be to revert c4cca2 for 1.2.0 and try again for 1.2.1.  [~slebresne]?;;;","21/Nov/12 17:26;slebresne;I'd rather avoid that if possible.

The problem in StorageServiceClientTest is imo a bug that this commit just happens to reveal. Namely, that for some reason {{SS.initClient()}} calls updateVersionAndAnnounce. But since fat clients are supposed to not have local tables, they will in particular have no schema and this is useless. If for some weird reason we really need fat client to send a 'I have no schema' on gossip, let's call {{MigrationManager.passiveAnnounce(Schema.emptyVersion)}} directly (but I don't see why we would need that).

As for RecoveryManager2Test, it's just that this test was somehow assuming the commit log was empty when the test started. Which was the case because we almost always flush when we write the system table, but it happens that for writing the schemaVersion in the local table I figured flushing was overkill (since it's only for client sake and we don't need to flush to have it visible).

Attaching simple fixes for both problems.;;;","21/Nov/12 18:45;yukim;patch lgtm. +1;;;","21/Nov/12 19:18;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress for cql3 is broken on 1.2/trunk,CASSANDRA-4979,12617116,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Nov/12 15:23,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/12 16:35,1.2.0 beta 3,,,,,,0,,,,,,,,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/12 15:24;slebresne;4979.txt;https://issues.apache.org/jira/secure/attachment/12554508/4979.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,259316,,,Wed Nov 21 16:35:35 UTC 2012,,,,,,,,,,"0|i0lobb:",124589,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Nov/12 15:33;jbellis;It's actually broken pretty deeply...  it gives explicit names to each cell for instance, when the more natural way to interpret ""make me a row with 100,000 cells"" is to make a wide row for slicing.

OTOH I could understand wanting to test explicit name queries on 5-cell rows.  Not sure how to generalize both, I think we might need more options. :(;;;","21/Nov/12 15:40;slebresne;Yes, cql3 support by stress is funky. It also only ever test compact storage, so it would be good to also allow testing compact storage. It would also make sense to allow testing the binary protocol, but the current code has thrift hardcoded in quite a few places. Lastly, the code is littered with code repetition. So imho, we should refactor the hell out of it.

But the code we have now doesn't even run with the -L3 option, and that's what this patch fixes.;;;","21/Nov/12 16:20;jbellis;+1;;;","21/Nov/12 16:35;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""AssertionError: Wrong class type"" at CounterColumn.diff()",CASSANDRA-4976,12616849,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,19/Nov/12 22:30,16/Apr/19 09:32,14/Jul/23 05:52,23/Nov/12 09:03,1.1.7,,,,,,0,counters,,,,,,"Thousands of the following errors are getting logged to system.log:

ERROR [ReadRepairStage:150152] 2012-11-15 12:31:02,815 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadRepairStage:150152,5,main]
java.lang.AssertionError: Wrong class type.
        at org.apache.cassandra.db.CounterColumn.diff(CounterColumn.java:110)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:248)
        at org.apache.cassandra.db.ColumnFamily.diff(ColumnFamily.java:342)
        at org.apache.cassandra.service.RowRepairResolver.scheduleRepairs(RowRepairResolver.java:117)
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:94)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

There are also many of the following errors intermingled with the above:

ERROR [ReadRepairStage:150158] 2012-11-15 12:30:34,148 CounterContext.java (line 381) invalid counter shard detected; (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, 916) and (b29a5480-e911-11e1-0000-ce481d6d2aff, 3, -1590) differ only in count; will pick highest to self-heal; this indicates a bug or corruption generated a bad counter shard

I am not 100% sure whether they are related.",,jblangston@datastax.com,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/12 10:48;slebresne;4976.txt;https://issues.apache.org/jira/secure/attachment/12554666/4976.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,258721,,,Fri Nov 23 09:03:43 UTC 2012,,,,,,,,,,"0|i0l2m7:",121074,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Nov/12 10:48;slebresne;I think that assertion is wrong and has always been (it's definitively possible that diff is called on a tombstone). Attaching patch to fix.

For the second type of errors, this is CASSANDRA-4417 and I'm almost certain this is not related (or rather if it is, I have no clue how).;;;","23/Nov/12 02:34;jbellis;+1;;;","23/Nov/12 09:03;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in Bounds when running word count,CASSANDRA-4975,12616796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,19/Nov/12 20:02,16/Apr/19 09:32,14/Jul/23 05:52,26/Nov/12 23:07,1.2.0 beta 3,,,,,,0,,,,,,,"Just run the word count in examples:

{noformat}
ERROR 20:01:24,693 Exception in thread Thread[Thrift:16,5,main]
java.lang.AssertionError: [DecoratedKey(663380155395974698, 6b6579333137),max(-8100212023555812159)]
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
        at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:34)
        at org.apache.cassandra.thrift.CassandraServer.get_paged_slice(CassandraServer.java:1041)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3478)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice.getResult(Cassandra.java:3466)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/12 18:24;brandon.williams;4975.txt;https://issues.apache.org/jira/secure/attachment/12554891/4975.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,258666,,,Mon Nov 26 23:07:06 UTC 2012,,,,,,,,,,"0|i0l23b:",120989,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"26/Nov/12 18:24;brandon.williams;WordCount is using RP, causing invalid bounds for M3P.  Something seems too fragile here since the job hangs forever after the AE, but for rc1 let's just change the partitioner to M3P.  Patch to do so.;;;","26/Nov/12 22:18;yukim;+1 for patch, but you missed WordCountCounters.java. You need to change that too.;;;","26/Nov/12 23:07;brandon.williams;Oops, thanks.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh renders bytes fields as strings,CASSANDRA-4970,12616535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jbellis,jbellis,17/Nov/12 17:17,16/Apr/19 09:32,14/Jul/23 05:52,17/Nov/12 18:13,1.2.0 beta 3,,,,,,0,,,,,,,Jake reports (CASSANDRA-4968) that this is a regression against 1.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/12 18:00;aleksey;4970.txt;https://issues.apache.org/jira/secure/attachment/12553907/4970.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,258407,,,Sat Nov 17 18:01:23 UTC 2012,,,,,,,,,,"0|i0ksjb:",119440,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"17/Nov/12 18:01;brandon.williams;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set max frame size in CLI to avoid OOM when SSL is enabled,CASSANDRA-4969,12616491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,16/Nov/12 22:12,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/12 22:37,1.2.0 beta 3,,,Legacy/Tools,,,0,,,,,,,"If SSL is enabled on Cassandra but not on the cli, the cli will OOM when connecting to Cassandra because it thinks it's getting a message with a frame size of ~350mb.",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/12 22:13;thobbs;4969-cli-max-frame-size.txt;https://issues.apache.org/jira/secure/attachment/12553848/4969-cli-max-frame-size.txt",,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,258357,,,Fri Nov 16 22:37:46 UTC 2012,,,,,,,,,,"0|i0krlz:",119290,,,,,Low,,,,,,,,,,,,,,,,,"16/Nov/12 22:13;thobbs;Attached patch hardcodes the max frame size to 15 MiB, matching Cassandra's default.;;;","16/Nov/12 22:37;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Descrepancies,CASSANDRA-4968,12616472,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,tjake,tjake,16/Nov/12 21:19,16/Apr/19 09:32,14/Jul/23 05:52,17/Nov/12 17:30,1.2.0 beta 3,,,,,,0,,,,,,,"I upgraded an environment from 1.1.6 to 1.2 beta

We found the following differences between cql3 versions:


The CLUSTERING ORDER BY requires all columns be specified in 1.2 whereas 1.1 defaulted to ASC

{code}
CREATE TABLE table1(
   col1 text,
   classtype ascii,
   kt bigint,
   id uuid,
   PRIMARY KEY (col1,classtype,kt)
) WITH COMPACT STORAGE
AND CLUSTERING ORDER BY(classtype ASC, kt DESC)
{code}

1.1 didn't require classtype in the ORDER BY, 1.2 does.


----------

In docs/cql3.textile there is mention of 'compaction_strategy' CREATE option but the actual option is 'compaction'

----------

cqlsh for some reason renders all bytes fields as strings in 1.2 whereas in 1.1 they were hex.






",,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,258338,,,Sat Nov 17 17:30:39 UTC 2012,,,,,,,,,,"0|i0krhr:",119271,,,,,Low,,,,,,,,,,,,,,,,,"17/Nov/12 17:30;jbellis;ORDER BY change is a bug fix.  leaving classtype implicit implies to SQL users that kt is the primary ordering/clustering which is incorrect.

Ninja-fixed compaction_strategy in 2806dedd9c412d3be6ae6979a86b155497284263.

Created CASSANDRA-4970 to deal with cqlsh bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in cqlsh, alter table with compaction_strategy_class does nothing",CASSANDRA-4965,12616167,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jeromatron,jeromatron,15/Nov/12 00:18,16/Apr/19 09:32,14/Jul/23 05:52,19/Nov/12 10:12,1.2.0 beta 3,,,,,,0,,,,,,,"The following cqlsh code appears to do nothing.
{code}
alter table blah with compaction_strategy_class = ‘LeveledCompactionStrategy’;
{code}

It completes as though it worked but when you describe the table, it's still STCS.",,aleksey,jeromatron,khahn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/12 01:42;aleksey;4965.txt;https://issues.apache.org/jira/secure/attachment/12554105/4965.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,257914,,,Mon Nov 19 08:48:53 UTC 2012,,,,,,,,,,"0|i0kktr:",118191,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Nov/12 01:45;aleksey;It's not a cqlsh issue. It's a CQL2 AlterTableStatement bug. Affects CQL2 1.1 and 1.2. Doesn't affect CQL3 in either C* version.;;;","19/Nov/12 08:48;jbellis;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy 1.0.4 doesn't work on OSX / Java 7,CASSANDRA-4958,12616012,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,coltnz,coltnz,14/Nov/12 03:10,16/Apr/19 09:32,14/Jul/23 05:52,17/May/13 15:30,1.2.6,,,,,,0,,,,,,,"Fixed in 1.0.5-M3 see :

https://github.com/xerial/snappy-java/issues/6

",,cherro,hsaputra,kzadorozhny,mkjellman,pmonfette,rcoli,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/13 14:58;yukim;0001-CASSANDRA-4958-1.2.patch;https://issues.apache.org/jira/secure/attachment/12583662/0001-CASSANDRA-4958-1.2.patch",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,257625,,,Wed Jul 24 14:14:07 UTC 2013,,,,,,,,,,"0|i0k3n3:",115407,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"14/Nov/12 09:21;slebresne;I'm fine with upgrading snappy-java, though is 1.0.5-M3 really released? I doesn't appear in http://code.google.com/p/snappy-java/downloads/list for instance. Does someone know if you're supposed to bake it yourself?;;;","14/Nov/12 12:27;jbellis;It looks like the M stands for Milestone, so it's beta (alpha?) quality.;;;","14/Nov/12 20:28;kzadorozhny;We have to compile M3 ourselves for our cluster on FreeBSD. It runs ok with 1.1.6 and openjdk7.;;;","14/Nov/12 21:40;coltnz;M3 is in maven repo:
http://search.maven.org/#search%7Cga%7C1%7Ca%3A%22snappy-java%22

The git history seems pretty stable https://github.com/xerial/snappy-java/commits/develop but maybe 1.5 isn't too far away.

The workaround for those who use the cassandra-maven-plugin is an explicit dependency:
{noformat} 
                <dependencies>
                    <dependency>
                        <groupId>org.xerial.snappy</groupId>
                        <artifactId>snappy-java</artifactId>
                        <version>1.0.5-M3</version>
                        <type>jar</type>
                    </dependency>
                </dependencies>
{noformat} ;;;","14/Dec/12 10:14;slebresne;I'm not too fan of using a beta version, no matter how stable the git history looks like. So I see two way of moving forward:
# maybe M3 is indeed pretty stable and the author of snappy-java just didn't take the time to do a stable release. But imo only the snappy-java author knows that and we should maybe bug him to see if he would be good doing a stable release or if not, when we can expect one.
# there's apparently a pure java implem of snappy (https://github.com/dain/snappy). It's slightly slower on decompression so I'm not sure we want to have it the default, but we could use it as a fallback when loading the native version fails. Which would also give us a fallback for more exotic platform where the native version is just not available.

I note that I list both possibilities, but I'd lie if I said any of them was on my priority list. So feel free to bug the snappy-java author if you want this issue resolved :);;;","14/Dec/12 14:40;tjake;You can also look at CASSANDRA-5038  I've been using it without issue and it should work fine with Java7.  ;;;","17/May/13 14:40;yukim;Finally snappy-java 1.0.5 is out.

http://search.maven.org/#artifactdetails%7Corg.xerial.snappy%7Csnappy-java%7C1.0.5%7Cbundle

Time to upgrade?;;;","17/May/13 14:52;slebresne;bq. Time to upgrade?

Sure, mind giving it a shot?;;;","17/May/13 14:58;yukim;Here it is, against 1.2 branch.;;;","17/May/13 15:04;slebresne;I'd have trust you going ninja style on that one :). But +1.;;;","17/May/13 15:30;yukim;Committed. :);;;","17/May/13 18:00;mkjellman;[~yukim] best ninja fix, maybe ever! thank you!;;;","06/Jul/13 03:03;pmonfette;I believe this fix (going to snappy 1.0.5) broke Cassandra on CentOS 5 for us. We're using the RPM package.

I upgraded from Cassandra 1.2.5 (which was using snappy 1.0.4) and I couldn't get 1.2.6 to start up properly and I had to rollback to 1.2.5.

Here's the error I was getting when starting Cassandra 1.2.6 after the upgrade.

{code}
ERROR [SSTableBatchOpen:5] 2013-07-05 16:35:22,466 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:5,5,main]
java.lang.RuntimeException: Cannot create CompressionParameters for stored parameters
        at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:99)
        at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:63)
        at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.complete(CompressedSegmentedFile.java:51)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:411)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:201)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:154)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:241)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: SnappyCompressor.create() threw an error: java.lang.NoClassDefFoundError Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.CompressionParameters.createCompressor(CompressionParameters.java:179)
        at org.apache.cassandra.io.compress.CompressionParameters.<init>(CompressionParameters.java:71)
        at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:95)
        ... 12 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.cassandra.io.compress.CompressionParameters.createCompressor(CompressionParameters.java:156)
        ... 14 more
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        ... 19 more
{code}

I did try to implement the following documented fix that seemed related to our issue but it did not work:

http://www.datastax.com/docs/1.0/troubleshooting/index#snappy

Furthermore, I suspect this is not the exact same problem since 1.2.5 was working perfectly fine for us without the above fix. We only did an update of the Cassandra package to 1.2.6, diffed and merged the yaml config file and tried to start it back up, like all the other previous upgrades we did as version 1.2.x came out.

I also fully updated all server packages in case it had something to do with an old library somwehere else or something but it did not work either.

We're running the latest CentOS 5.9, 64 bits, fully updated.

We're running java 1.6.0 from Sun (not OpenJDK)

{code}
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
{code}

Here is the content of my /tmp folder on the server (those get created when I start up Cassandra. The 1.0.5 is the one that got created when I was trying to start up Cassandra 1.2.6. It seems quite small compared to version 1.0.4 (which was created by version 1.2.5).

{code}
-rwxr-xr-x  1 cassandra       cassandra       991112 Jul  5 16:38 snappy-1.0.4.1-libsnappyjava.so
-rwxr-xr-x  1 cassandra       cassandra        48432 Jul  5 16:35 snappy-1.0.5-libsnappyjava.so
{code}

The only thing I haven't tried is to update our java version to a later 1.6.x release or go to 1.7.x or even try OpenJDK as this created some issues with other softwares we had since the server was coming up but queries to the keyspaces were coming back as errors or unknown keyspaces...

Let me know if you need more information, I'll be glad to help out.

Thanks guys !;;;","06/Jul/13 03:47;pmonfette;Me again, I believe I found the issue, it requires a much too recent glibc version than what is available for CentOS 5. Only CentOS 6 seems to have it for now.

{code}
 INFO 03:32:04,175 Not using multi-threaded compaction
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:322)
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:48)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:82)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:81)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:468)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:123)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:211)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:484)
Caused by: java.lang.UnsatisfiedLinkError: /tmp/snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by /tmp/snappy-1.0.5-libsnappyjava.so)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1807)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1703)
        at java.lang.Runtime.load0(Runtime.java:770)
        at java.lang.System.load(System.java:1003)
        at org.xerial.snappy.SnappyNativeLoader.load(SnappyNativeLoader.java:39)
        ... 17 more
 WARN 03:32:04,200 Cannot initialize native Snappy library. Compression on new tables will be disabled.
{code}

Hopefully this will help you fix this issue.

Thanks.;;;","06/Jul/13 04:10;pmonfette;Here's what CentOS 5 seems to provide (very latest update), very close but not 3.4.9 :-)

rpm -q --provides libstdc++
{code}
libstdc++ = 4.1.1-52.el5
libstdc++.so.6()(64bit)
libstdc++.so.6(CXXABI_1.3)(64bit)
libstdc++.so.6(CXXABI_1.3.1)(64bit)
libstdc++.so.6(GLIBCXX_3.4)(64bit)
libstdc++.so.6(GLIBCXX_3.4.1)(64bit)
libstdc++.so.6(GLIBCXX_3.4.2)(64bit)
libstdc++.so.6(GLIBCXX_3.4.3)(64bit)
libstdc++.so.6(GLIBCXX_3.4.4)(64bit)
libstdc++.so.6(GLIBCXX_3.4.5)(64bit)
libstdc++.so.6(GLIBCXX_3.4.6)(64bit)
libstdc++.so.6(GLIBCXX_3.4.7)(64bit)
libstdc++.so.6(GLIBCXX_3.4.8)(64bit)
libstdc++ = 4.1.2-54.el5
{code};;;","06/Jul/13 15:45;jbellis;I imagine you can just drop the old 1.0.4 library back in.;;;","19/Jul/13 03:59;pmonfette;Hello,

is there a fix for this issue for an upcoming release or is support for CentOS/RHEL 5 dropped starting with version 1.2.6 ?

I saw Jonathan's comment but I definitely would prefer not to go this way since we are using the rpm package, I would prefer that everything remains as distributed/packaged.

Should I open up a new bug for this instead of commenting here ?

Thanks.;;;","24/Jul/13 14:14;jbellis;Just to be clear, if we had known 1.0.5 would break RHEL5 we would have waited for 2.0.  But since we didn't find out until 1.2.6 was out in the wild, I think you can consider RHEL5 support dropped in Apache Cassandra, since reverting to the older version would break OS X users.

However, I note that DataStax Enterprise 3.1 (based on 1.2.x) still packages the old Snappy (and does not support OS X).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exclude system_traces from repair,CASSANDRA-4956,12615929,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,brandon.williams,brandon.williams,13/Nov/12 18:04,16/Apr/19 09:32,14/Jul/23 05:52,15/Nov/12 17:01,1.2.0 beta 3,,,,,,0,,,,,,,"When a repair is issued, the system ks is skipped but not system_traces.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/12 19:35;yukim;4956-1.2.0.txt;https://issues.apache.org/jira/secure/attachment/12553533/4956-1.2.0.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,257509,,,Thu Nov 15 17:01:36 UTC 2012,,,,,,,,,,"0|i0k2x3:",115290,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"13/Nov/12 18:57;jbellis;Can we generalize this to ""don't bother doing validation compactions for RF=1?"";;;","14/Nov/12 19:35;yukim;Patch attached for 1.2.0 branch.
Cassandra already skips repair when there is no endpoint to repair with. This patch goes further and makes it skip queueing repair session.
I also did refactoring around various repair methods.;;;","15/Nov/12 10:29;slebresne;+1;;;","15/Nov/12 17:01;yukim;Committed. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some confusion around KEY pseudocolumn from Thrift tables,CASSANDRA-4955,12615924,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jbellis,jbellis,13/Nov/12 17:50,16/Apr/19 09:32,14/Jul/23 05:52,29/Nov/12 19:54,1.2.0 beta 3,,,,,,0,,,,,,,"Inserting into the schema created by cassandra-stress.  cqlsh {{DESCRIBE TABLE}} says

{code}
CREATE TABLE ""Standard1"" (
  ""KEY"" blob PRIMARY KEY,
  ""C0"" blob,
  ""C1"" blob,
  ""C2"" blob,
  ""C3"" blob,
  ""C4"" blob
) WITH COMPACT STORAGE AND
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true';
{code}

but that casing doesn't actually work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""KEY"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
Bad Request: Unknown identifier KEY
{noformat}

lowercase does work:

{noformat}
cqlsh:Keyspace1> insert into ""Standard1"" (""key"", ""C0"", ""C1"", ""C2"", ""C3"", ""C4"") values ('FF', '00', '11', '22', '33', '44');
{noformat}
",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/12 19:37;aleksey;4955.txt;https://issues.apache.org/jira/secure/attachment/12555390/4955.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,257504,,,Thu Nov 29 19:54:31 UTC 2012,,,,,,,,,,"0|i0k2vz:",115285,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"29/Nov/12 19:42;brandon.williams;+1, this fixes the casing problem with the key (but CASSANDRA-4827 gets in the way now);;;","29/Nov/12 19:54;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3 does handle List append or prepend with a ""Prepared"" list",CASSANDRA-4945,12615647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,ardot,ardot,11/Nov/12 16:21,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/12 08:21,1.2.0 beta 3,,,,,,0,,,,,,,"I can successfully update a List using the ""literal"" syntax:

{code}
UPDATE testcollection SET L = [98,99,100] + L WHERE k = 1;
{code}

And I can successfully ""upsert"" a List using the ""Prepared"" syntax:


{code}
UPDATE testcollection SET L = ? WHERE k = 1
{code}

by providing a decoded List<Integer> in the bind values.

But using the ""prepared"" syntax for an prepend like:
{code}
UPDATE testcollection SET L = ? + L WHERE k = 1
{code}
fails with the following message:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:line 1:33 mismatched input '+' expecting K_WHERE)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}

and an append of a ""prepared"" syntax like:
{code}
UPDATE testcollection SET L = L + ? WHERE k = 1
{code}
fails as follows:
{code}
java.sql.SQLSyntaxErrorException: InvalidRequestException(why:invalid operation for non commutative columnfamily testcollection)
	at org.apache.cassandra.cql.jdbc.CassandraPreparedStatement.<init>(CassandraPreparedStatement.java:92)
...
...
{code}




",CQL3 Thrift methods (new),ardot,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/12 11:15;slebresne;4945.txt;https://issues.apache.org/jira/secure/attachment/12553101/4945.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256946,,,Wed Nov 21 08:21:55 UTC 2012,,,,,,,,,,"0|i0j0x3:",109077,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Nov/12 10:47;slebresne;Hum, apparently CASSANDRA-4739 hasn't been generic enough. Attaching patch to generalize the approach to support all of this.;;;","14/Nov/12 12:30;jbellis;Can you review, [~xedin]?;;;","20/Nov/12 21:17;jbellis;LGTM, +1;;;","21/Nov/12 08:21;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate doesn't clear row cache,CASSANDRA-4940,12615547,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jjordan,jjordan,09/Nov/12 20:25,16/Apr/19 09:32,14/Jul/23 05:52,12/Nov/12 14:29,1.2.0 beta 3,,,,,,0,,,,,,,"Truncate doesn't clear the row cache.  select * from <table> which skips the row cache returns no data, but selecting by key does.

cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> truncate temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

cqlsh:temp> select * from temp2;
cqlsh:temp> select v1..v3 from temp2 where k in (3,2,1);
 v1 | v2 | v3
----+----+----
 16 | 17 | 18
 12 | 13 | 14
  8 |  9 | 10

",,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/12 03:52;jbellis;4940.txt;https://issues.apache.org/jira/secure/attachment/12553068/4940.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256835,,,Mon Nov 12 14:29:48 UTC 2012,,,,,,,,,,"0|i0iy1r:",108610,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"12/Nov/12 03:52;jbellis;patch attached.;;;","12/Nov/12 14:00;slebresne;+1 (I've pushed a test in dtest for this);;;","12/Nov/12 14:29;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add debug logging to list filenames processed by o.a.c.db.Directories.migrateFile method,CASSANDRA-4939,12615525,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,jblangston@datastax.com,jblangston@datastax.com,09/Nov/12 17:52,16/Apr/19 09:32,14/Jul/23 05:52,13/Nov/12 01:05,1.2.1,,,,,,0,,,,,,,"Customer is getting the following error when starting Cassandra:

ERROR 20:20:06,635 Exception encountered during startup
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(Unknown Source)
        at org.apache.cassandra.db.Directories.migrateFile(Directories.java:556)
        at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:493)

It looks like this is caused by an file with an unexpected name in one of his keyspace directories. However, because we don't log the name of the file as it is processed, it is hard to tell which file is causing it to choke.

It would be good to add a logger.debug statement at the beginning of the method to list the file currently being processed.",,dbrosius,jblangston@datastax.com,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/12 05:49;dbrosius;4939.txt;https://issues.apache.org/jira/secure/attachment/12553075/4939.txt",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256813,,,Tue Nov 13 01:05:24 UTC 2012,,,,,,,,,,"0|i0isr3:",107752,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"12/Nov/12 04:57;dbrosius;fyi, filename doesn't end with .json and has no - in it;;;","12/Nov/12 05:48;dbrosius;ignore files that don't fit the pattern and log with warning
throw exception with file information for files that should have been migrated.;;;","12/Nov/12 11:17;slebresne;+1;;;","13/Nov/12 01:05;dbrosius;committed as 6677d075e9cadc073633fd84f810b9fc5174db45 to cassandra-1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Less than operator when comparing timeuuids behaves as less than equal.,CASSANDRA-4936,12615408,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,cesar.nataren,cesar.nataren,09/Nov/12 01:13,16/Apr/19 09:32,14/Jul/23 05:52,18/Jan/13 13:02,1.2.1,,,,,,0,,,,,,,"If we define the following column family using CQL3:

CREATE TABLE useractivity (
  user_id int,
  activity_id 'TimeUUIDType',
  data text,
  PRIMARY KEY (user_id, activity_id)
);

Add some values to it.

And then query it like:

SELECT * FROM useractivity WHERE user_id = '3' AND activity_id < '2012-11-07 18:18:22-0800' ORDER BY activity_id DESC LIMIT 1;

the record with timeuuid '2012-11-07 18:18:22-0800' returns in the results.

According to the documentation, on CQL3 the '<' and '>' operators are strict, meaning not inclusive, so this seems to be a bug.","Linux CentOS.

Linux localhost.localdomain 2.6.18-308.16.1.el5 #1 SMP Tue Oct 2 22:01:37 EDT 2012 i686 i686 i386 GNU/Linux",cesar.nataren,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/13 13:30;slebresne;4936.txt;https://issues.apache.org/jira/secure/attachment/12564408/4936.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256686,,,Fri Jan 18 13:02:55 UTC 2013,,,,,,,,,,"0|i0iqen:",107372,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"09/Nov/12 01:58;jbellis;Are you sure the UUIDs do not differ in their non-time-based component?  See CASSANDRA-4284.;;;","09/Nov/12 07:12;slebresne;This reminds me, I think that at the minimum we should provide a cqlsh option to display timeuuid as UUID rather than date. Otherwise it's painful to even check that the actual UUID differs. Though tbh I'm still not sure using dates by default is the best idea given this is a lossy representation and I'm afraid will confuse more people not used to type 1 UUID than anything else. Unless of course we adopt of the the format discussed in CASSANDRA-4284 (alternatively, we could just display the date *and* the uuid in parenthesis, though it'll be verbose). ;;;","09/Nov/12 19:23;cesar.nataren;Jonathan: What's the easiest way to find that out?;;;","10/Nov/12 00:06;thobbs;bq. Are you sure the UUIDs do not differ in their non-time-based component?

(Note: I suggested that Cesar open this ticket.)  It almost certainly is a problem with the non-time-based component.  However, the intention of the user is pretty clear for this query: select anything where the timestamp component is less than the provided one.

I've been handling this specially in pycassa for quite a while.  If the comparator is TimeUUID, and a datetime/timestamp is passed as a slice end, it will create a TimeUUID with special non-timestamp components.  For example, since slice ends are inclusive in the thrift api, when creating the TimeUUID for the slice 'finish', the highest possible TimeUUID with the given timestamp will be used.

We could do something similar with CQL, it would just require passing along information about whether to create the highest or lowest TimeUUID representation for a given datestamp based on the comparison operator that's used.

If details about what non-timestamp components pycassa uses would be useful, let me know.;;;","11/Jan/13 08:53;slebresne;I really think the bug here is that TimeUUIDType.fromString() accepts a date as input. But a date is *not* a valid representation of a timeuuid, and the fromString method does *arbitrarily* pick some 0's for parts of the resulting UUID.

In other words, the SELECT query above should be invalid.

Now don't get me wrong, selecting timeuuid based on dates is useful but that is a slightly different problem. So what I think we should do is:
# refuse dates as valid timeuuid values because they just are not.
# add convenience methods to translate dates to precise timeuuid. For querying we would have 'startOf()' and 'endOf()' (where 'startOf(<date> )' (resp. 'endOf(<date> )') would return the *smallest* (resp. *biggest*) possible timeuuid at time <date> ). And for insertion we could optionally add 'random(<date> )' that would return a random timeuuid at time '<date>' (we could even accept 'now' as syntactic sugar for 'random(now)' if we feel like it).

That would also mean that cqlsh should stop this non-sense of displaying timeuuid like date. Again, I understand the intention of making it more readable but this will confuse generations of CQL3 users. I do am in favor of finding a non confusing way to make it readable for users. In fact one solution could be to handle that on the CQL side and to allow 'SELECT dateOf(x ) FROM ...' that would return a date string from timeuuid x (but now it's clear, you've explicitly asked for a lossy representation of x).

I note that this suggestion pretty much fixes the problem discussed in CASSANDRA-4284 too.

I note that Tyler's solution of basically automatically generating the startOf() and endOf() method under the cover based on whether we've use an inclusive of exclusive operation may appear seductive but I don't think we should do that because:
# if you do that, what about SELECT ... WHERE activity_id = '2012-11-07 18:18:22-0800'. You still have no solution for that and by doing magic under the carpet for < and >, you've in fact blurred what = really does.
# ""it would just require passing along information about whether to create the highest or lowest TimeUUID representation for a given datestamp based on the comparison operator that's used"" <- while this seem simple on principle, this will yield very *very* ugly special cases internally. This is *not* 2 lines of code.
# more generally, this doesn't solve the fact that date *are not* valid representation of timeuuid. For example, I still think the first point mentioned in CASSANDRA-4284 is a bug in its own right.

Allowing dates as valid representation of timeuuid is a bug, let's fix it.
;;;","11/Jan/13 13:29;slebresne;Attaching a patch that implements what my previous comment describes.

A few precisions:
* the patch allows {{startOf('2012-11-07 18:18:22-0800')}} or even {{startOf(12432343423)}} but not {{startOf(? )}}. It's just that it's simpler to not support prepared marker for now. We can add it later, but I'd rather leave it for later. It's not excessively useful anyway since any good library will provide an equivalent to the {{startOf()}} method anyway (and so you can use that client side for prepared statements).
* the patch change the fact that dates are accepted as valid TimeUUID representation because, as arged previously, this is bogus. However, CQL2 has done that bogus thing to, and I'm not sure it's worth fixing there as there might be people relying on that buggy behavior. So the patch maintain the buggy behavior fro CQL2.
* I talks about adding a 'random(date)' method for insertions sake in my previous comment, but thinking about that a bit more, I'm not sure it's a good idea. Namely, the only way to generate a version 1 UUID according to the spec is based on the current time. Generating one from a timestamp is kind of not safe. Now I admit that if you use the timestamp but randomize all other bits, you probably end up with something having virtually no chance of collision, but still, I'm slightly reluctant to do that in Cassandra. I'd rather let people do that client side (and provide a UUID string) if they really want to. So instead the patch only provides a {{now()}} method that generate a new unique timeuuid based on the current time.
* The patch also adds the conversion for select statement I mention in my previous comment. In fact it adds 2 methods {{dateOf()}} and {{unixTimestampOf()}}. This part is kind of optional and I can rip it out if there is objections (I meant to separate in 2 patches but scewed up and got lazy). That being said, I kind of like it and with that I think we can just have cqlsh stop printing timeUUID as dates (which the patch doesn't include however).

Let's not shy away from the fact that this patch kinds of break backward compatibility. I say ""kinds of"" because as I've said, I really think allowing date litterals as timeuuid values is a bug so I really think this patch is a bug fix. And if we get that in 1.2.1, I really don't think they'll be any arm done. I also note that there isn't any way to fix this issue that don't ""break backward compatibility"". So I'm really sorry I didn't got that fix up before 1.2.0, but I still really think we should do it nonetheless.;;;","11/Jan/13 18:19;thobbs;I agree with all of your points.  The one thing I might change are the function names {{startOf()}} and {{endOf()}}.  Since these functions are dealing with dates and times, I think the names suggest they might be altering the time component.  Perhaps {{minTimeUUID()}} and {{maxTimeUUID()}}?

Regarding backwards compatibility: are we carefully tracking changes to CQL by version and documenting them somewhere?  These kinds of changes need to be easily discoverable, even if they are only considered bugfixes.;;;","14/Jan/13 09:43;slebresne;bq. Perhaps minTimeUUID() and maxTimeUUID()?

Agreed, that's probably less confusing. That's trivial to change though, I'll change before committing if it comes to that.

bq. are we carefully tracking changes to CQL by version and documenting them somewhere?

Not yet, but I do intend to do it now that CQL3 is final. It'll probably be listed in http://cassandra.apache.org/doc/cql3/CQL.html. Though if that patch is committed, I'll also mention it clearly in the NEWS file.;;;","16/Jan/13 08:27;jbellis;Tyler, can you review the patch?;;;","16/Jan/13 16:33;thobbs;Yes, I can review it.;;;","17/Jan/13 22:45;thobbs;+1 on the patch

I'd like to include the {{dateOf()}} and {{unixTimestampOf()}} functions; do you want update cqlsh here to not automatically convert timeUUIDs to a date format?;;;","18/Jan/13 13:02;slebresne;Alright, committed, thanks

bq. I'd like to include the dateOf() and unixTimestampOf() functions

I've left them in the commit. I've also renamed startOf/endOf to minTimeUUID/maxTimeUUID as suggested above.

bq. do you want update cqlsh here to not automatically convert timeUUIDs to a date format?

As that was simple, I did that too with the commit. Though if someone more knowledgeable of cqlsh wants to have a look at the commit and make sure I didn't forget something, that'd be appreciated.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
occasional TableTest failure,CASSANDRA-4935,12615348,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,brandon.williams,brandon.williams,08/Nov/12 16:33,16/Apr/19 09:32,14/Jul/23 05:52,09/Nov/12 22:30,1.2.0 beta 3,,,,,,0,,,,,,,"The TableTest unit test fails somewhat rarely:

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.TableTest
    [junit] Tests run: 11, Failures: 1, Errors: 0, Time elapsed: 4.339 sec
    [junit] 
    [junit] Testcase: testGetSliceWithExpiration(org.apache.cassandra.db.TableTest):	FAILED
    [junit] Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] junit.framework.AssertionFailedError: Columns [636f6c31:false:4@1,636f6c32:true:4@1!1,636f6c33:false:4@1,])] is not expected [col1,col2]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:574)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:541)
    [junit] 	at org.apache.cassandra.db.TableTest$5.runMayThrow(TableTest.java:353)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.TableTest.reTest(TableTest.java:56)
    [junit] 	at org.apache.cassandra.db.TableTest.testGetSliceWithExpiration(TableTest.java:362)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/12 21:56;yukim;4935.txt;https://issues.apache.org/jira/secure/attachment/12552897/4935.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255982,,,Fri Nov 09 22:30:40 UTC 2012,,,,,,,,,,"0|i0gec7:",93753,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Nov/12 19:49;yukim;Looks like the test TableTest#testGetSliceWithExpiration is from CASSANDRA-4761.
Since it is using TTL of 1 sec, test result will be different if it takes more than 1 sec before query.

Honestly, I don't understand the purpose of the test. It says ""// tests slicing against data from one row with expiring column in a memtable and then flushed to an sstable"", but it is forcing flush before slicing.;;;","08/Nov/12 20:26;jbellis;It does look like the forceBlockingFlush call should be removed to make the test do what was intended.;;;","09/Nov/12 21:56;yukim;Alright, I removed forceBlockingFlush and set TTL long enough to not get tombstoned so we get consistent test result.;;;","09/Nov/12 22:20;jbellis;+1;;;","09/Nov/12 22:30;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion error in offheap bloom filter,CASSANDRA-4934,12615341,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Nov/12 15:57,16/Apr/19 09:32,14/Jul/23 05:52,09/Nov/12 21:17,1.2.0 beta 3,,,,,,0,,,,,,,"Saw this while running the dtests:

{noformat}
 INFO [CompactionExecutor:2] 2012-11-08 09:35:18,206 CompactionTask.java (line 116) Compacting [SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]
ERROR [CompactionExecutor:2] 2012-11-08 09:35:18,257 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError: Memory was freed
    at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:134)
    at org.apache.cassandra.io.util.Memory.getByte(Memory.java:104)
    at org.apache.cassandra.utils.obs.OffHeapBitSet.get(OffHeapBitSet.java:60)
    at org.apache.cassandra.utils.BloomFilter.isPresent(BloomFilter.java:71)
    at org.apache.cassandra.db.compaction.CompactionController.shouldPurge(CompactionController.java:106)
    at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:64)
    at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
    at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:72)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at com.google.common.collect.Iterators$8.computeNext(Iterators.java:734)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146)
    at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:69)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:179)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/12 19:38;brandon.williams;log.txt;https://issues.apache.org/jira/secure/attachment/12552695/log.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255974,,,Fri Nov 09 21:17:55 UTC 2012,,,,,,,,,,"0|i0g6jz:",92492,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"08/Nov/12 19:35;vijay2win@yahoo.com;Looks like the compaction task is not referencing the SSTR which is understandable, not sure which part of the code is closing more than it is openings.... I can see one, but it is rare... any other error message in the logs (It will speedup the search for bug) :) ? Thanks!;;;","08/Nov/12 19:38;brandon.williams;I didn't see anything else useful, but here's the whole thing.;;;","09/Nov/12 05:41;vijay2win@yahoo.com;From the logs i see the following events: 

at, 2012-11-08 09:35:18,257 compaction failed on 

SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]

at, 2012-11-08 09:35:18,290 Compaction completed sucessfully on

SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-12-Data.db'), 
SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]

only thing which i can think of is to make Memory peer variable volatile but not sure i am looking at right places.;;;","09/Nov/12 16:12;jbellis;I think the problem is that compaction never bothered to mark overlapped sstables (that it uses for tombstone purge checking) as referenced, so those can get free'd out from under an ongoing compaction.  Before OHBS this was not a problem because the contents of those sstables was never referenced, only the bloom filter, which the GC kept around as long as it was needed.  But now we need to manually manage that.

Patchset pushed to http://github.com/jbellis/cassandra/branches/4934;;;","09/Nov/12 18:41;vijay2win@yahoo.com;+1 (ignore my first comment getOverlappingSSTables will return a diffrent set).
Honestly, i did think about this somehow missed the overlapping SST code path earlier :);;;","09/Nov/12 21:17;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in ConfigHelper.java,CASSANDRA-4930,12615269,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,mkjellman,mkjellman,08/Nov/12 02:00,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/12 08:43,1.2.0 beta 3,,,,,,0,,,,,,,"line 136 describing setOutputColumnFamily() is currently ""Set the column family for the input of this job.""

It should read ""Set the column family for the output of this job.""

similar typo on line 148.",,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255874,,,Thu Nov 08 08:43:26 UTC 2012,,,,,,,,,,"0|i0fvi7:",90702,,,,,Low,,,,,,,,,,,,,,,,,"08/Nov/12 08:43;slebresne;Thanks, I've committed the typo fix as commit c016b31.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deal with broken ALTER DROP support in CQL3,CASSANDRA-4929,12615198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Nov/12 17:29,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/12 11:37,1.2.0 beta 3,,,,,,0,,,,,,,"Currently, {{ALTER DROP}} only remove the metadata for the column, making it unavailable, but don't reclaim the data. This is unintuive and CASSANDRA-3919 is opened to fix it. However, that later issue won't make it for 1.2, and I think we should be very careful into shipping 1.2 with the current behavior because 1) it's unintuitive and 2) as unintuitive as it is, we don't want people to start relying on that behavior. So I thing we should do one of:
* remove support for {{ALTER DROP}} until CASSANDRA-3919 reintroduce it. After all, there is no real performance impact in keeping a colum that you don't use and if you really really want to get rid of the metadata, you still have the workaround of trashing the schema and recreating it without that column (obviously not user friendly, but at least it's vaguely possible).
* add a specific syntax for the current behavior of {{ALTER DROP}}, one that clearly imply that the data is not deleted, if we consider that this behavior can be sometimes useful (that is, even after CASSANDRA-3919 is resolved). One such syntax could one of (not sure which one I prefer):
{noformat}
ALTER TABLE foo DROP my_column SCHEMA ONLY
ALTER TABLE foo DROP my_column KEEP DATA
{noformat}

I have a slight preference for solution 2, but honestly because it will it easier to drop a column you've just added but maybe mispelled the name until CASSANDRA-3919. Once CASSANDRA-3919 is in, I'm not sure this will be so useful anymore.
",,jeromatron,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/12 14:18;slebresne;4929.txt;https://issues.apache.org/jira/secure/attachment/12553495/4929.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255796,,,Fri Nov 16 11:37:52 UTC 2012,,,,,,,,,,"0|i0fuyv:",90615,,,,,Low,,,,,,,,,,,,,,,,,"07/Nov/12 18:00;jbellis;Doesn't solution 2 have the problem that we'd be stuck with a ""wart"" on the language that doesn't make sense anymore post-1.2?  (""mispelled a column"" should properly be dealt with by supporting ALTER ... RENAME, for instance.);;;","07/Nov/12 18:14;slebresne;bq. Doesn't solution 2 have the problem that we'd be stuck with a ""wart"" on the language that doesn't make sense anymore post-1.2?

Yes, that's what I meant by ""I'm not sure this will be so useful anymore"". But I don't maybe it does have some narrow usefulness: say you're not sure you want to remove a column for real. You could do a 'DROP ... KEEP DATA'. If it turns out that it was a bad, you can just add it back and you're in your previous state. And if after some testing you're convince that's the correct things to do, you'd add it back and then do a real DROP just afterwards. I mean, that's definitively not an uber important things to have and there is a risk nobody will never used that, but I figured, maybe it's worth making life easier for 1.2 if there is an even remote chance that the feature can have some use from 1.3 onwards (of course I'm only suggesting that because supporting this will be 3 lines of code, and if all come to worst and nobody ever use it when 1.3 is out, then we won't have lost much). But again, I'm just suggesting.

bq. ""mispelled a column"" should properly be dealt with by supporting ALTER ... RENAME

While it would be nice, making that work for column not part of the PRIMARY KEY (the only ones concerned by CASSANDRA-3919 and this ticket) is about as much work as CASSANDRA-3919 (wouldn't be crazy to do it as part of CASSANDRA-3919 though).;;;","08/Nov/12 15:53;jbellis;If we think we could do 3919 for 1.2.1 or .2 then my preferred option would be ""leave DROP out entirely for now."";;;","08/Nov/12 17:32;jeromatron;I think there's a valid use case in wanting to drop metadata for individual column definitions.  That could go into a different form if the drop method is too confusing though.;;;","14/Nov/12 14:24;slebresne;bq. If we think we could do 3919 for 1.2.1 or .2 then my preferred option would be ""leave DROP out entirely for now.""

As said above, I'm fine with that. Worst case scenario, if #3919 proves trickier than we sought and/or if we find some compelling use case for a drop that only drops the metadata, we can add that later. For now, attaching patch to remove the support (the patch simply makes the syntax invalid, I figured there is no reason to remove code internally that we'll need for CASSANDRA-3919).

bq. I think there's a valid use case in wanting to drop metadata for individual column definitions

Which would be?;;;","15/Nov/12 16:19;jbellis;Let's go with ""remove support in 1.2.0 and add a note to NEWS that we expect to re-add it 'right' in 1.2.1"" then.;;;","16/Nov/12 11:37;slebresne;Alright, committed the removal with a note in the NEWS file (and in the CQL3 documentation).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 SelectStatement should not share slice query filters amongst ReadCommand,CASSANDRA-4928,12615193,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Nov/12 17:11,16/Apr/19 09:32,14/Jul/23 05:52,07/Nov/12 18:33,1.2.0 beta 3,,,,,,0,,,,,,,"In 1.2, SliceQueryFilter is stateful and should thus not be shared but SelectStatement is doing some sharing in the case of an IN query. This is the reason why cql_test:TestCQL.limit_multiget_test in the dtests is failing intermittently.

Let's be clear that the fact that SliceQueryFilter is stateful is ugly, but that is a concession made for performance until we can refactor all this more cleanly (which still being efficient). Such refactor being, as far as I can tell, far from trivial.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/12 17:12;slebresne;4928.txt;https://issues.apache.org/jira/secure/attachment/12552494/4928.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255791,,,Wed Nov 07 18:33:50 UTC 2012,,,,,,,,,,"0|i0fux3:",90607,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Nov/12 17:55;jbellis;+1;;;","07/Nov/12 18:33;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure Detector should log or ignore sudden time change to the past,CASSANDRA-4925,12615031,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,06/Nov/12 22:17,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/13 22:30,2.0.2,,,,,,0,,,,,,,"If a machine goes back in time all of a sudden because of a problem, Gossip will insert a negative interArrivalTime. 
This will decrease the mean value and can cause this machine to mark other nodes as down and then mark them up as time passed. 
But we should log such occurrences.",,cburroughs,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"17/Oct/13 21:43;kohlisankalp;trunk-4925-v1.patch;https://issues.apache.org/jira/secure/attachment/12609029/trunk-4925-v1.patch","17/Oct/13 23:26;kohlisankalp;trunk-4925-v2.patch;https://issues.apache.org/jira/secure/attachment/12609057/trunk-4925-v2.patch","08/Oct/13 17:21;kohlisankalp;trunk-4925.patch;https://issues.apache.org/jira/secure/attachment/12607391/trunk-4925.patch",,,,,,,,,,,,,,,3.0,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255583,,,Fri Oct 18 22:30:50 UTC 2013,,,,,,,,,,"0|i0fslz:",90233,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Mar/13 05:46;jbellis;Can we replace this with nanoTime measurements?  (nanoTime is guaranteed to be monotonically increasing, given appropriate OS support.);;;","21/Sep/13 20:11;kohlisankalp;Yes changing to nanotime will be a good improvement. ;;;","21/Sep/13 22:08;kohlisankalp;@Jonathan
Also why is BoundedStatsDeque has all values in double. Why it can't be Long? ;;;","08/Oct/13 02:59;kohlisankalp;Changed Double to Long in BoundedStatsDeque. Changed the unit test for it to take long instead of double.
Converted FD to use nano time. Also modified the test to use nano time. In this test I am comparing the values between passing millis and nano and checking whether the phi is the same for both. ;;;","08/Oct/13 03:07;jbellis;Thanks!

What about this? Looks like we're mixing millis + nanos.

{code}
        if (tLast != 0L)
        {
            interArrivalTime = (value - tLast);
        }
        else
        {
            interArrivalTime = Gossiper.intervalInMillis / 2;
        }
{code}
;;;","08/Oct/13 03:09;brandon.williams;Am I missing where we actually log that a negative value was received?  I'd be fine with a WARN since this should only happen rarely unless your clock is seriously crazy, but either way it's something you definitely want to know.;;;","08/Oct/13 17:21;kohlisankalp;Fixed the Gossip interval and updated the patch
[~brandon.williams]  If I am not wrong, we are now using nano which is a counter and difference of a counter(higher - lower) will always be positive even with overflow.;;;","09/Oct/13 13:28;jbellis;Is this the right patch?  I still see {{Gossiper.intervalInMillis}} being used.;;;","09/Oct/13 17:18;kohlisankalp;Yes now I am doing (Gossiper.intervalInMillis * MILLI_TO_NANO) ;;;","09/Oct/13 17:44;jbellis;Ah, my jira client was confused by re-using the same patch name.  I see it now.

Why double up the ArrivalWindowTest?;;;","10/Oct/13 04:27;kohlisankalp;I wanted to check whether passing  millis and nano gives out the same results. I can remove the millis part if you want. ;;;","17/Oct/13 00:36;kohlisankalp;[~jbellis] Should I make the test single? ;;;","17/Oct/13 08:59;jbellis;I think that would probably make more sense.;;;","17/Oct/13 21:43;kohlisankalp;Changed the test;;;","17/Oct/13 22:11;jbellis;Hmm, I get

patch: **** malformed patch at line 164: diff --git a/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java b/test/unit/org/apache/cassandra/utils/BoundedStatsDequeTest.java
;;;","17/Oct/13 23:26;kohlisankalp;Another patch;;;","18/Oct/13 22:30;jbellis;committed; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy.getRangeSlice sometimes returns incorrect number of columns,CASSANDRA-4919,12614979,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,06/Nov/12 15:55,16/Apr/19 09:32,14/Jul/23 05:52,12/Nov/12 22:00,1.1.8,1.2.0 beta 3,,,,,0,,,,,,,"When deployed on a single node, number of columns is correct.
When deployed on a cluster, total number of returned columns is slightly lower than desired. ",,pkolaczk,slebresne,tpatterson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/12 15:59;pkolaczk;0001-Fix-getRangeSlice-paging-reset-predicate-after-fetch.patch;https://issues.apache.org/jira/secure/attachment/12552302/0001-Fix-getRangeSlice-paging-reset-predicate-after-fetch.patch",,,,,,,,,,,,,,,,,1.0,pkolaczk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255497,,,Tue Nov 27 20:00:27 UTC 2012,,,,,,,,,,"0|i0esc7:",84355,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Nov/12 15:59;pkolaczk;Attaching a patch fixing paged column iteration.;;;","06/Nov/12 16:05;jbellis;Is there a dtest for this?;;;","06/Nov/12 16:58;brandon.williams;There's a wide row test and a range slice test, but not a combination of the two.;;;","12/Nov/12 22:00;jbellis;committed, and create https://github.com/riptano/cassandra-dtest/issues/5 to follow up w/ dtest.;;;","20/Nov/12 08:49;slebresne;I note for the testing part that this only concern getRangeSlice with the isPaging option (i.e. for thrift, get_paged_slice) and that currently CQL never calls that so this is not reproducible with CQL. We may end up using the isPaging thing in CQL with CASSANDRA-4851 however.;;;","27/Nov/12 20:00;tpatterson;Just added wide_slice_test to putget_test.py in the dtests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove CQL3 arbitrary select limit,CASSANDRA-4918,12614919,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,06/Nov/12 08:39,16/Apr/19 09:32,14/Jul/23 05:52,19/Nov/12 16:57,1.2.0 beta 3,,,,,,0,,,,,,,"Let it be clear however that until CASSANDRA-4415 is resolved, it will put us in a situation where it will be easy to write queries that timeout (and potentially OOM the server). That being said, even with the auto-limit it's not too hard to write queries that timeout if you're not at least a bit careful and so far we've always answer that by saying 'you have to be mindful of how much data your query is asking for'. And while I'm all for adding protection against OOMing the server like suggested by Jonathan on CASSANDRA-4304, I think the arbitrary auto-limit is the worst possible solution to this problem.

Note that until CASSANDRA-4415 is resolved I wouldn't be totally opposed to force people to provide a LIMIT to select queries if we're really thing it will avoids lots of surprise, though tbh I do think it would be enough to just continue to be vocal about the fact that 'you have to be mindful of how much data your query is asking for' and its follow-up 'you should use an explicit LIMIT if in doubt about how much data will be returned'.

But I am *strongly opposed* in keeping the current arbitrary limit because it makes very little sense imo, and the little sense it makes will completely vanish once CASSANDRA-4415 is here, and I don't want to break the API and do a CQL4 to be able to remove that limit later.
",,mauzhang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/12 08:43;slebresne;4918.txt;https://issues.apache.org/jira/secure/attachment/12552245/4918.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255431,,,Mon Nov 19 16:57:02 UTC 2012,,,,,,,,,,"0|i0erwn:",84285,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Nov/12 08:43;slebresne;Trivial patch attached;;;","15/Nov/12 17:55;jbellis;How about we remove this server-side, but add it to cqlsh?  (Maybe even cut it down to 1K rows there.)  That would address my worries about OOMing the server adequately.
;;;","15/Nov/12 17:56;jbellis;(that is: if querystring does not contain {{limit}}, cqlsh could add on {{LIMIT 1000}} as a default.);;;","16/Nov/12 11:36;slebresne;I'm clearly fine with that, though while the server-side patch is attached, I'd rather let the cqlsh part to someone that knows the cqlsh code better if possible.;;;","19/Nov/12 10:25;jbellis;+1 then.  Created CASSANDRA-4972 for the cqlsh change.  (Would like to get that into 1.2.0 but it's not critical.);;;","19/Nov/12 16:57;slebresne;Alright, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting Cassandra throws EOF while reading saved cache,CASSANDRA-4916,12614862,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,mkjellman,mkjellman,05/Nov/12 22:28,16/Apr/19 09:32,14/Jul/23 05:52,08/Jan/13 05:21,1.2.2,,,,,,0,,,,,,,"Currently seeing nodes throw an EOF while reading a saved cache on the system schema when starting cassandra

 WARN 14:25:54,896 error reading saved cache /ssd/saved_caches/system-schema_columns-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:278)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:365)
	at org.apache.cassandra.db.Table.initCf(Table.java:334)
	at org.apache.cassandra.db.Table.<init>(Table.java:272)
	at org.apache.cassandra.db.Table.open(Table.java:102)
	at org.apache.cassandra.db.Table.open(Table.java:80)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:320)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:395)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:438)


to reproduce delete all data files, start a cluster, leave cluster up long enough to build a cache. nodetool drain, kill cassandra process. start cassandra process in foreground and note EOF thrown (see above for stack trace)",,dbrosius,drew_kutchar,mauzhang,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5252,CASSANDRA-5253,,,,,CASSANDRA-5252,,,,,,,,"13/Nov/12 06:14;dbrosius;4916.txt;https://issues.apache.org/jira/secure/attachment/12553274/4916.txt","13/Feb/13 02:00;drew_kutchar;data.zip;https://issues.apache.org/jira/secure/attachment/12569138/data.zip",,,,,,,,,,,,,,,,2.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255360,,,Wed Feb 20 05:30:51 UTC 2013,,,,,,,,,,"0|i0eran:",84184,,,,,Low,,,,,,,,,,,,,,,,,"12/Nov/12 23:17;jbellis;Wouldn't be the first time I've seen InputStream.available lie.  But we don't want to write the number of items in the cache at the start of the file (the approach we usually take) because that would require making a copy of the cache's keySet which might be more memory than we can afford.

Suggested workarounds: write some kind of EOF value when we're done instead of just closing the file, and check for that on read.  Alternatively, just catch the EOF and log it at debug; a partially-read cache is harmless.;;;","13/Nov/12 05:58;jbellis;Michael, can you test Dave's patch?;;;","13/Nov/12 06:14;dbrosius;oops fix formatting;;;","13/Nov/12 16:12;mkjellman;[~jbellis] yes, will test now;;;","13/Nov/12 20:59;mkjellman;patch looks good. Ship it!;;;","08/Jan/13 05:21;dbrosius;committed to trunk as 1d641f5111613a5a049042a8723d0dd9ffc29c02;;;","13/Feb/13 01:59;drew_kutchar;I just saw this same exception happen on Cassandra 1.2.1. Was this part of the 1.2.1 release?
I'm on Mac OS X 10.8.2, Oracle JDK 1.7.0_11, using snappy-java 1.0.5-M3 from Maven (not sure if that's the cause).

I'm attaching my data and log directory as data.zip.
;;;","20/Feb/13 02:36;dbrosius;no mistakenly only applied to trunk... It is now applied to the cassandra-1.2 branch;;;","20/Feb/13 05:30;jbellis;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC KEYSPACE <ks> from cqlsh won't show cql3 cfs,CASSANDRA-4913,12614835,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,nickmbailey,nickmbailey,05/Nov/12 19:05,16/Apr/19 09:32,14/Jul/23 05:52,12/Nov/12 22:37,1.2.0,,,,,,0,,,,,,,I'm assuming because we made 'describe_keyspaces' from thrift not return cql3 cfs.,,aleksey,nickmbailey,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4907,CASSANDRA-4924,,,,,,,,,"12/Nov/12 04:30;aleksey;4913.txt;https://issues.apache.org/jira/secure/attachment/12553070/4913.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255329,,,Mon Nov 12 20:49:18 UTC 2012,,,,,,,,,,"0|i0er1z:",84145,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"12/Nov/12 04:31;aleksey;Fixes every issue with DESCRIBE * commands.;;;","12/Nov/12 20:49;brandon.williams;lgtm, +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 doesn't allow static CF definition with compact storage in C* 1.1,CASSANDRA-4910,12614758,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Nov/12 08:31,16/Apr/19 09:32,14/Jul/23 05:52,06/Nov/12 14:21,1.1.7,,,,,,0,,,,,,,"In Cassandra 1.1, the following CQL3 definition:
{noformat}
CREATE TABLE user_profiles (
    user_id text PRIMARY KEY,
    first_name text,
    last_name text,
    year_of_birth int
) WITH COMPACT STORAGE;
{noformat}
yields:
{noformat}
Bad Request: COMPACT STORAGE requires at least one column part of the clustering key, none found
{noformat}

This works fine in 1.2 however.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/12 09:16;slebresne;4910.txt;https://issues.apache.org/jira/secure/attachment/12552078/4910.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255184,,,Tue Nov 06 14:21:56 UTC 2012,,,,,,,,,,"0|i0epx3:",83961,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Nov/12 09:16;slebresne;Simple fix attached (not sure why we've refused that in the first place).;;;","05/Nov/12 16:15;jbellis;+1;;;","06/Nov/12 14:21;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug when composite index is created in a table having collections,CASSANDRA-4909,12614717,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,04/Nov/12 17:44,16/Apr/19 09:32,14/Jul/23 05:52,05/Nov/12 16:18,1.2.0 beta 2,,,,,,0,,,,,,,"CASSANDRA-4511 is open to add proper indexing of collection, but currently indexing doesn't work correctly if we index a value in a table having collection, even if that value is not a collection itself.

We also don't refuse creating index on collections, even though we don't support it. Attaching patch to fix both.",,henrikring,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/12 17:44;slebresne;4909.txt;https://issues.apache.org/jira/secure/attachment/12552029/4909.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255140,,,Mon Nov 05 22:49:01 UTC 2012,,,,,,,,,,"0|i0eplr:",83910,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Nov/12 16:02;henrikring;Working at the ApacheConEU Hackathon. Suggestions for testing the patch.
Below is my understanding of the issue and the solution as well as a suggestion for how to test it.

The patch changes the code so the exception: InvalidRequestException(""Indexes on collections are no yet supported"")
is raised if an attempt is made to create an index on an collection type column.

Creation of indexes on tables that have collection column values is allowed as long as 
the collection columns are NOT part of the index. 

Tests:
In CQL3 create CF with 2 collection type columns ""A1"" and ""A2"" and 2 noncollection type columns ""B1"" and ""B2"".
Create index on A1 or A2 should fail. 
Create an index on B1 or B2 should succeed.
Insert data.
Select on column with index to ensure index is in place.

-- Should succeed
CREATE KEYSPACE TEST_CASSANDRA_4909_KS;

-- Should succeed
USE TEST_CASSANDRA_4909_KS; 

-- Should succeed
CREATE TABLE TEST_CASSANDRA_4909_TLB1
   (A1 set<text>,
    A2 set<text>,
    B1 text,
    B2 text,
    PRIMARY KEY B1);

-- Should fail: InvalidRequestException(""Indexes on collections are no yet supported"")
CREATE INDEX TEST_CASSANDRA_4909_TLB1_INX1 ON TEST_CASSANDRA_4909_TLB1 (A1);

-- Should Succeed:
CREATE INDEX TEST_CASSANDRA_4909_TLB1_INX2 ON TEST_CASSANDRA_4909_TLB1 (B2);

-- Wait for schema agreement.

-- Should succeed:
INSERT INTO TEST_CASSANDRA_4909_TLB1 (A1, A2, B1, B2)
       VALUES({'A1-ROW1-ELM-1', 'A1-ROW1-ELM2'}, {'A2-ROW1-ELM-1', 'A2-ROW1-ELM2'}, 'B1-ROW1', 'B2-ROW1' );

-- Should succeed:
INSERT INTO TEST_CASSANDRA_4909_TLB1 (A1, A2, B1, B2)
       VALUES({'A1-ROW2-ELM-1', 'A1-ROW2-ELM2'}, {'A2-ROW2-ELM-1', 'A2-ROW2-ELM2'}, 'B1-ROW2', 'B2-ROW2' );

-- Should succeed (ensure index is in place):
SELECT * FROM TEST_CASSANDRA_4909_TLB1 WHERE B2 = 'B2-ROW2';

-- Should succeed:
DROP TABLE TEST_CASSANDRA_4909_TLB1;
;;;","05/Nov/12 16:03;jbellis;+1;;;","05/Nov/12 16:18;slebresne;Committed, thanks;;;","05/Nov/12 16:21;slebresne;bq. Working at the ApacheConEU Hackathon

I'm not sure what that means, but let me recall that there is a fair amount of tests here: https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py, including a test for this ticket. It's not necessarily perfect, but in any case it would be nice to avoid duplication of efforts.;;;","05/Nov/12 17:18;henrikring;Just to clarify: Comments above are my understanding of the issue and a suggestion for test. Since you agree I will next try and actually run the test to see if it does as expected. I have not done that yet. I need to get the development environment up and running first. I am new to all of this so it is taking a bit of time to get it all going :-). I'm not a Python developer, so I was hoping someone else with better skills that area could include the test in the automated tests as appropriate. ;;;","05/Nov/12 22:49;henrikring;Now I got it running and ran the test - it passed:

{code:title=CASSANDRA_4909.java|borderStyle=solid}
package org.apache.cassandra;

import org.apache.cassandra.thrift.*;
import org.apache.thrift.protocol.TProtocol;
import org.apache.thrift.transport.TFramedTransport;
import org.apache.thrift.transport.TSocket;
import org.apache.thrift.transport.TTransport;
import org.junit.AfterClass;
import org.junit.BeforeClass;

import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.nio.charset.CharsetDecoder;
import java.nio.charset.CharsetEncoder;
import java.sql.Connection;
import java.util.List;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertNotNull;

public class CASSANDRA_4909 {
    public static final Charset charset = Charset.forName(""UTF-8"");
    public static final CharsetEncoder encoder = charset.newEncoder();
    public static final CharsetDecoder decoder = charset.newDecoder();

    private static final String KEYSPACE_NAME = ""test_cassandra_4909_ks"";
    private static final String TABLE_NAME = ""test_cassandra_4909_tlb1"";
    private static Connection _con = null;
    private static Cassandra.Client _client = null;


    @BeforeClass
    public static void setUp() throws Exception {
        TTransport tr = new TFramedTransport(new TSocket(""localhost"", 9160));
        TProtocol proto = new TBinaryProtocol(tr);
        _client = new Cassandra.Client(proto);
        tr.open();
        _client.set_cql_version(""3.0.0"");
        _client.set_keyspace(""system"");

        // Create Keyspace
        executeCQL_legacy(""CREATE KEYSPACE "" + KEYSPACE_NAME + "" WITH strategy_class = SimpleStrategy AND strategy_options:replication_factor = 1"");

        // Switch default KS
        _client.set_keyspace(KEYSPACE_NAME);
    }

    @AfterClass
    public static void tearDown() throws Exception {
        if (_client != null) {
            executeCQL3(""DROP KEYSPACE "" + KEYSPACE_NAME);
        }
    }

    @org.junit.Test
    public void test1() throws Exception {
        assertNotNull(""Expected a connection to be defined?"", _client);

        executeCQL3(""CREATE TABLE "" + TABLE_NAME + ""\n"" +
                ""(A1 set<text>,\n"" +
                ""A2 set<text>,\n"" +
                ""B1 text PRIMARY KEY,\n"" +
                ""B2 text);"");

        //Should fail: InvalidRequestException(""Indexes on collections are no yet supported"")
        try {
            executeCQL3(""CREATE INDEX "" + TABLE_NAME + ""_INX1 ON "" + TABLE_NAME + "" (A1)"");
        } catch (InvalidRequestException e) {
            assertEquals(""Unexpected message in exception"", ""Indexes on collections are no yet supported"", e.getWhy());
        }

        //Should succeed
        executeCQL3(""CREATE INDEX "" + TABLE_NAME + ""_INX2 ON "" + TABLE_NAME + "" (B2)"");

        executeCQL3(""INSERT INTO "" + TABLE_NAME + "" (A1, A2, B1, B2)\n"" +
                ""VALUES({'A1-ROW1-ELM-1', 'A1-ROW1-ELM2'}, {'A2-ROW1-ELM-1', 'A2-ROW1-ELM2'}, 'B1-ROW1', 'B2-ROW1' );\n"");

        executeCQL3(""INSERT INTO "" + TABLE_NAME + "" (A1, A2, B1, B2)\n"" +
                ""VALUES({'A1-ROW2-ELM-1', 'A1-ROW2-ELM2'}, {'A2-ROW2-ELM-1', 'A2-ROW2-ELM2'}, 'B1-ROW2', 'B2-ROW2' );\n"");

        // The select would fail with InvalidRequestException(why:No indexed columns present in by-columns clause with Equal operator)
        // if the index is not in effect.
        CqlResult r = executeCQL3(""SELECT B1 FROM "" + TABLE_NAME + "" WHERE B2 = 'B2-ROW2';"");
        List<CqlRow> rows = r.getRows();
        CqlRow row = rows.get(0);
        Column c_b1 = row.getColumns().get(0); // B1
        String B1_value = bb2str(c_b1.bufferForValue());
        assertEquals(""Expected other value for B1"", ""B1-ROW2"", B1_value);
        int dummy = 0;
    }

    public static String bb2str(ByteBuffer buffer) {
        String data = """";
        try {
            int old_position = buffer.position();
            data = decoder.decode(buffer).toString();
            buffer.position(old_position);
        } catch (Exception e) {
            e.printStackTrace();
            return """";
        }
        return data;
    }

    private static ByteBuffer str2bb(String str) {
        try {
            return ByteBuffer.wrap(str.getBytes(""UTF-8""));
        } catch (UnsupportedEncodingException e) {
            throw new RuntimeException(""UTF-8 is unavailable?"", e);
        }
    }

    private static CqlResult executeCQL_legacy(String cql) throws Exception {
        return _client.execute_cql_query(str2bb(cql), Compression.NONE);
    }

    private static CqlResult executeCQL3(String cql) throws Exception {
        return _client.execute_cql3_query(str2bb(cql), Compression.NONE, ConsistencyLevel.ALL);
    }
}
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't throw internal exceptions over JMX,CASSANDRA-4893,12614410,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,01/Nov/12 18:33,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/12 23:13,1.2.0 beta 3,,,,,,0,,,,,,,"Similarly to how we don't return internal objects over JMX we shouldn't throw internal exceptions over jmx as well.

The one example I encountered was throwing ConfigurationException for the move() command. We should check the rest of our jmx as well.",,dbrosius,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/12 17:31;yukim;4893-1.2.txt;https://issues.apache.org/jira/secure/attachment/12554198/4893-1.2.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253738,,,Wed Nov 21 23:13:10 UTC 2012,,,,,,,,,,"0|i0e4wv:",80558,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,"16/Nov/12 22:16;yukim;Patch is against 1.2 branch. It removes or cassandra related exceptions or replace them with standard ones.
It may be better to put this into 1.2.0 since it changes method signatures which we don't want to do it in minor versions.;;;","17/Nov/12 22:59;jbellis;I want to get rc1 out really, really badly.  If you can expedite review of this [~nickmbailey] that would be great.  Otherwise, we've waited this long, it won't kill us to push it to 1.3.;;;","18/Nov/12 03:41;dbrosius;doesn't doing

throw new IllegalArgumentException(e);

still exhibit the problem of needing the underlying exception class of 'e' in the classpath of the caller?

it seems the thrown exception needs to lose it's exception history.;;;","19/Nov/12 17:31;yukim;Dave, thanks for review.
Updated patch not to wrap cause and instead use just error message.;;;","20/Nov/12 01:59;dbrosius;Cassandra throws a few custom exceptions that are non-checked. Not sure if any of these places can have these exceptions, but if the were, then we would still have bad exception types going back to jmx. To be paranoid, catching just 'Exception' in these circumstances would be needed. Don't know if this is a real issue tho.

Some of these methods are used as 'real code' outside of jmx, and thus we are weakening the exception model in these spots. Most of these look like client/tool code so i guess it's ok.

nit: not a fan of dangling statements after a try/catch->exit, as in setCompactionStrategyClass, i'd move maybeReloadCompactionStrategy into the try.


otherwise, if the above first issue, isn't an issue... lgtm.;;;","21/Nov/12 23:13;yukim;Committed to 1.2.0 and above with nit fixed.
Thanks for review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no need to keep tombstones in HintsColumnFamily,CASSANDRA-4892,12614400,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,mdennis,mdennis,01/Nov/12 17:13,16/Apr/19 09:32,14/Jul/23 05:52,01/Nov/12 19:19,1.2.0 beta 2,,,,,,0,,,,,,,"Once a hint is delivered, it is removed from the HintsColumnFamily.  Because it is local and would only be deleted after expiration or after a correct delivery, there is no need to keep any tombstones (i.e. gc_grace_seconds should be zero)",,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253724,,,Thu Nov 01 19:19:17 UTC 2012,,,,,,,,,,"0|i0e4tb:",80542,,,,,Low,,,,,,,,,,,,,,,,,"01/Nov/12 19:19;jbellis;Done in d1dd9a10295e408daf9107d4fe4c47dbece75195 for trunk -- this was a regression in the 1.2 hints rewrite, it is already set to zero in earlier releases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't allow prepared marker inside collections,CASSANDRA-4890,12614336,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,01/Nov/12 10:39,16/Apr/19 09:32,14/Jul/23 05:52,01/Nov/12 14:32,1.2.0 beta 2,,,,,,0,,,,,,,"Currently the parser don't disallow preparing queries like (where l is a list<string>):
{noformat}
INSERT INTO test (k, l) VALUES (0, [1, ?, 2])
{noformat}

However, we don't handler it correctly. And in fact we can't really handle it properly currently since we return the name of the prepared column during prepare and here the marker don't correspond to a column (concretely, the code currently return l and list<string> for the name and type of the prepared value, which is incorrect). We also don't handle it during execute, though that last could in theory be fixed with some effort.

But overall I don't think allowing that kind of things is really useful (you can of course prepare the whole collection), so I suggest just refusing it for now.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/12 10:40;slebresne;4890.txt;https://issues.apache.org/jira/secure/attachment/12551694/4890.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253580,,,Thu Nov 01 14:32:44 UTC 2012,,,,,,,,,,"0|i0dvnj:",79058,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"01/Nov/12 13:15;jbellis;+1;;;","01/Nov/12 14:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composites index bug,CASSANDRA-4884,12614187,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,31/Oct/12 14:50,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/12 15:41,1.2.0 beta 2,,,,,,0,,,,,,,Indexes on composite tables is currently broken (due to CASSANDRA-2897). Attaching patch to fix.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/12 14:52;slebresne;4884.txt;https://issues.apache.org/jira/secure/attachment/12551552/4884.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253379,,,Wed Oct 31 15:41:49 UTC 2012,,,,,,,,,,"0|i0dkrr:",77294,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"31/Oct/12 15:35;jbellis;+1;;;","31/Oct/12 15:41;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection don't count columns correctly for CQL3,CASSANDRA-4882,12614100,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,pmcfadin,pmcfadin,30/Oct/12 23:08,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/12 17:50,1.2.0 beta 2,,,,,,0,,,,,,,"Using the same schema defined in https://issues.apache.org/jira/browse/CASSANDRA-4881

select * from video_event;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 | 2012-09-02 18:35:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346634300.0 | 2012-09-02 18:05:00+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346636250.0 | 2012-09-02 18:37:30+0000
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop |    1346634330.0 | 2012-09-02 18:05:30+0000

And this:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;
 videoid_username                           | event | event_timestamp | video_timestamp
--------------------------------------------+-------+-----------------+-----------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start |    1346636100.0 |            null

As you can see, we have some different output based on the select statement. The timestamp in both is formatted as a float or a double. ",,pmcfadin,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/12 14:15;slebresne;4882.txt;https://issues.apache.org/jira/secure/attachment/12551542/4882.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253228,,,Wed Oct 31 17:50:05 UTC 2012,,,,,,,,,,"0|i0dclj:",75970,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Oct/12 23:10;brandon.williams;Formatting appears fixed in cqlsh trunk, but the null is strange.;;;","31/Oct/12 14:15;slebresne;So I'm ignoring the formatting issue since that has been fixed.

Howe the returned null is a legit bug. The problem is that the StorageProxy short read protection hasn't been updated after the changed made by CASSANDRA-3647, namely the fact that SliceQueryFilter now group columns by prefix before counting them.

Attaching a patch that fixes that.;;;","31/Oct/12 17:19;jbellis;LGTM.

Nit: would prefer avoiding allocating unnecessary HashSet in trim when the common case is not to need to trim.;;;","31/Oct/12 17:50;slebresne;Committed with nit fixed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force provided columns in clustering key order in 'CLUSTERING ORDER BY',CASSANDRA-4881,12614094,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,pmcfadin,pmcfadin,30/Oct/12 22:18,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/12 14:33,1.2.0 beta 2,,,,,,0,,,,,,,"Using this table:
CREATE TABLE video_event (
  videoid_username varchar,
  event varchar,
  event_timestamp timestamp,
  video_timestamp timestamp,
  PRIMARY KEY (videoid_username, event, event_timestamp)
)WITH CLUSTERING ORDER BY (event_timestamp DESC);

Inserting these records:

INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:05:00','2012-09-02 18:05:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:05:30','2012-09-02 18:05:30');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','start','2012-09-02 18:35:00','2012-09-02 18:35:00');
INSERT INTO video_event (videoid_username, event, event_timestamp, video_timestamp) 
VALUES ('99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd','stop','2012-09-02 18:37:30','2012-09-02 18:37:30');

Running this select:

select * from video_event where videoid_username = '99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd' limit 1;

I get this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd | start | 2012-09-02 18:05:00+0000 | 2012-09-02 18:05:00+0000

I would expect to see this:

 videoid_username                           | event | event_timestamp          | video_timestamp
--------------------------------------------+-------+--------------------------+--------------------------
 99051fe9-6a9c-46c2-b949-38ef78858dd0:ctodd |  stop | 2012-09-02 18:37:30+0000 | 2012-09-02 18:37:30+0000

where the first record pulled was the sorted record by event_timestamp in reverse order.
",,pmcfadin,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/12 08:39;slebresne;4881.txt;https://issues.apache.org/jira/secure/attachment/12551503/4881.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253222,,,Wed Oct 31 14:33:09 UTC 2012,,,,,,,,,,"0|i0dcjb:",75960,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"31/Oct/12 08:39;slebresne;I think there is indeed one thing that should be improved, but I'm not sure this is what you think :)

Currently, the {{CLUSTERING ORDER BY (event_timestamp DESC)}} is a shorthand for {{CLUSTERING ORDER BY (event ASC, event_timestamp DESC)}}. This isn't very clear however and so I think we should refuse the former and require the latter. Attaching a patch to do that.

Now with the caveat above, the rest work as designed. If you don't specify any ordering for your request (like in the select above), we are free to return what's most convenient and in practice we return row in disk order. But since the disk order will be pretty much the one of the inserts (because 'start' sorts before 'stop', which trumps any sorting for event_timestamp in that example), the result is correct.

To have things sorted by event_timestamp independently of the event, you will have to put the {{event_timestamp}} before {{event}} in the primary key definition.
;;;","31/Oct/12 08:41;slebresne;I've updated the title to reflect what the patch attached actually fixes.;;;","31/Oct/12 13:48;jbellis;LGTM.

Nit: ""Too many columns"" check could be moved out of the for loop for greater clarity.;;;","31/Oct/12 14:33;slebresne;Committed (with nit fixed), thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endless loop flushing+compacting system/schema_keyspaces and system/schema_columnfamilies,CASSANDRA-4880,12614077,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,minaguib,minaguib,30/Oct/12 20:18,16/Apr/19 09:32,14/Jul/23 05:52,24/Jan/14 18:47,1.1.7,1.2.0 beta 3,,,,,2,,,,,,,"After upgrading a node from 1.1.2 to 1.1.6, the startup sequence entered a loop as seen here:

http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

Stopping and starting the node entered the same loop.

Reverting back to 1.1.2 started successfully.","Linux x86_64 3.4.9, sun-jdk 1.6.0_33",cburroughs,cherro,christianmovi,cscetbon,jeromatron,mheffner,minaguib,mishail,tiberiusteng,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4837,,,,,,,,,,,"03/Dec/13 12:12;tiberiusteng;131203-schema-1.txt;https://issues.apache.org/jira/secure/attachment/12616759/131203-schema-1.txt","03/Dec/13 12:12;tiberiusteng;131203-schema-2.txt;https://issues.apache.org/jira/secure/attachment/12616760/131203-schema-2.txt","20/Nov/12 22:30;xedin;CASSANDRA-4880-fix.patch;https://issues.apache.org/jira/secure/attachment/12554418/CASSANDRA-4880-fix.patch","14/Nov/12 22:14;xedin;CASSANDRA-4880.patch;https://issues.apache.org/jira/secure/attachment/12553575/CASSANDRA-4880.patch",,,,,,,,,,,,,,4.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253202,,,Fri Jan 24 14:07:06 UTC 2014,,,,,,,,,,"0|i0dcen:",75939,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"08/Nov/12 18:09;mheffner;We also see this after upgrading a node from 1.1.0 -> 1.1.6 in our ring. We also saw increased flush writes begin continuously on the other 1.1.0 nodes in the ring after the 1.1.6 was started, but not as rapidly as on the 1.1.6 node. There were also periodic HintedHandoffs occurring around the ring after the 1.1.6 node came up.;;;","08/Nov/12 18:34;jbellis;Is schema synchronization confused?  I can't think of any other reason for continuous flush here.;;;","08/Nov/12 19:20;brandon.williams;I'm pretty sure that's what's happening, but it's not clear why.  You can artificially reproduce this by manually injecting a conflict, such as a ""Column family ID mismatch"" though that error isn't present here.;;;","08/Nov/12 21:04;brandon.williams;Bisects to CASSANDRA-4561;;;","08/Nov/12 21:14;xedin;can you try 1.1.6 + CASSANDRA-4837 ?;;;","08/Nov/12 21:42;brandon.williams;No help :(;;;","08/Nov/12 21:45;xedin;[~brandon.williams] Can you attach schema_* sstables that could trigger this behavior to the ticket please (if you have them)?;;;","08/Nov/12 21:50;brandon.williams;Sent to Pavel privately.;;;","09/Nov/12 02:03;xedin;[~mheffner] and [~minaguib] Can you please execute following commands in CLI and post output here - ""use system;"" and ""list schema_keyspaces""?;;;","09/Nov/12 02:37;minaguib;[~xedin] Here's mine (after some very minor name obfuscation):

{code}
[default@system] list schema_keyspaces;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: AdKS
=> (column=durable_writes, value=true, timestamp=1336233003898)
=> (column=name, value=AdKS, timestamp=1336233003898)
=> (column=strategy_class, value=org.apache.cassandra.locator.NetworkTopologyStrategy, timestamp=1336233003898)
=> (column=strategy_options, value={""DCMTL"":""2"",""DCLA"":""2"",""DCLON"":""2""}, timestamp=1336233003898)

1 Row Returned.
Elapsed time: 3 msec(s).
{code}

Tomorrow I'll run a test with cassandra 1.1.6 + CASSANDRA-4837 just to add a second vote to Brandon's test whether it helps or not.;;;","09/Nov/12 03:00;mheffner;This is after we removed the 1.1.6 node, but:

{code}
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: KSName
=> (column=durable_writes, value=true, timestamp=5397847727343)
=> (column=name, value=KSName, timestamp=5397847727343)
=> (column=strategy_class, value=org.apache.cassandra.locator.NetworkTopologyStrategy, timestamp=5397847727343)
=> (column=strategy_options, value={""us-east"":""3""}, timestamp=5397847727343)

1 Row Returned.
Elapsed time: 132 msec(s).
{code};;;","09/Nov/12 05:09;xedin;it looks like it wasn't a good idea to involve timestamps in computing schema version (was super easy to do), I will work on improving things to make a migration from older versions (which System.nanoTime() bug) as smooth as possible.;;;","14/Nov/12 22:14;xedin;I made start timestamp schema timestamp repair account and fix any timestamps that are grater than current timestamp and introduced new version in MessagingVersion as it's the only way to fix infinite loop in schema due to broken nano timestamps that we can't properly fix in <= 1.1.6.

Edit: Also note that once this is committed we would have to increase VERSION number for cassandra-1.2.;;;","14/Nov/12 22:15;xedin;Assigning Brandon as reviewer because he was initially involved.;;;","15/Nov/12 17:24;brandon.williams;Hmm, this unfortunate but I don't see any way around it, either.  +1;;;","16/Nov/12 22:36;xedin;Committed.;;;","20/Nov/12 21:30;brandon.williams;I think something's still wrong in 1.1, concurrent schema changes aren't settling: http://buildbot.datastax.com:8010/builders/cassandra-1.1/builds/567/steps/shell/logs/stdio

Everything works in 1.2.;;;","20/Nov/12 21:35;xedin;Could that be that those two stub nodes running on different versions - one on recent and one on previous? That would explain why they have disagreement. It's tested on rolling restart by CASSANDRA-4837.;;;","20/Nov/12 21:39;brandon.williams;Nope, those tests all run with the same version.;;;","20/Nov/12 21:42;xedin;Interesting... Ok, I will try to figure it out asap.;;;","20/Nov/12 22:30;xedin;We don't need special RM deserialization handler any more, verified that tests are now passing.;;;","21/Nov/12 19:37;brandon.williams;+1;;;","22/Nov/12 00:02;xedin;Committed.;;;","03/Dec/13 12:11;tiberiusteng;I have encountered similar bug after I upgrade one of my 1.2.2 node to 1.2.12.
Was using FreeBSD 8.2 + diablo-jre-1.6.0.07.02_18.

Before I upgrading the node to 1.2.12, I changed JVM to openjdk-7.25.15_2 (in retrospective probably not a good idea ...), saw flipping Memtable flushes, changes JVM back to diablo-jre-1.6.0.07.02_18, and still seeing rapid flushes. Now I've taken the 1.2.12 node offline (but not decommissioned it).

After that I see tens of Memtable flushes on the 1.2.12 per second, while once or twice a second on nodes with 1.2.2.

Attached files are the flipping schema versions.;;;","03/Dec/13 12:12;tiberiusteng;Two versions of schema observed after upgrading one machine in a cluster of 1.2.2 to 1.2.12.;;;jira-users","06/Dec/13 08:11;tiberiusteng;Run the node with 1.2.2 again, and the schema converged. Further upgrading JVM to openjdk-7.25.15_2 and Cassandra to 1.2.12 didn't reproduce the problem. Strange, but at least my cluster now upgrading fine ...;;;","22/Jan/14 01:08;cscetbon;I can easily reproduce it by having one DC with 1.2.2 and another with 1.2.12/1.2.13. see http://pastebin.com/YZKUQLXz
If I grep for only InternalResponseStage logs I get http://pastebin.com/htnXZCiT which always displays same account of ops and serialized/live bytes per column family. Column family system/schema_columns is also concerned by this issue.;;;","24/Jan/14 08:28;cscetbon;When I upgrade one node from 1.2.2 to 1.2.13 for 2h I get the previous messages with a raise of CPU(as it flush and compact continually) on all nodes http://picpaste.com/pics/Screen_Shot_2014-01-24_at_09.18.50-ggcCDVqd.1390551670.png
After that, everything is fine and I can upgrade other nodes without any important raise of cpus load. when I start the upgrade, the more nodes I upgrade at the same time (at the beginning), the higher the cpu load is http://picpaste.com/pics/Screen_Shot_2014-01-23_at_17.45.56-I3fdEQ2T.1390552036.png;;;","24/Jan/14 13:46;brandon.williams;This ticket was closed more than a year ago.  Please open a new ticket if you encounter the issue, because it's probably not related to this one.;;;","24/Jan/14 14:07;cscetbon;Done. see [CASSANDRA-6614|https://issues.apache.org/jira/browse/CASSANDRA-6614];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL help in trunk/doc/cql3/CQL.textile outdated,CASSANDRA-4879,12614076,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,khahn,khahn,30/Oct/12 20:07,16/Apr/19 09:32,14/Jul/23 05:52,06/Nov/12 14:14,1.2.0 beta 3,,,,,,0,documentation,,,,,,"https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile doesn't include the new create keyspace syntax or the collections. Last time, I updated the CQL.textile for Paul Cannon to review. Want me to do it again? 

BNR-like formatting needs to be replaced, right?, because the brackets now have literal meaning. I test-applied this custom formatting to commands and it seems ok: Uppercase means literal (lowercase nonliteral), italics mean optional, the | symbol means OR, ... means repeatable. The ... in italics doesn't strictly explain things like nested [...] does, but it's easier on the eyes and loosely understandable. Any doubt could be erased by examples, I think. ",,khahn,slebresne,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4959,,,,,,,,,,,"05/Nov/12 16:53;urandom;v2-0001-CASSANDRA-4879-update-CQL-doc-for-collections.txt;https://issues.apache.org/jira/secure/attachment/12552125/v2-0001-CASSANDRA-4879-update-CQL-doc-for-collections.txt","05/Nov/12 16:53;urandom;v2-0002-update-CREATE-KEYSPACE-for-map-syntax.txt;https://issues.apache.org/jira/secure/attachment/12552126/v2-0002-update-CREATE-KEYSPACE-for-map-syntax.txt",,,,,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253201,,,Tue Nov 06 14:14:27 UTC 2012,,,,,,,,,,"0|i0dce7:",75937,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"31/Oct/12 14:25;slebresne;That's on my todo list and I'll make sure that's ready for 1.2 release. However:

bq. BNR-like formatting needs to be replaced, right?

I'm not sure about that. The fact that we use '<' and '>' for collection types is not a big deal in the sense that we can quote those characters in the grammar (much like we do when we have other signs like '=', ':', etc...). Since that's a reference documentation, I'd rather have the grammar be as precise as can be (nested [..] do matter) and not rely on things like italics (that don't translate if you say copy-paste the grammar into a text file). Yes, it might be slightly less readable, but that's what the examples are for. Of course the datastax documentation should feel free to use a syntax that is easier on the eye.;;;","05/Nov/12 16:21;urandom;Patch attached to document collection types;;;","05/Nov/12 16:54;urandom;bq. Patch attached to document collection types

And another to fix {{CREATE KEYSPACE}} for map vs. properties;;;","06/Nov/12 14:14;slebresne;Alright, I've committed those patches with a number of additions:
* I've added a few missing collections operation and I've included the collections to the insert/update/delete syntax
* I've also updated the new map syntax for create table statements.
* I've added the new boolean support
* I've removed the consistency level bits (since it's now in the protocol);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No entry in TypesMap for InetAddressType,CASSANDRA-4878,12614024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,30/Oct/12 14:03,16/Apr/19 09:32,14/Jul/23 05:52,30/Oct/12 14:43,1.2.0 beta 2,,,Legacy/CQL,,,0,,,,,,,{{InetAddressType}} was added to {{o.a.c.db.marshal}} and {{JdbcInetAddress}} was added to {{o.a.c.cql.jdbc}} but no bridging entry was made for it in the {{o.a.c.cql.jdbc.TypesMap}} Class.,,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 14:06;ardot;4878-v1.text;https://issues.apache.org/jira/secure/attachment/12551355/4878-v1.text",,,,,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253146,,,Tue Oct 30 14:43:09 UTC 2012,,,,,,,,,,"0|i0dbz3:",75869,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/Oct/12 14:43;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range queries return fewer result after a lot of delete,CASSANDRA-4877,12614009,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,julien_campan,julien_campan,30/Oct/12 09:55,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/12 13:04,1.2.0 beta 3,,,,,,0,,,,,,,"Hi, I'm testing on the trunk version
I'm using : [cqlsh 2.3.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]

My use case is :
I create a table
CREATE TABLE premier (
id int PRIMARY KEY,
value int
) WITH
comment='' AND
caching='KEYS_ONLY' AND
read_repair_chance=0.100000 AND
dclocal_read_repair_chance=0.000000 AND
gc_grace_seconds=864000 AND
replicate_on_write='true' AND
compression={'sstable_compression': 'SnappyCompressor'};

1) I insert 10 000 000 rows (they are like id = 1 and value =1)
2) I delete 2 000 000 rows (i use random method to choose the key value)
3) I do select * from premier ; and my result is 7944 instead of 10 000.
4) if if do select * from premier limit 20000 ; my result is 15839 .

So after a lot of delete, the range operator is not working.",,julien_campan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/12 17:31;slebresne;0001-4877.patch;https://issues.apache.org/jira/secure/attachment/12554370/0001-4877.patch","20/Nov/12 17:31;slebresne;0002-Rename-maxIsColumns-to-countCQL3Rows.patch;https://issues.apache.org/jira/secure/attachment/12554371/0002-Rename-maxIsColumns-to-countCQL3Rows.patch",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253123,,,Wed Nov 21 13:04:45 UTC 2012,,,,,,,,,,"0|i0dbp3:",75824,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"20/Nov/12 17:31;slebresne;Attaching patch to fix this. The problem is that our handling of LIMIT was still not correct, in particular when NamesQueryFilter where used, as delete rows were wrongfully counted.

One problem with that patch is that we may still under-count in a mixed 1.1/1.2 cluster because 1.1 nodes won't know how to count correctly. That's sad, but at the same time changing this in 1.1 would be hard and dangerous and CQL3 is beta in 1.1 after all.

Note that I'm attaching 2 patches. The first one is the bulk of the fix. The second one is mostly a renaming of the 'maxIsColumn' parameters that is used in a number of place to 'countCQL3Rows' because that describe more faithfully what this parameter actually do.;;;","21/Nov/12 12:07;jbellis;+1;;;","21/Nov/12 13:04;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL2 CREATE COLUMNFAMILY checks wrong permission,CASSANDRA-4864,12613595,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,26/Oct/12 00:39,16/Apr/19 09:32,14/Jul/23 05:52,26/Oct/12 01:12,1.1.7,1.2.0 beta 2,,,,,0,,,,,,,"Permission is asked for resource [cassandra, keyspaces, <column family name>] instead of [cassandra, keyspaces, <keyspace name>]",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 00:40;aleksey;4864.txt;https://issues.apache.org/jira/secure/attachment/12550901/4864.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,251153,,,Fri Oct 26 01:12:36 UTC 2012,,,,,,,,,,"0|i0b53r:",62939,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Oct/12 01:12;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh can't describe system tables,CASSANDRA-4863,12613503,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,jbellis,jbellis,25/Oct/12 15:12,16/Apr/19 09:32,14/Jul/23 05:52,26/Oct/12 11:42,1.2.0 beta 2,,,Legacy/Tools,,,0,cqlsh,,,,,,"{noformat}
cqlsh> describe table system_traces.sessions;

Unconfigured column family 'sessions'
{noformat}",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/12 18:00;aleksey;4863.txt;https://issues.apache.org/jira/secure/attachment/12550818/4863.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,251043,,,Fri Oct 26 11:42:24 UTC 2012,,,,,,,,,,"0|i0b3h3:",62675,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"25/Oct/12 17:59;aleksey;This has nothing to do with cql3. Well, not exclusively with cql3. It's about system cql3 tables. The fix is trivial now that we have info on system tables in system.schema_*.
;;;","26/Oct/12 11:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Estimated Row Cache Entry size incorrect (always 24?),CASSANDRA-4860,12613379,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,cburroughs,cburroughs,24/Oct/12 22:11,16/Apr/19 09:32,14/Jul/23 05:52,13/May/13 20:04,1.2.0 beta 3,,,,,,0,qa-resolved,,,,,,"After running for several hours the RowCacheSize was suspicious low (ie 70 something MB)  I used  CASSANDRA-4859 to measure the size and number of entries on a node:

In [3]: 1560504./65021
Out[3]: 24.0

In [4]: 2149464./89561
Out[4]: 24.0

In [6]: 7216096./300785
Out[6]: 23.990877204647838


That's RowCacheSize/RowCacheNumEntires  .  Just to prove I don't have crazy small rows the mean size of the row *keys* in the saved cache is 67 and Compacted row mean size: 355.  No jamm errors in the log

Config notes:
row_cache_provider: ConcurrentLinkedHashCacheProvider
row_cache_size_in_mb: 2048

Version info:
 * C*: 1.1.6
 * centos 2.6.32-220.13.1.el6.x86_64
 * java 6u31 Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)",,alexzar,carlyeks,cburroughs,enigmacurry,hsn,jjordan,omid,rcoli,tjake,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/13 19:51;vijay2win@yahoo.com;0001-4860-v2.patch;https://issues.apache.org/jira/secure/attachment/12575941/0001-4860-v2.patch","30/Mar/13 03:55;vijay2win@yahoo.com;0001-4860-v3.patch;https://issues.apache.org/jira/secure/attachment/12576219/0001-4860-v3.patch","13/Nov/12 04:00;vijay2win@yahoo.com;0001-CASSANDRA-4860-for-11.patch;https://issues.apache.org/jira/secure/attachment/12553261/0001-CASSANDRA-4860-for-11.patch","13/Nov/12 04:00;vijay2win@yahoo.com;0001-CASSANDRA-4860.patch;https://issues.apache.org/jira/secure/attachment/12553262/0001-CASSANDRA-4860.patch","13/May/13 19:37;carlyeks;4860-fix-row-index-entry.patch;https://issues.apache.org/jira/secure/attachment/12582969/4860-fix-row-index-entry.patch","02/Apr/13 16:50;vijay2win@yahoo.com;4860-perf-test.zip;https://issues.apache.org/jira/secure/attachment/12576601/4860-perf-test.zip","07/May/13 16:38;carlyeks;4860-tests.patch;https://issues.apache.org/jira/secure/attachment/12582117/4860-tests.patch","28/Mar/13 04:54;enigmacurry;trunk-4860-revert.patch;https://issues.apache.org/jira/secure/attachment/12575829/trunk-4860-revert.patch",,,,,,,,,,8.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250881,,,Mon May 13 20:04:48 UTC 2013,,,,,,,,,,"0|i0b27z:",62472,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,enigmacurry,,,"13/Nov/12 04:00;vijay2win@yahoo.com;Simple patch to fix the issue....

Looks like the issue is that we do Mesure (instead of MeasureDeep) which will not measure the byte[] attached to the RK.

*-for-11.patch is for 1.1 and *.patch is for 1.2.;;;","13/Nov/12 14:06;jbellis;+1;;;","13/Nov/12 18:48;vijay2win@yahoo.com;Committed Thanks!;;;","28/Mar/13 04:53;enigmacurry;I've been working on a 2.0/1.2 read performance regression analysis ([report is here|http://goo.gl/KHZfL]) and it brought me back to this ticket.

The patch as it was applied in revision 94fa82558f198489e0eefb0c14392607d5d23224 introduces a read performance penalty of about ~20%.

I've tested reverting this patch and applying on top of the latest 2.0 branch and it greatly improves read performance. 

I don't claim to know what this patch really does, so not sure if that's an appropriate action to take or not, but I've attached my patch here regardless (trunk-4860-revert.patch).;;;","28/Mar/13 05:52;vijay2win@yahoo.com;Ryan, so the results has row caching enabled? (For the record this shows up only when caching is enabled and using InHeap RowCache).
20% over head is a lot more than i initially anticipated.

I could think of one alternative: I have to dig my history, but i had a alternative to measureDeep() which was very similar to http://goo.gl/oqD89, but it doesn't cover all the platforms and was lot more complicated so it was never committed. Should we resurrect it?;;;","28/Mar/13 06:31;enigmacurry;Hi [~vijay2win@yahoo.com], I have not tweaked any row cache settings from the default. This is in my cassandra.yaml:

{code}
# Maximum size of the row cache in memory.
# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.
#
# Default value is 0, to disable row caching.
row_cache_size_in_mb: 0
{code}

From the description, it sounds like I should have row caching turned off, and yet this code is still being run as evidenced by the change in performance by reverting your patch. I don't yet have a deep understanding of the features involved here, so if you have any other suggestions for things to test here, please let me know. Thanks!;;;","28/Mar/13 16:46;vijay2win@yahoo.com;Ahaa missed it, it is the key cache which is slowing down the performance.;;;","28/Mar/13 19:51;vijay2win@yahoo.com;Hi Ryan, 
Since you have the environment do you mind testing -v2?
It is not a final patch, I have to verify the accuracy of the estimate though.;;;","28/Mar/13 20:59;enigmacurry;With your v2 patch applied I get an average read rate of 14276. That's much worse actually than the first patch. To make sure something is not amiss, I re-ran the 2.0 baseline and got comparable results as before (22524). The number we're hoping to back to is ~28000.;;;","30/Mar/13 03:55;vijay2win@yahoo.com;Hi Ryan, can you try this one? 

I am really optimistic that this patch should improve the performance without sacrificing the accuracy of measurement of the memory footprint.
Attached patch doesn't use any reflection, Micro benchmark shows a better performance than any other approach.

{code}
Completed warmup!, Number of Iteratoions: 1000000
Using reflection took: 8113
Using 4860-v3 took: 95
Using MemoryMeter meter.measure(key) took: 190
Using MemoryMeter meter.measureDeep(key) took: 982
{code}

Note: We don't have this optimization when we have a range tombstone in KeyCache (coz the code becomes really complex), and while using RowCache.

Let me know if you want me to publish the accuracy test and perf test code.;;;","02/Apr/13 15:46;enigmacurry;This one (v3 patch) is much better. Average read op rate is 26484. That's the best run I've seen on 1.2+ except for the run with the trunk-4860-revert.patch applied which has an average read op rate of 27809 (4.8% faster).;;;","02/Apr/13 16:50;vijay2win@yahoo.com;Ryan, Micro benchmark shows v3 is better, is there any chance you are hitting the same key in the key cache often? The reason for asking is that you might have a smaller key cache since the calculation is more accurate. If yes then i would increase the keycache and try. 

To be clear the Meter.measure() is bug and cannot be used in production and can cause OOM.;;;","11/Apr/13 20:14;enigmacurry;I'm hitting the same key multiple times in the write, but not the read:

stress -F 2000000 -n 20000000 -i 1
stress -n 2000000 -o read -i 1
;;;","12/Apr/13 04:00;vijay2win@yahoo.com;This is my theory:

2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 * 2000000 bytes) will fit in key cache.
2M KV with measureDeep() will take 96M + 48M (48 * 2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key size.

Hence there is a eviction overhead on the keycache which you dont have in Measure.measure().
Give the above if you have the key cache of 300M and re test both v3 should show a better performance.

{code}
Completed warmup!, Number of Iteratoions: 1000000
Using reflection took: 8037
Using 4860-v3 took: 90
Using MemoryMeter meter.measure(key) took: 190
Using MemoryMeter meter.measureDeep(key) took: 1002
Using 4860-v3 RowIndexEntry took: 14
Using MemoryMeter meter.measure(RowIndexEntry(i)) took: 104
Using MemoryMeter meter.measureDeep(RowIndexEntry(i)) took: 459
Size of Meter.measure key: 24
Size of Meter.measure index: 24
Size of Meter.measureDeep key: 48
Size of Meter.measureDeep index: 24
Size of key: 48
Size of index: 24
{code};;;","12/Apr/13 04:39;vijay2win@yahoo.com;Added unit test and pushed to https://github.com/Vijay2win/cassandra/commits/4860-v4, Thanks!;;;","16/Apr/13 16:25;enigmacurry;I increased my key_cache_size_in_mb and did in fact get better results:

{code}
Averages from the middle 80% of values:
interval_op_rate          : 27848
interval_key_rate         : 27848
latency median            : 0.7
latency 95th percentile   : 1.4
latency 99.9th percentile : 22.4
Total operation time      : 00:01:20
{code}

The old measure() way with 300M key cache:
{code}
Running stress : -n 2000000 -o read -i 1
output is hidden while collecting stats...
Averages from the middle 80% of values:
interval_op_rate          : 27877
interval_key_rate         : 27877
latency median            : 0.7
latency 95th percentile   : 1.5
latency 99.9th percentile : 23.4
Total operation time      : 00:01:20
{code}

This is roughly equal to the original messure() method in read performance, I'm happy with it!;;;","16/Apr/13 16:38;jbellis;{quote}
2M KV with Measure.measure() will take 96,000,000 or 96M (2 *24 * 2000000 bytes) will fit in key cache.
2M KV with measureDeep() will take 96M + 48M (48 * 2000000 + 24 * 2000000) where 48 is the index min size and 24 is the key size.
{quote}

Clarifying for my own benefit: Vijay is saying that before the original fix, the key cache underestimated the real entry size by 1/3, so a configured size of 2M would actually allow caching 3M worth of entries.  So to compare performance apples-to-apples, we need to allow the fixed code to use an equivalent size.;;;","16/Apr/13 16:44;jbellis;Suggest standardizing on {{memorySize}} instead of {{size}} in IMeasureableMemory, to avoid confusion with size = number of contained elements.  Otherwise +1.;;;","17/Apr/13 01:44;vijay2win@yahoo.com;Wow that sounds lot better than my cryptic explanation :)

Committed to 1.2 and trunk, Thanks Ryan and Jonathan!;;;","07/May/13 15:02;tjake;I just rolled this patch and am hitting this exception:

{code}
) liveRatio is 7.8818923401518655 (just-counted was 7.8818923401518655).  calculation took 825ms for 78373 columns
ERROR [ReadStage:14] 2013-05-07 11:01:44,555 CassandraDaemon.java (line 175) Exception in thread Thread[ReadStage:14,5,main]
java.lang.AssertionError: Serialized size cannot be more than 2GB/Integer.MAX_VALUE
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:58)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache$1.weightOf(ConcurrentLinkedHashCache.java:54)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher.weightOf(ConcurrentLinkedHashMap.java:1447)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:764)
	at com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap.put(ConcurrentLinkedHashMap.java:743)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:101)
	at org.apache.cassandra.cache.ConcurrentLinkedHashCache.put(ConcurrentLinkedHashCache.java:27)
	at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
	at org.apache.cassandra.io.sstable.SSTableReader.cacheKey(SSTableReader.java:696)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:834)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:717)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:43)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:274)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
	at org.apache.cassandra.db.Table.getRow(Table.java:347)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code};;;","07/May/13 15:09;jbellis;I don't suppose you can turn that into a failing unit test?  It's not obvious to me what could be different about what you're doing, and what we're already doing in the [passing] tests.;;;","07/May/13 15:19;tjake;[~carlyeks] is on the case... ;;;","07/May/13 15:38;tjake;Looking at the code this line looks suspect....

https://github.com/apache/cassandra/blob/da93a1cfe483a1522b2c149d287279a74e43a8a9/src/java/org/apache/cassandra/utils/ObjectSizes.java#L65

If you pass in a mmapped bytebuffer I don't think capacity is the right thing to use...;;;","07/May/13 15:50;jbellis;Agreed, the way we use BB it should be .remaining;;;","07/May/13 16:13;tjake;Also do we even care about direct byte buffers? it's not on heap so we should just skip those?;;;","07/May/13 16:16;jbellis;Don't we make a copy for the row cache entry?  If we don't we probably should.;;;","07/May/13 16:38;carlyeks;Providing a couple of tests which show the broken byte buffer. Don't think that we should be comparing against the meter for these, but since it appears to be compared against that in the other parts, I followed the same specification.;;;","07/May/13 17:27;vijay2win@yahoo.com;We should probably check BB isDirect and skip adding the BB overhead, honestly it was an oversight. 
We should probably just switch to remaining() for the rest to be safe.

{quote}
Don't we make a copy for the row cache entry? If we don't we probably should.
{quote}

Do you want to open a separate ticket on this? Since this happens on CLHM (in-heap cache) i am not sure if it will benefit from bytes copying.;;;","07/May/13 17:50;jbellis;I'm more worried about safety issues after a mapped buffer like Jake's gets unmapped.

Edit: but if he's using 1.2.4 safely then I guess the ""clean out row cache after compaction"" code is working pretty well.;;;","07/May/13 18:08;tjake;I'm not using 1.2.4. We went from 1.2.3 -> 1.2.5

Our workaround is to disable the keycache;;;","07/May/13 19:55;jbellis;I did some code diving but just got more confused.  Here's the KeyKacheKey relevant parts:

{code}
    public final byte[] key;

    public KeyCacheKey(Descriptor desc, ByteBuffer key)
    {
        this.desc = desc;
        this.key = ByteBufferUtil.getArray(key);
        assert this.key != null;
    }
{code}

# we are indeed making a defensive copy
# we don't even have a ByteBuffer involved here, so while {{.capacity}} could sometimes not be what we want, it's not causing the problem here.  (Other times, {{capacity}} *could* be what we want; I don't think there's a one-size-fits-all answer here.  JAMM has an {{omitSharedBufferOverhead}} setting to configure this behavior.) 

I think we need to look elsewhere for our smoking gun.;;;","07/May/13 21:43;tjake;I think the issue is the RowIndexEntry size. If you look at IndexInfo.memorySize() those bytebuffers would have the wrong capacity since we mmap them...

{code}
        public long memorySize()
        {
            long fields = ObjectSizes.getSize(firstName) + ObjectSizes.getSize(lastName) + 8 + 8; 
            return ObjectSizes.getFieldSize(fields);
        }
{code}
;;;","07/May/13 21:44;vijay2win@yahoo.com;{quote}
Providing a couple of tests which show the broken byte buffer. 
{quote}
No it is not broken, MeasureDeep was omitting the shared buffer size... which is kind of wrong... try the following.

{code}
        ByteBuffer bb = ByteBuffer.allocate(1000);
        long objectSize = ObjectSizes.getSize(bb);
        MemoryMeter meter2 = new MemoryMeter();
        long meterSize = meter2.measureDeep(bb);
        Assert.assertEquals(meterSize, objectSize);
{code}

{quote}
Here's the KeyKacheKey relevant parts
{quote}

Agreed! Looks like error is from CLHC, i am still not sure where its broken MappedFileDataInput.readBytes copies anyways.... [~tjake]?;;;","07/May/13 22:03;jbellis;bq. i am still not sure where its broken MappedFileDataInput.readBytes copies anyways

Also a good point.  For reference:

{code}
        // we have to copy the data in case we unreference the underlying sstable.  See CASSANDRA-3179
        ByteBuffer clone = ByteBuffer.allocate(bytes.remaining());
        clone.put(bytes);
        clone.flip();
        return clone;
{code};;;","13/May/13 19:37;carlyeks;The issue is in the following code from RowIndexEntry:208

{code}
long internal = 0;
            for (IndexHelper.IndexInfo idx : columnsIndex)
                internal += idx.memorySize();
            long listSize = ObjectSizes.getFieldSize(ObjectSizes.getArraySize(columnsIndex.size(), internal) + 4);
            return ObjectSizes.getFieldSize(deletionTime.memorySize() + listSize);
{code}

The list size is *not* getArraySize(columnsIndex.size(), internal). That multiplies the number of elements by the size that was just calculated.

It should instead be:
{code}
ObjectSizes.getFieldSize(internal + 4)
{code}

There are no other suspicious usages of getArraySize, the only other ones are from the ObjectSizes file for byte array sizes (those make sense).;;;","13/May/13 20:04;jbellis;Moved to CASSANDRA-5564;;;",,,,,,,,,,,,,,,,,,,,,,,
Debian packaging doesn't do auto-reloading of log4j properties file,CASSANDRA-4855,12613352,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,rbranson,rbranson,24/Oct/12 19:16,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/12 16:11,1.1.7,,,Packaging,,,0,,,,,,,Cassandra isn't starting the log4j auto-reload thread because it requires -Dlog4j.defaultInitOverride=true on initialization. Is there a reason to not do this when installed from the Debian package?,,rbranson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/12 21:06;brandon.williams;4855.txt;https://issues.apache.org/jira/secure/attachment/12550684/4855.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250848,,,Thu Oct 25 16:11:48 UTC 2012,,,,,,,,,,"0|i0b1z3:",62432,,rbranson,,rbranson,Low,,,,,,,,,,,,,,,,,"24/Oct/12 21:06;brandon.williams;I don't see why not.  I guess when we added the reloading to bin/cassandra we neglected to also add it to the init.;;;","25/Oct/12 16:07;rbranson;Looks good to me, +1;;;","25/Oct/12 16:11;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeException when bootstrapping a node without an explicitely set token,CASSANDRA-4850,12613134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,slebresne,slebresne,23/Oct/12 13:21,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/12 04:55,1.2.0 beta 2,,,,,,0,,,,,,,"Trying to boostrap a node for which no initial token has been set result in:
{noformat}
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:603)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:490)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:386)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
{noformat}

This has been broken by CASSANDRA-4416. More specifically, now that we storage the system metadata in the schema on startup, the check
{noformat}
                // if we see schema, we can proceed to the next check directly
                if (!Schema.instance.getVersion().equals(Schema.emptyVersion))
                {
                    logger.debug(""got schema: {}"", Schema.instance.getVersion());
                    break;
                }
{noformat}
in StorageService.joinTokenRing is broken. This result in the node trying to check the Load map to pick a token before any gossip state has been received.

Note sure what is the best fix (an easy would be to always wait RING_DELAY before attempting to pick the token, at least in the case where an initial token isn't set, but that's a big hammer).",,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 19:18;xedin;CASSANDRA-4850.patch;https://issues.apache.org/jira/secure/attachment/12551411/CASSANDRA-4850.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250564,,,Wed Oct 31 04:55:40 UTC 2012,,,,,,,,,,"0|i0azbj:",62000,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Oct/12 04:25;jbellis;IMO we should not include system tables in serializedSchema at all, for version or for sending to other nodes, since they are hardcoded.  (Sending a diff to another node will not have the desired effects.);;;","30/Oct/12 19:18;xedin;schema is now set to ignore system rows when version is calculated and when migrations are to be send to the remote node.;;;","30/Oct/12 19:29;jbellis;+1;;;","31/Oct/12 04:55;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typo in tuning cassandra doc,CASSANDRA-4849,12613020,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,mkjellman,mkjellman,22/Oct/12 18:56,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 19:04,,,,Legacy/Documentation and Website,,,0,,,,,,,"http://www.datastax.com/docs/1.1/operations/tuning#tuning-bloomfilters has a typo

ALTER TABLE addamsFamily WITH bloomfilter_fp_chance = 0.01; should be ALTER TABLE addamsFamily WITH bloom_filter_fp_chance = 0.01;",,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250394,,,Mon Oct 22 19:04:22 UTC 2012,,,,,,,,,,"0|i0ay3r:",61798,,,,,Low,,,,,,,,,,,,,,,,,"22/Oct/12 19:04;jbellis;thanks, emailed docs@datastax.com;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad disk causes death of node despite disk_failure_policy,CASSANDRA-4847,12613006,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kirktrue,kirktrue,kirktrue,22/Oct/12 18:11,16/Apr/19 09:32,14/Jul/23 05:52,13/Dec/12 16:20,1.2.1,,,,,,0,,,,,,,"Steps:

# Create a bad disk via device mapper
# Specify good disk and bad disk is data directory
# Set {{disk_failure_policy}} to {{best_effort}} in cassandra.yaml
# Start node

Expected:

Attempts to create system directories to fail (as expected) on bad disk, and have it added to blacklisted directories.

Actual:

Node start up aborts due to uncaught error:

{noformat}
FSWriteError in /mnt/bad_disk/system_traces/sessions
        at org.apache.cassandra.io.util.FileUtils.createDirectory(FileUtils.java:258)
        at org.apache.cassandra.db.Directories.<init>(Directories.java:104)
        at org.apache.cassandra.db.Directories.create(Directories.java:90)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:404)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:227)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:393)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:436)
Caused by: java.io.IOException: Failed to mkdirs /mnt/bad_disk/system_traces/sessions
        ... 7 more
{noformat}
",,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/12 02:04;kirktrue;trunk-4847.txt;https://issues.apache.org/jira/secure/attachment/12560705/trunk-4847.txt",,,,,,,,,,,,,,,,,1.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250379,,,Thu Dec 13 16:20:29 UTC 2012,,,,,,,,,,"0|i0ay0f:",61783,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Nov/12 04:58;kirktrue;I'd like to know if dying on startup with a bad disk regardless of disk_failure_policy setting is desired behavior. I'd lean toward it being 'no' from a pragmatic standpoint, but would like to know if there's a precedent against _simply_ black-listing the bad disk and moving on. I'd like to fix it from a robustness POV.;;;","13/Nov/12 15:30;jbellis;I do think we should respect the policy wrt {{stop}}.  But I would agree that {{best_effort}} and {{ignore}} should both blacklist and move on here.;;;","13/Dec/12 16:20;jbellis;LGTM, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkLoader throws NPE at start up,CASSANDRA-4846,12612999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,yukim,yukim,22/Oct/12 17:10,16/Apr/19 09:32,14/Jul/23 05:52,23/Oct/12 20:27,1.2.0 beta 2,,,,,,0,bulkloader,,,,,,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 13:43;jbellis;4846.txt;https://issues.apache.org/jira/secure/attachment/12550454/4846.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250372,,,Tue Oct 23 20:27:00 UTC 2012,,,,,,,,,,"0|i0axyf:",61774,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"23/Oct/12 13:43;jbellis;fix attached;;;","23/Oct/12 14:55;yukim;yeah, let's (almost) revert CASSANDRA-4732 with this patch until we find more cleaner way to use SSTR without caches.;;;","23/Oct/12 20:27;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
When upgrading from 1.1.6 to 1.20 change in partitioner causes nodes not to start,CASSANDRA-4843,12612879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,appodictic,appodictic,21/Oct/12 14:15,16/Apr/19 09:32,14/Jul/23 05:52,27/Nov/12 15:36,1.2.0 beta 3,,,,,,0,,,,,,,"ERROR 10:17:20,341 Cannot open /home/edward/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-hf-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner != org.apache.cassandra.dht.Murmur3Partitioner

This is because 1.2 has a new default partitioner, why are we changing the default? Is this wise? The current partitioner has been rock solid for years. 

Should the previously known partition be stored in the schema like the previously know seed nodes, and schema?",,appodictic,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/12 15:31;jbellis;4843.txt;https://issues.apache.org/jira/secure/attachment/12555016/4843.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250189,,,Tue Nov 27 15:36:18 UTC 2012,,,,,,,,,,"0|i0audr:",61195,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"21/Oct/12 14:18;appodictic;The error message is strange as well.

Changing paritioner on a existing cluster can cause data loose, Please verify your partitioner in cassandra.yaml

""Can cause data *loss"" is impossible because Cassandra will not even start. If it did start it would definitely cause data loss.;;;","21/Oct/12 14:20;appodictic;Also the comments say, 

{noformat}
# - RandomPartitioner distributes rows across the cluster evenly by md5.
#   When in doubt, this is the best option.
{noformat}

If this is the best option why is another option chosen as the default?;;;","22/Oct/12 07:38;slebresne;bq. why are we changing the default?

The reason is that md5 is a bit cpu intensive and in some 2ndary index requests (that does a token computation for each key on disk it scans for internal reason that can't be changed easily) this was a bottleneck. The new default is the Murmur3Partitioner that is much cheaper to compute. Besides, for every query we do compute a bunch of token and vnodes will probably not reduce that, so it's a generic improvement.

That being said, I fully agree that the current upgrade experience is pretty harsh (even the NEWS file don't clearly explain the action to take to avoid this error). And since we save the partitonner and don't start if the user change it in the yaml, maybe it's time to change the behavior so that if a partitioner is saved in the system table, we use that (and log a warning if it differs from the yaml configured one).

;;;","22/Oct/12 21:15;jbellis;bq. maybe it's time to change the behavior so that if a partitioner is saved in the system table, we use that

Partitioner is saved per-sstable so we can do this panic doublecheck, but you have to know (or think you know) the partitioner before you can actually open up a system table.  I could be wrong, but I don't think it's a quick fix.

In the meantime, I've updated the comments and error message in 8f3d9b8371fa7c5dea83d45d83ec7fe4911a96c0.;;;","23/Oct/12 06:19;slebresne;bq. but you have to know (or think you know) the partitioner before you can actually open up a system table

I don't think you do since the system table keyspace uses LocalStrategy (and this is hardcoded). That being said, even if we start saving the partitioner in the system table, it's possibly a little late for the upgrade to 1.2. I still think we should do it for 1.1.7 though (but to be clear, I'm all for keeping the panic doublecheck when we open a sstable because that protects again a different problem anyway).;;;","27/Nov/12 15:31;jbellis;We missed the 1.1.7 window so I think it's pretty safe to guess that most 1.2 upgraders won't have any extra safety information we add to 1.1.8.

Patch attached to clarify the exception message. ;;;","27/Nov/12 15:33;slebresne;+1;;;","27/Nov/12 15:36;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DateType in Column MetaData causes server crash,CASSANDRA-4842,12612807,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,devdazed,devdazed,20/Oct/12 01:20,16/Apr/19 09:32,14/Jul/23 05:52,24/Oct/12 18:41,1.1.7,1.2.0,,,,,0,,,,,,,"when creating a column family with column metadata containing a date, there is a server crash that will prevent startup.

To recreate from the cli:
{code}
create keyspace test;
use test;
create column family foo
  with column_type = 'Standard'
  and comparator = 'CompositeType(LongType,DateType)'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and column_metadata = [ 
    { column_name : '1234:1350695443433', validation_class : BooleanType} 
  ];
{code}

Produces this error in the logs:

{code}
ERROR 21:11:18,795 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:194)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:141)
	at org.apache.cassandra.thrift.CassandraServer.system_add_column_family(CassandraServer.java:931)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3410)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family.getResult(Cassandra.java:3398)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 11 more
Caused by: org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
ERROR 21:11:18,795 Exception in thread Thread[MigrationStage:1,5,main]
org.apache.cassandra.db.marshal.MarshalException: unable to coerce '2012-10-19 21' to a  formatted date (long)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:117)
	at org.apache.cassandra.db.marshal.DateType.fromString(DateType.java:85)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.fromString(AbstractCompositeType.java:213)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:257)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1318)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1250)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.text.ParseException: Unable to parse the date: 2012-10-19 21
	at org.apache.commons.lang.time.DateUtils.parseDate(DateUtils.java:285)
	at org.apache.cassandra.db.marshal.DateType.dateStringToTimestamp(DateType.java:113)
	... 14 more
{code}

This error is repeated when attempting to restart the server, and results in the server failing to start.",All,devdazed,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 12:32;slebresne;4842.txt;https://issues.apache.org/jira/secure/attachment/12550448/4842.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250070,,,Wed Oct 24 18:41:32 UTC 2012,,,,,,,,,,"0|i0ao7b:",60194,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Oct/12 16:33;jbellis;verified that this only affects 1.1, not 1.0, probably due to changes in schema handling.;;;","22/Oct/12 18:22;xedin;This is the problem with CompositeType, because DateType would convert input to a human readable format with has *colons* inside and on AbstractCompositeType.fromString it tries to spit composite into parts based on colons as well.;;;","23/Oct/12 12:32;slebresne;Oups, it's indeed time we fix that (it's in fact a problem for any string that have a ':', so not only dates). Attaching a patch that makes the string representation of composites safer. Namely, the patch escapes ':' character in the components to know where to split correctly on fromString. There is a slight complication if a '\' is at the end of one part, but the patch deals with that (and hopefully handle all cases).

Unfortunately, this won't fix the getString for existing data. However since the only time we were using fromString/getString of the comparator is in CFDefinition.to/fromSchema, and since things used to crash right away, this is probably not a big deal. People may have to trash/recreate their schema however if they ran into this problem.;;;","24/Oct/12 18:30;jbellis;+1;;;","24/Oct/12 18:41;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remnants of removed nodes remain after removal,CASSANDRA-4840,12612800,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jeromatron,jeromatron,19/Oct/12 22:28,16/Apr/19 09:32,14/Jul/23 05:52,02/Nov/12 13:14,1.1.7,,,,,,0,,,,,,,"After nodes are removed from the ring and no longer appear in any of the nodes' nodetool ring output, some of the dead nodes show up in the o.a.c.net.FailureDetector SimpleStates metadata.  Also, some of the JMX stats are updating for the removed nodes (ie RecentTimeoutsPerHost and ResponsePendingTasks).",,btoddb,jeromatron,omid,rcoli,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 18:07;brandon.williams;4840.txt;https://issues.apache.org/jira/secure/attachment/12550992/4840.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250060,,,Fri Nov 02 13:14:39 UTC 2012,,,,,,,,,,"0|i0ao3b:",60176,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"19/Oct/12 22:36;brandon.williams;I believe this may be evidence of the issue I've heard reported where nodes are still trying to connect to dead IPs that were removed.  I suspect a message might be getting stuck in OTC and causing this.

As far as the FD goes, we definitely remove it there in Gossiper.removeEndpoint, so something must be adding it back.;;;","22/Oct/12 15:58;jbellis;Does this happen in 1.1 as well?;;;","26/Oct/12 12:46;brandon.williams;Yes, it can happen in 1.1.  Even on restart some kind of message sits in MS for the host that I haven't tracked down yet.  Worth noting that at least the FD portion is a red herring, it's dumping the gossiper's endpoint state map which of course contains hosts with dead state.;;;","26/Oct/12 18:07;brandon.williams;The gossiper has to notify subscribers of joins, even on dead state.  Specifically it needs to notify SS in case some action needs to be taken, however SS in turn needs to notify the gossiper to remove the endpoint if the endpoint is already a non-member.  When removing an endpoint, the gossiper should notify MS to destroy any conn pools and remove the timeout tracking.  Patch to do so.;;;","26/Oct/12 22:15;vijay2win@yahoo.com;Wondering if there might be a race condition since we remove and disconnect should we take a membership lock on the node and do this? (Not sure if this is a real problem);;;","30/Oct/12 20:46;brandon.williams;I'm not sure what you mean, what is a membership lock on the node?;;;","31/Oct/12 08:46;vijay2win@yahoo.com;Membership lock so no one else add's the node back into the map during the remove sequence, it would be rare... +1 other than that.;;;","02/Nov/12 13:14;brandon.williams;The node would need a new generation for that to happen, and if so, it should be processed accordingly.  Since we're backed by NBHM, I don't see a reason to worry about this.  Committed as is.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Appending/Prepending items to list using BATCH,CASSANDRA-4835,12612680,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,krzysztof cieslinski,krzysztof cieslinski,19/Oct/12 14:03,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/12 06:52,1.2.0 beta 2,,,,,,0,,,,,,,"As I know, there is no any guarantee that commands that are inside BATCH block will execute in same order, as they are stored in the BATCH block. But...

I have made two tests:
First appends some items to the empty list, and the second one, prepends items, also to the empty list. Both of them are using UPDATE commands stored in the BATCH block. 

Results of those tests are as follow:
First:
      When appending new items to list, USING commands are executed in the same order as they are stored i BATCH.

Second:
      When prepending new items to list, USING commands are executed in random order.  

So, in other words below code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = list_name + [ '1' ]  
 UPDATE... list_name = list_name + [ '2' ]
 UPDATE... list_name = list_name + [ '3' ] 
APPLY BATCH;{code}

 always results in [ '1', '2', '3' ],
 but this code:
{code:xml}
BEGIN BATCH
 UPDATE... list_name = [ '1' ] + list_name   
 UPDATE... list_name = [ '2' ] + list_name
 UPDATE... list_name = [ '3' ] + list_name
APPLY BATCH;{code}

results in randomly ordered list, like [ '2', '1', '3' ]    (expected result is [ '3', '2', '1' ])

So somehow, when appending items to list, commands from BATCH are executed in order as they are stored, but when prepending, the order is random.",,krzysztof cieslinski,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/12 08:49;slebresne;0001-Fix-prepends-within-same-millis.txt;https://issues.apache.org/jira/secure/attachment/12550605/0001-Fix-prepends-within-same-millis.txt","24/Oct/12 08:49;slebresne;0002-Ensure-same-timestamp-in-batches.txt;https://issues.apache.org/jira/secure/attachment/12550606/0002-Ensure-same-timestamp-in-batches.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249929,,,Thu Oct 25 06:52:58 UTC 2012,,,,,,,,,,"0|i0an87:",60036,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Oct/12 14:10;jbellis;You should think of multiple prepends in a batch as guaranteed to be prepended before any existing data, but not ordered among themselves.  Otherwise we could not parallelize within the batch.  (It's random chance that your appends appear to maintain batch order.)

If you want to retain order you should combine into one update: {{list = [3, 2, 1] + list}}, which will also be more performant.;;;","19/Oct/12 21:15;krzysztof cieslinski;Ok, thanks, but I'm afraid that the fact, that my appends are in same order as in BATCH is not a result of a random chance, due the fact that i did this test using BATCH that contains 5000 update commands. And all of them(these 5000 values in list) are in same order as update commands in BATCH(i have executed this test ~10 times and result was always same). However prepending new items is totally random even for BATCH that contains 10 or less updates.. So this shows that for sure, order of updates execution is different for BATCH with prependings and BATCH with appendings. ;;;","22/Oct/12 15:57;jbellis;You should consider this an implementation quirk (possibly even a bug), not a guarantee.;;;","23/Oct/12 20:18;jbellis;Sylvain points out that we actually do expect update order to be preserved *within the same row*.  Reopening.;;;","24/Oct/12 08:49;slebresne;Alright, this is in fact a legit bug in prepend and is not specific to batches (though it's probably harder to reproduce without them). Basically the logic in prepend to make sure we were always generating a decreasing keys even in the same millisecond was broken. It was working only for the same update, but was broke for successive update in the same millisecond. Patch attached to fix that.

That being said, I do think that people should be very careful in assuming that statements in a batch are applied in order *even within the same row* because that's just not true in general. Batch applies everything ""at the same time"".  So for instance:
{noformat}
BEGIN BATCH
  UPDATE user SET name = 'Goo' WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always (that's not quite true currently, see below) end up setting 'Goo' as the name because the way the reconciliation rules work, the biggest value wins for equal timestamp. Similarly,
{noformat}
BEGIN BATCH
  DELETE FROM user WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always (again, see below) end up with the user deleted because on timestamp ties, tombstone wins.

In other words, there was indeed a bug with prepend, and append/prepend do respect the order in batches within the same partition key because we happen to process the statements of a batch in order and there is no good reason to do otherwise, but I don't think we should make that a guarantee either (as in, it's true now, it could change tomorrow, it's an implementation detail). And so user shouldn't rely on it, and if the order is important, they should combine into one statement.

Now, it is unrelated to lists, but when I said that
{noformat}
BEGIN BATCH
  UPDATE user SET name = 'Goo' WHERE userid = 1;
  UPDATE user SET name = 'Foo' WHERE userid = 1;
APPLY BATCH
{noformat}
will always end up with 'Goo', it's not quite true currently, because batches don't guarantee that all update will use the same timestamp (in other words, the result of the batch above randomly depends of the timing of the operation).  I think that *that* is a guarantee we should provide: that unless the timestamp is user provided, all statement of a batch uses the same timestamp. I'm attaching a second patch that implements that.
;;;","24/Oct/12 20:26;jbellis;+1;;;","25/Oct/12 06:52;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Old-style mapred interface only populates row key for first column when using wide rows,CASSANDRA-4834,12612496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bkempe,bkempe,bkempe,18/Oct/12 21:37,16/Apr/19 09:32,14/Jul/23 05:52,06/Nov/12 12:25,1.1.7,,,,,,0,,,,,,,"When using the ColumnFamilyRecordReader with the old-style Hadoop interface to iterate over wide row columns, the row key is only populated on the first column.
See attached tests.

",,bkempe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/12 21:38;bkempe;TestJob.java;https://issues.apache.org/jira/secure/attachment/12549758/TestJob.java","18/Oct/12 21:38;bkempe;TestJobOldHadoop.java;https://issues.apache.org/jira/secure/attachment/12549759/TestJobOldHadoop.java","05/Nov/12 16:01;bkempe;cassandra-1.1-CASSANDRA-4834.txt;https://issues.apache.org/jira/secure/attachment/12552117/cassandra-1.1-CASSANDRA-4834.txt","25/Oct/12 23:50;bkempe;trunk-CASSANDRA-4834.txt;https://issues.apache.org/jira/secure/attachment/12550891/trunk-CASSANDRA-4834.txt",,,,,,,,,,,,,,4.0,bkempe,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249675,,,Tue Nov 06 12:25:30 UTC 2012,,,,,,,,,,"0|i0agbr:",58918,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Oct/12 21:38;bkempe;TestJob is using the new mapred interface and produces the correct output.
TestJobOldHadoop does not populate the row key for columns after the first.;;;","18/Oct/12 21:40;bkempe;patch;;;","25/Oct/12 23:50;bkempe;lastColumn also needs a duplicated ByteBuffer.
Otherwise, if the column name is consumed in the map/reduce code, the expression in
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java#L458
will evaluate to false and the CFRR can get into an infinite loop.;;;","02/Nov/12 14:32;brandon.williams;[~bkempe] can you rebase to 1.1?  I don't see any reason to not include this there.  Looks good otherwise, thanks!;;;","05/Nov/12 16:01;bkempe;added patch for cassandra-1.1 branch;;;","06/Nov/12 12:25;brandon.williams;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_count with 'count' param between 1024 and ~actual column count fails,CASSANDRA-4833,12612449,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,thobbs,thobbs,18/Oct/12 16:56,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 18:56,1.1.7,1.2.0 beta 2,,,,,0,,,,,,,"If you run get_count() with the 'count' param of the SliceRange set to a number between 1024 and (approximately) the actual number of columns in the row, something seems to silently fail internally, resulting in a client side timeout.  Using a 'count' param outside of this range (lower or much higher) works just fine.

This seems to affect all of 1.1 and 1.2.0-beta1, but not 1.0.",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/12 16:08;yukim;4833-1.1-v2.txt;https://issues.apache.org/jira/secure/attachment/12550308/4833-1.1-v2.txt","19/Oct/12 17:05;yukim;4833-1.1.txt;https://issues.apache.org/jira/secure/attachment/12550008/4833-1.1.txt","18/Oct/12 16:58;thobbs;4833-get-count-repro.py;https://issues.apache.org/jira/secure/attachment/12549708/4833-get-count-repro.py",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249596,,,Thu Jan 10 17:50:23 UTC 2013,,,,,,,,,,"0|i0aft3:",58834,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"18/Oct/12 16:58;thobbs;Attached script reproduces the issue with pycassa.;;;","18/Oct/12 20:36;yukim;get_count runs into infinite loop when requesting with count param around a multiple of page size(1024).
Patch attached with unit test.;;;","18/Oct/12 21:06;thobbs;I tested out the patch, and although the infinite loop isn't hit, the resulting count numbers are off.  For example, the repro script has 3050 columns, and when run produces these counts (manually edited for clarity):

{noformat}
specified count=1024: 1024 expected, got 1024
specified count=2^31: 3050 expected, got 2047
specified count=4000: 3050 expected, got 2047
specified count=3051: 3050 expected, got 2047
specified count=1025: 1025 expected, got 1024
{noformat};;;","19/Oct/12 17:05;yukim;You are right.
New version attached. I also modified test to match yours.

get_count pages when requesting count more than page size (determined by average column size but max at 1024). Paging starts with the last column of previously fetched page, so newly fetched page may contains one overlapped column.
When page size is 1024, and we have more than 1024 columns in a row, counting with limit of 1025 columns always fails because we fetch 1 (1025 - 1024 page size) column on 2nd page and it contains only already fetched column. Same thing can happen around the actual number of columns in a row.

Attached patch modified so that paging will fetch at least two columns.;;;","19/Oct/12 23:43;thobbs;The latest patch fixes the issue and passes all of the pycassa tests.

One comment on this conditional:
{code}
    if (requestedCount == 0 || columns.size() < predicate.slice_range.count)
        break;
{code}

Since you're no longer decrementing requestedCount, the first half of the disjunction isn't needed.  If the user actually set a requestedCount of 0, the first column slice would be empty, so we wouldn't get this far.

Other than that, I'm +1 on the changes;;;","22/Oct/12 16:08;yukim;Thanks for review, Tyler.

What we want here is to fetch last page with remainder, not whole page size. So we still need requestedCount -= newColumns.

Attaching v2 for this.;;;","22/Oct/12 16:28;thobbs;The patch needs to be rebased, but I'm +1 on the code changes;;;","22/Oct/12 18:56;yukim;Committed, thanks!;;;","10/Jan/13 17:50;jbellis;Updating affects-version to 1.1.0 since ""this seems to affect all of 1.1,"" and since the fix caused a regression with TTL data (CASSANDRA-5099).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: keys must not be empty,CASSANDRA-4832,12612366,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mithrandi,mithrandi,mithrandi,18/Oct/12 05:15,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 16:50,1.1.7,,,,,,0,indexing,,,,,,"I'm getting errors like this logged:

 INFO 07:08:32,104 Compacting [SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-114-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-113-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-110-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-108-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-106-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hd-107-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-112-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-109-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Fusion/quoteinfo/Fusion-quoteinfo.quoteinfo_search_value_idx-hf-111-Data.db')]
ERROR 07:08:32,108 Exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError: Keys must not be empty
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:154)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:154)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

I'm not really sure when this started happening; they tend to be logged during a repair but I can't reproduce the error 100% reliably.",Debian 6.0.5,cherro,christianmovi,e1zorro,mithrandi,omid,scottfines,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4991,,,,,,,,,,,"20/Oct/12 22:35;cherro;FlushWriterKeyAssertionBlock.txt;https://issues.apache.org/jira/secure/attachment/12550155/FlushWriterKeyAssertionBlock.txt",,,,,,,,,,,,,,,,,1.0,mithrandi,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249477,,,Sun Nov 18 09:59:09 UTC 2012,,,,,,,,,,"0|i0aeqn:",58661,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Oct/12 05:31;mithrandi;Actually, it seems like this gets logged any time Cassandra tries to flush this column family now.;;;","18/Oct/12 06:55;mithrandi;After some further investigation, it looks like this began after I upgraded from 1.1.2 to 1.1.6. The assertion seems to have been introduced as part of some debugging in CASSANDRA-4687. I'm not familiar with the code at all, but it seems to me that perhaps the assertion is bogus? If I have a secondary index on a particular column, and that column has an empty value in some row, would this not result in an empty key in the secondary index column family?;;;","19/Oct/12 00:08;mithrandi;I modified my application to avoid inserting the empty column in question (which was a bug anyway; if that particular column was empty, the application should not have been doing an insert at all) and the problem has gone away, so it seems like my hypothesis is at least partially correct. I'm going to attempt to reproduce this in a more controlled environment now that my immediate issue in production has been resolved.;;;","19/Oct/12 22:45;jbellis;you're right that the assert is bogus.  removed it in 72dcc298d335721c053444249c157e9a6431ebea.;;;","20/Oct/12 22:35;cherro;Came across this investigating an apparent deadlock in Schema Migrations.

If this assertion fails on the flushWriter executor, it blocks indefinitely. Anything upstream locking-wise gets stuck also. This was on 1.1.6.

Log output below, thread dump attached.

ERROR [FlushWriter:3] 2012-10-19 22:27:56,948 org.apache.cassandra.service.AbstractCassandraDaemon Exception in thread Thread[FlushWriter:3,5,main]
java.lang.AssertionError: Keys must not be empty
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:176)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:295)
        at org.apache.cassandra.db.Memtable.access$600(Memtable.java:48)
        at org.apache.cassandra.db.Memtable$5.runMayThrow(Memtable.java:316)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

;;;","22/Oct/12 06:44;e1zorro;I've run into the same symptoms as Chris, but while attempting to migrate data between a 1.1.1 cluster and 1.1.6.

Whilst using sstableloader to import the data, nodes would hit certain column indexes and stop processing further requests once the assertion was thrown - restarting the node would almost immediately throw the assertion again, and the node would just fail to rejoin the ring.

tpstats would show active flushwriter tasks but no further node activity.

We had to work around the issue by:

1. Removing all indexes from our target cluster schema
2. Importing the data via sstableloader
3. Scanning through relevant column families and inserting data into any empty indexed columns
4. Re-applying the indexes to the target cluster schema

Only then was the migration successful.

I'll also note that attempting to apply an index to a column which has null data will also throw the cluster out of sync as the nodes which throw the assertion fail to migrate their schemas properly

 INFO [Creating index: Transactions.TransactionsCountryCode] 2012-10-21 07:45:25,978 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsCountryCode@1802367190(38862/169512 serialized/live bytes, 762 ops)
 INFO [Creating index: Transactions.TransactionsStatus] 2012-10-21 07:45:25,980 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsStatus@673679943(38862/125966 serialized/live bytes, 762 ops)
 INFO [FlushWriter:1] 2012-10-21 07:45:25,987 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsCountryCode@1802367190(38862/169512 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,004 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsStatus@673679943(38862/125966 serialized/live bytes, 762 ops)
ERROR [FlushWriter:1] 2012-10-21 07:45:26,004 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError: Keys must not be empty
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:176)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:295)
	at org.apache.cassandra.db.Memtable.access$600(Memtable.java:48)
	at org.apache.cassandra.db.Memtable$5.runMayThrow(Memtable.java:316)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,049 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/xxxxxx/Transactions/xxxxxx-Transactions.TransactionsStatus-hf-2-Data.db (35259 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0)
 INFO [Creating index: Transactions.TransactionsLastUpdateDate] 2012-10-21 07:45:26,313 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Transactions.TransactionsLastUpdateDate@1912098049(38862/240277 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,314 Memtable.java (line 264) Writing Memtable-Transactions.TransactionsLastUpdateDate@1912098049(38862/240277 serialized/live bytes, 762 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:26,743 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/xxxxxx/Transactions/xxxxxx-Transactions.TransactionsLastUpdateDate-hf-2-Data.db (37024 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0)
 INFO [main] 2012-10-21 07:45:27,052 CommitLogReplayer.java (line 272) Finished reading /var/lib/cassandra/commitlog/CommitLog-1350768744805.log
 INFO [main] 2012-10-21 07:45:27,054 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-Versions@1851630436(83/103 serialized/live bytes, 3 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:27,054 Memtable.java (line 264) Writing Memtable-Versions@1851630436(83/103 serialized/live bytes, 3 ops)
 INFO [FlushWriter:2] 2012-10-21 07:45:27,061 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/system/Versions/system-Versions-hf-1-Data.db (247 bytes) for commitlog position ReplayPosition(segmentId=1350805525722, position=0);;;","18/Nov/12 07:17;scottfines;Is there an actual patch for this, or a Workaround? We see this same AssertionError when attempting to start our nodes; restarting the node usually works correctly, however.;;;","18/Nov/12 09:59;jbellis;The patch to fix this is git commit 72dcc298d335721c053444249c157e9a6431ebea.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcDate.compose is not null safe,CASSANDRA-4830,12612363,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,skuppa,skuppa,skuppa,18/Oct/12 05:03,16/Apr/19 09:32,14/Jul/23 05:52,06/Dec/12 19:23,1.1.7,,,,,,0,,,,,,,"I am using the cassandra-jdbc for CQL.  I have a table with timestamp column.  When timestamp column is null it throws, IndexOutOfBoundsException exception since JdbcDate.compose calls the new Date(ByteBufferUtil.toLong(value)).  The ByteBufferUtil.toLong(bytes) throws exception the exception since position and limit pointers are same (similar to null).  This has to be handled gracefully in the JdbcDate.compose method instead of throwing exception.  I would like to see implementation something like,

    public Date compose(ByteBuffer bytes)
    {
        if(bytes.limit() - bytes.position() > 0) 
        {
            return new Date(ByteBufferUtil.toLong(bytes));
        } 
      
        return  null;
    }

BTW, this matches exactly reverse with decompose method.  Logically it supposed to be implemented in the first place ;)

",Any,skuppa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,skuppa,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249473,,,Thu Oct 18 22:44:18 UTC 2012,,,,,,,,,,"0|i0aepr:",58657,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Oct/12 22:44;jbellis;done in 9d7ba39cbb6f93759f654c7df1771b52354dec36, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default cache provider does not match default yaml,CASSANDRA-4828,12612327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,17/Oct/12 22:18,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/12 01:49,1.1.7,,,,,,0,,,,,,,"The yaml indicates SerializingCacheProvider is the default, however the default when not present in the yaml is actually ConcurrentLinkedHashCacheProvider.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/12 22:22;brandon.williams;4828.txt;https://issues.apache.org/jira/secure/attachment/12549592/4828.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249423,,,Thu Oct 18 01:49:55 UTC 2012,,,,,,,,,,"0|i0a85z:",57596,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Oct/12 01:42;jbellis;+1;;;","18/Oct/12 01:49;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh --cql3 unable to describe CF created with cli,CASSANDRA-4827,12612325,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jbellis,jbellis,17/Oct/12 21:55,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/12 15:07,1.2.0 rc1,,,Legacy/Tools,,,0,cql3,,,,,,"created CF with cli:

{noformat}
create column family playlists
with key_validation_class = UUIDType
 and comparator = 'CompositeType(UTF8Type, UTF8Type, UTF8Type)'
 and default_validation_class = UUIDType;
{noformat}

Then get this error with cqlsh:

{noformat}
cqlsh:music> describe table playlists;

/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/Users/jonathan/projects/cassandra/git-trunk/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)'
CREATE TABLE playlists (
  ""KEY"" uuid PRIMARY KEY
)
{noformat}",,aleksey,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/12 15:40;aleksey;4827-1.2.txt;https://issues.apache.org/jira/secure/attachment/12560091/4827-1.2.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249421,,,Mon Dec 10 15:07:24 UTC 2012,,,,,,,,,,"0|i0a85j:",57594,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Oct/12 21:59;jbellis;Have only tested in 1.1 since CASSANDRA-4823 is not yet done for trunk.

Expected result would be

{code}
CREATE TABLE playlists (
  key uuid,
  column1 text,
  column2 text,
  column3 text,
  column4 uuid
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE
{code}

(See column name ""forging"" in CFDefinition.);;;","19/Oct/12 20:26;aleksey;cql3 on trunk:

cqlsh:music> describe table playlists;

/Users/aleksey/Repos/ASF/cassandra/bin/../pylib/cqlshlib/cql3handling.py:1505: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. Dynamic storage CF does not have UTF8Type added to comparator;;;","31/Oct/12 21:02;mkjellman;similar issue when trying to use COPY in cqlsh3. cf was created with cli

/opt/cassandra/apache-cassandra-1.1.6/bin/../pylib/cqlshlib/cql3handling.py:771: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected composite key CF to have column aliases, but found none
/opt/cassandra/apache-cassandra-1.1.6/bin/../pylib/cqlshlib/cql3handling.py:794: UnexpectedTableStructure: Unexpected table structure; may not translate correctly to CQL. expected [u'KEY'] length to be 3, but it's 1. comparator='org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.IntegerType)';;;","09/Dec/12 15:41;aleksey;4827-1.2 eliminates most if not all DESCRIBE issues in 1.2, no matter what kind of table we are dealing with.;;;","09/Dec/12 17:25;aleksey;Oh, and this only fixes DESCRIBE in CQL3 mode. Didn't bother with CQL2.;;;","10/Dec/12 14:55;jbellis;+1;;;","10/Dec/12 15:07;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subcolumn slice ends not respected,CASSANDRA-4826,12612301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,thobbs,thobbs,17/Oct/12 20:17,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 07:22,1.2.0 beta 2,,,,,,0,,,,,,,"When performing {{get_slice()}} on a super column family with the {{supercolumn}} argument set as well as a slice range (meaning you're trying to fetch a slice of subcolumn from a particular supercolumn), the slice ends don't seem to be respected.",,slebresne,thobbs,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/12 19:44;vijay2win@yahoo.com;0001-CASSANDRA-4826.patch;https://issues.apache.org/jira/secure/attachment/12550064/0001-CASSANDRA-4826.patch","17/Oct/12 20:19;thobbs;4826-repro.py;https://issues.apache.org/jira/secure/attachment/12549568/4826-repro.py",,,,,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249388,,,Mon Oct 22 07:22:46 UTC 2012,,,,,,,,,,"0|i0a7b3:",57457,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"17/Oct/12 20:19;thobbs;The attached script reproduces using pycassa.  The assertions fail when running against 1.2.0-beta1 or trunk, but pass when running against 1.0 or 1.1.

The pycassa test suite covers these areas a bit more thoroughly, which may be useful to check a patch against.;;;","18/Oct/12 20:07;jbellis;Could you have a look at this, Vijay?;;;","18/Oct/12 20:12;vijay2win@yahoo.com;Will do thanks!;;;","19/Oct/12 19:44;vijay2win@yahoo.com;Attached patch fixes the bug.;;;","19/Oct/12 23:47;thobbs;The patch passes all of the pycassa tests. I'll leave the code review to Sylvain.;;;","22/Oct/12 07:22;slebresne;+1 (committed).

For the record, the reason this affects trunk but not 1.1 is that on trunk we've removed a end-of-slice check in SliceQueryFilter.colllectReducedColumns because this was redundant with the job of the sstable and memtable iterators. We forgot the super column case however, which this patch fixes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix cqlsh after move of CL to the protocol level,CASSANDRA-4823,12612167,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,slebresne,slebresne,17/Oct/12 07:59,16/Apr/19 09:32,14/Jul/23 05:52,19/Oct/12 19:48,1.2.0 beta 2,,,Legacy/Tools,,,0,,,,,,,"CASSANDRA-4734 moved the consistency level at the protocol level (and in doing so, separated the cql3 thrift methods from the cql2 ones). We should adapt cqlsh to that change.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/12 23:58;aleksey;CASSANDRA-4823.txt;https://issues.apache.org/jira/secure/attachment/12549785/CASSANDRA-4823.txt","19/Oct/12 16:48;aleksey;cql-internal-only-1.4.0.zip;https://issues.apache.org/jira/secure/attachment/12550003/cql-internal-only-1.4.0.zip",,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249230,,,Fri Oct 19 19:48:29 UTC 2012,,,,,,,,,,"0|i0a68n:",57284,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"19/Oct/12 19:48;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: Allow renaming PK columns to ease upgrade from thrift,CASSANDRA-4822,12612166,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Oct/12 07:56,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/12 16:00,1.2.0 beta 2,,,Legacy/CQL,,,0,,,,,,,"Say you have a clicks CF in thrift storing for each user a timeline of which links it clicked on. It may have a definition like:
{noformat}
create column family clicks with key_validation_class = UUIDType and comparator = TimeUUIDType and default_validation_class = UTF8Type
{noformat}

In CQL3, you can access that thrift created CF as if it had been defined by:
{noformat}
CREATE TABLE clicks (
  key uuid,
  column timeuuid,
  value text,
  PRIMARY KEY (key, column)
) WITH COMPACT STORAGE
{noformat}
In other words, CQL3 will pick default names for the key_alias, column_aliases and value_alias metadata. It's ok but it would be more user friendly to use if the user could rename those to something better. Today, the only solution would be to remove the schema and re-create the table in CQL3. We can make that simpler by adding support for:
{noformat}
ALTER TABLE clicks RENAME key to user_id;
ALTER TABLE clicks RENAME column to insertion_time;
ALTER TABLE clicks RENAME value to url_clicked; 
{noformat}

Of course such rename statement won't be applicable to all columns. Namely, we can only allow renaming PK columns and in some compact storage cases the value. But that's probably still worth adding.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/12 15:29;slebresne;4822-2.txt;https://issues.apache.org/jira/secure/attachment/12550784/4822-2.txt","17/Oct/12 08:33;slebresne;4822.txt;https://issues.apache.org/jira/secure/attachment/12549467/4822.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249229,,,Thu Oct 25 16:00:20 UTC 2012,,,,,,,,,,"0|i0a68f:",57283,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Oct/12 20:13;jbellis;I think this is backwards:

+                newList.add(i < l.size() ? null : l.get(i));

Nits: would prefer {{l}} to be given a more meaningful name, like {{oldNames}}.  Comments for the aliases lists that they can be null-padded would be nice.;;;","25/Oct/12 15:29;slebresne;Attaching v2 of the patch. It addresses the remarks above but now that I've been able to test it, it also includes a few new stuffs. Basically we weren't very accurate in our detection of whether the table was compact or not. Let recall that we don't ""store"" if a table is compact or not, we infer that from the comparator and how many column aliases exist. We probably should have store something to distinguish cql3 tables from other ones from day one, but unless we want to break all existing CQL3 table now we have to stick to our little detection dance.

Anyway, I've updated said detection code (in CFDefinition) to be more precise.  However, there is still a small catch if we allow users to provide column aliases, which is that if a thrift user with a composite type declare all but the last column alias, we will interpret the table as non-compact but that's not correct. I don't know how to fix that, but to make sure this have little change to happen in practice, I've modified the rename to allow renaming multiple columns in the same ALTER statement. If User upgrading from thrift alter all the columns at once, they'll always be fine.
;;;","25/Oct/12 15:49;jbellis;+1;;;","25/Oct/12 16:00;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a close() method to CRAR to prevent leaking file descriptors.,CASSANDRA-4820,12612139,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,17/Oct/12 01:31,16/Apr/19 09:32,14/Jul/23 05:52,17/Oct/12 18:08,1.1.7,,,,,,0,,,,,,,"The problem is that under heavy load Finalizer daemon is unable to keep up with number of ""source"" and ""channel"" fields from CRAR to finalize (as FileInputStream has custom finalize() which calls close inside) which creates memory/cpu pressure on the machine.",,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/12 03:38;xedin;CASSANDRA-4820.patch;https://issues.apache.org/jira/secure/attachment/12549437/CASSANDRA-4820.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249186,,,Wed Oct 17 18:08:50 UTC 2012,,,,,,,,,,"0|i0a5l3:",57178,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"17/Oct/12 03:11;jbellis;FIS.close says ""If this stream has an associated channel then the channel is closed as well.""  So closing both source and channel is unnecessary.;;;","17/Oct/12 03:38;xedin;Right, sorry, they share the same fd. Re-attached the patch.;;;","17/Oct/12 14:52;jbellis;+1;;;","17/Oct/12 18:08;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update sstableloader for 1.1.x,CASSANDRA-4818,12612102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,j.casares,j.casares,16/Oct/12 20:51,16/Apr/19 09:32,14/Jul/23 05:52,20/Sep/13 22:40,,,,,,,0,datastax_qa,,,,,,"This was done on Cassandra 1.1.5:
{CODE}
$ ls -1 Keyspace1/Keyspace1/Standard1/
Standard1-51-Data.db
Standard1-51-Index.db
Standard1-hc-51-Data.db
Standard1-hc-51-Index.db
Standard1-tmp-hc-46-Data.db
Standard1-tmp-hc-46-Index.db
$ ~/repos/cassandra/bin/sstableloader -d localhost Keyspace1/Keyspace1/Standard1/
 WARN 15:41:56,023 Invalid file 'Standard1-51-Data.db' in data directory Keyspace1/Keyspace1/Standard1.
 WARN 15:41:56,024 Invalid file 'Standard1-51-Index.db' in data directory Keyspace1/Keyspace1/Standard1.
Skipping file Standard1-hc-51-Data.db: column family Keyspace1.hc doesn't exist
Skipping file Standard1-tmp-hc-46-Data.db: column family Keyspace1.tmp doesn't exist
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]
{CODE}",,j.casares,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249126,,,Tue Oct 16 20:58:06 UTC 2012,,,,,,,,,,"0|i0a4xr:",57073,,,,,Normal,,,,,,,,,,,,,,,,,"16/Oct/12 20:58;nickmbailey;The sstable file names there don't appear to be valid. Shouldn't they be 

{noformat}
<keyspace>-<cf>-....db
{noformat}

rather than just

{noformat}
<cf>-....db
{noformat};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update sstable2json for 1.1.x,CASSANDRA-4817,12612088,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,j.casares,j.casares,16/Oct/12 19:17,16/Apr/19 09:32,14/Jul/23 05:52,20/Sep/13 22:40,,,,,,,0,datastax_qa,,,,,,"This format is still needed in 1.1.5:

bin/json2sstable -K KS -c CF CF.json KS-CF-he-2-Data.db

to comply with this method:

https://github.com/apache/cassandra/blob/cassandra-1.1.5/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L160

even though 1.1.x now uses the directory structure KS/CF/CF-he-2-Data.db.",,j.casares,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249104,,,2012-10-16 19:17:25.0,,,,,,,,,,"0|i0a4sn:",57050,,,,,Low,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken get_paged_slice,CASSANDRA-4816,12612041,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,16/Oct/12 14:21,16/Apr/19 09:32,14/Jul/23 05:52,24/Oct/12 19:48,1.1.7,1.2.0 beta 2,,,,,0,,,,,,,"get_paged_slice doesn't reset the start column filter for the second returned row sometimes. So instead of getting a slice:

row 0: <start_column>...<last_column_in_row>
row 1: <first column in a row>...<last_column_in_row>
row 2: <first column in a row>...

you sometimes get:

row 0: <start_column>...<last_column_in_row>
row 1: <start_column>...<last_column_in_row>
row 2: <first column in a row>...
",,alexliu68,cdaw,jeromatron,pkolaczk,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/12 17:23;slebresne;4816-2.txt;https://issues.apache.org/jira/secure/attachment/12549345/4816-2.txt","17/Oct/12 07:52;pkolaczk;4816-3.txt;https://issues.apache.org/jira/secure/attachment/12549461/4816-3.txt",,,,,,,,,,,,,,,,2.0,pkolaczk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249001,,,Wed Oct 24 19:48:43 UTC 2012,,,,,,,,,,"0|i0a3ev:",56826,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"16/Oct/12 15:33;slebresne;My bet would be that this is due to the memtable iterator. The way we implement get_paged_slice, we reset the start of the QueryFilter used after the first row have been read. Which means that for each row, we need to make sure the filter is not used until we've processed the preceding row. This is the case for the sstable iterator, but not for the memtable one. Attaching patch to change that (I haven't tested the patch though).;;;","16/Oct/12 16:14;slebresne;Scratch that patch, brain fart. The AbstractIterator.computeNext() is called no sooner than on hasNext() so we should be good on that front. Still don't know what is the problem here.;;;","16/Oct/12 17:23;slebresne;Ok, second attempt. Looking more closing it does seem that this is a problem with the interaction of the mergeIterator and the SSTableScanner. Basically the mergeIterator always needs to know what his ""the next row"" (during the reducing phase). If that next row was the one that the reducer ended up returning, we were fine (so with 1 sstable or if all sstables had the same rows, it was ok), but otherwise it might end up using the QueryFilter before it should for our get_paged_slice ""hack"".

Anyway, all that the mergeIterator needs during its reduction phase is to know the next key. So attaching a patch that delay the use of the filter until the row data is actually queried.;;;","17/Oct/12 07:52;pkolaczk;Attaching 3rd version of the patch. LazyColumnIterator is used both for SSTableScanners and Memtable.
This version works for me.;;;","24/Oct/12 06:36;cdaw;+1 this version fixes my tests as well.;;;","24/Oct/12 06:57;slebresne;To be clear, I'm also good on version 3 but I'll let Jonathan review since I've wrote version 2 on which version 3 is based.;;;","24/Oct/12 19:48;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem using BulkOutputFormat while streaming several SSTables simultaneously from a given node.,CASSANDRA-4813,12611994,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,ralph.romanos,ralph.romanos,16/Oct/12 08:58,16/Apr/19 09:32,14/Jul/23 05:52,13/Nov/12 22:39,1.2.0 beta 3,,,,,,0,Bulkoutputformat,Hadoop,SSTables,,,,"The issue occurs when streaming simultaneously SSTables from the same node to a cassandra cluster using SSTableloader. It seems to me that Cassandra cannot handle receiving simultaneously SSTables from the same node. However, when it receives simultaneously SSTables from two different nodes, everything works fine. As a consequence, when using BulkOutputFormat to generate SSTables and stream them to a cassandra cluster, I cannot use more than one reducer per node otherwise I get a java.io.EOFException in the tasktracker's logs and a java.io.IOException: Broken pipe in the Cassandra logs.","I am using SLES 10 SP3, Java 6, 4 Cassandra + Hadoop nodes, 3 Hadoop only nodes (datanodes/tasktrackers), 1 namenode/jobtracker. The machines used are Six-Core AMD Opteron(tm) Processor 8431, 24 cores and 33 GB of RAM. I get the issue on both cassandra 1.1.3, 1.1.5 and I am using Hadoop 0.20.2.",mauzhang,mkjellman,ralph.romanos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4922,,,,,,,,,,"08/Nov/12 21:55;yukim;4813.txt;https://issues.apache.org/jira/secure/attachment/12552717/4813.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248935,,,Wed Nov 14 12:52:00 UTC 2012,,,,,,,,,,"0|i0a2l3:",56692,,mkjellman,,mkjellman,Low,,,,,,,,,,,,,,,,,"16/Oct/12 22:09;brandon.williams;Can you post a stacktrace?;;;","17/Oct/12 06:58;ralph.romanos;I get the following error in the tasktracker's logs when SSTables 
are streamed into the Cassandra cluster:

Exception in thread ""Streaming to /172.16.110.79:1"" java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:628)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:194)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:94)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
Exception in thread ""Streaming to /172.16.110.92:1"" java.lang.RuntimeException: java.io.EOFException
	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:628)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:194)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:94)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more;;;","17/Oct/12 19:47;mkjellman;Same issue with 1.1.6 and Hadoop 1.0.3

I have the following from the Cassandra logs as well

ERROR 12:46:06,256 Exception in thread Thread[Thread-1224,5,main]
java.lang.AssertionError: We shouldn't fail acquiring a reference on a sstable that has just been transferred
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:188)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:182)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78);;;","17/Oct/12 20:42;mkjellman;Just confirmed that limiting the reducer to 1 does not change the behavior in my environment. Also noticed that BulkRecordWriter will always throw an IOException (as mentioned in the original bug) if mapreduce.output.bulkoutputformat.maxfailedhosts is ever > 0 (assuming defaults).

In my case future.getFailedHosts() always returns every node in my cluster when the condition occurs. I'm doing about 50 million insertions into 50 million rows and the EOFExceptions seem to crop up after a good number of the sstables have already been successfully sent.;;;","17/Oct/12 20:50;ralph.romanos;Do you stream the SSTables in your reducer? And when you tried to limit the reducer to 1, did you restart the cassandra cluster?;;;","17/Oct/12 20:57;mkjellman;Yes, in my reducer. Didn't restart the Cassandra cluster -- what would that accomplish? Limited the reducer in my job configuration.;;;","17/Oct/12 21:06;ralph.romanos;I had the same error (IOException) when I did not restart Cassandra. It is caused by the EOFException and because the streaming failed in your previous job. Restarting Cassandra should allow you to make it work with 1 reducer.;;;","18/Oct/12 00:45;mkjellman;you're quite right. no issues after a rolling restart and only one reducer.;;;","19/Oct/12 20:21;mkjellman;just reproduced this again with one reducer.;;;","22/Oct/12 19:08;yukim;This is limitation of BulkOutputFormat right now. Currently, streaming session uses (IP, counter) for its ID. Since counter is per JVM, running two or more reducers on same node streaming to one cassandra node likely cause session conflict, and I think that is causing the issue here.
To resolve this, we need to change the way to distinguish each session(possibly by changing to use UUID for session ID).

[~mkjellman] Do you run your reducer on top of cassandra node? If that is the case, session conflict I described above may be the cause. If not, there is another issue in your one reducer case I think.;;;","22/Oct/12 19:15;mkjellman;[~nakagawayk] Yes - the single reducer was running on top of a Cassandra node. Why would having the reducer+the cassandra node cause a session ID collision if there is only one reducer?;;;","23/Oct/12 07:35;ralph.romanos;I never get this exception when I run my reducer on a single node (either on top of a cassandra node or on a Hadoop only node). In addition, when I run multiple reducers, I create a java process for each reducer; therefore my reducers should be running on different JVMs right? If that is the case we shouldn't get counter conflicts when running multiple reducers.;;;","23/Oct/12 15:07;yukim;Session counter is per JVM, so when you spawn two or more stream session on different JVM on same host, you will have chance to get same session ID, say (10.x.x.x, 1) on both JVM.
This is also true with Cassandra on one JVM and reducer on the other JVM on the same machine(but session collision in this set up is less likely than two reducers spawned simultaneously on the same node).;;;","23/Oct/12 20:29;jbellis;If we need to change streaming protocol to fix this then we should target 1.2.;;;","30/Oct/12 20:14;yukim;Attaching patch to change streaming session ID from (host, counter) pair to Time UUID. This should hugely drop probability of session ID collision. I haven't tested with BOF, but I spawned 3 sstableloader simultaneously on the same node with C* and could finish streaming without getting errors.;;;","31/Oct/12 15:38;mkjellman;Yuki- Thanks for your work on the patch. I'll test BOF with 1.1.6 today.;;;","31/Oct/12 15:41;yukim;Michael,

Patch is for trunk(version 1.2). The change breaks messaging compatibility in 1.1, so this won't be ported for next 1.1.7 release.
;;;","31/Oct/12 15:45;mkjellman;Good to know - I'll get a 1.2 cluster setup on trunk then and test.;;;","31/Oct/12 16:12;ralph.romanos;Me too, I will let u know as soon as I test it. Thank you for this quick patch Yuki.;;;","31/Oct/12 21:16;yukim;Attaching newer version. Since with this patch, we only distinguish streaming session by UUID, we don't need to carry around broadcast address(CASSANDRA-3503), so I removed it from StreamHeader.;;;","01/Nov/12 15:42;mkjellman;[~yukim] not sure if this will be an issue for ""normal"" upgraders or if this was because i went between patch versions but just wanted you to be aware of the EOF i saw on startup. Node only has the system schema at this point.

WARN 08:40:29,821 error reading saved cache /ssd/saved_caches/system-local-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:278)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:365)
	at org.apache.cassandra.db.Table.initCf(Table.java:334)
	at org.apache.cassandra.db.Table.<init>(Table.java:272)
	at org.apache.cassandra.db.Table.open(Table.java:102)
	at org.apache.cassandra.db.Table.open(Table.java:80)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:320)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:395)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:438)

Edit: actually thinking this is a totally unrelated bug?;;;","01/Nov/12 22:59;yukim;Michael,

I think it is not related to this issue.
Do you upgrade from which version? '-b' in system-local-KeyCache-b.db indicates it's from version 1.2.
Also, it possible that the saved cache file is really corrupted for some reason(shutting down in the middle of cache saving?), so C* was just WARNing you about it.;;;","01/Nov/12 23:15;mkjellman;Brand new cluster running 1.2 with your original patch, nodetool drain, put new jar with new patch in place, started Cassandra and got error. had to delete the cache file as subsequent initializations threw the EOF as well. Happened on all three nodes in the 3 node test cluster.;;;","05/Nov/12 16:48;yukim;Michael,

I couldn't reproduce your error, and I believe that is not related to this issue.
So if you see that error constantly, please open another issue.;;;","05/Nov/12 16:53;mkjellman;Sounds good Yuki. I've been trying to get a secondary cluster setup since the 31st..keep getting pulled away. I promise i'll get multiple reducers tested today :);;;","05/Nov/12 23:19;mkjellman;Exception in thread ""Streaming to /10.25.9.5:1"" java.lang.RuntimeException: java.net.SocketException: Already bound
        at com.google.common.base.Throwables.propagate(Throwables.java:156)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.SocketException: Already bound
        at sun.nio.ch.Net.translateToSocketException(Net.java:109)
        at sun.nio.ch.Net.translateException(Net.java:141)
        at sun.nio.ch.Net.translateException(Net.java:147)
        at sun.nio.ch.SocketAdaptor.bind(SocketAdaptor.java:147)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:128)
        at org.apache.cassandra.streaming.FileStreamTask.connectAttempt(FileStreamTask.java:236)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:88)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more
Caused by: java.nio.channels.AlreadyBoundException
        at sun.nio.ch.SocketChannelImpl.bind(SocketChannelImpl.java:556)
        at sun.nio.ch.SocketAdaptor.bind(SocketAdaptor.java:145)
        ... 7 more

is it intended that we fail that reducer? this looks like just a more elegant collision, no? seeing the same failure on every node.

;;;","06/Nov/12 18:26;mkjellman;Also, while we don't throw the EOF anymore further down the stack as far as I can tell from my testing, the net result is an IOException being thrown in the Reducer.;;;","08/Nov/12 21:55;yukim;ok, it looks like we have CASSANDRA-3839 for BOF.
Updated patch to avoid socket re-binding.

[~mkjellman] How about this one?;;;","08/Nov/12 23:13;mkjellman;new problem now with current patch.

2012-11-08 15:29:05,323 ERROR org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor: Error in ThreadPoolExecutor
java.lang.RuntimeException: java.io.IOException: Broken pipe
        at com.google.common.base.Throwables.propagate(Throwables.java:156)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:89)
        at sun.nio.ch.IOUtil.write(IOUtil.java:60)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:450)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:98)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at com.ning.compress.lzf.LZFChunk.writeCompressedHeader(LZFChunk.java:77)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:132)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
        at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:218)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:164)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more;;;","09/Nov/12 21:16;yukim;[~mkjellman] Just checking, did you use patched version for both Cassandra and hadoop job?;;;","09/Nov/12 21:19;mkjellman;yes, actually the first time I tested I did forget to ensure both the Hadoop jar and Cassandra nodes had the newest patched version. I've tested a few times now just to make sure i'm not missing anything. Seems to die as soon as it tries to stream the first sstable to the nodes. Never progresses past 0% on the streaming and then throws the exception.

applied patch to trunk when it was at commit f09a89f4cd13af2087fcc92f09f6cf1ee4785feb. i rebuilt the entire cluster, and ensured my maven dependencies were all set. Still reproduced the problem unfortunately (i actually thought it had been resolved but i just reproduced the java.io.IOException: Broken pipe again).

MD5 (build/apache-cassandra-1.2.0-beta2-SNAPSHOT.jar) = 92d8ffacb3963116dd153a2c8c83fbe9;;;","12/Nov/12 18:04;yukim;[~mkjellman] Hmm, I tried standalone and psuedo-cluster hadoop on my machine and haven't seen that error. I will try in fully distributed mode.
By the way, what kind of error did you see on cassandra side? Can you post stacktrace?;;;","12/Nov/12 19:06;mkjellman;[~yukim] I can produce it in local mode (standalone) and distributed mode with the current revision of the patch. I haven't ran it in psuedo-cluster mode. Also should mention I have reproduced it even when I limit to 1 reducer.;;;","13/Nov/12 18:45;yukim;[~mkjellman] OK, I did test on distributed hadoop cluster with multiple reducers and job succeeded.
At first I was getting the same error as yours (java.io.IOException: Broken pipe), but I figured out it was due to the mismatch in partitioner(My cluster was configured as RandomPartitioner and I used Murmur3Partitioner for BOF).
Can you check that also?
If that's not your case, can you upload Cassandra system.log?;;;","13/Nov/12 20:16;mkjellman;[~yukim] Looks like you are right. Was Murmur Partitioner and my job was configured for Random. On a first run looks like this patch is good to go. Sorry for the wild goose chase there.;;;","13/Nov/12 21:54;mkjellman;Just tested a few more times. Looks good. Ship it!;;;","13/Nov/12 22:39;yukim;Thanks for testing! Committed for 1.2 release.

For partitioner mismatch part, I opened CASSANDRA-4957 to follow up.;;;","14/Nov/12 12:52;ralph.romanos;Hello, 
I just tested the patch on 1.2 with 2 reducers per node and 7 nodes. Everything works fine now.

Thank you Yuki!;;;",,,,,,,,,,,,,,,,,,,,,
"Some cqlsh help topics don't work (select, create, insert and anything else that is a cql statement)",CASSANDRA-4811,12611952,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,16/Oct/12 02:37,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/12 10:51,1.1.7,1.2.0 beta 2,,,,,0,cqlsh,,,,,,"cqlsh> help select
Improper help command.

Same will happen if you look up a help topic for any other cql statement.
38748b43d8de17375c7cc16e7a4969ca4c1a2aa1 broke it (#4198) 5 months ago.
",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/12 19:12;aleksey;CASSANDRA-4811.txt;https://issues.apache.org/jira/secure/attachment/12550149/CASSANDRA-4811.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248887,,,Thu Oct 25 10:51:56 UTC 2012,,,,,,,,,,"0|i0a29j:",56640,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"20/Oct/12 19:15;aleksey;There is a deep underlying issue with lexing/parsing, but I gave up trying to fix it. If something like this ever happens again, I'll make changes, otherwise it's not worth it.

The patch is simple, fixes the issue's symptoms perfectly and should be enough.;;;","25/Oct/12 10:51;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unit test failing under long-test,CASSANDRA-4810,12611905,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,bbucher,bbucher,15/Oct/12 20:59,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/12 16:47,1.2.0 beta 2,,,Legacy/Testing,,,0,,,,,,,"the following failure occurs when running ant long-test

junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionsTest
    [junit] Tests run: 5, Failures: 1, Errors: 0, Time elapsed: 31.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=1 colsper=200000: 2173 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=2 rowsper=200000 colsper=1: 4531 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionsTest: sstables=100 rowsper=800 colsper=5: 1864 ms
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStandardColumnCompactions(org.apache.cassandra.db.compaction.LongCompactionsTest):	FAILED
    [junit] expected:<9> but was:<99>
    [junit] junit.framework.AssertionFailedError: expected:<9> but was:<99>
    [junit] 	at org.apache.cassandra.db.compaction.CompactionsTest.assertMaxTimestamp(CompactionsTest.java:207)
    [junit] 	at org.apache.cassandra.db.compaction.LongCompactionsTest.testStandardColumnCompactions(LongCompactionsTest.java:141)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionsTest FAILED
",,bbucher,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248821,,,Tue Oct 16 16:47:37 UTC 2012,,,,,,,,,,"0|i0a07b:",56306,,,,,Low,,,,,,,,,,,,,,,,,"16/Oct/12 01:59;jbellis;is this 1.1 or trunk?;;;","16/Oct/12 02:53;bbucher;trunk.;;;","16/Oct/12 16:47;yukim;AssertionError is caused by one of tests moved from unit test recently in trunk.
testStandardColumnCompactions should have cleared SSTables generated in other tests in order to run properly.
Committed fix in 8c471240d0ffafeef53df2fd69693294257ed730.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool doesnt work well with negative tokens,CASSANDRA-4808,12611860,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,15/Oct/12 17:22,16/Apr/19 09:32,14/Jul/23 05:52,23/Oct/12 18:02,1.2.0 beta 2,,,,,,0,,,,,,,"./apache-cassandra-1.2.0-beta1-SNAPSHOT/bin/nodetool move \-2253536297082652573
Unrecognized option: -2253536297082652573
usage: java org.apache.cassandra.tools.NodeCmd --host <arg> <command>
            
 -cf,--column-family <arg>   only take a snapshot of the specified column
                             family
",,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/12 18:14;vijay2win@yahoo.com;0001-CASSANDRA-4808.patch;https://issues.apache.org/jira/secure/attachment/12549181/0001-CASSANDRA-4808.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248767,,,Tue Oct 23 18:02:37 UTC 2012,,,,,,,,,,"0|i09ztb:",56243,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"15/Oct/12 18:14;vijay2win@yahoo.com;There are 2 option, 

Make nt to accept '-' by making ""Options"" ignore any -xxxx values (dont verify if they are valid options), this can cause confusion on other commands.

Other option (Attached patch) is to support escape character for '-' 
{code}
Example: ./apache-cassandra-1.2.0-beta1-SNAPSHOT/bin/nodetool move \\-2253536297082652571
{code};;;","15/Oct/12 19:53;brandon.williams;I'm confused as to why we need negative tokens.;;;","15/Oct/12 20:31;vijay2win@yahoo.com;M3P supports -ve tokens the range is from Long.MIN_VALUE to Long.MAX_VALUE, 

explanation is in 
https://issues.apache.org/jira/browse/CASSANDRA-4621?focusedCommentId=13452829&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13452829

Example for generating tokens:

For RandomPartitioner

Node 0	: 0
Node 1	: 56713727820156410577229101238628035242
Node 2	: 113427455640312821154458202477256070484

For Murmur3Partitioner

Node 0	: 0
Node 1	: 6148914691236517204
Node 2	: -6148914691236517208;;;","16/Oct/12 06:20;slebresne;bq. I'm confused as to why we need negative tokens.

To elaborate, M3P uses a long token internally and since long in java are signed, we end up with a tokens that can be negative. We could chose to interpret the long as unsigned, but even if we do that, we need the minimum token to not be a valid token, so tokens would have to be a value in [1, 2^64-1], not [0, 2^64-1] and so people would have to adapt whatever algorithm they use to compute tokens anyway and hence I'm not sure it's worth bothering changing from negative tokens.;;;","23/Oct/12 08:39;slebresne;+1

nit: could be nice to add how to do the escaping in the help of nodetool move (I think that's the only command that take a token as argument).;;;","23/Oct/12 18:02;vijay2win@yahoo.com;Added help comment and committed, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction progress counts more than 100%,CASSANDRA-4807,12611845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,omid,omid,15/Oct/12 15:10,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/12 23:12,1.1.7,1.2.0 beta 2,,,,,1,,,,,,,"'nodetool compactionstats' compaction progress counts more than 100%:

{code}
pending tasks: 74
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Validation        KSP        CF1           56192578305         84652768917    66.38%
               Compaction        KSP        CF2           162018591           119913592     135.11%
{code}

Hadn't experienced this before 1.1.3. Is it due to changes in 1.1.4-1.1.6 ?",,azotcsit,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/12 19:52;yukim;4807-1.1.txt;https://issues.apache.org/jira/secure/attachment/12549369/4807-1.1.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248720,,,Tue Oct 16 23:12:03 UTC 2012,,,,,,,,,,"0|i09xzz:",55949,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Oct/12 18:15;yukim;[~omid] What compaction strategy are you using? Do you turn on multithreaded compaction?;;;","16/Oct/12 18:21;azotcsit;Yuki, I've seen that too. I've used the following configs:
* LCS with singlethreaded compaction (CompactionIterable).
* snappy compression with chunk_length_kb = 32. 
* Cassandra 1.1.4.;;;","16/Oct/12 18:59;omid;[~yukim] LCS with snappy with chunk_length_kb = 64, sstable_size_in_mb = 10 and multithreaded compaction disabled on Cassandra 1.1.6.;;;","16/Oct/12 19:52;yukim;Alexey, Omid,

Thank you both for providing information.
I think this regression comes from CASSANDRA-4587.
compactionstats' bytes compacted comes from LeveledScanner#getCurrentPosition(https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L261) when using LCS.

We need to reset currentScanner to null after scanning through to the end, otherwise we would add twice for last scanned SSTable.

Patch attached as well as unit test for this.
(github: https://github.com/yukim/cassandra/tree/4807);;;","16/Oct/12 20:13;jbellis;LGTM, +1.

When committing, can you add a comment to the computeNext code explaining why we set to null?;;;","16/Oct/12 20:21;omid;Thanks Yuki. It also probably affects compaction throttling, by throttling more than it should (https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L612);;;","16/Oct/12 23:12;yukim;Committed with comment.

[~omid] Correct, though you pointed to cleanup source code which is not affected by this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong assumption for KeyRange about range.end_token in get_range_slices().,CASSANDRA-4804,12611757,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nmmm,nmmm,nmmm,14/Oct/12 15:45,16/Apr/19 09:32,14/Jul/23 05:52,17/Oct/12 04:01,1.1.7,1.2.0 beta 2,,Legacy/CQL,,,0,,,,,,,"In get_range_slices() there is parameter KeyRange range.

There you can pass start_key - end_key, start_token - end_token, or start_key - end_token.

This is described in the documentation.

in thrift/ThriftValidation.java there is validation function validateKeyRange() (line:489) that validates correctly the KeyRange, including the case start_key - end_token.

However in thrift/CassandraServer.java in function get_range_slices() on line: 686 wrong assumption is made:

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), RowPosition.forKey(range.end_key, p));

   }

This means if there is start key, no end token is checked.
The opposite - null is ""inserted"" as end_key.

Solution:
same file - thrift/CassandraServer.java on next function - get_paged_slice(), on line:741 same code is written correctly

   if (range.start_key == null)

   {

      ... // populate tokens

   }

   else

   {

      RowPosition end = range.end_key == null ? p.getTokenFactory().fromString(range.end_token).maxKeyBound(p)

                           : RowPosition.forKey(range.end_key, p);

      bounds = new Bounds<RowPosition>(RowPosition.forKey(range.start_key, p), end);

   }

",,dbrosius,nmmm,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,"15/Oct/12 19:53;nmmm;cassa.1.1.6.diff.txt;https://issues.apache.org/jira/secure/attachment/12549192/cassa.1.1.6.diff.txt","15/Oct/12 19:53;nmmm;cassa.1.2.x.diff.txt;https://issues.apache.org/jira/secure/attachment/12549193/cassa.1.2.x.diff.txt",,,,,,,,,,,,,,,,2.0,nmmm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248569,,,Wed Oct 17 04:01:27 UTC 2012,,,,,,,,,,"0|i09w3z:",55643,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,"15/Oct/12 19:54;nmmm;patch tested in 1.1.5 and 1.1.6
did not check in 1.2, but is trivial.;;;","16/Oct/12 01:20;jbellis;Can you review, Dave?;;;","16/Oct/12 02:13;dbrosius;it seems to me you should check range.isSetEnd_key() and range.isSetEnd_token() to see what option you should use as i believe it's valid for the value to be null, meaning end of range.


bah... ignore this comment. new byte[0] is the way to specify end of range.
;;;","16/Oct/12 02:30;dbrosius;1.2 patch doesn't apply cleanly. 

remove commented out code

otherwise patch works as expected.;;;","17/Oct/12 04:01;dbrosius;committed to cassandra-1.1 as commit 4d637f1f1b62593e0c52e49966e3f286bf65c3e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFRR wide row iterators improvements,CASSANDRA-4803,12611668,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pkolaczk,pkolaczk,pkolaczk,13/Oct/12 08:16,16/Apr/19 09:32,14/Jul/23 05:52,01/Dec/12 07:05,1.1.8,,,,,,0,,,,,,,"{code}
 public float getProgress()
    {
        // TODO this is totally broken for wide rows
        // the progress is likely to be reported slightly off the actual but close enough
        float progress = ((float) iter.rowsRead() / totalRowCount);
        return progress > 1.0F ? 1.0F : progress;
    }
{code}

The problem is iter.rowsRead() does not return the number of rows read from the wide row iterator, but returns number of *columns* (every row is counted multiple times). ",,jeromatron,pkolaczk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/12 21:08;pkolaczk;0007-Fallback-to-describe_splits-v3.patch;https://issues.apache.org/jira/secure/attachment/12554809/0007-Fallback-to-describe_splits-v3.patch",,,,,,,,,,,,,,,,,1.0,pkolaczk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248417,,,Sat Dec 01 07:05:21 UTC 2012,,,,,,,,,,"0|i09uxz:",55446,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Oct/12 13:48;pkolaczk;I attach a list of patches affecting operation of CFRR:

# Fix for obvious counting bug in wide row iterator that was counting columns instead of rows.
# Several fixes in describe_splits:
   fixed non-uniform splitting - caused by integer math roundoff errors
   fixed insane behaviour when number of splits was higher than number of key samples
   added estimated size of the split to the result, and make use of it in CFIF
# This is a patch for broken get_paged_slice; addressed in a separate ticket, but I had to include it in order to test my code
# Fix for creating excessively small splits (and wrong progress reporting) due to range wrap around.
# get_range_slices allows for (start_key, end_token) exactly the same as get_paged_slice 
# I tried to de-spaghettize CFRR code a little. This also fixes some bug that accidentally slipped in with previous patches.;;;","19/Oct/12 23:31;jbellis;reviewed + committed patches 01 and 02, rest still pending.;;;","24/Oct/12 19:57;jbellis;03 committed in CASSANDRA-4816.

Not sure about 04 -- I'm a fan of the simplifications we get from letting CFRR only need to deal with non-wrapping splits.;;;","05/Nov/12 09:36;pkolaczk;#04 - what about virtual nodes in 1.2? Do we insist that split may not span more than one contiguous token range? It will be harder to avoid too small splits. And too small split = bigger task book-keeping overhead.;;;","05/Nov/12 10:13;pkolaczk;Hold on with applying patch 2 for a while. We just discovered it breaks running hive queries while doing rolling upgrade. There is a need for falling back to old describe_splits method if describe_splits_ex is not found.;;;","06/Nov/12 16:11;pkolaczk;Attaching a patch allowing to generate splits also when talking to an older version of thrift server.;;;","12/Nov/12 09:40;pkolaczk;Rebased patches for recent 1.1. Some patches have been already applied, so removed them from the list.;;;","17/Nov/12 13:02;jbellis;bq. what about virtual nodes in 1.2? Do we insist that split may not span more than one contiguous token range?

That's kind of orthogonal to wrapping ranges per se -- you'll still only have a single [virtual] node whose range wraps.  So vnodes won't make that worse.  Moreover, you're still going to need two scans at the disk level since a wrapping range won't be contiguous there.  (Currently wrapping ranges are split by StorageProxy.getRestrictedRanges but this may change for CASSANDRA-4858.)  Doing an extra Thrift or CQL query is negligible overhead compared to the actual scan.

Finally, getRestrictedRanges *will* split it up into scan-per-vnode which I agree is something we should fix but I don't think this patch does it.  As an optimization I don't think it's something we should block 1.2.0 for.  Should we split this into a separate ticket?;;;","17/Nov/12 13:02;jbellis;0006: Can you split out the bug fix and rebase?  Refactoring is fine but let's keep it separate from bug fixes.

0007: I'm unclear what this is useful for, anyone running a 1.1 recent enough to have this patch, would also have describe_splits_ex, no?;;;","20/Nov/12 15:48;pkolaczk;0007 - this is for rolling upgrade. When you upgrade one node and the other nodes don't have describe_splits_ex yet, starting a hadoop job on a newly upgraded node fails.

As for 0004 / 0006 fixes - I agree. Let's move them to a separate ticket. ;;;","20/Nov/12 15:49;pkolaczk;BTW: I don't get email notifications from ASF Jira. It is because I registered long, long time ago and my email is obsolete. How to change my email to a newer one? I can't see an option in the user profile for doing that. ;;;","21/Nov/12 11:48;jbellis;Is there a more specific exception we can catch besides TException?;;;","21/Nov/12 11:49;jbellis;Without trying it, I think it would be TApplicationException.WRONG_METHOD_NAME.;;;","21/Nov/12 20:33;pkolaczk;[~jbellis] right, I change it.;;;","22/Nov/12 08:58;pkolaczk;0007-Fallback-to-describe-splits-v2.patch: Catching TApplicationException and checking its type. ;;;","25/Nov/12 21:08;pkolaczk;Attached third version of the patch - this time it works. The right error code is UNKNOWN_METHOD (1), not WRONG_METHOD_NAME.;;;","01/Dec/12 07:05;jbellis;(committed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regular startup log has confusing ""Bootstrap/Replace/Move completed!"" without boostrap, replace, or move",CASSANDRA-4802,12611644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,kmueller,kmueller,12/Oct/12 22:21,16/Apr/19 09:32,14/Jul/23 05:52,17/Oct/12 19:09,1.2.0,,,,,,0,,,,,,,"A regular startup completes successfully, but it has a confusing message the end of the startup:

""  INFO 15:19:29,137 Bootstrap/Replace/Move completed! Now serving reads.""

This happens despite no bootstrap, replace, or move.

While purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?!  It should simply read something like ""Startup completed! Now serving reads"" unless it actually has done one of the actions in the error message.



Complete log at the end:


INFO 15:13:30,522 Log replay complete, 6274 replayed mutations
 INFO 15:13:30,527 Cassandra version: 1.0.12
 INFO 15:13:30,527 Thrift API version: 19.20.0
 INFO 15:13:30,527 Loading persisted ring state
 INFO 15:13:30,541 Starting up server gossip
 INFO 15:13:30,542 Enqueuing flush of Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,543 Writing Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,550 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-274-Data.db (80 bytes)
 INFO 15:13:30,563 Starting Messaging Service on port 7000
 INFO 15:13:30,571 Using saved token 31901471898837980949691369446728269823
 INFO 15:13:30,572 Enqueuing flush of Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,573 Writing Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,579 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-275-Data.db (163 bytes)
 INFO 15:13:30,581 Node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal
 INFO 15:13:30,598 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 15:13:30,600 Will not load MX4J, mx4j-tools.jar is not in the classpath
","RHEL6, JDK1.6",kmueller,rcoli,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248134,,,Wed Oct 17 19:09:37 UTC 2012,,,,,,,,,,"0|i09lqf:",53954,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Oct/12 19:48;vijay2win@yahoo.com;How about just saying:
Bootstrap completed! Now serving reads.

? Do we need any additional information?;;;","15/Oct/12 19:51;brandon.williams;I think the point is that we should not print it if we didn't actually bootstrap, and we should be able to distinguish between bootstrap/replace/move.;;;","15/Oct/12 19:59;vijay2win@yahoo.com;Move doesnt use the same code anymore, replace uses this but there are other log info explaining that....

If Bootstrap is a wrong word then how about: Startup completed?
(I am still looking for an abstract word :));;;","15/Oct/12 22:10;kmueller;Bootstrap means something specifically with cassandra in that you think some data has streamed in.

I think ""Startup completed"" would be great.

If there IS a bootstrap/replace/move then I think the message ought to specify which has happened and that it's ready now (if it's easy to do) :);;;","16/Oct/12 04:28;vijay2win@yahoo.com;Committed https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/service/StorageService.java;h=8de0bd24632c89ea1b41c952ee6ec2db58808894;hp=7d92fbe0ff15c8c686a93425f4fccca49b921c0b;hb=d525cf969c042b21a9375446f5449ee82d7d1484;hpb=7e937b3d1308c0774e4b0366b6e66b14af1dd5f6

Let me know if you need more info, i will reopen this ticket.;;;","16/Oct/12 20:48;brandon.williams;This isn't quite what I had in mind.  It's not a semantic issue, it's a logical issue.  We should clearly indicate the operation that was actually performed, which after a quick glance at the code means we need to store this state somewhere to do so.;;;","17/Oct/12 19:09;vijay2win@yahoo.com;Had a discussion with Brandon offline, 
There is enough information in the logs to show the operation was Bootstrap vs Repair vs Startup, so closing the ticket for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inet datatype does not work with cqlsh on windows,CASSANDRA-4801,12611640,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jbellis,jbellis,12/Oct/12 21:37,16/Apr/19 09:32,14/Jul/23 05:52,07/Mar/13 17:00,1.1.11,1.2.3,,Legacy/Tools,,,0,cqlsh,windows,,,,,"{noformat}
create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
use foo;
create table one (id int primary key, c int);
TRACING ON;
insert into one (id, c) values (1, 2);

value '\x7f\x00\x00\x01' (in col 'source') can't be deserialized as inet: 'module' object has no attribute 'inet_ntop'
{noformat}","Windows 7, Python 2.7.2",aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/13 03:26;aleksey;4801.txt;https://issues.apache.org/jira/secure/attachment/12572480/4801.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248129,,,Thu Mar 07 17:00:37 UTC 2013,,,,,,,,,,"0|i09lp3:",53948,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"12/Oct/12 21:43;brandon.williams;It looks like we can check socket.has_ipv6 to decide between inet_ntop and inet_ntoa, and if there are more than 4 bytes just display the escaped bytes.  In other words, ipv4 would work on windows but not ipv6.;;;","07/Mar/13 02:44;aleksey;bq. It looks like we can check socket.has_ipv6 to decide between inet_ntop and inet_ntoa, and if there are more than 4 bytes just display the escaped bytes.

I'm thinking about just using inet_ntoa if there are 4 bytes, and inet_ntop if there are more - instead of using inet_ntop for both. It's slightly uglier, but will at least support ipv4 properly on Windows, which is still the most common case.. and let it continue raising an error on Windows when facing ipv6 addresses.

Don't want to just display the escaped bytes - this breaks cqlsh COPY FROM, for example. We could also reimplement inet_ntop and inet_pton in Python ourselves, since they won't be added until Python 3.4, but I'd rather not to - it's not *that* important. Working ipv4 and failing ipv6 on Windows is good enough for me.;;;","07/Mar/13 02:49;brandon.williams;Agreed.;;;","07/Mar/13 03:26;aleksey;Attached the cassandra-dbapi2 patch (committed, but haven't pushed it yet).;;;","07/Mar/13 16:38;brandon.williams;+1;;;","07/Mar/13 17:00;aleksey;Thanks. Committed the fix to cassandra-dbapi2 and updated the bundled cql-internal-only-1.4.0.zip in 1.1/1.2/trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh help is obsolete for cql3,CASSANDRA-4800,12611627,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jbellis,jbellis,12/Oct/12 20:24,16/Apr/19 09:32,14/Jul/23 05:52,01/Nov/12 23:18,1.2.0 beta 2,,,Legacy/Tools,,,0,cql3,,,,,,"For example, new syntax for CREATE KEYSPACE is

create keyspace foo with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}
",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 20:53;aleksey;CASSANDRA-4800-v2.txt;https://issues.apache.org/jira/secure/attachment/12551418/CASSANDRA-4800-v2.txt","30/Oct/12 18:27;aleksey;CASSANDRA-4800.txt;https://issues.apache.org/jira/secure/attachment/12551399/CASSANDRA-4800.txt",,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248112,,,Thu Nov 01 23:18:04 UTC 2012,,,,,,,,,,"0|i09lj3:",53921,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"24/Oct/12 02:01;aleksey;I've noticed that a lot of help topics in cqlsh are outdated, not just ddl ones. outdated is the wrong word here.. they are for cql2 and don't have cql3 analogues. Anything that mentions USING CONSISTENCY for example.

Should they all be handled as part of this issue or should we deal with DDL in this one and create an issue for all others?;;;","24/Oct/12 07:03;slebresne;I've updated the title to reflect my opinion :)

But basically, we need to fix all the cqlsh help for 1.2 release anyway, so I don't see a point in creating many different tickets.;;;","30/Oct/12 18:53;jbellis;Why move everything out of docstrings?  Nice to have it inline IMO.;;;","30/Oct/12 18:57;aleksey;Only 8 commands or so had help in docstrings, everything else wasn't.
I can move (some of) them back - those that are purely cqlsh.
As for everything else, I'd rather have it all in one place.;;;","30/Oct/12 19:03;jbellis;Ah, right.  I skimmed too quickly. :);;;","01/Nov/12 23:18;brandon.williams;Committed v2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion failure in leveled compaction test,CASSANDRA-4799,12611533,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,12/Oct/12 14:47,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/12 14:41,1.1.7,1.2.0,,,,,0,lcs,,,,,,"It's somewhat rare, but I'm regularly seeing this failure on trunk:

{noformat}
    [junit] Testcase: testValidationMultipleSSTablePerLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testValidationMultipleSSTablePerLevel(LeveledCompactionStrategyTest.java:78)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED
{noformat}

I suspect there's a deeper problem, since this is a pretty fundamental assertion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247970,,,Tue Oct 16 14:41:51 UTC 2012,,,,,,,,,,"0|i095zb:",51401,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Oct/12 18:25;yukim;I found two problems here.

1) Test does not completely setup necessary SSTables and causes above AssertionError.

LCS test uses CFS#forceFlush to generate enough SSTables at the beginning of the test, but since the method is asynchronous call, there is a chance that test proceeds without sufficient SSTables to fill up L2 and causes AE at strat.getLevelSize(2) > 0.

Fix for this is to change forceFlush to forceBlockingFlush.
(diff: https://github.com/yukim/cassandra/commit/0d16efc6d7592e61f15598d5a4e3cc81d2760007)

2) Repair sometimes causes AssertionError with LCS

During the test, I encountered below error several times.

{code}
ERROR [ValidationExecutor:1] 2012-10-12 14:39:18,660 SchemaLoader.java (line 73) Fatal exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:183)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:879)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:743)
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:71)
        at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:481)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

AssertionError comes from following assert code in LCS#getScanners:

{code}
for (SSTableReader sstable : sstables)
{
    int level = manifest.levelOf(sstable);
    assert level >= 0;
    byLevel.get(level).add(sstable);
}
{code}

LeveledManifest#levelOf method returns level of SSTable or -1 if SSTable is not yet added to LeveledManifest. Here, each SSTable comes from CF's DataTracker.
Every time SSTable is written by flush/compaction, it gets added to DataTracker and  after that, added to LeveledManifest if you are using LCS.
If above repair code is executed between those two, you will get AssertionError.

My proposed fix is to remove assertion and treat SSTables that do not belong LeveledManifest yet as L0 SSTables.
(diff: https://github.com/yukim/cassandra/commit/c8a0fb9a9128e47ec3d07926eb26f6fd93664f52)

Problem (2) also exists in 1.1 branch, but without assertion. We may want to fix this in 1.1 too.;;;","16/Oct/12 02:16;jbellis;+1

(I wish we could keep that assert but I don't have a better solution.);;;","16/Oct/12 14:41;yukim;Committed to trunk as well as 1.1 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range queries return incorrect results,CASSANDRA-4797,12611454,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,brandon.williams,brandon.williams,11/Oct/12 22:55,16/Apr/19 09:32,14/Jul/23 05:52,26/Oct/12 12:48,1.2.0,,,,,,1,,,,,,,"I've only seen this fail once, but it's quite obviously returning incorrect results since the query is ""SELECT * FROM clicks WHERE userid >= 2 LIMIT 1"" and it's getting userid 0 in return.

{noformat}
======================================================================
FAIL: cql_tests.TestCQL.limit_ranges_test
Validate LIMIT option for 'range queries' in SELECT statements
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/var/lib/buildbot/cassandra-dtest/tools.py"", line 187, in wrapped
    f(obj)
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 302, in limit_ranges_test
    assert res == [[ 2, 'http://foo.com', 42 ]], res
AssertionError: [[0, u'http://foo.com', 42]]
{noformat}",,julien_campan,liqusha,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247820,,,Tue Oct 30 09:55:59 UTC 2012,,,,,,,,,,"0|i08q3r:",48828,,,,,Normal,,,,,,,,,,,,,,,,,"11/Oct/12 23:06;brandon.williams;I take it back, this reproduces reliably.;;;","16/Oct/12 12:18;slebresne;Can you reproduce on current trunk? I can't and I suspect this may have been due to CASSANDRA-4796.;;;","26/Oct/12 12:46;julien_campan;Hi, I have the same problem when using cassandra 1.2 beta 1 with vnode :
I 'm using cql 3.
My table has 66 000 rows : 

{noformat}
cqlsh:pns_fr> select count(*) from syndic  limit 100 ;
count
-------
   100

cqlsh:pns_fr> select count(*) from syndic  limit 1000 ;
count
-------
   999

cqlsh:pns_fr> select count(*) from syndic  limit 10000 ;
count
   9983

cqlsh:pns_fr> select count(*) from syndic limit 100000 ;
count
-------
65883
{noformat};;;","26/Oct/12 12:48;brandon.williams;Passes on trunk now.;;;","30/Oct/12 09:41;julien_campan;Hi, i took the trunk version this morning  and i still have the problem.
I'm using : [cqlsh 2.3.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]

My use case is : 
I create a table 
CREATE TABLE premier (
  id int PRIMARY KEY,
  value int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compression={'sstable_compression': 'SnappyCompressor'};

1) I insert 10 000 000 rows (they are like  id = 1 and value =1)
2) I delete 2 000 000 rows (i use random method to choose the key value)
3) I do select * from premier ; and my result is 7944 instead of 10 000.

So after a lot of delete, the range operator is not working.;;;","30/Oct/12 09:47;slebresne;Hum ok, but that's actually a different problem from what the initial test case was catching (the test was returning a wrong value, the problem you're describing is about select returning fewer results than expected). Would you mind opening a separate ticket with that ""new"" problem?;;;","30/Oct/12 09:55;julien_campan;Ok, i did it 
CASSANDRA-4877 - Range queries return fewer result after a lot of delete ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
composite indexes don't always return results they should,CASSANDRA-4796,12611438,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,11/Oct/12 21:37,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/12 06:41,1.2.0 beta 2,,,,,,0,,,,,,,"composite_index_with_pk_test in the dtests is failing and it reproduces manually.

{noformat}
cqlsh:foo>            CREATE TABLE blogs (                 blog_id int,                 time1 int,                 time2 int,                 author text,                 content text,                 PRIMARY KEY (blog_id, time1, time2)             ) ;
cqlsh:foo> create index on blogs(author);
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 0, 'foo', 'bar1');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (1, 0, 1, 'foo', 'bar2');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (2, 1, 0, 'foo', 'baz');
cqlsh:foo> INSERT INTO blogs (blog_id, time1, time2, author, content) VALUES (3, 0, 1, 'gux', 'qux');
cqlsh:foo> SELECT blog_id, content FROM blogs WHERE time1 = 1 AND author='foo';
cqlsh:foo>
{noformat}

The expected result is:
{noformat}

 blog_id | time1 | time2 | author | content
---------+-------+-------+--------+---------
       2 |     1 |     0 |    foo |     baz
{noformat}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/12 13:37;slebresne;4726.txt;https://issues.apache.org/jira/secure/attachment/12549145/4726.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247803,,,Tue Oct 16 06:41:24 UTC 2012,,,,,,,,,,"0|i08pzr:",48810,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Oct/12 13:37;slebresne;Hum, I think that is due to some bad merge or something along that way. Basically we were using a column value instead of a key because SelectStatement.buildBound() was used on keys but was (wrongfully) using columns internally instead.

Patch attached to change that and that makes buildBound static to make it harder to do that kind of mistake again.;;;","16/Oct/12 01:30;jbellis;+1;;;","16/Oct/12 06:41;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"replication, compaction, compression? options are not validated",CASSANDRA-4795,12611434,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,brandon.williams,brandon.williams,11/Oct/12 21:21,16/Apr/19 09:32,14/Jul/23 05:52,05/Mar/13 11:05,1.2.1,,,,,,0,,,,,,,"When creating a keyspace and specifying strategy options, you can pass any k/v pair you like.",,dbrosius,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/13 12:02;slebresne;0001-Reallow-unexpected-strategy-options-for-thrift.txt;https://issues.apache.org/jira/secure/attachment/12568965/0001-Reallow-unexpected-strategy-options-for-thrift.txt","12/Feb/13 12:02;slebresne;0002-Reallow-unexpected-strategy-options-for-thrift.txt;https://issues.apache.org/jira/secure/attachment/12568966/0002-Reallow-unexpected-strategy-options-for-thrift.txt","12/Feb/13 12:02;slebresne;0003-Adds-application_metadata-field-to-ks-metadata.txt;https://issues.apache.org/jira/secure/attachment/12568967/0003-Adds-application_metadata-field-to-ks-metadata.txt","15/Jan/13 07:24;dbrosius;4795.compaction_strategy.txt;https://issues.apache.org/jira/secure/attachment/12564894/4795.compaction_strategy.txt","01/Dec/12 01:48;dbrosius;4795.replication_strategy.txt;https://issues.apache.org/jira/secure/attachment/12555602/4795.replication_strategy.txt","23/Jan/13 05:53;dbrosius;4795_compaction_strategy_v2.txt;https://issues.apache.org/jira/secure/attachment/12566094/4795_compaction_strategy_v2.txt","24/Jan/13 05:59;dbrosius;4795_compaction_strategy_v3.txt;https://issues.apache.org/jira/secure/attachment/12566255/4795_compaction_strategy_v3.txt",,,,,,,,,,,7.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247798,,,Tue Mar 05 11:05:28 UTC 2013,,,,,,,,,,"0|i08pxr:",48801,,slebresne,,slebresne,Low,,,,,,,,,,,,,,xedin,,,"11/Oct/12 21:22;jbellis;We've been WARNing about bad options since 1.1.1 (CASSANDRA-4046) so I would think making it stricter would be fair game for 1.2.;;;","11/Oct/12 22:46;brandon.williams;I didn't realize we were warning about it already.  It's probably better to leave it at that since we've allowed this for so long in case someone is relying on it.;;;","11/Oct/12 23:01;jbellis;I still want to fix it -- as we found out today, it's definitely confusing when you think an option belongs to replication strategy but it actually belongs to the keyspace -- but we can push it out past 1.2.;;;","31/Oct/12 18:50;slebresne;I think that ticket was initially targeting the replication strategy, but there is also a lack of validation for the compaction strategies. In that case, we don't even validate that the value of a know option is correct (or rather, for some of the options we log a warning in the log but just carry on, and AbstractCompactionStrategy.tombstoneThreshold is not validated at all. I do not that in the case of the compaction strategy, we don't currently build the compaction strategy until it's actually needed (i.e. when migration are apply), so we shouldn't ""just"" throw an exception in the constructor: we need to do as we do for the replication strategy in CreateKeyspaceStatement, i.e. do a trial run of building the strategy to make sure everything's ok. 

I do think we should correctly validate options (and by that I mean doing more than a warning in the log) for both replication and compaction strategy for 1.2. At least we should do it for CQL3 if we've extremely afraid of breaking thrift (but I don't really buy that people would rely on our lack of validation).  ;;;","12/Nov/12 05:02;jbellis;Can you take a stab at Sylvain's proposal, Dave?;;;","01/Dec/12 01:47;dbrosius;oops, just saw this...

part1, replication strategy validation
4795.replication_strategy.txt

(more to come);;;","09/Jan/13 10:20;slebresne;+1 on part1, committed).

Dave, are you still planning on working on the compaction and compression validation? ;;;","11/Jan/13 06:29;dbrosius;yeah, sorry. fell asleep at the wheel.;;;","15/Jan/13 04:28;dbrosius;so, for compaction options, it appears to me that you can't create a CompactionStrategy object to do validation, because the ColumnFamilyStore needs to exist for the ACS constructor, which means it's too late to safe guard invalid options, right?

The validation would need to be in CFMetaData i think, and CFMetaData would validate options needed by fetching the valid set from a static call thru reflection on the compactionStrategyClass....

which seems ugly... so am i missing something easier?;;;","15/Jan/13 04:59;jbellis;What if we required a static {{validate}} method on ACS subclasses that we call with the options and maybe CFMetaData?  Ugly, since there's no polymorphism on statics, but since we're doing ACS creation via reflection anyway it's not really a problem.

Edit: which is pretty much what you suggested with the difference being where the logic lives.  Either way WFM.;;;","15/Jan/13 05:22;dbrosius;the 'right'?? solution perhaps is to remove the cfs from AbstractCompactionStrategy ctor, and instead pass it into the various methods that need it. Problem there is that you'd need a 'postCreateInitialize(ColumnFamilyStore cfs)' on ACS, as SizeTiered and Leveled do things with cfs in the ctor, that would need to move to that method.

It would also break backwards compatibility with potential client ACS's.;;;","15/Jan/13 05:29;jbellis;That's what I thought at first but ACS is free to maintain state as well, e.g. LCS's manifest.  It would suck to turn those all into Map<CFS, X>.  Alternativley you could split off the construction into a StrategyFactory, which IMO is clunkier than the reflection idea.;;;","15/Jan/13 05:34;dbrosius;ok, and just fail with ConfigurationException if that static method doesn't exist?;;;","15/Jan/13 05:43;jbellis;I'd no-op it for backwards compatibility with user-provided ACS implementations.;;;","15/Jan/13 07:25;dbrosius;validate compaction options. in 4795.compaction_strategy.txt

pretty ugly... if you have suggestions, please do.;;;","15/Jan/13 10:39;slebresne;On the compaction strategy patch:
* I would move the call to {{validateCompactionOptions}} in {{CFPropDefs.validate()}}.  First because ALTER also need the validation and putting it in CFPropDefs handles both ""for free"". And also because there is no guarantee that {{CFPropDefs.compactionStrategyClass}} won't be {{null}} which I think is not handled correctly by the patch (and moving things to {{CFPropDefs.validate()}} makes it easier to deal with).
* We should not only validate that there is no unknown option provided, but we should also move the validation of the options themselves from the ctors to the validateOptions method. Also, I'd prefer adding a validateOptions to AbstractCompactionStrategy to handle the tombstone related option and have subclasses call that method explicitly rather than handling everything in the sublcasses.
;;;","23/Jan/13 03:19;dbrosius;not sure where CFPropDefs.validate gets used with cql3

ah, never mind. two CFPropDefs.;;;","23/Jan/13 10:23;slebresne;Patch looks good but a few remaining remarks/nits:
* I think the comment in AbstractCompactionStrategy ctor is a bit confusing as we don't really fully repeat all the ""checks"" (we don't catch NumberFormatException nor validate the tombstoneThreshold value). So I agree we should probably repeat the checks, but let's repeat them fully.
* In SizeTiered validation, we might want to catch NumberFormatException.
* Since validateOptions is supposed to return a map of the options it don't know about, let's maybe check it's empty at the end of CFMetaData.validateCompactionOptions and get rid of the individual checks in Leveled and SizeTiered (which avoid needing VALID_OPTIONS).
* Might I suggest adding a few static getInt(map, property_name, defaut_value), getDouble, ... helper methods to simplify stuff like
{noformat}
try
{
    String optionValue = options.get(MIN_SSTABLE_SIZE_KEY);
    long minSSTableSize = optionValue == null ? DEFAULT_MIN_SSTABLE_SIZE : Long.parseLong(optionValue);
}
catch (NumberFormatException e)
{
    ...
}
{noformat}
* In cql3.CFPropDefs, we should move the validateCompactionOptions line inside the preceding 'if' statement.
* In cql3.CreateColumnFamilyStatement, let's remove the validate method override since it's not doing anything.

As a side note, there's a bit too much line changed not related to the patch to my taste. I understand this is your editor doing that automatically, but while reordering imports is ok, removing static imports to inline the class name in the code (in CFMetaData) is less justified imo.
;;;","23/Jan/13 13:27;dbrosius;{quote}Since validateOptions is supposed to return a map of the options it don't know about, let's maybe check it's empty at the end of CFMetaData.validateCompactionOptions and get rid of the individual checks in Leveled and SizeTiered (which avoid needing VALID_OPTIONS).{quote}


If user supplied compaction strategies derive from these classes, that won't work.;;;","23/Jan/13 13:40;slebresne;bq. If user supplied compaction strategies derive from these classes

Why? tTe contract of validateOptions would be that it should return a list with *only* the options it doesn't know about (which is kind of what it is supposed to do (and does) in your patch already anyway). Thus if a user supplied compaction derives from an existing one, it would have to either 1) return an empty map if he wants to ignore on purpose options or 2) call the super class validateOptions first (which is what Leveled and SizeTiered do with AbstractStrategy). Of course, in the case where the compaction strategy do not implement a validateOptions, we should skip the check for invalid options, but that's no different from what the patch does now.;;;","23/Jan/13 15:19;dbrosius;yeah, your right. i shouldn't form opinions early in the morning.;;;","24/Jan/13 17:05;slebresne;Alright, +1, committed (I did move the unrecognized option check inside CFMetadata so all interface gets it).;;;","11/Feb/13 18:55;xedin;Ok, so changing this in 1.2.1 probably was a bit unfair to people, because e.g. Titan relies on providing custom strategy option 'titan-version' and it can't do custom strategy because there is no control of what version people install, can we please start warning people about unrecognized options in the strategy again instead of restricting them, because those options are sometimes very convenient to safe meta-information.;;;","11/Feb/13 19:29;xedin;Also, All of the old users how were adding custom properties to replication strategy (at least) wouldn't be able to start-up after upgrade to 1.2.1 because validation is also done on Table initialization.

{noformat}
java.lang.RuntimeException: org.apache.cassandra.exceptions.ConfigurationException: Unrecognized strategy option {titan-version} passed to SimpleStrategy for keyspace titan
	at org.apache.cassandra.db.Table.<init>(Table.java:269)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.Table.open(Table.java:88)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:223)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:379)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:422)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Unrecognized strategy option {titan-version} passed to SimpleStrategy for keyspace titan
	at org.apache.cassandra.locator.AbstractReplicationStrategy.validateExpectedOptions(AbstractReplicationStrategy.java:281)
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:72)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:234)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:288)
	at org.apache.cassandra.db.Table.<init>(Table.java:265)
	... 5 more
{noformat};;;","12/Feb/13 07:26;slebresne;bq. changing this in 1.2.1 probably was a bit unfair to people

If you mean ""in a minor release"", then I agree, mea culpa.

But on the principle, I disagree that we should allow people to shove random metadata in the stategy options. Especially because that means not validating correctly the actual strategy options (and no, warning in the log is not a good enough because 1) application developer may not have easy access to the server log and 2) asking people to check the server log when they do query to see if they hadn't mispelled some option is a ridiculously bad user experience).

If application want to store data, they should create a column family and store data in there. But to be clear, I'm open to discussing adding some new application_metadata field to the column family metadata that would explicitely be uninterpreted by Cassandra (not that I'm particularly thrilled by the idea, but I'm at least open to it), but the strategy options is just not the right place for that.

So I'm fine reverting this from 1.2 (in which case I'd still prefer keeping the validation at least on the CQL3 side), not so much from trunk.
;;;","12/Feb/13 07:42;xedin;bq. If you mean ""in a minor release"", then I agree, mea culpa.

That's exactly what I mean.

bq. 1) application developer may not have easy access to the server log 2) asking people to check the server log when they do query to see if they hadn't mispelled some option is a ridiculously bad user experience

If the original intention was to protect people from mistypes then maybe we need to put more work and throw exception only if typed in is similar to what it should be, like git does e.g. ""option rplicatian_factor is not recognized maybe you mean 'replication_factor'?"" ? Requiring those options to be typed in by the user especially with such a long names that we have already is a ""ridiculously bad user experience"", especially when those names/options change frequently between releases.

bq. If application want to store data, they should create a column family and store data in there. But to be clear, I'm open to discussing adding some new application_metadata field to the column family metadata that would explicitely be uninterpreted by Cassandra (not that I'm particularly thrilled by the idea, but I'm at least open to it), but the strategy options is just not the right place for that.

So in the case of Titan when they just want to save one 'last seen version' attribute, creating a separate column family or even having that per-cf doesn't make any sense but it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives) because it's not updated or even read that often.;;;","12/Feb/13 08:00;slebresne;bq. If the original intention was to protect people from mistypes

How can that *not* be one's intent?

bq. like git does e.g. ""option rplicatian_factor is not recognized maybe you mean 'replication_factor'?""

Sure, that would be great: ""reject errors with fix suggestion in the error message"" is indeed even better than ""reject errors"" that is itself better than ""not reject errors silently"". So be my guest, do open a ticket to do even better than what this patch does. But that patch is still progress.

bq. it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives)

Again, I'm ok talking about alternatives that would be right if there is a need, but you will not convince me that ""shoving random application data inside the strategy options"" is right, or even a good idea.
;;;","12/Feb/13 08:17;xedin;I don't think this is about right or wrong but rather about living by means instead of re-inventing broken bicycle. I'm not arguing or trying to convince, I'm simply saying that committed patch made situation even worse and it should be reconsidered all together. Also, if somebody ever had mistyped and didn't fix or used unrecognized attributes in replication strategy or compaction, after upgrade to 1.2.1 Cassandra just *wouldn't start up* which is also a ""ridiculously bad user experience"", this is why I think this patch should be reverted.

Edit: I think that concerns DSE as well, because AFAIK they used custom attributes in the compaction strategy as well as in keyspace replication.;;;","12/Feb/13 12:02;slebresne;I'm a little bit confused about what you are trying to say. If you're saying we've broke stuff in a minor release, it's bad, we should revert the breakage, then I though I had made it pretty clear that I agree. But for 2.0 onwards, allowing to store random data in the replication strategy option *is* wrong: it doesn't even work with NTS (which will interpret the random data as a datacenter).

But anyway, less arguing more doing, I'm attaching patches that do the following:
* patch 00001 re-allow unknown options in the replication strategy options on the thrift side (but not for CQL3 as I doubt there is any legacy CQL3 app using this since CQL3 is kind of new). It also allows startup if there was unknown options in the first place (that part was definitevely an oversight, the compaction strategy is clearly making sure to only log a warning in that case for instance).
* patch 00002 does the same for the compaction strategy.
* patch 00003 is my suggestion for an alternative to the problem ""I want to attach some tiny piece of metadata to a keyspace and creating a CF for that is overkill"" (for that patch the thrift file must be regen but I don't attach that). Could totally go into a separate ticket.

Those patches are against 1.2 but to be clear, my intent is to keep disallowing unknown options even for thrift in trunk.;;;","12/Feb/13 15:36;jbellis;bq. So in the case of Titan when they just want to save one 'last seen version' attribute, creating a separate column family or even having that per-cf doesn't make any sense but it's pretty convenient to keep it in the keypsace strategy options (without have any other alternatives) because it's not updated or even read that often.

No doubt everyone practicing this extremely awful hack has a perfectly good excuse. :)

But fundamentally creating a last_seen_versions table with cfname (ksname?) is simply not that onerous and is the Right Thing To Do.

bq. patch 00003 is my suggestion for an alternative to the problem ""I want to attach some tiny piece of metadata to a keyspace and creating a CF for that is overkill""

-1 from me, I get that change is painful for people who are abusing schema right now but we really shouldn't be encouraging this.;;;","12/Feb/13 22:33;xedin;bq. But fundamentally creating a last_seen_versions table with cfname (ksname?) is simply not that onerous and is the Right Thing To Do.

For that specific case it's actually not worth it because it's always a single version as there is no reason to keep older version around, so creating separate CF for one row with empty value is overkill as specially as it's even not read that often and query wouldn't be as convenient as it was with describe keyspace for that single piece of data.

bq. -1 from me, I get that change is painful for people who are abusing schema right now but we really shouldn't be encouraging this.

It's usually one/two attributes that go there so I think it's pretty convenient to store it that way instead of dealing with separate CF/querying, but I'm welcome to new ideas;;;","12/Feb/13 22:39;jbellis;Show me an existing database that lets you attach random crap to a table definition and I will reconsider my position. :);;;","13/Feb/13 08:03;slebresne;Do we agree on the first 2 patches in the meantime?;;;","01/Mar/13 13:38;slebresne;So, does the absence of review of my 2 first patches mean that everyone decided the status quo was ok? Cause I could live with that. But if we think this should be reverted from 1.2 then let's do that sooner than later.;;;","01/Mar/13 19:24;xedin;bq. Show me an existing database that lets you attach random crap to a table definition and I will reconsider my position. 

What is HBase?

bq. So, does the absence of review of my 2 first patches mean that everyone decided the status quo was ok? Cause I could live with that. But if we think this should be reverted from 1.2 then let's do that sooner than later.

I'm fine with those, I'm fine even with removing those as soon as system doesn't crap on startup.;;;","04/Mar/13 17:39;jbellis;bq. I'm fine with those

To clarify, is that ""I'm fine in principle"" or ""I've reviewed it, +1?""
;;;","04/Mar/13 20:52;xedin;I've reviewed those two, +1.;;;","05/Mar/13 11:05;slebresne;Alright. I committed the 2 first patch. For trunk however, I've kept the behavior of rejecting unknown options (though Cassandra won't refuse to start if there is unknown option in the first place), and I've added a proper not in the NEWS file for that.;;;",,,,,,,,,,,,,,,,,,,,,
Commitlog files replayed but not in the order of their ids,CASSANDRA-4793,12611368,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,11/Oct/12 15:10,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 21:23,1.2.0 beta 2,,,,,,0,commitlog,,,,,,"I noticed that the commitlog files were not replayed in the order of their ids.
It seems that they are sorted by ""last modification date"" before being replayed, but this does not corresponds to their ids.

Moreover, ""last modification date"" is changed when a file is copied, so, this could also change the order of archived commitlogs.

Maybe it's safer to sort them using the id in the file name ?
",,frousseau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 15:12;frousseau;4793-potential-patch-for-correctly-ordering-commit-log-fi.patch;https://issues.apache.org/jira/secure/attachment/12548751/4793-potential-patch-for-correctly-ordering-commit-log-fi.patch","12/Oct/12 09:36;frousseau;4793-trunk-potential-patch-for-correctly-ordering-commit-log-fi.patch;https://issues.apache.org/jira/secure/attachment/12548874/4793-trunk-potential-patch-for-correctly-ordering-commit-log-fi.patch",,,,,,,,,,,,,,,,2.0,frousseau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247643,,,Mon Oct 22 21:23:54 UTC 2012,,,,,,,,,,"0|i08mw7:",48308,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Oct/12 20:30;jbellis;Can you rebase to trunk?;;;","12/Oct/12 09:36;frousseau;Sure,

Attached the patch rebased to trunk;;;","12/Oct/12 15:27;jbellis;committed, thanks!;;;","22/Oct/12 21:23;jbellis;belatedly marking resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digest mismatch doesn't wait for writes as intended,CASSANDRA-4792,12611365,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Oct/12 14:54,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/12 20:07,1.1.6,1.2.0 beta 2,,,,,0,,,,,,,"As reported by Niklas Ekström on the dev list:

I’m looking in the file StorageProxy.java (Cassandra 1.1.5), and line 766 seems odd to me.

FBUtilities.waitOnFutures() is called with the repairResults from the RowRepairResolver resolver.

The problem though is that repairResults is only assigned when the object is created at line 737 in StorageProxy.java, and there it is assigned to Collections.emptyList(), and in the resolve() method in RowRepairResolver, which is indirectly called from line 771 in StorageProxy.java, that is, after the call to FBUtilities.waitOnFutures().

So the effect is that line 766 in StorageProxy.java is essentially a no-op.",,christianmovi,mauzhang,rcoli,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 14:55;jbellis;4792.txt;https://issues.apache.org/jira/secure/attachment/12548749/4792.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247569,,,Thu Oct 18 20:10:38 UTC 2012,,,,,,,,,,"0|i08fbj:",47081,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"11/Oct/12 14:55;jbellis;patch attached.;;;","11/Oct/12 17:40;vijay2win@yahoo.com;+1;;;","11/Oct/12 20:07;jbellis;committed;;;","17/Oct/12 23:59;rcoli;Is this bug's practical effect summarizable as :

""Because read repair didn't actually wait around for repair writes to ack, there was a non-zero chance of multiple concurrent read requests to a needing-repair key triggering multiple repair writes?"";;;","18/Oct/12 20:10;jbellis;That, and ""it was possible for a quorum read to return an old value, even after a quorum read had returned a newer value [but the newer value had not yet been sent to the other replicas]."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage.getNextWide produces corrupt data,CASSANDRA-4789,12611242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,oberman,oberman,oberman,10/Oct/12 20:48,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/12 19:01,1.1.6,,,,,,0,,,,,,,"This took me a while to track down.  I'm seeing the problem when the ""key changes"" case happens.  The intended behavior (as far as I can tell) when the key changes is the method returns the current tuple, and picks up where it left off on the next call to getNextWide().  The problem I'm seeing is the sometimes the current key advances between method calls, sometimes not.  ""Not"" being the correct behavior, since the code is saving the value into an instance variable, but when the key advances there is a key/value mismatch (the result being the values for two different keys are being glued together).  I think the problem might be related to keys that only have a single column???  I'm still trying to track that down to help assist in solving this case...

Maybe this will be clearer from me pasting a bunch of logging I added to the class.  The log messages are fairly self documenting (I hope):  

...lots of previous logging...
enter getNextWide
hasNext = true
set key = dVNhbXAxMzQ3ODM1OA%3D%3D
lastRow != null
added 1 items to bag from lastRow
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 669392df09572d0045b964bc65f86a2c
exit getNextWide
enter getNextWide
hasNext = true
//!!!THIS IS THE PROBLEM HERE I THINK!!!
//!!!Usually the key here == key before ""exit getNextWide""!!!
set key = 5f900ee4bb1850f8cf387cc3d5fc23ca
//!!! lastRow is data for 669392df09572d0045b964bc65f86a2c !!! 
//!!! but it's being added to key 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
lastRow != null
added 1 items to bag from lastRow
//!!! Here are the real values for 5f900ee4bb1850f8cf387cc3d5fc23ca !!!
added 1 items to bag from row
hasNext = true
added 1 items to bag from row
hasNext = true
key changed, new key = 50438549-cdb6-8c44-f93a-d18d7daeffd8
exit getNextWide
enter getNextWide
hasNext = true
set key = 50438549-cdb6-8c44-f93a-d18d7daeffd8",,oberman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 15:22;oberman;patch.txt;https://issues.apache.org/jira/secure/attachment/12548752/patch.txt","11/Oct/12 14:55;oberman;patch.txt;https://issues.apache.org/jira/secure/attachment/12548748/patch.txt",,,,,,,,,,,,,,,,2.0,oberman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247116,,,Thu Oct 11 19:01:22 UTC 2012,,,,,,,,,,"0|i07z87:",44474,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"10/Oct/12 20:50;oberman;Apparently using !!! is some kind of special syntax, sorry.  I was just trying to distinguish my comments from my logging.;;;","10/Oct/12 21:05;oberman;I'm now 99% sure the problem is keys that map to a single column are being skipped over, and their values glued into the key after them.  But I'm not sure the most elegant fix...;;;","10/Oct/12 21:33;jbellis;Are you mutating the ByteBuffers returned by CFIF?  That could definitely produce this kind of problem.;;;","11/Oct/12 14:04;oberman;It's definitely a problem using widerows=true and having keys that only map to one column.  Here is my patch on 1.1.5 (I also have a one line patch to fix the wide rows bug):

112a113
>     private ByteBuffer lastKey;
116d116
< 
153a154
> 			    //check key == lastKey?
159a161
> 			lastKey = null;
177a180
> 		    lastKey = (ByteBuffer)reader.getCurrentKey();
185a189,200
> 		    if(lastKey != null && !(key.equals(lastKey))) // last key only had one value
> 		    {
> 			tuple.append(new DataByteArray(lastKey.array(), lastKey.position()+lastKey.arrayOffset(), lastKey.limit()+lastKey.arrayOffset()));
> 			for (Map.Entry<ByteBuffer, IColumn> entry : lastRow.entrySet())
> 			{
> 			    bag.add(columnToTuple(entry.getValue(), cfDef, parseType(cfDef.getComparator_type())));
> 			}
> 			tuple.append(bag);
> 			lastKey = key;
> 			lastRow = (SortedMap<ByteBuffer,IColumn>)reader.getCurrentValue();
> 			return tuple;
> 		    }
194a210
> 		    lastKey = null;
551c567
<             widerows = Boolean.valueOf(System.getProperty(PIG_WIDEROW_INPUT));
---
>             widerows = Boolean.valueOf(System.getenv(PIG_WIDEROW_INPUT));;;;","11/Oct/12 14:51;brandon.williams;Could you post your patches as attachments?;;;","11/Oct/12 14:55;oberman;Sorry about that, new to this.;;;","11/Oct/12 14:59;brandon.williams;Thanks, but this doesn't appear to be in patch format:

patch: **** Only garbage was found in the patch input.
;;;","11/Oct/12 15:02;oberman;I don't really have a proper development environment for cassandra, so it's a diff of the file...  I'll see how to get something better going.;;;","11/Oct/12 15:22;oberman;Does this work?;;;","11/Oct/12 15:27;brandon.williams;It applies, thanks.;;;","11/Oct/12 15:30;oberman;Good!  At some point I'll actually try to figure out how to setup a real development environment, as I feel really uncomfortable showing code without unit tests.;;;","11/Oct/12 19:01;brandon.williams;Committed the relevant portion of this with formatting fixes.  I'll leave the PIG_WIDEROW_INPUT fix to CASSANDRA-4749, though it's needed to test this patch.  Thanks, Will!  I know this function is especially tricky.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming can put files in the wrong location,CASSANDRA-4788,12611230,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,10/Oct/12 19:35,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 21:48,1.2.0 beta 2,,,,,,2,,,,,,,"Some, but not all streaming incorrectly puts files in the top level data directory.  Easiest way to repro that I've seen is bootstrap where it happens 100% of the time, but other operations like move and repair seem to do the right thing.",,azotcsit,mauzhang,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/12 23:07;yukim;4788.txt;https://issues.apache.org/jira/secure/attachment/12550102/4788.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247101,,,Mon Nov 12 15:19:14 UTC 2012,,,,,,,,,,"0|i07z47:",44456,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"19/Oct/12 23:07;yukim;You are right. Streaming writes file directly under the data directory.
Patch attached.;;;","19/Oct/12 23:12;brandon.williams;Can you explain why this only seemed to affect bootstrap?;;;","22/Oct/12 15:53;yukim;Actually, it does regardless of streaming type. I observed it with sstableloader.
SSTables are put in right place when they get compacted, so maybe that's why you only see when bootstrapping.;;;","22/Oct/12 21:22;brandon.williams;Thanks. +1;;;","22/Oct/12 21:48;yukim;Committed in 8c99a376980b90faf7b1ca1aa33d9fde0a356cda.;;;","12/Nov/12 15:11;jbellis;What would have caused this that is 1.2-specific?  Superficially this sounds like it is related to CASSANDRA-2749.;;;","12/Nov/12 15:19;yukim;This came from CASSANDRA-4292, so it was only affected on 1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in migration stage after creating an index,CASSANDRA-4786,12611102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,10/Oct/12 01:58,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/12 06:41,1.2.0 beta 2,,,,,,0,,,,,,,"The dtests are generating this error after trying to create an index in cql2:

{noformat}

ERROR [MigrationStage:1] 2012-10-09 20:54:12,796 CassandraDaemon.java (line 132) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:1] 2012-10-09 20:54:12,797 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:714)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:816)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1656)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 13 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat}",,mharris,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/12 08:50;slebresne;4786.txt;https://issues.apache.org/jira/secure/attachment/12549121/4786.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,246571,,,Thu Jan 24 01:54:43 UTC 2013,,,,,,,,,,"0|i07rqn:",43260,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"10/Oct/12 20:04;xedin;[~brandon.williams] Can you please elaborate which of dtests indicates that? I tried cloning riptano/cassandra-dtests and running secondary_indexes_test.py on trunk but it can't even start a node because of cassandra.yaml changes...;;;","10/Oct/12 20:06;brandon.williams;It's actually snapshot_test in concurrent_schema_changes_test.py that does it about 50% of the time.;;;","10/Oct/12 20:09;xedin;Interesting, I will try to deal with cassandra.yaml problem and run those tests then.;;;","10/Oct/12 20:09;brandon.williams;I had to comment out the native_transport_address line in ccmlib/node.py to get it to run.;;;","11/Oct/12 20:15;brandon.williams;Here is a similar error I've seen when creating a CF:

{noformat}
ERROR [InternalResponseStage:1] 2012-10-11 15:12:16,695 CassandraDaemon.java (line 132) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:500)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:434)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:66)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:44)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat};;;","11/Oct/12 22:14;brandon.williams;Also doesn't appear to be tied to cql2, it happens with cql3 index creation too sometimes:

{noformat}

ERROR [Thrift:1] 2012-10-11 17:09:39,616 CustomTThreadPoolServer.java (line 214) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:348)
    at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:238)
    at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:209)
    at org.apache.cassandra.cql3.statements.CreateIndexStatement.announceMigration(CreateIndexStatement.java:113)
    at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:82)
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
    at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1658)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:344)
    ... 15 more
Caused by: java.lang.NullPointerException
    at org.apache.cassandra.db.ColumnFamilyStore.reload(ColumnFamilyStore.java:162)
    at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:549)
    at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:479)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:344)
    at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:256)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
{noformat};;;","12/Oct/12 01:11;xedin;It looks like it's not a good idea to try to switch memtables as part of the CF reload process as NPE happens in CFS.reload():162 which is related to ability to change CF comparators feature (introduced in CQL3?), we can probably minimize the effect by actually somehow checking if comparator was changed before requesting memtable switch...;;;","12/Oct/12 15:02;brandon.williams;If what we're seeing here is racy memtable switch behavior, then this is only being seen by schema changes since they flush two tables so close together, so we have a greater overall problem.  [~slebresne], can you take a look?;;;","15/Oct/12 08:50;slebresne;So the NPE is because when we switch the memtable during reload, there could be a race where some other thread flush the current memtable first and thus maybeSwitchMemtable returns null.

This can be fixed by storing the comparator at the time a memtable is created, and when we try to change the memtable in reload, try switching the memtable until we know it has the right comparator (this also has the advantage that we won't switch the memtable unless there has been a comparator change). Patch attached to implement that.

Note that the patch also switch all the non-final variables from CFMetadata to volatile as they are definitively accessed from multiple threads.;;;","15/Oct/12 21:19;xedin;+1;;;","16/Oct/12 06:41;slebresne;Committed, thanks;;;","23/Jan/13 23:23;mharris;Do you guys know what version this bug was introduced in?  This is affecting us, but we're hesitant to move to 1.2 so soon after release.  Do you have a recommendation for a version of cassandra to use that would not be affected by this?;;;","23/Jan/13 23:59;jbellis;This bug was introduced in 1.2.0 beta1.;;;","24/Jan/13 00:21;mharris;Weird, we're seeing this on 1.1.6.  Do you know of a fix we could try for this on that version?  We saw a similar bug had been fixed for 1.1.1, so it's surprising to see it again in 1.1.6.  I'll double check our deployment setup, but I'm pretty sure that's the case.

;;;","24/Jan/13 01:54;mharris;Apologies, I think I was barking up the wrong tree here.  The stack trace I'm seeing actually matches CASSANDRA-4219, which looks similar but isn't quite the same thing, so I'll start being obnoxious on that thread instead.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in cql3 select,CASSANDRA-4783,12611037,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,09/Oct/12 18:39,16/Apr/19 09:32,14/Jul/23 05:52,23/Oct/12 08:31,1.2.0 beta 2,,,,,,0,,,,,,,"Caused by 'select * from foo where key='blah' and column in (...)

{noformat}
ERROR 18:35:46,169 Exception in thread Thread[Thrift:11,5,main]
java.lang.AssertionError
        at org.apache.cassandra.cql3.statements.SelectStatement.getRequestedColumns(SelectStatement.java:443)
        at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:312)
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:200)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:125)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1658)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:196)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Causes cqlsh to hang forever.",,iflatness,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/12 08:22;slebresne;0001-Fix-validation-of-IN-queries.txt;https://issues.apache.org/jira/secure/attachment/12549119/0001-Fix-validation-of-IN-queries.txt","15/Oct/12 08:22;slebresne;0002-Fix-mixing-list-set-operation-and-regular-updates.txt;https://issues.apache.org/jira/secure/attachment/12549120/0002-Fix-mixing-list-set-operation-and-regular-updates.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,246240,,,Tue Oct 23 08:31:07 UTC 2012,,,,,,,,,,"0|i07j8f:",41883,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Oct/12 00:28;iflatness;I have also found this issue when running update or insert on an existing column with both list and noncollection modifications. 
For example ""update my_table insert set list_col = [...], noncollection_col = 'val' where key='row_key';""

{noformat}
ERROR [Thrift:1] 2012-10-12 17:08:10,911 CassandraDaemon.java (line 132) Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.getCollection(ColumnGroupMap.java:85)
	at org.apache.cassandra.cql3.statements.UpdateStatement.mutationForKey(UpdateStatement.java:251)
	at org.apache.cassandra.cql3.statements.UpdateStatement.getMutations(UpdateStatement.java:134)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:83)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:138)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1677)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:193)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat};;;","15/Oct/12 08:22;slebresne;The reason for the assertionError in select is that we were not validating correctly IN queries. But basically, we don't support yet the kind of IN queries that was attempted. We certainly should, and now that we can do multi-slice queries we should, but that's on the todo list. So attaching a patch that just fix the validation for now, and let's leave lifting the limitation to another ticket.

Ian's assertion error is unrelated but is fairly easy to fix so attaching a second patch for that too (not sure it's worth the trouble of spawning a separate ticket but we can if someone prefers it).;;;","22/Oct/12 21:21;jbellis;+1;;;","23/Oct/12 08:31;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commitlog not replayed after restart,CASSANDRA-4782,12611021,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,frousseau,frousseau,09/Oct/12 16:51,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/12 13:59,1.1.6,,,,,,0,,,,,,,"It seems that there are two corner cases where commitlog is not replayed after a restart :

 - After a reboot of a server + restart of cassandra (1.1.0 to 1.1.4)
 - After doing an upgrade from cassandra 1.1.X to cassandra 1.1.5

This is due to the fact that the commitlog segment id should always be an  incrementing number (see this condition : https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReplayer.java#L247 )

But this assertion can be broken :
In the first case, it is generated by System.nanoTime() but it seems that System.nanoTime() is using the boot time as the base/reference (at least on java6 & linux), thus after a reboot, System.nanoTime() can return a lower number than before the reboot (and the javadoc says the reference is a relative point in time...)
In the second case, this was introduced by #4601 (which changes System.nanoTime() by System.currentTimeMillis() thus people starting with 1.1.5 are safe)

This could explain the following tickets : #4741 and #4481
",,arya,brandon.williams,christianmovi,frousseau,htoulan,omid,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/12 17:01;jbellis;4782.txt;https://issues.apache.org/jira/secure/attachment/12548588/4782.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,246195,,,Thu Oct 29 08:57:37 UTC 2015,,,,,,,,,,"0|i07i2n:",41695,,frousseau,,frousseau,Critical,,,,,,,,,,,,,,,,,"09/Oct/12 17:33;brandon.williams;Thanks for the summary.  What are you proposing we do?;;;","10/Oct/12 08:44;frousseau;To solve the first case, it's probably better, when possible, to upgrade to a newer version, then rewrite all SSTables (or at least all SSTables metadata)
For the second case, just rewrite all SSTables (or at least all SSTables metadata)

Each SSTable metadata contains the ReplayPosition (and the max is taken to know which commitlog to replay), thus if System.nanoTime() returned a number which is a timestamp in the future, previous commitlogs will be ignored),
thus a drain should prevent from losing data (because there is no commitlog to replay).
And because SSTables are immutables, then rewriting them completely seems a better option (rather than modifying previously written metadata)

Maybe the simplest to do is to have a new option to nodetool (or a new option to nodetool upgradesstables) which only changes the metadata using the following rule :
if the replayPosition.segment of the SSTable is in the future, then reset it to NONE otherwise, let it to its current value. (NONE is valid value if node was drained and restarted)

By using this rule :
 - if a sstable was generated previously using a higher System.nanoTime() then it is reset
 - if a sstable was generated previously using a lower System.nanoTime() OR System.currentTimeMillis() then it is left as is
 - if a sstable is generated between the start & this rewriting process, then it is left as is

Thus the upgrade should be :
 - drain node
 - upgrade
 - start
 - run the process described above

What do you think ?
;;;","10/Oct/12 16:52;jbellis;Thanks, Fabien.  I admit that I didn't realize at first the implications of the millis fix on existing sstable metadata.

Patch attached that bumps the sstable version to hf as a marker that we know metadata with that version has sane replay positions.  Replay positions from older metadata will be treated as NONE.

This will force a full replay the first restart on 1.1.6; afterwards, any newly flushed sstables will have the sane-replay-position marker and future restarts will not need to replay data unnecessarily.

What do you think?;;;","10/Oct/12 17:01;jbellis;(patch revised to update CURRENT_VERSION as well);;;","11/Oct/12 12:08;frousseau;Thanks for the patch. This solution is more simple and elegant than the one I proposed.

I tested it and it worked like a charm.
Nevertheless, if there are counters CF, a drain is probably necessary to avoid replaying the full commitlog and avoid having overcounts. (I don't think it is a problem, just something to know before the upgrade...);;;","11/Oct/12 12:44;frousseau;By the way, I just noticed that the commitlog files were not replayed in the order of their ids.
It seems that they are sorted by ""last modification date"" before being replayed, but this does not corresponds to their ids.
Moreover, ""last modification date"" is changed when a file is copied, so, this could also change the order of archived commitlogs.

I suppose the sort order of commit log files is for schemas ?

Maybe it's safer to sort them using the id in the file name ?;;;","11/Oct/12 13:59;jbellis;Committed with a warning in NEWS to drain.

Go ahead and open a separate ticket to fix sort order.;;;","15/Oct/12 17:05;rcoli;If drain is required between versions to avoid this issue then CASSANDRA-4446, where drain sometimes doesn't actually drain, seems to have become more significant.;;;","16/Oct/12 12:00;omid;CASSANDRA-4446 does not happen to me any more (so far) when restarting 1.1.6 into 1.1.6. I could observe CASSANDRA-4446 on upgrade from 1.1.3 to 1.1.6 though.;;;","18/Oct/12 16:35;arya;+1. I think the issue I had with data loss after node restart, actually had to do with this bug. Thanks for the fix.;;;","21/Oct/15 07:53;htoulan;Hi,
I know this is bug is fixed now, but I got  a loss of data in production with several nodes in Cassandra 1.1.0 (java 6 + Red Hat)
I've been told that NTP and network issues occured, also the Cassandra servers have been restarted  probably due to power outage.
How can I identify that the loss of data are due to the bug described here? Is it reproducible ?

I can't decide to upgrade my servers in production without a solid evidence...

Thanks,

Hervé;;;","26/Oct/15 09:37;htoulan;anyone can help ? how can I confirm I reproduced the bug ? 
something in the logs?
a specific cassandra-cli command ?

Thanks in advance.;;;","28/Oct/15 17:59;rcoli;[~htoulan] :

1) JIRA is not a support forum, generally. You would get a better response from the #cassandra IRC channel on freenode or the cassandra-user@ mailing list.

2) Cassandra 1.1.0 is an extremely old and extremely broken version. You should update to at least the final released version of the 1.1 line. Much better would be (first 1.2.x and then) at least the most recent 2.0.x version.

3) No commit log replay bug, including this one, should be capable of losing data if you write with a sufficient ConsistencyLevel and Replication Factor and repair regularly.;;;","29/Oct/15 08:57;htoulan;Hi Robert,

1) Thank you, I did know that, I almost never encountered issue with Cassandra (even with the 1.1.0)
2) That's what I thought. I plan to upgrade to 1.2.9.
3) Now, I lose data on all 1.1.0 platforms if I insert data and reboot servers (ring composed by 2 servers)... 
I can't believe we never saw that before. 
I am not sure I understand how Cassandra works with System.nanoTime() and replay position but couold we imagine current timestamp reaches a value for wich this bug is now reproducibke systematically?? 

Anyway you're right, and upgrade is mandatory.


Hervé
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sometimes Cassandra starts compacting system-shema_columns cf repeatedly until the node is killed,CASSANDRA-4781,12611005,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,aleksey,aleksey,09/Oct/12 15:39,16/Apr/19 09:32,14/Jul/23 05:52,30/Oct/12 16:59,1.2.0 beta 2,,,,,,0,,,,,,,"Cassandra starts flushing system-schema_columns cf in a seemingly infinite loop:

 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,804 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32107-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,824 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.182486MB/s.  Time: 20ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,825 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32108-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.096045MB/s.  Time: 38ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,864 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32109-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.121657MB/s.  Time: 30ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,894 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32110-Data.db')]
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 239) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db,].  3,827 to 3,827 (~100% of original) bytes for 3 keys at 0.202762MB/s.  Time: 18ms.
 INFO [CompactionExecutor:7] 2012-10-09 17:55:46,914 CompactionTask.java (line 119) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-ia-32111-Data.db')]
.........

Don't know what's causing it. Don't know a way to predictably trigger this behaviour. It just happens sometimes.","Ubuntu 12.04, single-node Cassandra cluster",aleksey,mauzhang,minaguib,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 22:00;yukim;4781-v2.txt;https://issues.apache.org/jira/secure/attachment/12551032/4781-v2.txt","29/Oct/12 16:18;yukim;4781-v3.txt;https://issues.apache.org/jira/secure/attachment/12551205/4781-v3.txt","25/Oct/12 19:58;yukim;4781.txt;https://issues.apache.org/jira/secure/attachment/12550850/4781.txt",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,246173,,,Tue Oct 30 20:18:38 UTC 2012,,,,,,,,,,"0|i07hxb:",41671,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"09/Oct/12 15:43;brandon.williams;Here is something possibly related that is easy to reproduce.  Given nodes X, Y and Z, start X.  Now start Y, and see X flush schema_columns and schema_keyspaces in addition to peers/local, even though there's no schema and nothing changed.  Now start Z and see both X and Y exhibit this behavior, and sometimes even flush the schema twice in a row.;;;","23/Oct/12 17:58;brandon.williams;Interesting to note that if you hit this and then add nodes to the cluster, they will inherit the problem too.  Issuing a schema changes seems to stop the infinite compactions.;;;","25/Oct/12 19:57;yukim;I found the cause of this recursive compaction.

We calculate how many columns in sstable are expected to be dropped, and if that exceeds threshold we do single sstable compaction. Cassandra does not drop tombstones when overlapping keys exist in other sstables, so when we have overlap, we calculate what percentage of columns that do not belong to overlapped keys can be dropped.
Here we use sstable.estimatedKeys to calculate, but since the value comes from index interval and index summary, we have chance to calculate wrong value if the number of keys in sstable is small, like in this case, schema_columns. (schema_columns also has lot of overlapped keys among sstable since its key is Keyspace name.)

So I propose to skip single sstable compaction if sstable contains small number of keys.
Patch is attached for this.;;;","26/Oct/12 08:16;slebresne;I believe you are right that this is a problem. But I think there is another problem in that computation (that do not only impact small number of keys), namely in the estimation of remaining columns:
{noformat}
long columns = sstable.getEstimatedColumnCount().percentile(remainingKeysRatio) * remainingKeys;
{noformat}
I think the use of percentile here is not correct. For instance, say the remaingKeysRatio is very big (say 99%), and say that your rows are such that you have many small rows and a handful (5%) of very big ones. In that case, percentile will give you the number of columns the very big row have (it will give you a number such that 99% of the rows have less than this number of columns), and you'll end up with an estimate of columns that is way off (that is, you could end up with a number of remaining column that is order of magnitude bigger than the total number of columns). I believe we should simply use:
{noformat}
long columns = sstable.getEstimatedColumnCount().mean() * remainingKeys;
{noformat}

For the estimated key number, I'm good with going with your solution, but an alternative one would be to use a more conservative estimated key number that would be:
{noformat}
public long conservativeKeyEstimate()
{
    return indexSummary.getKeys().size() < 2
         ? 1
         : (indexSummary.getKeys().size() - 1) * DatabaseDescriptor.getIndexInterval();
}
{noformat}
That advantage being that this would always under-estimate the number of keys, while estimatedKeys() always over-estimate it, which seems a better option here because we don't have a choose a rather random value of minimum samples after which we consider that the over-estimation is ""acceptable"" in proportion.

But all this being said, and while we should definitively fix the things above, they will only make the estimation better, but it still an estimation. So at least in theory, we could always end up in a case where the estimate thinks there is enough droppable tombstones, but in practice all the droppable tombstones are in overlapping ranges. Meaning that I'd suggest skipping the worthDroppingTombstones check for sstables that have been compacted (using the creation time of the file is probably good enough) since less than some time threshold (say maybe gcGrace/4). After all, if I've just been compacted and still have a high ratio of droppable, it's probably that those are in fact not droppable due to overlapping sstables.
;;;","26/Oct/12 21:58;yukim;Sylvain, thanks for the feedback. v2 attached.

I changed use of percentile to mean. For estimated keys, I just kept mine since estimatedKeysForRanges might return greater value than under estimated keys count.

bq. But all this being said, and while we should definitively fix the things above, they will only make the estimation better, but it still an estimation. So at least in theory, we could always end up in a case where the estimate thinks there is enough droppable tombstones, but in practice all the droppable tombstones are in overlapping ranges.

You are right, we need safe stopper. I followed your advice of using sstable creation time. But I made time threshold configurable via compaction strategy option with default value of 5 minutes, since gcGraceSecond can be set to 0, and makes testing easy.

Speaking of schema_* columnfamilies, can we lower minimum compaction threshold to 2? schema_* have almost always overlapped key and tombstones, and less likely to updated.;;;","29/Oct/12 10:33;slebresne;Looks overall ok, but a few small remaining remarks:
* I'm good with making the ""tombstone compaction interval"" configurable but I would have gone with a longer default one (say a day, or at least a few hours). The tombstone compaction can be fairly improductive if we do them too often: compacting a sstable for tombstones, even if you do collect tombstone, is kinda bad if you're going to compact the sstable a short time later. In other words, I don't think this interval is only useful for avoiding an infinite loop. Besides, if it's configurable, the few people that have very heavy delete/expiring workload can set that lower if that helps them.
* It's probably not worth exposing an interval in milliseconds, seconds would be more than good enough. I also don't dislike putting the unit the option use in the name too, so ""tombstone_compaction_interval_seconds"", though maybe it's too long a name.
* Could be nice to validate the user input for the new option (should be > 0). 
;;;","29/Oct/12 16:18;yukim;v3 attached. Default interval is now 1 day in second unit with validation.;;;","29/Oct/12 22:59;minaguib;FWIW I've just hit this bug, on the very first startup of a node after it was upgraded from 1.1.2 to 1.1.6;;;","30/Oct/12 02:25;minaguib;Restarting the node produced the same endless loop.  Downgrading back to 1.1.2 fixed it.;;;","30/Oct/12 07:55;slebresne;Alright, v3 lgtm, +1 (I had in mind of throwing an exception if compaction_compaction_interval < 0 but I realize that it's slightly more complex than I though and not only related to that option so let's leave it like that here and I'll open a separate ticket for improving validation).

@Mina you cannot have hit this bug in 1.1.6 because it concerns code that is 1.2.0 only. So if you had a problem upgrading, that would be something else.;;;","30/Oct/12 12:58;minaguib;@[~slebresne] Here's the relevant portion of cassandra 1.1.6 starting up (after loading the SStables and replaying the commitlog): http://mina.naguib.ca/misc/cassandra_116_startup_loop.txt

It loops and never ends until I kill cassandra.

As I've said, restarting cassandra repeated the behavior.  Downgrading from 1.1.6 to 1.1.2 resolved it.

If you still feel it's unrelated to this ticket, let me know and I'll file a new one.
;;;","30/Oct/12 14:04;slebresne;bq. If you still feel it's unrelated to this ticket

I feel nothing, I know it's unrelated. It *cannot* be related to this ticket since that ticket is due to code that does not exist in 1.1. Besides, your log shows System.schema_keyspaces memtables being flush over and over again (and they are not even empty) so that is what triggers compaction. Why you get continuous activity on the schema_keyspaces CF is a good question though, but definitively not related to this ticket.;;;","30/Oct/12 16:59;yukim;Committed v3 to trunk.

[~minaguib] I'm closing this one because it was caused by functionality only available in trunk (as Sylvain stated) and I pushed the fix to trunk. Feel free to open another ticket for investigation in 1.1 version.;;;","30/Oct/12 19:26;brandon.williams;[~minaguib] your issue is slightly different; there is constant flushing of the schema too.;;;","30/Oct/12 20:18;minaguib;I've filed CASSANDRA-4880;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
leveled compaction does less work in L0 than intended,CASSANDRA-4778,12610898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Oct/12 22:40,16/Apr/19 09:32,14/Jul/23 05:52,09/Oct/12 03:49,1.1.6,,,,,,0,compaction,lcs,,,,,"We have this code in the candidate loop:

{code}
.               if (SSTable.getTotalBytes(candidates) > maxSSTableSizeInBytes)
                {
                    // add sstables from L1 that overlap candidates
                    candidates.addAll(overlapping(candidates, generations[1]));
                    break;
                }
{code}

thus, as soon as we have enough to compact to make one L1 sstable's worth of data, we stop collecting candidates.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/12 22:41;jbellis;4778.txt;https://issues.apache.org/jira/secure/attachment/12548322/4778.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,245693,,,Wed Oct 10 17:04:37 UTC 2012,,,,,,,,,,"0|i06cqf:",34997,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"08/Oct/12 22:41;jbellis;patch to move combine-with-L1 logic outside the candidates loop;;;","08/Oct/12 22:42;jbellis;(introduced by CASSANDRA-4341);;;","08/Oct/12 23:03;yukim;+1;;;","09/Oct/12 03:49;jbellis;committed;;;","10/Oct/12 17:04;jbellis;Also added a fix in f34bd79b9a92f23c1fc5e185e074d7faa880fc0b to avoid infinite compaction on a single L0 sstable that isn't large enough to promote.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't require quotes for true and false,CASSANDRA-4776,12610780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jsanda,jsanda,08/Oct/12 01:20,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/12 16:00,1.2.0 beta 2,,,,,,0,cql3,,,,,,"The docs at http://cassandra.apache.org/doc/cql3/CQL.html#identifiers describe using double quotes for an identifier that is a reserved word. The following works as expected,

cqlsh:test> select ""columnfamily"" from system.schema_columnfamilies;

I have a table with a boolean column. In order to insert a boolean value, I have to enclose it in single quotes. The table looks like,

CREATE TABLE bool_test (
  id int PRIMARY KEY,
  val boolean
);

Here is what happens when I try using double quotes,

cqlsh:rhq> insert into bool_test (id, val) values (4, ""false"");
Bad Request: line 1:43 no viable alternative at input 'false'


The use of single quotes here seems inconsistent with what is described in the docs, and makes things a bit confusing. It would be nice if single or double quotes could be used for identifiers that are reserved words. I also think it is a bit counter-intuitive to require quotes for true and false which are literal values.","Mac OS X, Fedora 16",aleksey,jsanda,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/12 13:51;aleksey;4776-cqlsh.txt;https://issues.apache.org/jira/secure/attachment/12550774/4776-cqlsh.txt","25/Oct/12 10:47;slebresne;4776.txt;https://issues.apache.org/jira/secure/attachment/12550765/4776.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,245004,,,Thu Oct 25 16:00:01 UTC 2012,,,,,,,,,,"0|i05xiv:",32530,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Oct/12 15:59;jbellis;# it sounds like you're complaining that the server uses single quotes to denote the input it couldn't parse.  that sounds completely unreleated to how keywords must be quoted, and a non-problem.
# open to making true/false not require quoting but if it's a pita i'm also fine with leaving this alone;;;","08/Oct/12 16:07;slebresne;In fact there is a documentation bug as the doc pretends that <identifier> are valid <term> but they are not (a <term> is a value, an <identifier> is not a value, not in CQL at least). So that should be fixed. So, with the documentation fixed, it should be clear that you should use single quotes because that's a string you should pass.

That being said, I'm also open at making true/false being values themselves (without quoting).;;;","25/Oct/12 02:05;aleksey;I agree with making true/false valid identifiers, but don't understand what's otherwise expected from this issue. It's a documentation bug, and that doc has quite some outdated stuff (assuming it's intended to reflect latest CQL3 and not 3.0.0-beta1).;;;","25/Oct/12 03:24;jbellis;Updated title.;;;","25/Oct/12 03:38;jsanda;Where/how can I access the latest CQL3 docs? I just ran ant generate-cql-html from trunk where my HEAD is a commit from 10/17. I am not sure how different that is from the doc I cited in the description. ;;;","25/Oct/12 10:47;slebresne;Attaching the fairly trivial patch to make boolean literals.;;;","25/Oct/12 10:50;slebresne;bq. Where/how can I access the latest CQL3 docs?

The good question is when. There has been many change to CQL3 on trunk but the doc hasn't been updated yet. We'll update it for the release though.;;;","25/Oct/12 13:38;jbellis;+1;;;","25/Oct/12 13:40;aleksey;Please don't commit it yet - I'll attach a patch for cqlsh as well. Soon.;;;","25/Oct/12 16:00;slebresne;Alright, committed both patches, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException in org.apache.cassandra.gms.Gossiper.sendGossip,CASSANDRA-4774,12610704,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cnlwsu,bcoverston,bcoverston,06/Oct/12 13:14,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/13 14:36,1.2.9,,,,,,0,,,,,,,"ERROR [GossipTasks:1] 2012-10-06 10:47:48,390 Gossiper.java (line 169) Gossip error
java.lang.IndexOutOfBoundsException: Index: 13, Size: 5
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:541)
	at org.apache.cassandra.gms.Gossiper.doGossipToUnreachableMember(Gossiper.java:575)
	at org.apache.cassandra.gms.Gossiper.access$300(Gossiper.java:59)
	at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:141)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",Saw this when looking through some logs in version 1.0.0 system was under a lot of load.,bcoverston,cnlwsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/13 18:45;cnlwsu;patch.txt;https://issues.apache.org/jira/secure/attachment/12595236/patch.txt",,,,,,,,,,,,,,,,,1.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,244709,,,Thu Aug 01 14:36:40 UTC 2013,,,,,,,,,,"0|i05q87:",31346,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Nov/12 18:47;brandon.williams;Line 541 in 1.0.0 is in doGossipToSeed, are you certain about the version?  I'm not sure what to look at.;;;","31/Jul/13 18:28;cnlwsu;Saw this once on a 1.1.5 node under high load with significant heap pressure, GCInspector reported 95% full right before.

{code}
 WARN [ScheduledTasks:1] 2013-07-07 19:37:03,834 GCInspector.java (line 145) Heap is 0.9552299542288667 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 WARN [ScheduledTasks:1] 2013-07-07 19:37:03,834 StorageService.java (line 2855) Flushing CFS(Keyspace='x', ColumnFamily='x') to relieve memory pressure
 INFO [ScheduledTasks:1] 2013-07-07 19:37:03,834 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-x@766608353(261434/1801824 serialized/live bytes, 5150 ops)
 INFO [GossipStage:1] 2013-07-07 19:37:05,125 Gossiper.java (line 816) InetAddress /10.x.x.x is now UP
 INFO [GossipStage:1] 2013-07-07 19:37:05,146 Gossiper.java (line 816) InetAddress /10.x.x.x is now UP
ERROR [GossipTasks:1] 2013-07-07 19:37:05,155 Gossiper.java (line 171) Gossip error
java.lang.IndexOutOfBoundsException: Index: 10, Size: 10
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:560)
	at org.apache.cassandra.gms.Gossiper.doGossipToUnreachableMember(Gossiper.java:594)
	at org.apache.cassandra.gms.Gossiper.access$300(Gossiper.java:61)
	at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:143)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}
;;;","31/Jul/13 18:46;cnlwsu;doGossipToUnreachableMember calls
{code}
sendGossip(prod, unreachableEndpoints.keySet());
{code}
the keyset returned is backed by the set so changes are reflected in the set.  Since sendGossip gets the size, then picks the random number, then does a get on a list created it created a race condition where the list is smaller then the keyset.  attached a possible change to do this.

note: patch off of 1.1;;;","01/Aug/13 14:36;brandon.williams;Your analysis sounds spot on and the timing proximity in the log seems to back it up.  Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails to deliver hints after first repaired node,CASSANDRA-4772,12610635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,azotcsit,azotcsit,azotcsit,05/Oct/12 21:01,16/Apr/19 09:32,14/Jul/23 05:52,09/Oct/12 13:55,1.1.6,,,,,,1,hintedhandoff,,,,,,"If some node has hints for a few nodes it will deliver hints only for the first one of them. After all hints delivery for the first node compaction process is started. After compaction all data from hints cf is removed.

target fix for 1.2 version:
{code}
diff --git a/src/java/org/apache/cassandra/db/HintedHandOffManager.java b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
index e5ff163..c02997e 100644
--- a/src/java/org/apache/cassandra/db/HintedHandOffManager.java
+++ b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
@@ -189,7 +189,7 @@ public class HintedHandOffManager implements HintedHandOffManagerMBean
         ArrayList<Descriptor> descriptors = new ArrayList<Descriptor>();
         for (SSTable sstable : hintStore.getSSTables())
             descriptors.add(sstable.descriptor);
-        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, Integer.MAX_VALUE);
+        return CompactionManager.instance.submitUserDefined(hintStore, descriptors, (int) System.currentTimeMillis() / 1000);
     }

 
     private static boolean pagingFinished(ColumnFamily hintColumnFamily, ByteBuffer startColumn)
{code}

Can I expect to see that fix in 1.1.6 version?",,azotcsit,christianmovi,sergeant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3972,,,,,,"09/Oct/12 04:32;jbellis;4772-1.0.txt;https://issues.apache.org/jira/secure/attachment/12548358/4772-1.0.txt","08/Oct/12 11:00;azotcsit;cassandra-1.2-4772-hh_compact.txt;https://issues.apache.org/jira/secure/attachment/12548223/cassandra-1.2-4772-hh_compact.txt",,,,,,,,,,,,,,,,2.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,244077,,,Tue Oct 09 13:55:08 UTC 2012,,,,,,,,,,"0|i05gtr:",29823,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Oct/12 21:15;jbellis;the timestamp is ""time before which we can throw away deleted columns.""  columns that have not already been deleted are unaffected.;;;","08/Oct/12 11:00;azotcsit;That timestamp is passed to ColumnFamilyStore.removeDeleted() method. That method removes columns with tombstones and expired columns. All columns in ""hints"" column family are ""expiring columns"" and they should be removed if gcBefore is greater than localExpirationTime. We pass gcBefore as Integer.MAX_VALUE, so all columns should be deleted because of Integer.MAX_VALUE >> localExpirationTime.
I've attached some test, that fails without the fix. Please take a look on it. 
;;;","09/Oct/12 04:31;jbellis;You're right, Alexey.  Thanks for the followup.

Thought at first this would affect 1.0 as well -- compaction code is the same in 1.0.x -- but the bug needs on both compaction w/ MAX_TIMESTAMP *and* CASSANDRA-3716.;;;","09/Oct/12 04:32;jbellis;attaching backport of test to 1.0;;;","09/Oct/12 13:55;jbellis;committed fix to 1.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting TTL to Integer.MAX causes columns to not be persisted.,CASSANDRA-4771,12610619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dbrosius@apache.org,tnine,tnine,05/Oct/12 19:34,16/Apr/19 09:32,14/Jul/23 05:52,24/Jan/18 16:49,1.1.6,,,,,,0,,,,,,,"When inserting columns via batch mutation, we have an edge case where columns will be set to Integer.MAX.  When setting the column expiration time to Integer.MAX, the columns do not appear to be persisted.

Fails:

Integer.MAX_VALUE 
Integer.MAX_VALUE/2

Works:
Integer.MAX_VALUE/3

",,christianmovi,dbrosius@apache.org,pauloricardomg,rbfblk,sayap,slebresne,tnine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14092,,,,,,,,"07/Oct/12 13:15;dbrosius@apache.org;4771.txt;https://issues.apache.org/jira/secure/attachment/12548161/4771.txt","09/Oct/12 03:15;dbrosius@apache.org;4771_b.txt;https://issues.apache.org/jira/secure/attachment/12548355/4771_b.txt",,,,,,,,,,,,,,,,2.0,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,244052,,,Wed Jan 24 16:49:18 UTC 2018,,,,,,,,,,"0|i05fkn:",29620,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"07/Oct/12 13:00;dbrosius@apache.org;The value written is delta-ed from the current time as

(System.currentTimeMillis() / 1000) + timeToLive

which causes the written ttl to go negative

the error back to the client could be better, and perhaps pre-flighted.;;;","07/Oct/12 13:15;dbrosius@apache.org;fix preflighting to catch ttl too large problems. 4771.txt;;;","08/Oct/12 14:02;slebresne;I think we'll want to fix that for CQL too (in ModificationStatement.validate(); and yes, that would be nice to not have duplication of the validation code but ...). Also, the patch is against trunk, we should push that to 1.1. Last thing: just returning ""ttl too large"" doesn't really help to know why it is too large, and it can be hard to check in the application that your ttl won't be too large. Maybe it would be simpler to pick a fixed max TTL value, like say 20 years (which would give us something like 16 years to lift that limitation)?;;;","09/Oct/12 03:15;dbrosius@apache.org;makes sense

1) patch against 1.1
2) limit to 20 years
3) exception contains requested vs max limit
4) added to cql;;;","09/Oct/12 06:58;slebresne;nit: I would have put MAX_TTL in ExpiringColumn rather than IColumn.

But +1 in any case.;;;","10/Oct/12 02:00;dbrosius@apache.org;committed to 1.1 as 46fc843bbd39bf6b007fb6d1c8e823f8b3ba2425;;;","13/Mar/14 17:21;rbfblk;Isn't this going to break again in less than 4 years when (now + 20 years) > 2038-01-19?;;;","24/Jan/18 12:54;christianmovi;[~rbfblk]: You are a smart guy. It indeed broke 4 years later :)

 

Edit: I will buy you a beer, should we ever meet ;) ;;;","24/Jan/18 13:30;christianmovi;Can't we get another 68 years if we cast to long instead of int?

 

Class SerializationHeader:
{code:java}
public int readLocalDeletionTime(DataInputPlus in) throws IOException
{
return (int)in.readUnsignedVInt() + stats.minLocalDeletionTime;
}{code}
 ;;;","24/Jan/18 16:00;rbfblk;[~christianmovi]: I forgot about this! I just tried on a 2.1 cluster and have also confirmed it is broken. I get an AssertionError back when doing an insert with ttl 630720000 (20 years). Are you on Cassandra 3+? I was hoping this was fixed as part of CASSANDRA-8099, but from that snippet I guess not.

It looks like expiration time is encoded per cell as an offset in seconds from the minimum deletion time in the SSTable. If I'm reading it right the offset is serialized as a variable length integer, so reading it into a long would give a very large range of offsets that would be more than sufficient. However it looks to me like the minimum deletion time at the SSTable metadata level is saved as a fixed-size 4-byte signed integer. Without changing the serialization format we could change the interpretation of that metadata field to assume it is an unsigned integer (and de-serialize it also into a long variable) which should indeed give us another 68 years. A better fix would probably be to also change the serialization format of the minLocalDeletionTime to an 8 byte integer or a variable size integer so it could hold values in the far far future. Changing the SSTable format might be a 4.0+ thing though. This probably needs a new ticket.

I would gladly accept that beer! But I probably deserve negative points for not following up on my comment several years ago...;;;","24/Jan/18 16:11;christianmovi;[~rbfblk]: Nop, it seems it was not fixed with the refactoring.

For now changing the typecast to signed int would be a big improvement already.

 Edit: It happened in C* 3.0.15;;;","24/Jan/18 16:49;pauloricardomg;Normally we don't reopen issues which were already released, the idea is to open a new issue if there is a regression of a previous defect. Since there is CASSANDRA-14092 for the reappearance of this issue I will close this and add a link to this on that ticket. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) data type not in lowercase are not handled correctly.,CASSANDRA-4770,12610588,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Oct/12 15:56,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/12 16:56,1.1.6,,,,,,0,cql3,,,,,,"Seems that we accept {{int}} but we don't accept {{INT}} (that is, the parser accepts it, but we fail later to recognize it).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/12 16:00;slebresne;4770.txt;https://issues.apache.org/jira/secure/attachment/12547998/4770.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,243598,,,Fri Oct 05 16:56:12 UTC 2012,,,,,,,,,,"0|i04bun:",23185,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Oct/12 16:00;slebresne;Trivial fix attached.;;;","05/Oct/12 16:44;jbellis;+1;;;","05/Oct/12 16:56;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError in CompactionExecutor thread,CASSANDRA-4765,12610368,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,abashir,abashir,04/Oct/12 18:17,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/12 20:20,1.1.7,1.2.0 beta 2,,,,,0,compaction,,,,,,"Seeing the following error:


Exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.StackOverflowError
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)
        at com.google.common.collect.Sets$1.iterator(Sets.java:578)

",,abashir,christianmovi,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/12 18:59;jbellis;4765.txt;https://issues.apache.org/jira/secure/attachment/12547800/4765.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,240690,,,Thu Oct 18 20:20:39 UTC 2012,,,,,,,,,,"0|i014ov:",4529,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"04/Oct/12 18:34;brandon.williams;Which java version is this on?  You might be able to increase the Xss flag as a workaround.;;;","04/Oct/12 18:50;jbellis;This is from the SetView returned by Sets.union:

{code}
.     @Override public Iterator<E> iterator() {
        return Iterators.unmodifiableIterator(
            Iterators.concat(set1.iterator(), set2minus1.iterator()));
      }
{code}

I don't think we're taking 100s of unions so increasing Xss is unlikely to help.;;;","04/Oct/12 18:59;jbellis;Actually we do take union in a loop in getOverlappingSSTables.  Give this patch a try.;;;","17/Oct/12 17:31;omid;I just got a similar StackOverflowError but on Iterators.java:

{code}
2012-10-17_14:35:09.95258 ERROR 14:35:09,942 Exception in thread Thread[CompactionExecutor:2773,1,main]
2012-10-17_14:35:09.95260 java.lang.StackOverflowError
2012-10-17_14:35:09.95261 	at java.util.AbstractList$Itr.hasNext(Unknown Source)
2012-10-17_14:35:09.95269 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95281 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
2012-10-17_14:35:09.95293 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95305 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
2012-10-17_14:35:09.95320 	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
2012-10-17_14:35:09.95331 	at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
{code}

And iterators.java:517 and :514 belong to concat call that appears on the SetView returned by Sets::union.;;;","18/Oct/12 13:00;omid;I could only reproduce the error by mistakenly running size tiered compaction in test environment on originally LCS-compacted data and the patch fixes the stack overflow. ;;;","18/Oct/12 15:31;yukim;+1;;;","18/Oct/12 20:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean out STREAM_STAGE vestiges,CASSANDRA-4764,12610366,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,04/Oct/12 17:46,16/Apr/19 09:32,14/Jul/23 05:52,23/Oct/12 14:57,1.2.0 beta 2,,,,,,0,streaming,,,,,,Currently it appears as though bulk loading operations don't run in any stage. Seems like they should be running in STREAM_STAGE.,,nickmbailey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/12 19:14;jbellis;4764-v2.txt;https://issues.apache.org/jira/secure/attachment/12550338/4764-v2.txt","23/Oct/12 14:15;jbellis;4764-v3.txt;https://issues.apache.org/jira/secure/attachment/12550465/4764-v3.txt","04/Oct/12 21:07;jbellis;4764.txt;https://issues.apache.org/jira/secure/attachment/12547824/4764.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,240576,,,Tue Oct 23 14:57:58 UTC 2012,,,,,,,,,,"0|i013mv:",4358,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"04/Oct/12 21:07;jbellis;Looks like post-CASSANDRA-3494 Stage.STREAM is obsolete, since MessagingService has a per-target Executor for streams.  Patch attached to clean out the vestiges.;;;","04/Oct/12 21:13;nickmbailey;So the reason i noticed this is because we bulk loads weren't showing up in the stream stage mbean. It sounds like nothing will show up there though. Removing the stage is good so that the mbean doesn't show up at all anymore but should we make a ticket to expose similar stats from messaging service?

I guess the only thing that mbean provided that the streaming service mbean doesn't is completed tasks. So maybe just add a counter there?;;;","04/Oct/12 21:22;jbellis;Do we need it?  Would we count sessions or files or ranges?;;;","04/Oct/12 21:29;nickmbailey;Well just the number of completed tasks probably isn't particularly useful you are right.

I feel like we need some additional tracking around streaming though. Currently if streaming fails mid stream for bootstrap or move or decom and the like there isn't necessarily any indication that something went wrong unless you examine the logs. So more useful woud be successful count and failed count on a per session basis or something like that.

That is perhaps best decided in a different ticket though.;;;","04/Oct/12 21:41;jbellis;Yeah, it sounds like we're not really sure what we need.  Maybe just return session id to the user so they can ask ""hey, how's session X doing?"";;;","22/Oct/12 17:58;yukim;For removing STREAM stage, +1.
Attached patch seems against 1.1 and contains diff from different issue though.;;;","22/Oct/12 19:14;jbellis;clean v2 attached against trunk;;;","22/Oct/12 19:50;yukim;+1;;;","22/Oct/12 21:29;jbellis;committed;;;","23/Oct/12 14:00;slebresne;That has completely broke bootstrap by removing Verb.STREAM_REQUEST from the list of known verbs in MessagingService (and StreamRequests were executed on teh stream stage, which we can change I suppose).;;;","23/Oct/12 14:13;jbellis;Reverted.;;;","23/Oct/12 14:15;jbellis;v3 attached w/ fix.;;;","23/Oct/12 14:52;yukim;+1
(StreamReply is actually not handled in MISC stage but in FileStreamTask and streamExecutor now, but let's leave it here for later fix.);;;","23/Oct/12 14:57;jbellis;committed, w/ comment for STREAM_REPLY;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Predicate logic bug when using composite columns,CASSANDRA-4759,12610142,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,03/Oct/12 21:23,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/12 15:03,1.1.6,,,,,,0,cql3,,,,,,"Looks like a predicate logic bug that only happens when you have > 2 primary keys and use COMPACT STORAGE (meaning its using composite columns under the hood)

First I'll show it works with just 2 
{code}
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate)
       ...          ) WITH COMPACT STORAGE
       ...            AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
 key | rdate                    | num
-----+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 10.5
{code}

Now we create with 3 parts to the PRIMARY KEY
{code}
cqlsh:dev> drop TABLE testrev ;
cqlsh:dev> CREATE TABLE testrev (
       ...          key text,
       ...          rdate timestamp,
       ...          rdate2 timestamp,
       ...          num double,
       ...          PRIMARY KEY(key,rdate,rdate2)
       ...          ) WITH COMPACT STORAGE
       ...          AND CLUSTERING ORDER BY(rdate DESC);

cqlsh:dev> INSERT INTO testrev(key,rdate,rdate2,num) VALUES ('foo','2012-01-01','2012-01-01',10.5);
cqlsh:dev> select * from testrev where key='foo' and rdate > '2012-01-01';
 key | rdate                    | rdate2                   | num
-----+--------------------------+--------------------------+------
 foo | 2012-01-01 00:00:00-0500 | 2012-01-01 00:00:00-0500 | 10.5

cqlsh:dev> select * from testrev where key='foo' and rdate >= '2012-01-01';
{code}

The last query should return the row...
",,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/12 09:08;slebresne;4759.txt;https://issues.apache.org/jira/secure/attachment/12547701/4759.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239428,,,Thu Oct 04 15:03:20 UTC 2012,,,,,,,,,,"0|i00h53:",710,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"03/Oct/12 21:25;tjake;Also the first query in the second example is wrong, it should return nothing.;;;","03/Oct/12 21:32;jbellis;Is this 1.1 or 1.2?;;;","04/Oct/12 09:08;slebresne;Seems like the fix for CASSANDRA-4716 didn't fix the whole problem. The (hopefully) last problem is in the handling of the compositeType 'end-of-component'. Namely, ReversedType don't apply to that eoc so we should take that into account. Patch attached to fix (the patch is against 1.1 because this does affect 1.1). It also fixes CASSANDRA-4760 (because that was in fact the same problem).;;;","04/Oct/12 14:55;tjake;Tested +1;;;","04/Oct/12 15:03;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bulk loader won't work with CQL3,CASSANDRA-4755,12610134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,nickmbailey,nickmbailey,03/Oct/12 20:46,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 16:48,1.2.0 beta 2,,,Legacy/Tools,,,0,,,,,,,"Currently the bulk loader uses thrift to get the schema and validate it before bulk loading a cf. Since we stopped returning cql3 cfs through describe_keyspaces, the bulk loader will be unable to load those cfs.

If we figure out/add a way to validate the schema over jmx, we could use that for getting token ranges as well and drop thrift completely from the bulk loader.

Another option might be querying system tables manually to validate things.",,aleksey,mbulman,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/12 22:17;aleksey;CASSANDRA-4755.txt;https://issues.apache.org/jira/secure/attachment/12550154/CASSANDRA-4755.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,240680,,,Mon Oct 22 16:48:36 UTC 2012,,,,,,,,,,"0|i014jb:",4504,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Oct/12 21:46;jbellis;I think I'd prefer just converting that code to use a CQL query or two, all it cares about is ""is this ks/cf pair valid,"" which isn't a big deal to fix.

Side note: looks like some code duplication in BulkLoader.ExternalClient and BulkRecordWriter.External client, be nice to clean that up.;;;","20/Oct/12 22:28;aleksey;Haven't dealt with BulkRecordWriter.External/BulkLoader.ExternalClient duplication. The attached patch fixes the BulkLoader+CQL3 issue and does nothing beyond that.

Please create a new issue for removing the duplication (or tell me to create it) if you still think it's worth the effort. But that would have a different priority compared to this issue, so I've dealt with the bug first.;;;","22/Oct/12 16:48;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is an inconsistency between default configuration in cassandra.yaml and java code,CASSANDRA-4754,12610091,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,03/Oct/12 15:57,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/12 18:48,1.1.6,,,,,,0,configurations,docs,,,,,Options max_hint_window_in_ms and in_memory_compaction_limit_in_mb have different values in cassandra.yaml and in java code. I suggest to lead java code values to cassandra.yaml values.,,azotcsit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 15:59;azotcsit;cassandra-1.1-4754_default_configs.txt;https://issues.apache.org/jira/secure/attachment/12547547/cassandra-1.1-4754_default_configs.txt",,,,,,,,,,,,,,,,,1.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,240585,,,Thu Oct 04 06:04:15 UTC 2012,,,,,,,,,,"0|i013q7:",4373,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"03/Oct/12 18:48;brandon.williams;Committed, without yaml comments since they seem a bit redundant and out of place.;;;","04/Oct/12 06:04;azotcsit;Ok, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout reporter writes hints for the local node,CASSANDRA-4753,12610089,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,aleksey,aleksey,03/Oct/12 15:47,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/12 20:56,1.2.1,,,,,,0,,,,,,,"MessagingService.java:330 calls StorageProxy.scheduleLocalHint() without checking if the local node is the target. This causes StorageProxy.scheduleLocalHint to throw AssertionError sometimes.

Got this error when some batches are timed out. This can happen because even local batches now go through StorageProxy.sendMessages and aren't just rm.apply()'d.",,aleksey,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/12 09:00;jbellis;4753-v2.txt;https://issues.apache.org/jira/secure/attachment/12554140/4753-v2.txt","19/Nov/12 11:03;aleksey;4753-v3.txt;https://issues.apache.org/jira/secure/attachment/12554154/4753-v3.txt","19/Nov/12 03:15;aleksey;4753.txt;https://issues.apache.org/jira/secure/attachment/12554108/4753.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250367,,,Wed Dec 19 20:56:54 UTC 2012,,,,,,,,,,"0|i0axxb:",61769,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,"03/Oct/12 15:51;aleksey;See https://issues.apache.org/jira/browse/CASSANDRA-4542?focusedCommentId=13451479&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13451479
and https://issues.apache.org/jira/browse/CASSANDRA-4542?focusedCommentId=13451486&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13451486

;;;","03/Oct/12 15:54;aleksey;Special-casing for local inserts might be worth it now that abm is the default and we went even futher with CASSANDRA-4738.;;;","03/Oct/12 15:54;jbellis;Sounds like the right solution is to add a local-only apply the way we do for normal Mutations.;;;","19/Nov/12 03:17;aleksey;That particular error has been fixed some time ago (same issue that was causing batchlog timeouts in beta1). So there is really no need to special-case timeout-reporter for local node - such cases just should not happen.

However, I added a check for local node to StorageProxy.shouldHint so that no issue like this one pops in the future.;;;","19/Nov/12 09:00;jbellis;It's valid for local writes to timeout, but we don't want to just drop them on the floor if they do.  v2 attached.;;;","19/Nov/12 10:48;aleksey;You are right. There is still the option of special-casing local endpoint for syncWriteToBatchlog and asyncRemoveFromBatchlog. I know single-node clusters are not a pirority, but still. That would also make this issue go away.
;;;","19/Nov/12 11:03;aleksey;Something like v3.;;;","19/Nov/12 15:19;jbellis;Hmm.  We actually have a couple problems here.

# We do allow local writes to be dropped, but
# Nobody writes hints for dropped local writes
# If local hints were attempted, they would error out (what v2 was trying to fix)

We actually do want to hint batchlog inserts to maintain the contract of ""timed out means it's in progress, you do not have to retry.""

I'm not sure what the best way to add hinting (actually, retry-on-hint-stage) code for insertLocal is though.  One option would be to just make local writes non-droppable, but then we lose our backpressure mechanism of a full hint stage causing OverloadedException, and a poorly behaving client could OOM the coordinator with a lot of local writes.;;;","20/Nov/12 21:23;jbellis;Here is one solution: http://github.com/jbellis/cassandra/branches/4753-5

- Created LocalMutationRunnable that retries with hint backpressure if it gets dropped
- Added an updated v3, with an additional assert to make sure we don't introduce a bug if we decide to allow using the coordinator as a batchlog member in non-single-node clusters;;;","19/Dec/12 20:45;aleksey;+1;;;","19/Dec/12 20:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop keyspace causes schema disagreement,CASSANDRA-4752,12610062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,03/Oct/12 10:38,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/12 15:29,1.1.6,,,,,,0,,,,,,,"The fix for CASSANDRA-4698 introduced a bug whereby when drop keyspace is issued a schema disagreement immediately occurs. This seems to be because the 

{code}ColumnFamily cf = ColumnFamily.create(modification.getValue().metadata());{code}

in {{RowMutation.deserializeFixingTimestamps}} does not preserve deletion info for the cf in the modification. In most cases, this doesn't cause a problem, but for a drop keyspace modification, there are no columns in the cf, so the deletion info is effectively lost leading to an incorrect digest being created and ultimately a schema disagreement.

Replacing the {{create}} with {{cloneMeShallow}} does preserve the deletion info and avoids the schema disagreement issue.
",,samt,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 10:44;samt;0001-CASSANDRA-4752.txt;https://issues.apache.org/jira/secure/attachment/12547518/0001-CASSANDRA-4752.txt","03/Oct/12 15:30;jbellis;4752-v2.txt;https://issues.apache.org/jira/secure/attachment/12547542/4752-v2.txt",,,,,,,,,,,,,,,,2.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239448,,,Thu Oct 04 15:29:52 UTC 2012,,,,,,,,,,"0|i00hef:",752,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Oct/12 15:30;jbellis;Thanks, Sam!  I think you nailed it with your diagnosis of the problem.  But the fix will not actually correct the timestamp (neither did the original code).  Attaching v2 that makes the code do what I think was originally intended -- copy the old timestamp into the clone, or cap it at current time if necessary.;;;","04/Oct/12 13:57;tjake;Verified Jonathans fix +1;;;","04/Oct/12 15:29;jbellis;committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User rpc_address for binary protocol and change default port,CASSANDRA-4751,12610056,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,03/Oct/12 09:41,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/12 09:12,1.2.0 beta 2,,,,,,0,,,,,,,"The events mechanism of the binary protocol require that we know the address on which other nodes can be joined (for the binary protocol). Hence CASSANDRA-4501. However, in 1.2 we've already burned all the padding in 1.1 gossip, so we can't gossip a new info (the binary protocol address), so CASSANDRA-4501 will have to move to 1.3.

But we do already gossip the rpc_address value, so an option is to make the binary protocol bind on the rpc_address (but a specific port) rather than having it's own setting. This ticket suggests to do that. Imo, there is little downside to do it: the thrift and binary transport are not meant to be used together except during the transition from one to the other, and even then having to use the same network interface is hardly a limitation (in other words, even for 1.3, we might want to hold on CASSANDRA-4501 until someone comes with a compelling use case for it).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 09:43;slebresne;4751.txt;https://issues.apache.org/jira/secure/attachment/12547514/4751.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,240241,,,Thu Oct 04 09:12:49 UTC 2012,,,,,,,,,,"0|i00yp3:",3554,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Oct/12 09:43;slebresne;Attaching patch for this. The patch also change the default port from 8000 since it's a bit crowded as a port number.;;;","03/Oct/12 17:52;jbellis;I don't follow {{InetAddress.getByName(StorageService.instance.getRpcaddress(endpoint));}} -- why convert it to a String, to convert back w/ getByName?;;;","03/Oct/12 18:27;slebresne;{{endpoint}} is the listen_address of the node, but for event, we want to provide the address on which the node can be joined by a client, hence StorageService.instance.getRpcaddress.;;;","03/Oct/12 21:29;jbellis;Feels a bit hackish but since we're not doing it in performance critical areas, +1;;;","04/Oct/12 09:12;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible problem with widerow in Pig URI,CASSANDRA-4749,12609917,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,oberman,oberman,02/Oct/12 14:30,16/Apr/19 09:32,14/Jul/23 05:52,12/Oct/12 03:03,1.1.6,,,,,,0,,,,,,,"I don't have a good way to test this directly, but I'm concerned the Uri parsing for widerows isn't going to work.  setLocation 
1.) calls setLocationFromUri (which sets widerows to the Uri value)
2.) sets widerows to a static value (which is defined as false)
3.) sets widerows to the system setting if it exists.  
That doesn't seem right...

But setLocationFromUri also gets called from setStoreLocation, and I don't really know the difference between setLocation and setStoreLocation in terms of what is going on in terms of the integration between cassandra/pig/hadoop.",AWS running Centos 5.6 using Sun build 1.6.0_24-b07,jeromatron,oberman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 18:45;brandon.williams;4749.txt;https://issues.apache.org/jira/secure/attachment/12548790/4749.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247767,,,Fri Oct 12 03:03:51 UTC 2012,,,,,,,,,,"0|i08ppj:",48764,,jeromatron,,jeromatron,Normal,,,,,,,,,,,,,,,,,"02/Oct/12 19:29;brandon.williams;You're right.  Patch attached to instead init the vars as default, then parse from the uri, and finally allow overriding with env vars.;;;","12/Oct/12 02:51;jeromatron;+1 to the changes, though don't we want to expose a way for them to set those variables with standard hadoop config, possibly namespaced with pig?  e.g. cassandra.pig.wide.row?;;;","12/Oct/12 02:58;jeromatron;I was looking at 1.1.5 that didn't yet have the URL location for Cassandra accepting the widerows or use_secondary flags.  That's in there in the 1.1 branch.

in other words +1 :);;;","12/Oct/12 03:03;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh timestamp formatting is broken - displays wrong timezone info (at least on Ubuntu),CASSANDRA-4746,12609853,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,02/Oct/12 02:09,16/Apr/19 09:32,14/Jul/23 05:52,30/Oct/12 18:51,1.1.7,1.2.0 beta 2,,,,,0,cqlsh,,,,,,"cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> create table ts (id int primary key, ts timestamp);
cqlsh:test> insert into ts (id, ts) values (1, '2012-05-14 07:53:20+0000');
cqlsh:test> select * from ts;
 id | ts
----+--------------------------
  1 | 2012-05-14 10:53:20+0000


Should've been 2012-05-14 10:53:20+0300.

cqlsh formats timestamps using '%Y-%m-%d %H:%M:%S%z' format-string and 'the %z escape that expands to the preferred hour/minute offset is not supported by all ANSI C libraries'. In this case it's just replaced with all zeroes.","64-bit Ubuntu 12.04, python 2.7.3",aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4437,,,,,,,,,,,"26/Oct/12 12:59;aleksey;4746-1.1.txt;https://issues.apache.org/jira/secure/attachment/12550958/4746-1.1.txt","25/Oct/12 17:21;aleksey;4746.txt;https://issues.apache.org/jira/secure/attachment/12550811/4746.txt",,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,246153,,,Tue Oct 30 19:02:44 UTC 2012,,,,,,,,,,"0|i07hs7:",41648,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"26/Oct/12 11:31;brandon.williams;Is there any reason not to put this in 1.1?;;;","26/Oct/12 13:00;aleksey;Not anymore.;;;","26/Oct/12 13:48;brandon.williams;Committed.;;;","30/Oct/12 15:45;jbellis;reopening because this was not merged to trunk.  (I got a conflict on merge so I just kept the trunk version as a temporary fix.);;;","30/Oct/12 18:31;aleksey;Huh? I have it in trunk. Seems like it was merged just fine.;;;","30/Oct/12 19:02;brandon.williams;I used both patches and did a 'merge -s ours' on the second one for trunk which may have caused confusion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared Statements don't support collections,CASSANDRA-4739,12609767,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,titanous,titanous,01/Oct/12 16:32,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/12 07:33,1.2.0 beta 2,,,,,,0,,,,,,,"I'm putting a collection onto the wire in an EXECUTE request with exactly the same bytes that Cassandra encodes the same data in a response:

""Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type""

Here's the full trace log from cassandra:

{noformat}
DEBUG 19:24:15,414 Received: PREPARE INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,414 CQL QUERY: INSERT INTO things (id, set_text) VALUES (?, ?);
TRACE 19:24:15,415 Stored prepared statement #413587006 with 2 bind markers
DEBUG 19:24:15,415 Responding: RESULT PREPARED 413587006 [id(gocql_collections, things), org.apache.cassandra.db.marshal.TimeUUIDType][set_text(gocql_collections, things), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]
DEBUG 19:24:15,416 Received: EXECUTE 413587006 with 2 values
TRACE 19:24:15,416 [1] 'java.nio.HeapByteBuffer[pos=18 lim=34 cap=53]'
TRACE 19:24:15,416 [2] 'java.nio.HeapByteBuffer[pos=38 lim=51 cap=53]'
DEBUG 19:24:15,417 Responding: ERROR INVALID: Can't apply operation on column with org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type) type.
{noformat}


The prepared statement is:

{noformat}
INSERT INTO things (id, set_text) VALUES (?, ?);
{noformat}

and the collection value ({'asdf', 'sdf'}) is encoded as:

{noformat}
00 02 00 04 61 73 64 66 00 03 73 64 66
{noformat}

which is byte-for-byte exactly the same as what I get from Cassandra off the wire when I do a query for the same data.

I already have the driver working with other queries/inserts with all other types, so this is just a collection encoding problem.","Cassandra 937f15e1
OS X 10.8.2
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10-428-11M3811)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01-428, mixed mode)",slebresne,titanous,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/12 10:14;slebresne;4739.txt;https://issues.apache.org/jira/secure/attachment/12547704/4739.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239432,,,Fri Oct 05 07:33:02 UTC 2012,,,,,,,,,,"0|i00h6n:",717,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Oct/12 10:26;slebresne;This is actually not related to the binary protocol; collections are just not supported correctly in prepared statements (I've updated the title to reflect that).

And this require a little bit of specific code because we must ""deserialize"" the sent list to be able to insert internally one column per element. Attaching patch to fix that.;;;","02/Oct/12 16:08;jbellis;bq. this require a little bit of specific code because we must ""deserialize"" the sent list to be able to insert internally one column per element

How is this different from Collection support in non-prepared statements?;;;","02/Oct/12 16:35;slebresne;bq. How is this different from Collection support in non-prepared statements?

That's dealt with by the parser.;;;","03/Oct/12 19:19;titanous;I've been testing this patch out a bit, and it's mostly working, but I ran into an issue when using a map<timestamp,int> ({1349286846012: 2}):

""Expected 8 or 0 byte long for date (4)""

The map is encoded in the exact same format that I get from the server when querying the same value:

{noformat}
00 01 00 08 00 00 01 3a 27 c3 5e 3c 00 04 00 00 00 02
{noformat};;;","04/Oct/12 10:14;slebresne;Oups, that last one is just a copy-paste type (MapType was trying to validate values with the keys comparator). Patch updated.;;;","04/Oct/12 17:51;jbellis;+1, although it seems weird to have partly-prepared statements to me.  Weird in the ""I hope this doesn't get a lot more complicated and cause us grief in the future"" kind of way. :);;;","05/Oct/12 07:33;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
doc/native_protocol.txt isn't up to date,CASSANDRA-4737,12609728,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,tux21b,tux21b,01/Oct/12 09:22,16/Apr/19 09:32,14/Jul/23 05:52,01/Oct/12 14:34,1.2.0 beta 2,,,,,,0,binary_protocol,,,,,,CASSANDRA-4449 seems to have changed the datatype of the query id returned by a RESULT-PREPARED message from an {{int}} to a {{short}} n followed by n bytes (representing a md5sum). The specification at doc/native_protocol.txt doesn't cover this change yet.,,slebresne,tux21b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255196,,,Mon Oct 01 14:34:23 UTC 2012,,,,,,,,,,"0|i0epzr:",83973,,,,,Low,,,,,,,,,,,,,,,,,"01/Oct/12 10:21;slebresne;Fixed in commit 76107078438211816153722b86f8bf2c49b34ddc. Thanks.;;;","01/Oct/12 12:26;tux21b;Thanks for looking into this. Unfortunately, the specification is still wrong in my opinion:

[short]        A 2 bytes unsigned integer
[short bytes]  A [short] n, followed by n bytes if n >= 0. If n < 0, no byte should follow and the value represented is `null`.

I'm not entirely sure how I should represent those n<0 using an unsigned integer.;;;","01/Oct/12 14:34;slebresne;Oups, fixed in commit 859473db505bd4e5953f69f06066af815da9a0ce.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NPE with some load of writes, but possible snitch setting issue for a cluster",CASSANDRA-4728,12609349,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,alexliu68,alexliu68,27/Sep/12 05:11,16/Apr/19 09:32,14/Jul/23 05:52,28/Sep/12 14:41,1.2.0 beta 2,,,,,,0,snitch,,,,,,"The following errors are showing under height load

ERROR [MutationStage:8294] 2012-09-25 22:01:47,628 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:8294,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.locator.PropertyFileSnitch.getDatacenter(PropertyFileSnitch.java:104)
	at com.datastax.bdp.snitch.DseDelegateSnitch.getDatacenter(DseDelegateSnitch.java:69)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122)
	at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:93)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1984)
	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1972)
	at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:262)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:248)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:505)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:56)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


ERROR [MutationStage:13164] 2012-09-25 22:19:06,486 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13164,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [MutationStage:13170] 2012-09-25 22:19:07,349 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:13170,5,main]
java.lang.NullPointerException
ERROR [Thrift:12] 2012-09-25 22:19:07,433 Cassandra.java (line 3462) Internal error processing batch_mutate
java.lang.NullPointerException
ERROR [Thrift:16] 2012-09-25 22:19:07,437 Cassandra.java (line 2999) Internal error processing get


java.lang.NullPointerException
 INFO [GossipStage:280] 2012-09-26 00:15:15,371 Gossiper.java (line 818) InetAddress /172.16.233.208 is now dead.
ERROR [GossipStage:280] 2012-09-26 00:15:15,372 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:280,5,main]
j

ERROR [MutationStage:40529] 2012-09-26 00:15:21,527 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MutationStage:40529,5,main]
java.lang.NullPointerException
 INFO [GossipStage:281] 2012-09-26 00:15:23,013 Gossiper.java (line 818) InetAddress /172.16.232.159 is now dead.
ERROR [GossipStage:281] 2012-09-26 00:15:23,014 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:281,5,main]
",,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/12 20:56;jbellis;4728.txt;https://issues.apache.org/jira/secure/attachment/12546903/4728.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255201,,,Thu Sep 27 21:36:10 UTC 2012,,,,,,,,,,"0|i0eq0v:",83978,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"27/Sep/12 17:10;brandon.williams;Duplicate of CASSANDRA-4675;;;","27/Sep/12 17:30;jbellis;Actually not a duplicate, this is a PFS NPE for an unknown host.;;;","27/Sep/12 18:07;jbellis;Would it be sufficient for each (server-mode) node to check that it's present in the PFS file when starting up?;;;","27/Sep/12 20:05;brandon.williams;That sounds like a good solution.;;;","27/Sep/12 20:56;jbellis;attached, with extra assert for good measure.;;;","27/Sep/12 21:36;brandon.williams;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid ConcurrentModificationExceptions on relocateTokens,CASSANDRA-4727,12609344,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,dbrosius@apache.org,dbrosius@apache.org,27/Sep/12 04:47,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/12 12:11,1.2.0 beta 2,,,,,,0,,,,,,,"code loops over a HashMap and deletes items from the hashmap without using the iterator.

will result in ConcurrentModificationExceptions... remove thru the iterator instead.",,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/12 04:47;dbrosius@apache.org;cme_patch.txt;https://issues.apache.org/jira/secure/attachment/12546804/cme_patch.txt",,,,,,,,,,,,,,,,,1.0,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255202,,,Thu Sep 27 12:10:20 UTC 2012,,,,,,,,,,"0|i0eq13:",83979,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Sep/12 05:01;jbellis;+1;;;","27/Sep/12 12:10;dbrosius@apache.org;committed to trunk as 1078e6f514bcfbcaadb2517099baab5f3d21d510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some operations of HintedHandOffManager bean have wrong output,CASSANDRA-4724,12609250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,26/Sep/12 14:25,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/12 15:10,1.1.6,,,Legacy/Tools,,,0,hintedhandoff,jmx,,,,,"I suggest to change output of listEndpointsPendingHints and countPendingHints operations in HintedHandOffManager bean.

Current output:
  - listEndpointsPendingHints:
{code}
c�.@ÁM��JprV���c�.@ÁM��JprV���
{code}
  - countPendingHints:
{code}
116570217535704627=1170
{code}

Suggested output:
  - listEndpointsPendingHints:
{code}
localhost/127.0.0.1
{code}
  - countPendingHints:
{code}
localhost/127.0.0.1=1170
{code}

",Cassandra 1.1.2,azotcsit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/12 14:28;azotcsit;cassandra-1.2-4724-handoff_bean.txt;https://issues.apache.org/jira/secure/attachment/12546702/cassandra-1.2-4724-handoff_bean.txt",,,,,,,,,,,,,,,,,1.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255205,,,Wed Sep 26 18:39:55 UTC 2012,,,,,,,,,,"0|i0eq1r:",83982,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Sep/12 15:10;jbellis;Every Row will have the same key for all the hints it contains.  Could use that to avoid needing the extra Map<ByteBuffer, InetAddress>.

That said, I don't think using the inetaddress instead of the token is really an improvement... it's keyed off of token on purpose so that if the actual machine owning that token changes, we can still deliver the hints.

(For {{listEndpointsPendingHints}}, using new String(key.array) is a bug, I've changed it to use the same tokenFactory.toString that {{count}} was using in commit e752de96f7e3ae676b5dba0564e1321d6661a0cc on the 1.1 branch.);;;","26/Sep/12 18:39;azotcsit;Ok, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Have Cassandra return the right error for keyspaces with dots,CASSANDRA-4721,12609127,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,j.casares,j.casares,25/Sep/12 21:02,16/Apr/19 09:32,14/Jul/23 05:52,25/Sep/12 21:51,1.1.6,,,,,,0,,,,,,,"cqlsh> CREATE KEYSPACE 'solr.test' WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
Bad Request: Invalid keyspace name: shouldn't be empty nor more than 48 characters long (got ""solr.test"")",,j.casares,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255207,,,Tue Sep 25 21:51:56 UTC 2012,,,,,,,,,,"0|i0eq27:",83984,,,,,Low,,,,,,,,,,,,,,,,,"25/Sep/12 21:46;jbellis;Not a cqlsh issue.;;;","25/Sep/12 21:51;jbellis;committed fix in 05a5ede91ac558998f93024439185fdd1e04345e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh fails to format timeuuid values,CASSANDRA-4720,12609124,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,25/Sep/12 20:54,16/Apr/19 09:32,14/Jul/23 05:52,25/Sep/12 22:39,1.2.0 beta 2,,,Legacy/Tools,,,0,cqlsh,,,,,,"If a user has a table with a timeuuid column, and the user tries to select some rows containing timeuuid values, a weird error results:

{noformat}
global name 'unix_time_from_uuid1' is not defined
{noformat}

Not very helpful. It should probably display the timeuuid somehow.",,aleksey,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 20:56;thepaul;4720.patch.txt;https://issues.apache.org/jira/secure/attachment/12546580/4720.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255208,,,Tue Sep 25 23:50:23 UTC 2012,,,,,,,,,,"0|i0eq2f:",83985,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"25/Sep/12 20:56;thepaul;fixes the problem.;;;","25/Sep/12 22:36;aleksey;It does. (why is it assigned to me? I made sure it works, though, just in case).;;;","25/Sep/12 22:39;brandon.williams;Committed.;;;","25/Sep/12 23:45;thepaul;sorry Aleksey, my fault, I guessed you were the one to automatically get new cqlsh tickets. But then I went and fixed it myself anyway, because my memory sucks. :);;;","25/Sep/12 23:50;aleksey;np, I don't mind getting issues with working patches attached to them (:;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"binary protocol: when an invalid event type is watched via a REGISTER message, the response message does not have an associated stream id",CASSANDRA-4719,12609115,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,25/Sep/12 20:25,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/12 11:19,1.2.0,,,Legacy/CQL,,,0,binary_protocol,,,,,,"I tried sending a REGISTER message with an eventlist including the string ""STATUS_FOO"", in order to test error handling in the python driver for that eventuality. But the response from the server (a ""Server error"" with a message of ""java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.transport.Event$Type.STATUS_FOO"") had a stream_id of 0, so the driver was not able to associate it with the request.",,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239452,,,Thu Nov 08 11:19:30 UTC 2012,,,,,,,,,,"0|i00he7:",751,,,,,Low,,,,,,,,,,,,,,,,,"08/Nov/12 11:19;slebresne;I've fixed that some times ago in a ninja-commit so closing this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh fails to format values of ReversedType-wrapped classes,CASSANDRA-4717,12609084,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,25/Sep/12 16:54,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/12 15:31,1.1.6,,,Legacy/Tools,,,0,cqlsh,,,,,,"See the test case for CASSANDRA-4715, but run it on trunk. The ReversedType-wrapped column (rdate) will be displayed as a floating-point integer (it gets deserialized into a native type correctly, but cqlsh's format-according-to-type machinery doesn't know how to handle the cqltypes.ReversedType subclass.",,slebresne,thepaul,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 07:49;slebresne;4717-1.1.txt;https://issues.apache.org/jira/secure/attachment/12547509/4717-1.1.txt","25/Sep/12 17:18;thepaul;4717-test.patch.txt;https://issues.apache.org/jira/secure/attachment/12546552/4717-test.patch.txt","25/Sep/12 17:18;thepaul;4717.patch.txt;https://issues.apache.org/jira/secure/attachment/12546551/4717.patch.txt","03/Oct/12 07:49;slebresne;4717.txt;https://issues.apache.org/jira/secure/attachment/12547508/4717.txt",,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255210,,,Wed Oct 03 15:31:45 UTC 2012,,,,,,,,,,"0|i0eq2v:",83987,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"25/Sep/12 17:03;jbellis;Is cql3 not giving back the ""base"" type in resultset info?  If not I think that should be addressed server-side.

But if this is a a cql2-only problem then it's not on my priority list...;;;","25/Sep/12 17:04;jbellis;Also should we close CASSANDRA-4715 as duplicate?;;;","25/Sep/12 17:18;thepaul;4717.patch.txt is a fix; 4717-test.patch.txt adds a test for it that fits in the framework from CASSANDRA-3920, if you guys decide to add that.;;;","25/Sep/12 17:21;thepaul;I'm not sure whether it's better for the thrift interface to include ReversedType in the resultset or not, but the cql library can handle it either way. And this fix for cqlsh is pretty simple too.

If you do want to change the interface so that ReversedType doesn't get reported, then 4715 probably is a dupe. Otherwise, it's a different problem with a different solution.

This isn't specific to cql2.;;;","26/Sep/12 12:37;slebresne;bq. Is cql3 not giving back the ""base"" type in resultset info? If not I think that should be addressed server-side.

It's not currently but that's definitively an oversight. I'll fix it (and thus CASSANDRA-4715 is a dupe).

That being said, I'm fine committing Paul's patch in the meantime to solve the issue for cql2, but there seems to be some kind of mess in the cqlsh sources on the cassandra-1.1 as both displaying.py and formatting.py are missing (as shown here: https://github.com/apache/cassandra/tree/cassandra-1.1/pylib/cqlshlib; trunk is fine).;;;","26/Sep/12 16:36;thepaul;Sorry, you're right, this patch was only for trunk. The 1.1 branch doesn't have formatting.py or displaying.py. The patch will be easy to adapt- I can do it within a day or two, if no one else picks it up;;;","03/Oct/12 07:49;slebresne;Attaching patch to skip returning a ReversedType to CQL3 clients. I'm attaching a separate patch for 1.1 and trunk because they are different enough.;;;","03/Oct/12 12:57;tjake;+1 I like this idea;;;","03/Oct/12 15:31;slebresne;Alright, committed, thanks.

I'm closing that ticket because this solves the original issue. But I'm still fine committing paul's patch for the sake of CQL2 once the 1.1 version of the patch is attached.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 predicate logic is reversed when used on a reversed column,CASSANDRA-4716,12609060,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,25/Sep/12 15:00,16/Apr/19 09:32,14/Jul/23 05:52,02/Oct/12 16:52,1.1.6,,,,,,1,,,,,,,"Example:

{code}
cqlsh:test>
cqlsh:test> CREATE TABLE testrev (
        ... key text,
        ... rdate timestamp,
        ... num double,
        ... PRIMARY KEY(key,rdate)
        ... ) WITH COMPACT STORAGE
        ...   AND CLUSTERING ORDER BY(rdate DESC);
cqlsh:test> INSERT INTO testrev(key,rdate,num) VALUES ('foo','2012-01-01',10.5);
cqlsh:test> select key from testrev where rdate > '2012-01-02' ;
 key 
-----
 foo 

cqlsh:test> select key from testrev where rdate < '2012-01-02' ;
cqlsh:test>
{code}",,liqusha,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/12 14:35;slebresne;4716.txt;https://issues.apache.org/jira/secure/attachment/12547388/4716.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253218,,,Tue Oct 02 16:52:20 UTC 2012,,,,,,,,,,"0|i0dcif:",75956,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"01/Oct/12 19:46;tjake;How deeply do we want to fix this? It goes pretty deep into how we deal with isReversed flag vs ReversedType.

We can, however easily fix it in the CQLParser by just flipping the predicates signs around.;;;","02/Oct/12 14:35;slebresne;This can't be fixed in the parser as this all depends on what's the clustering order and the requested order (if any).

But attaching a patch to fix this (I've pushed dtests to make sure we're good with different variation of clustering order/order by). ;;;","02/Oct/12 16:35;tjake;+1;;;","02/Oct/12 16:52;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate column names with DynamicCompositeType,CASSANDRA-4711,12608897,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thrykol,thrykol,24/Sep/12 16:35,16/Apr/19 09:32,14/Jul/23 05:52,29/Oct/12 10:14,1.1.7,,,,,,0,,,,,,,"I have a column family whose comparator is DynamicCompositeType and validation is CounterColumnType.  During automated testing, there have been occasions where a counter column is created twice, throwing off the query results for the column.

Doing a 'get' via the cli, I see the following output for the row:

=> (counter=s@language:b@00000001:s@pt_BR, value=198)
=> (counter=s@language:s@possible, value=200)
=> (counter=s@language:b@00000001:s@pt_BR, value=0)

If I print out the byte value of the column names along with their MD5 sum I see:

Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653
Name: [language, possible]
Byte array: ffffff8073086c616e67756167650ffffff807308706f737369626c650
MD5: 82cad9b6a65c794e97cf1d4613e2e367
Name: [language, java.nio.HeapByteBuffer[pos=0 lim=4 cap=4], pt_BR]
Byte array: ffffff8073086c616e67756167650ffffff80620400010ffffff80730570745f42520
MD5: 2db353a9a72a0d7cb6cb277ac5125653

Unfortunately, I have been unable to duplicate this manually or via a generic test script and our QA department can only duplicate ~25% of the time.","CentOS 5.8 x86_64
Cassandra 1.1.5
Hector 1.1-2",christianmovi,slebresne,thrykol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 09:45;slebresne;4711.txt;https://issues.apache.org/jira/secure/attachment/12550944/4711.txt","25/Oct/12 20:47;thrykol;DuplicateColumnName.java;https://issues.apache.org/jira/secure/attachment/12550858/DuplicateColumnName.java",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,251121,,,Wed Nov 14 18:41:44 UTC 2012,,,,,,,,,,"0|i0b4vr:",62903,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Sep/12 18:20;thrykol;Rough example of how counter column is being used.;;;","25/Oct/12 20:47;thrykol;Java class which demonstrates unexpected behavior with counter columns.

Sample output from the CLI is included in inline comment blocks.  Each time I ran the script, the column ordering changed slightly.;;;","26/Oct/12 09:45;slebresne;This is a bug in the patch of CASSANDRA-3625. We are mistakenly using ReversedType with our FixedValueComparator but that doesn't correctly reverse the output. Attaching simple patch to fix.;;;","26/Oct/12 09:47;slebresne;Note that the patch is against 1.1 but 1.0 would also be affected so we can commit there if we want (the patch probably apply there without modifications).;;;","26/Oct/12 10:02;slebresne;Oh, and the use of counters has nothing to do with anything so I've update the title to reflect that.;;;","26/Oct/12 14:36;thrykol;Thanks for the quick response on this.  Do you have a time frame for when 1.1.7 is expected to be released?  I'm seeing this issue in a production environment and would like to get the fix in place as quickly as possible.  If the release won't be happening in the very near future, can you patch this into 1.1.6 as well.  While I can download the source and apply the patch locally, it'd be my preference not to.;;;","26/Oct/12 18:38;jbellis;+1 on the fix.

The next release will be 1.1.7, probably in a couple weeks.  If you want a hotfix sooner than that, then rolling your own patched 1.1.6 is your best option.;;;","29/Oct/12 10:14;slebresne;Committed, thanks.;;;","14/Nov/12 18:41;thrykol;As and FYI, after patching 1.1.6 in my environment I've found that some of my unit tests failed (I'm assuming the same would be true for 1.1.7).  Going through the failing tests I found that the logic for my slice ranges had to be modified.  It may be good to put a warning along those lines in the change log.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) Missing validation for IN queries on column not part of the PK,CASSANDRA-4709,12608856,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,24/Sep/12 12:12,16/Apr/19 09:32,14/Jul/23 05:52,09/Oct/12 14:16,1.1.6,,,,,,0,,,,,,,"Copy-pasting from the original mail (http://mail-archives.apache.org/mod_mbox/cassandra-user/201209.mbox/%3C20120922185826.GO6205@pslp2%3E):
{noformat}
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> 
cqlsh> create keyspace xpl1 WITH strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;
cqlsh> use xpl1;
cqlsh:xpl1> create table t1 (pk varchar primary key, col1 varchar, col2 varchar);
cqlsh:xpl1> create index t1_c1 on t1(col1);
cqlsh:xpl1> create index t1_c2 on t1(col2);
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1a','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1b','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk1c','foo1','bar1');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk2','foo2','bar2');
cqlsh:xpl1> insert into t1  (pk, col1, col2) values ('pk3','foo3','bar3');
cqlsh:xpl1> select * from t1 where col2='bar1';
 pk   | col1 | col2
------+------+------
 pk1b | foo1 | bar1
  pk1 | foo1 | bar1
 pk1a | foo1 | bar1
 pk1c | foo1 | bar1

cqlsh:xpl1> select * from t1 where col2 in ('bar1', 'bar2') ;
cqlsh:xpl1> 
{noformat}

We should either make that last query work or refuse the query but returning nothing is wrong.",,psergey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 13:31;slebresne;4709.txt;https://issues.apache.org/jira/secure/attachment/12546513/4709.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239445,,,Tue Oct 09 14:16:27 UTC 2012,,,,,,,,,,"0|i00han:",735,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Sep/12 13:31;slebresne;For now I think we should just refuse the query since this would require secondary indexes to do an OR which they can't do right now. Attaching patch that simply refuse such queries.;;;","25/Sep/12 16:52;jbellis;+1;;;","09/Oct/12 14:16;slebresne;Forgot to mark that one resolved somehow, doing it now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy slow-down and memory leak,CASSANDRA-4708,12608822,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,danielnorberg,danielnorberg,danielnorberg,24/Sep/12 07:09,16/Apr/19 09:32,14/Jul/23 05:52,24/Sep/12 15:11,1.0.12,1.1.6,,,,,0,,,,,,,"I am consistently observing slow-downs in StorageProxy caused by the NonBlockingHashMap used indirectly by MessagingService via the callbacks ExpiringMap.

This seems do be due to NBHM having unbounded memory usage in the face of workloads with high key churn. As monotonically increasing integers are used as callback id's by MessagingService, the backing NBHM eventually ends up growing the backing store unboundedly. This causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. 

This behavior is especially noticable for high throughput workloads where the dataset is completely in ram and I'm doing up to a hundred thousand reads per second.

Replacing NBHM in ExpiringMap with the java standard library ConcurrentHashMap resolved the issue and allowed me to keep a consistent high throughput.

An open issue on NBHM can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362",,danielnorberg,dehora,marcuse,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/12 07:28;danielnorberg;0001-MessagingService-don-t-use-NBHM-in-ExpiringMap.patch;https://issues.apache.org/jira/secure/attachment/12546260/0001-MessagingService-don-t-use-NBHM-in-ExpiringMap.patch",,,,,,,,,,,,,,,,,1.0,danielnorberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256348,,,Tue Sep 25 04:07:59 UTC 2012,,,,,,,,,,"0|i0gynr:",97045,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Sep/12 07:28;danielnorberg;Attached patch with the proposed fix; replaces NBHM with CHM in ExpiringMap.;;;","24/Sep/12 15:11;jbellis;committed, thanks!

(also commented on NBHM SF issue.);;;","24/Sep/12 15:19;jbellis;did a quick audit of NBHM usage elsewhere.  think we're okay, everything else has a pretty tiny amount of possible keys.  the main ""large"" maps are in the column containers where NBHM is a non-candidate since we need sorting.;;;","24/Sep/12 15:44;jbellis;also committed to 1.0 branch;;;","25/Sep/12 04:07;scode;Nice catch!

FWIW, ConcurrentSkipListMap should probably be considered to avoid locking (though I believe it's generally slower and it'll be less memory compact).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 CREATE TABLE with set and counter causes java.lang.IllegalArgumentException,CASSANDRA-4706,12608789,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,titanous,titanous,23/Sep/12 15:40,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/12 12:10,1.2.0 beta 2,,,,,,0,,,,,,,"Running a freshly compiled cassandra with no data, and a brand new keyspace (SimpleStrategy, replication_factor 1)


{noformat}
cqlsh:test> CREATE TABLE test (id bigint PRIMARY KEY, count counter, things set<text>);
TSocket read 0 bytes
{noformat}

{noformat}
ERROR 11:25:54,926 Error occurred during processing of message.
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:143)
	at org.apache.cassandra.config.CFMetaData.validate(CFMetaData.java:1064)
	at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:123)
	at org.apache.cassandra.cql3.statements.CreateColumnFamilyStatement.announceMigration(CreateColumnFamilyStatement.java:100)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:83)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:116)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1677)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3709)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}","rev 60bf68ca (tagged 1.2.0-beta1-tentative)

cqlsh 2.2.0 | Cassandra 1.2.0-beta1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.34.0

OS X 10.8.2
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10-428-11M3811)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01-428, mixed mode)

Ubuntu 12.04.1 LTS
java version ""1.7.0_07""
Java(TM) SE Runtime Environment (build 1.7.0_07-b10)
Java HotSpot(TM) 64-Bit Server VM (build 23.3-b01, mixed mode)",slebresne,titanous,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 13:56;slebresne;4706.txt;https://issues.apache.org/jira/secure/attachment/12546515/4706.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256347,,,Wed Sep 26 12:10:54 UTC 2012,,,,,,,,,,"0|i0gyn3:",97042,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Sep/12 13:10;slebresne;The code lacks proper validation and I'll look into it but let me note that mixing counter column with non-counter columns (set being non-counter) is not allowed (not more than it is on thrift).;;;","24/Sep/12 14:37;titanous;Ah, thanks, I missed that in the docs.;;;","25/Sep/12 13:56;slebresne;We were actually validating correctly but the IAE was triggered while building the InvalidRequestException. Attached simple patch to fix.;;;","25/Sep/12 16:53;jbellis;+1;;;","26/Sep/12 12:10;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.service.StorageProxy - compilation issue,CASSANDRA-4703,12608756,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kaykay.unique,kaykay.unique,kaykay.unique,22/Sep/12 16:38,16/Apr/19 09:32,14/Jul/23 05:52,22/Sep/12 20:20,1.2.0 beta 2,,,,,,0,,,,,,,"Running ant on master yields the following compilation error. 



build-project:
     [echo] apache-cassandra: /home/user/workspace/cassandra/build.xml
    [javac] Compiling 1 source file to /home/user/workspace/cassandra/build/classes/main
    [javac] /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java:803: local variable remotes is accessed from within inner class; needs to be declared final
    [javac]                             sendToHintedEndpoints(cm.makeReplicationMutation(), remotes, responseHandler, localDataCenter, consistency_level);
    [javac]                                                                                 ^
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: /home/user/workspace/cassandra/src/java/org/apache/cassandra/service/StorageProxy.java uses unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 1 error




","$ java -version
java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)




$ uname -a
Linux user-System-Product-Name 3.5.0-13-generic #14-Ubuntu SMP Wed Aug 29 16:48:44 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux


Ubuntu 12.10",kaykay.unique,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/12 16:39;kaykay.unique;CASSANDRA-4703.patch;https://issues.apache.org/jira/secure/attachment/12546169/CASSANDRA-4703.patch",,,,,,,,,,,,,,,,,1.0,kaykay.unique,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256346,,,Sat Sep 22 20:20:05 UTC 2012,,,,,,,,,,"0|i0gym7:",97038,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Sep/12 20:20;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql 2 counter validations need to use default consistencylevel if none is explicitly given,CASSANDRA-4700,12608695,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,niteesh.kumar,niteesh.kumar,niteesh.kumar,21/Sep/12 21:05,16/Apr/19 09:32,14/Jul/23 05:52,24/Sep/12 17:47,1.1.6,,,,,,0,,,,,,,"i was trying to run cql 2 query

cqlsh:stats> UPDATE Minutewise_Product_Stats SET '2LX:OQ:XYZ.com:664230591:1:totalView'='2LX:SOQ:XYZ.com:664230591:1:totalView'+1, '2LX:OQ:XYZ.com:664230591:1:keywordClick'='2LX:SOQ:xyz.com:664230591:1:keywordClick'+1 WHERE KEY='2017:4' ;

WHEN I GOT  this error

ERROR 20:38:46,220 Error occurred during processing of message.
java.lang.NullPointerException
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:151)
    at org.apache.cassandra.cql.UpdateStatement.prepareRowMutations(UpdateStatement.java:128)
    at org.apache.cassandra.cql.QueryProcessor.batchUpdate(QueryProcessor.java:245)
    at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:563)
    at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:817)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1675)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3721)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)


attached is the patch file  to fix this bug

CQL2fix.patch

diff --git a/src/java/org/apache/cassandra/cql/UpdateStatement.java b/src/java/org/apache/cassandra/cql/UpdateStatement.java
index 3a47712..0caa61b 100644
--- a/src/java/org/apache/cassandra/cql/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql/UpdateStatement.java
@@ -146,8 +146,11 @@ public class UpdateStatement extends AbstractModification
         }
 
         CFMetaData metadata = validateColumnFamily(keyspace, columnFamily, hasCommutativeOperation);
-        if (hasCommutativeOperation)
-            cLevel.validateCounterForWrite(metadata);
+        
+        if (hasCommutativeOperation){
+        	ConsistencyLevel currentCLevel = getConsistencyLevel();
+        	currentCLevel.validateCounterForWrite(metadata);
+        }
 
         QueryProcessor.validateKeyAlias(metadata, keyName);
 
",java,niteesh.kumar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/12 21:08;niteesh.kumar;CQL2fix.patch;https://issues.apache.org/jira/secure/attachment/12546090/CQL2fix.patch",,,,,,,,,,,,,,,,,1.0,niteesh.kumar,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256343,,,Mon Sep 24 17:47:07 UTC 2012,,,,,,,,,,"0|i0gyl3:",97033,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Sep/12 21:07;niteesh.kumar;
diff --git a/src/java/org/apache/cassandra/cql/UpdateStatement.java b/src/java/org/apache/cassandra/cql/UpdateStatement.java
index 3a47712..0caa61b 100644
--- a/src/java/org/apache/cassandra/cql/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql/UpdateStatement.java
@@ -146,8 +146,11 @@ public class UpdateStatement extends AbstractModification
         }
 
         CFMetaData metadata = validateColumnFamily(keyspace, columnFamily, hasCommutativeOperation);
-        if (hasCommutativeOperation)
-            cLevel.validateCounterForWrite(metadata);
+        
+        if (hasCommutativeOperation){
+        	ConsistencyLevel currentCLevel = getConsistencyLevel();
+        	currentCLevel.validateCounterForWrite(metadata);
+        }
 
         QueryProcessor.validateKeyAlias(metadata, keyName);
 
;;;","24/Sep/12 17:47;jbellis;committed to trunk and backported to 1.1 (where the error has existed since 0.8, only more subtly since it doesn't NPE);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keyspace disappears when upgrading node from cassandra-1.1.1 to cassandra-1.1.5,CASSANDRA-4698,12608662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,21/Sep/12 16:21,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/12 14:18,1.1.6,,,,,,0,,,,,,,"Here is how I got the problem to happen:

1. Get this zipped data directory (about 33Mb):
  scp cass@50.57.69.32:/home/cass/cassandra.zip ./ (password cass)
2. Unzip it in /var/lib/
3. clone the cassandra git repo
4. git checkout cassandra-1.1.1; ant jar;
5. bin/cassandra 
6. Run cqlsh -3, then DESC COLUMNFAMILIES; Note the presence of Keyspace performance_tests
7. pkill -f cassandra; git checkout cassandra-1.1.5; ant realclean; ant jar;
8. bin/cassandra
9. Run cqlsh -3, then DESC COLUMNFAMILIES; Note that there is no performance_tests keyspace",ubuntu. JNA not installed.,cdaw,hudson,jeromatron,sdelmas,tpatterson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 00:00;xedin;CASSANDRA-4698.patch;https://issues.apache.org/jira/secure/attachment/12546412/CASSANDRA-4698.patch","21/Sep/12 16:24;tpatterson;start_1.1.1_system.log;https://issues.apache.org/jira/secure/attachment/12546056/start_1.1.1_system.log","21/Sep/12 16:24;tpatterson;start_1.1.5_system.log;https://issues.apache.org/jira/secure/attachment/12546057/start_1.1.5_system.log",,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,241134,,,Thu Oct 11 19:06:53 UTC 2012,,,,,,,,,,"0|i019iv:",5312,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Sep/12 16:24;tpatterson;These are the log files after starting 1.1.1 and 1.1.5 respectively;;;","24/Sep/12 22:03;cdaw;[~tpatterson]
Can you try this test out and let me know if the keyspace reappears?

* In 1.1.1, show keyspaces to get schema definitions
* Upgrade to 1.1.5
* Recreate table definitions:
** rm data/system/system_schema*
** restart
** CREATE KEYSPACE + CREATE TABLE as necessary;;;","24/Sep/12 22:05;jbellis;It sounds like this reproduces on a single node?  No schema disagreement should be necessary/possible.;;;","24/Sep/12 22:13;tpatterson;{quote}It sounds like this reproduces on a single node? No schema disagreement should be necessary/possible.{quote}
yes, this happened on a single node.

{quote} Can you try this test out and let me know if the keyspace reappears? ...{quote}
Yes. I did those steps on one of the columnfamilies in the keyspace and the data did indeed come back.;;;","25/Sep/12 00:00;xedin;Attaching the patch to fix the problem which was related to fixing timestmaps in ColumnSerializer, instead of doing that patch does fixing only schema RowMutation timestamps from remote nodes.

Tyler, can you please test the scenario when local correct schema data are getting overriden by incorrect remote (as seen at CASSANDRA-4561)?;;;","25/Sep/12 04:03;jbellis;ISTM that only fixing remote timestamps is the wrong solution in general -- it fixes this particular scenario, but re-introduces ""unmodifiable"" keyspaces, since the [local] timestamp is now permanently left unnaturally high.

I think that we need to fix it at a higher level than serialization: once the rows [from different sstables] have all been deserialized and merged, THEN fix the timestamp.  Then we won't have the problem of an adjusted tombstone clobbering a value that it didn't, pre-adjustment.;;;","25/Sep/12 09:43;xedin;bq. ISTM that only fixing remote timestamps is the wrong solution in general – it fixes this particular scenario, but re-introduces ""unmodifiable"" keyspaces, since the [local] timestamp is now permanently left unnaturally high.

The local timestamp fixing didn't go anywhere and still fixed _before_ we accept remote schema mutations that it why it's useful to fix only schema mutation without touching anything else...;;;","26/Sep/12 03:00;tpatterson;{quote}Tyler, can you please test the scenario when local correct schema data are getting overriden by incorrect remote (as seen at CASSANDRA-4561)?{quote}
[~xedin] Could you explain what you mean by local and remote schemas, and how I can go about testing this? Thanks;;;","26/Sep/12 10:00;xedin;[~tpatterson] By local I mean - definitions from system.schema_*, by remote - mutations that could be sent (pushed) to the node from others or could be requested if local schema version is different from remote ones.;;;","26/Sep/12 18:06;tpatterson;[~xedin] I ran this test:

I set up a 1-node 1.1.1 cluster with that broken keyspace mentioned at the beginning of this test, then I bootstrapped another 1.1.1 node and verified that the keyspace was visible to the second node. The timestamps were 16 digits long on both nodes. 

Then I took down node 2, upgraded it to 1.1.5, and started it back up. The keyspace was not visible. In cassandra-cli I did was not able to see the timestamps on node 2, here is what the output looked like:
{code}
[default@system] list schema_columns;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: performance_tests

1 Row Returned.
Elapsed time: 2 msec(s).
{code}
I got the same results for list schema_keyspaces and list schema_columnfamilies.

On node1 (which was still on 1.1.1) the timestamps were still 16-digits long. I don't know how to get the now-incorrect remote schema (on node2) to overwrite the local correct schema (on node1).;;;","26/Sep/12 18:14;xedin;Did you apply patch attached to this issue on top of 1.1.5?;;;","26/Sep/12 18:23;jbellis;Patch LGTM.;;;","26/Sep/12 19:50;tpatterson;I hadn't applied the patch, but I just tried it again with the patch. With the patch the keyspace disappearing bug didn't happen. The timestamps all were 16-digits long on both nodes after upgrading one node.;;;","26/Sep/12 20:20;xedin;Great! Tyler will be running the last test to confirm that new node correctly handles timestamps from old nodes and we are good to go.;;;","26/Sep/12 22:31;tpatterson;I re-ran the test like the one described earlier today, but after node2 was upgraded to 1.1.5 (with the patch), I followed Pavel's instructions:
start first and second, stop second, run resetschema on first, stop first, start second, start first

Both nodes then showed 16-digit timestamps with 3 zeros at the end. ;;;","27/Sep/12 00:02;tpatterson;Per Pavel's request I ran the test again. Everything checked out, here are the steps followed:

1: get that data onto both nodes in a 1.1.1 cluster
2: upgrade and start node2 on 1.1.5+patch
3: stop both nodes
4: start node1 (1.1.1) and verify that timestamps are fixed (zero last 3 digits) 
5: stop node1 
6: start node2 (1.1.5+patch), verify that timestamps are fixed (3 zeros)
7: nodetool resetlocalschema on node2
8: Verify that the troublesome keyspace is *not* present on node2
9: stop node2
10: start node1
11: start node2 
12: verify that the troublesome keyspace appears on node2 and timestamps are fixed (3 zeros);;;","27/Sep/12 14:18;xedin;Committed.;;;","27/Sep/12 15:06;hudson;Integrated in Cassandra #2190 (See [https://builds.apache.org/job/Cassandra/2190/])
    adopt RM code of CASSANDRA-4698 to trunk (Revision a0d7d9713d776506ce6c2eea577eea3b7c5099bd)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/db/RowMutation.java
;;;","11/Oct/12 19:06;tpatterson;Verified that the issue is fixed for 1.1.6-tentative;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactionsTest fails with timeout,CASSANDRA-4695,12608517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,20/Sep/12 19:05,16/Apr/19 09:32,14/Jul/23 05:52,21/Sep/12 15:03,1.2.0 beta 2,,,Legacy/Testing,,,0,test-fail,,,,,,"{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.CompactionsTest:testStandardColumnCompactions:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsTest FAILED (timeout)
{code}","Ubuntu 12.04, Java 1.6, Ant 1.8.2, Maven 3.0.4
Ubuntu 11.10, Java 1.6, Ant 1.8.2, Maven 2.2.1
",azotcsit,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256340,,,Fri Sep 21 15:03:25 UTC 2012,,,,,,,,,,"0|i0gyjb:",97025,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Sep/12 06:28;slebresne;Downgrading the priority of this issue because I'm pretty sure this isn't really a problem. I also have CompactionsTest timeouting for ages on my desktop box with the default timeout (but is passes on some other box). Bumping the junit timeout fixes the issue however, so I think it's really just that CompactionsTest is a bit too long for the default timeout on system with a slow hard drive. ;;;","21/Sep/12 08:07;azotcsit;May be CompactionTest.testStandardColumnCompactions() method should be moved to LongCompactionSpeedTest or somewhere in the long-test testsuite. What do you think about it?;;;","21/Sep/12 15:03;jbellis;good idea.  done in trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log error when using IN together with ORDER BY,CASSANDRA-4689,12608254,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,19/Sep/12 14:57,16/Apr/19 09:32,14/Jul/23 05:52,20/Sep/12 13:59,1.1.6,,,,,,0,,,,,,,"{code}
$ bin/cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use ks;
cqlsh:ks> drop TABLE test;
cqlsh:ks> CREATE TABLE test (my_id varchar, time_id uuid, value int, PRIMARY KEY (my_id, time_id));
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key1', 1, 1);
cqlsh:ks> INSERT INTO test (my_id, time_id, value) VALUES ('key2', 2, 2);
cqlsh:ks> select * from test where my_id in('key1', 'key2') order by time_id;
TSocket read 0 bytes
{code}

The log shows this:
{code}
ERROR [Thrift:5] 2012-09-19 08:44:59,859 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.IllegalArgumentException: Column time_id wasn't found in select clause.
	at org.apache.cassandra.cql3.statements.SelectStatement.getColumnPositionInSelect(SelectStatement.java:866)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:836)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:807)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:137)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1242)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

NOTE: This issue appears similar to https://issues.apache.org/jira/browse/CASSANDRA-4612 from the user perspective, even though 4612 was verified as fixed.","built from source on cassandra-1.1 (b43cc362aa568a46bc53e545c68518b0bd350b76)
os: ubuntu",slebresne,tpatterson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/12 22:13;xedin;CASSANDRA-4689.patch;https://issues.apache.org/jira/secure/attachment/12545814/CASSANDRA-4689.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256338,,,Thu Sep 20 13:59:07 UTC 2012,,,,,,,,,,"0|i0gyhb:",97016,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"19/Sep/12 17:16;xedin;Jonathan, I can easily handle this one if Sylvain has a lot on his plate.;;;","19/Sep/12 22:13;xedin;changes from CASSANDRA-4612 aren't covering extended selections properly.;;;","20/Sep/12 11:05;slebresne;+1;;;","20/Sep/12 13:59;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
comments and error messages use wrong method names,CASSANDRA-4688,12608242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,crotwell,crotwell,crotwell,19/Sep/12 13:50,16/Apr/19 09:32,14/Jul/23 05:52,21/Sep/12 17:21,1.1.6,,,,,,0,,,,,,,comments and error messages in ColumnFamilyInputFormat do not reflect the actual method names in ConfigHelper,,crotwell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/12 13:52;crotwell;trunk-4688.txt;https://issues.apache.org/jira/secure/attachment/12545732/trunk-4688.txt",,,,,,,,,,,,,,,,,1.0,crotwell,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256337,,,Fri Sep 21 17:21:49 UTC 2012,,,,,,,,,,"0|i0gygv:",97014,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Sep/12 13:52;crotwell;patch;;;","19/Sep/12 13:52;crotwell;patch file attached to fix comments and error messages;;;","21/Sep/12 17:21;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scrubbing of CQL3 created tables,CASSANDRA-4685,12608233,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,19/Sep/12 13:00,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/12 12:11,1.2.0 beta 2,,,,,,0,,,,,,,"{noformat}
 INFO 12:20:42,822 Scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db')
 WARN 12:20:42,826 Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Error validating row DecoratedKey(61935297886570031978600740763604084078, 4b6579737061636531)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:244)
        at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:111)
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:157)
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173)
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:495)
        at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:484)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:66)
        at org.apache.cassandra.db.compaction.CompactionManager$3.perform(CompactionManager.java:223)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:193)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
        at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:65)
        at org.apache.cassandra.db.Column.validateFields(Column.java:287)
        at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:378)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:240)
        ... 15 more
 WARN 12:20:42,826 Row at 19 is unreadable; skipping to next
 WARN 12:20:42,827 No valid rows found while scrubbing SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ia-1-Data.db'); it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot
{noformat}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 12:29;slebresne;4685.txt;https://issues.apache.org/jira/secure/attachment/12546502/4685.txt","24/Sep/12 20:40;jbellis;scrub-transparency.txt;https://issues.apache.org/jira/secure/attachment/12546374/scrub-transparency.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256336,,,Wed Sep 26 12:11:11 UTC 2012,,,,,,,,,,"0|i0gyfz:",97010,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Sep/12 20:40;jbellis;attached patch demonstrates that it's trying to validate gossip_generation as utf8, when it's declared int.  bug in schema compiling?

{noformat}
Caused by: org.apache.cassandra.db.marshal.MarshalException: Failed to validate column gossip_generation
        at org.apache.cassandra.db.Column.validateFields(Column.java:295)
        at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:378)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTabl
eIdentityIterator.java:240)
        ... 15 more
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
        at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:65)
{noformat};;;","24/Sep/12 20:41;jbellis;(patch not intended for inclusion, since we can't trust wide row names to not blow up our log file);;;","25/Sep/12 12:29;slebresne;The problem was that scrub was correctly handling the CQL3 ColumnDefinition.  Basically it needs the same thing that we've introduced in CASSANDRA-4377, so attaching patch that move said code to CFMetadata to be used more generally.
;;;","25/Sep/12 16:51;jbellis;+1;;;","26/Sep/12 12:11;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix binary protocol NEW_NODE event,CASSANDRA-4679,12608065,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Sep/12 14:01,16/Apr/19 09:32,14/Jul/23 05:52,05/Nov/12 16:36,1.2.0 beta 2,,,,,,0,,,,,,,"As discussed on CASSANDRA-4480, the NEW_NODE/REMOVED_NODE of the binary protocol are not correctly fired (NEW_NODE is fired on node UP basically). This ticket is to fix that.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 07:24;slebresne;0001-4679.txt;https://issues.apache.org/jira/secure/attachment/12551313/0001-4679.txt","30/Oct/12 07:24;slebresne;0002-Start-RPC-binary-protocol-before-gossip.txt;https://issues.apache.org/jira/secure/attachment/12551314/0002-Start-RPC-binary-protocol-before-gossip.txt","29/Oct/12 17:24;slebresne;0003-Remove-hardcoded-initServer-from-AntiEntropyServiceTes.txt;https://issues.apache.org/jira/secure/attachment/12551212/0003-Remove-hardcoded-initServer-from-AntiEntropyServiceTes.txt",,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239456,,,Mon Nov 05 16:36:48 UTC 2012,,,,,,,,,,"0|i00hgn:",762,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"18/Sep/12 14:19;slebresne;Attaching patch that adds a new listener interface for ""higher level"" events than the gossip ones.;;;","01/Oct/12 10:15;slebresne;I've rebased the patch and added a short paragraph to the protocol doc to explain that UP events are usually sent a few milliseconds before the binary protocol server is fully up (since gossip is started before the protocol server).;;;","01/Oct/12 13:54;jbellis;We can always change that ordering.  Maybe we should.;;;","01/Oct/12 14:15;slebresne;I suppose we can, yes. Though I suspect there may be races so that some queries (I'm thinking schema altering ones though honestly I haven't checked) might trigger assertion errors or related if gossip isn't started. Probably worth checking though. Maybe in a separate ticket however since it's not directly related (clients will have a to handle the case where after a NEW_NODE the node cannot be joined anyway).;;;","23/Oct/12 14:03;slebresne;I've rebased the patch and attaching a second patch to start the binary protocol server before gossip. This seems to be working as expected but the fact that bootstrap is broken on trunk (due to CASSANDRA-4764) makes it a bit harder to test correctly.;;;","26/Oct/12 11:24;slebresne;I've also updated the first patch to send a TOPOLOGY_CHANGE event when a node has moved since that was missing and trivial to add.;;;","29/Oct/12 16:43;yukim;Patch looks good, though AntiEntropyServiceStandardTest/AntiEntropyServiceCounterTest is failing after applying this patch.;;;","29/Oct/12 17:24;slebresne;Interesting. This is due to the hardcoded call to StorageServer.initServer() in AntiEntropyServiceTestAbstract. But I have absolutely no clue what we have that call. In fact, removing that call (patch 3 attached) fixes the test. I'm not totally sure why the test was working previously, maybe the 2 patch of this ticket just changed the timing of the server initialization triggering that issue?;;;","29/Oct/12 21:04;yukim;I found the difference between patched and trunk.
Your initServer tries to ""join ring"" even after server is initialized, whilst in trunk it is guarded by ""initialized"" check.
I think it is better to check if initialized before calling matbeJoinRing in your patch.;;;","30/Oct/12 07:26;slebresne;Ok, you're right and I've updated the first patch to add back the same 'is initialized' check. I leave the third patch though since that hardcoded call is still useless (but the test pass even if we don't remove it).;;;","30/Oct/12 14:28;yukim;+1 for all patches.;;;","30/Oct/12 15:13;slebresne;Committed, thanks;;;","05/Nov/12 16:03;brandon.williams;Reopening because (at least sometimes) this allows thrift to start before MS and gossip, which is wrong, but also other things seem to be breaking like bootstrap.;;;","05/Nov/12 16:14;slebresne;As much as I'm happy to revert the part about starting the thrift/binary protocol server first, I'd prefer some precision about ""breaks all kind of things"". That is, does that break things only if people query the thrift interface before MS and gossip are up (which would make sense, but in real life doesn't seem like very likely to happen (which doesn't mean we shouldn't fix it btw)), or does it break things in other cases?;;;","05/Nov/12 16:16;brandon.williams;Sorry, edited my comment to clarify while you were posting yours :);;;","05/Nov/12 16:36;slebresne;Alright, I've reverted the 'start thrift before gossip part of this patch'. That has never been the most important part anyway. This should fix any regression at least as far as this ticket is concerned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in NTS when using LQ against a node (DC) that doesn't have replica,CASSANDRA-4675,12607993,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,18/Sep/12 01:23,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/12 14:31,1.1.6,,,,,,0,,,,,,,"in a NetworkTopologyStrategy where there are 2 DC:

{panel}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
127.0.0.1       dc1         r1          Up     Normal  115.78 KB       50.00%  0                                           
127.0.0.2       dc2         r1          Up     Normal  129.3 KB        50.00%  85070591730234615865843651857942052864  
{panel}
I have a KS that has replica is 1 of the dc (dc1):

{panel}
[default@unknown] describe Keyspace3;                                                                                                                     
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

But if I connect to a node in dc2, using LOCAL_QUORUM, I get NPE in the Cassandra node's log:

{panel}
[default@unknown] consistencylevel as LOCAL_QUORUM;                       
Consistency level is set to 'LOCAL_QUORUM'.
[default@unknown] use Keyspace3;                                          
Authenticated to keyspace: Keyspace3
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                     
Internal error processing get
org.apache.thrift.TApplicationException: Internal error processing get
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:511)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}

node2's log:
{panel}
ERROR [Thrift:3] 2012-09-17 18:15:16,868 Cassandra.java (line 2999) Internal error processing get
java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.getReplicationFactor(NetworkTopologyStrategy.java:142)
        at org.apache.cassandra.service.DatacenterReadCallback.determineBlockFor(DatacenterReadCallback.java:90)
        at org.apache.cassandra.service.ReadCallback.<init>(ReadCallback.java:67)
        at org.apache.cassandra.service.DatacenterReadCallback.<init>(DatacenterReadCallback.java:63)
        at org.apache.cassandra.service.StorageProxy.getReadCallback(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:609)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.internal_get(CassandraServer.java:383)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:401)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2989)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{panel}

I could workaround it by adding dc2:0 to the option:

{panel}
[default@Keyspace3] describe Keyspace3;                                           
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc2:0, dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

Now you get UA:

{panel}
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                           
null
UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6506)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:519)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}


On a side note, is there a thought on having a CL.LOCAL_ONE? Ie if local node (wrt the dc) does not have replica, on a LOCAL_ONE, it won't try to go across DC to try to get it. It would be similar to LOCAL_QUORUM.",,cywjackson,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/12 22:47;jbellis;4675.txt;https://issues.apache.org/jira/secure/attachment/12546768/4675.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256332,,,Mon Oct 01 22:12:57 UTC 2012,,,,,,,,,,"0|i0gycn:",96995,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Sep/12 15:14;jbellis;Not sure what version this stacktrace is supposed to be against.  Doesn't appear to be against a recent 1.0 or 1.1.;;;","18/Sep/12 22:21;cywjackson;1.0.10;;;","26/Sep/12 22:47;jbellis;Following ""explicit is better than implicit"" convention, I was inclined to make it raise an InvalidRequestException instead of failing with NPE.  But the request isn't invalid; it's a configuration problem.

After discussion on IRC, decided the best thing to do is just default undefined DC to zero replicas.  Patch attached to do that.;;;","26/Sep/12 23:37;brandon.williams;+1;;;","27/Sep/12 14:31;jbellis;committed;;;","01/Oct/12 22:12;cywjackson;+1 thx;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh COPY TO and COPY FROM don't work with cql3,CASSANDRA-4674,12607992,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,18/Sep/12 01:06,16/Apr/19 09:32,14/Jul/23 05:52,15/Oct/12 17:33,1.2.0 beta 2,,,,,,0,cqlsh,,,,,,cqlsh COPY TO and COPY FROM don't work with cql3 due to previous cql3 changes.,,aleksey,marco.matarazzo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/12 16:50;aleksey;CASSANDRA-4674.txt;https://issues.apache.org/jira/secure/attachment/12549164/CASSANDRA-4674.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248677,,,Mon Oct 15 17:33:13 UTC 2012,,,,,,,,,,"0|i09xpr:",55903,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"18/Sep/12 13:07;aleksey;Mostly irrelevant now.;;;","15/Oct/12 10:47;marco.matarazzo;Well, We would love to have it fixed.;;;","15/Oct/12 15:28;aleksey;[~marco.matarazzo] It's a new issue, caused by a different reason. Affects cql3 in trunk, not cql2.
I'm looking into it.;;;","15/Oct/12 16:03;aleksey;Though COPY TO and COPY FROM still work in CQL3 as long as you don't provide any optional parameters to it.;;;","15/Oct/12 16:16;marco.matarazzo;I realized I hijacked another ticket thinking that mine was a well-known problem, when it's obvious it's not. I'm very sorry about that.

I have some specific column families on which COPY FROM does not work from cqlsh -3, and works perfectly with cqlsh -2. 

If it's ok with you, I'll hunt down the condition that makes it not work and, as soon as I have them pinned down, I will open a new bug.

Again, I'm sorry about that.
;;;","15/Oct/12 16:25;aleksey;Well, you've accidentally uncovered a COPY TO/COPY FROM bug, so there is nothing to apologize for. Thank you (:
Turns out COPY TO/COPY FROM has had broken completion for a really long time, which in combination with a recent commit (2f979ed60fc4f9dab2db7ce9921ff2953acd714c for CASSANDRA-4488) broke COPY TO/COPY FROM with non-default parameters.;;;","15/Oct/12 17:33;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
_TRACE verb is not droppable which causes an AssertionError,CASSANDRA-4672,12607934,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dr-alves,dr-alves,dr-alves,17/Sep/12 18:49,16/Apr/19 09:32,14/Jul/23 05:52,22/Oct/12 21:32,1.2.0 beta 2,,,,,16/Sep/12 00:00,0,,,,,,,When a big enough statement is traced (like select *) an assertion error is fired because the _TRACE verb is not droppable.,,dr-alves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/12 18:52;dr-alves;4672.patch;https://issues.apache.org/jira/secure/attachment/12545454/4672.patch",,,,,,,,,,,,,,,,,1.0,dr-alves,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250448,,,Mon Oct 22 21:32:30 UTC 2012,,,,,,,,,,"0|i0ayl3:",61876,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Sep/12 18:52;dr-alves;attaching trivial fix ;;;","17/Sep/12 18:52;dr-alves;trivial fix attached
;;;","26/Sep/12 22:16;jbellis;I get making _TRACE droppable, but what is the rest of the patch doing?;;;","22/Oct/12 21:32;jbellis;committed the droppableVerb part;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty .cqlsh_history file causes cqlsh to crash on startup.,CASSANDRA-4669,12607713,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,boneill,boneill,boneill,14/Sep/12 20:55,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 01:44,1.1.6,,,Legacy/Tools,,,0,,,,,,,"Not sure how I got it, but I ended up with an empty .cqlsh_history file.  In that state, when starting cqlsh, you end up with:

bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 2588, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""bin/cqlsh"", line 2543, in main
    readline.read_history_file(HISTORY)
IOError: [Errno 22] Invalid argument

Its a simple fix to check for a non-empty history file.  I'll attach the patch.",Python 2.7.1 on Mac OSX ,aleksey,boneill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/12 02:31;aleksey;fix-cqlsh-history.patch;https://issues.apache.org/jira/secure/attachment/12545516/fix-cqlsh-history.patch","14/Sep/12 20:58;boneill;trunk-4669.txt;https://issues.apache.org/jira/secure/attachment/12545209/trunk-4669.txt",,,,,,,,,,,,,,,,2.0,boneill,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256330,,,Tue Sep 18 02:42:50 UTC 2012,,,,,,,,,,"0|i0gyav:",96987,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"14/Sep/12 20:58;boneill;Here is the patch.;;;","14/Sep/12 20:58;boneill;Patch attached.;;;","14/Sep/12 21:47;brandon.williams;Committed.;;;","18/Sep/12 01:42;aleksey;This patch breaks cqlsh history loading. Also, the original issue isn't an issue at all - readline handles empty history files perfectly well.
Reverting the commit should fix the new issue.;;;","18/Sep/12 01:44;brandon.williams;Reverted.;;;","18/Sep/12 01:59;boneill;I trust you guys, but you may just want to double check that this isn't an issue.  I can easily reproduce the problem.  Here is a log.  You can see it working initially.  I truncate the file, and no joy.

bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> quit
bone@zen:~/dev/boneill42/cassandra-> rm -fr ~/.cqlsh_history 
bone@zen:~/dev/boneill42/cassandra-> touch ~/.cqlsh_history
bone@zen:~/dev/boneill42/cassandra-> bin/cqlsh 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 2588, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""bin/cqlsh"", line 2543, in main
    readline.read_history_file(HISTORY)
IOError: [Errno 22] Invalid argument
bone@zen:~/dev/boneill42/cassandra-> ;;;","18/Sep/12 02:03;brandon.williams;Maybe we should just catch any error from loading the history, issue a warning, and move along.;;;","18/Sep/12 02:13;aleksey;Brian, what are your python/readline versions and OS?

{quote}
 ➤ bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra unknown | CQL spec 3.0.0 | Thrift protocol 19.34.0]
Use HELP for help.
cqlsh> quit
 ➤ rm -fr ~/.cqlsh_history 
 ➤ touch ~/.cqlsh_history
 ➤ bin/cqlsh 
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra unknown | CQL spec 3.0.0 | Thrift protocol 19.34.0]
Use HELP for help.
cqlsh>
{quote}

Also, shell will write empty .cqlsh_history when you open it for the first time and close it without entering any commands. And will open just fine next time.

Brandon, or do that, yes. Then we can get rid of os.path.exists(HISTORY) check.;;;","18/Sep/12 02:28;boneill;
Not sure how to tell exact version of readline. (I'm a java head =)
This is on OSX.  Looks like the version is the one provided by apple in the python install.

Here is what i've got:
bone@zen:~/dev/boneill42/cassandra-> python
Python 2.7.1 (r271:86832, Jul 31 2011, 19:30:53) 
[GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import readline
>>> readline
<module 'readline' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/readline.so'>

FWW, this can be a really annoying error unless you can read Python.  It doesn't get far enough to recrete the history file.  So, the problem persists. Your average user would be stuck.  I first reinstalled Cassandra because I thought something was corrupt.  Fortunately, python isn't compiled. =)
;;;","18/Sep/12 02:36;aleksey;Ah, OS X. Maybe it really does fail on OS X. Works on Ubuntu though. Attached a patch that catches IOError when reading/writing from/to history file.;;;","18/Sep/12 02:36;boneill;Beautiful.  Thanks guys. ;;;","18/Sep/12 02:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix consistency ALL parsing in CQL3,CASSANDRA-4659,12607311,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,12/Sep/12 17:35,16/Apr/19 09:32,14/Jul/23 05:52,12/Sep/12 19:21,1.2.0 beta 1,,,,,,0,,,,,,,"CASSANDRA-4490 made some change to the parsing of ALL for consistency levels (introducing a specific token K_ALL). It's unclear why since that new token is not used (that is, except for the consistency level), probably some left over of a previous version of the patch.

In any case, this doesn't work. K_ALL and K_LEVEL being both tokens, the string 'ALL' will always generate K_ALL and never K_LEVEL and thus 'USING CONSISTENCY ALL' doesn't parse anymore.",,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 17:41;slebresne;4659.txt;https://issues.apache.org/jira/secure/attachment/12544850/4659.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256323,,,Wed Sep 12 19:21:27 UTC 2012,,,,,,,,,,"0|i0gy73:",96970,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"12/Sep/12 17:41;slebresne;Trivial patch that revert the change from CASSANDRA-4490 since it's not used anyway.;;;","12/Sep/12 19:17;xedin;+1, it was a left behind, grant/revoke commands actually using {full, no}_access.;;;","12/Sep/12 19:21;slebresne;Committed, thanks
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql version race condition with rpc_server_type: sync,CASSANDRA-4657,12607257,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ecourreges,ecourreges,12/Sep/12 15:10,16/Apr/19 09:32,14/Jul/23 05:52,08/Oct/12 16:05,1.1.6,,,,,,0,features,,,,,,"If clients connect to a cassandra cluster configured with rpc_server_type: sync with heterogeneous cql versions (2 and 3), the cql version used for execution on the server changes seemingly randomly.
It's due to the fact that CustomTThreadPoolServer.java does not set the remoteSocket anytime, or does not clear the cql version in the ThreadLocal clientState object.
When CassandraServer.java calls state() it gets the ThreadLocal object clientState, which has its cqlversion already changed by a previous socket that was using the same thread.


The easiest fix is probably to do a SocketSessionManagementService.instance.set when accepting a new client and SocketSessionManagementService.instance.remove when the client is closed, but if you really want to use the ThreadLocal clientState and not alloc/destroy a ClientState everytime, then you should clear this clientState on accept of a new client.

The problem can be reproduced with cqlsh -3 on one side and a client that does not set the cql version, expecting to get version 2 by default, but actually gettingv v2/v3 depending on which thread it connects to.

The problem does not happen with other rpc_server_types, nor with clients that set their cql version at connection.",Ubuntu 12.04,dbrosius@apache.org,ecourreges,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 15:54;ecourreges;4657.patch;https://issues.apache.org/jira/secure/attachment/12544831/4657.patch",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,244165,,,Mon Oct 08 16:05:40 UTC 2012,,,,,,,,,,"0|i05iu7:",30149,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,"12/Sep/12 15:54;ecourreges;Patch with set and remove;;;","06/Oct/12 03:56;jbellis;Thanks Emmanuel!

Your patch will work, but I'd like to do a deeper cleanup.

I've pushed this to https://github.com/jbellis/cassandra/branches/4657.

In order these commits

# fix cql version reset under the existing regime of CassandraServer.clientState threadlocal
# centralizes all session management into ThriftSessionManager
# removes the threadlocal approach in favor of standardizing on the socket-based approach everywhere;;;","06/Oct/12 05:17;dbrosius@apache.org;+1 patch lgtm.;;;","08/Oct/12 16:05;jbellis;committed reset fix to 1.1.6, remainder to trunk (after much rebasing over CASSANDRA-4608, grr);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate operation doesn't delete rows from HintsColumnFamily.,CASSANDRA-4655,12607241,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,azotcsit,azotcsit,12/Sep/12 13:48,16/Apr/19 09:32,14/Jul/23 05:52,23/May/13 15:07,1.2.6,,,,,,0,hintedhandoff,truncate,,,,,"Steps to reproduce:
1. Start writing of data to some column family, let name it 'MyCF'
2. Stop 1 node
3. Wait some time (until some data will be collected in HintsColumnFamily)
4. Start node (HintedHandoff will be started automatically for 'MyCF')
5. Run 'truncate' command for 'MyCF' column family from command from cli
6. Wait until truncate will be finished
7. You will see that 'MyCF' is not empty because HintedHandoff is copying data 

So, I suggest to clean HintsColumnFamily (for truncated column family) before we had started to discard sstables. 
I think it should be done in CompactionManager#submitTrucate() method. I can try to create patch but I need to know right way of cleaning HintsColumnFamily. Could you clarify it?
","Centos 6.2, Cassandra 1.1.4 (DataStax distribution), three-nodes cluster.",azotcsit,christianmovi,jdilloyd,rcoli,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/13 23:24;jbellis;4655-v3.txt;https://issues.apache.org/jira/secure/attachment/12584401/4655-v3.txt","06/Dec/12 11:48;azotcsit;cassandra-1.2-4655-hints_truncation-v2.txt;https://issues.apache.org/jira/secure/attachment/12556263/cassandra-1.2-4655-hints_truncation-v2.txt","26/Sep/12 08:22;azotcsit;cassandra-1.2-4655-hints_truncation.txt;https://issues.apache.org/jira/secure/attachment/12546659/cassandra-1.2-4655-hints_truncation.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250235,,,Thu May 23 15:07:46 UTC 2013,,,,,,,,,,"0|i0awfj:",61527,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"21/Sep/12 16:35;jbellis;Maybe it would be easier to record truncate time and drop hints older than that on replay, than to make truncate a two-pass operation.;;;","24/Sep/12 08:57;azotcsit;Good idea. I'll try to implement it.;;;","26/Sep/12 08:24;azotcsit;Patch has been attached. Please review it.;;;","26/Sep/12 15:20;jbellis;Two things:

# Need to store last truncation time across server restart.  Storing these as a Map in {{system.local}} is one option.  ({{system}} columnfamilies are defined in CFMetaData.)
# Instead of skipping the entire hint if any component CF has been truncated, should generate a new RowMutation with only untruncated CF data in it.;;;","07/Nov/12 21:33;jbellis;Are you still working on this, Alexey?;;;","06/Dec/12 11:48;azotcsit;Jonathan, I'm sorry for so long delay and a silence from my side. 
I've created another patch against current trunk (46f1c7fabd6a7102a7a363bcfdc40c62a3bdc461). It uses local column family as you suggested. I tested it on three-nodes ccm cluster with replication factor 2. Steps:
1. load 150KB per node
2. stop one of them
3. wait until both live nodes will have about 20MB of data
4. stop loading of data
5. start downed node
6. wait 3-5 seconds after start of hints delivery process and truncate data
7. wait until data will be successfully truncated
Results:
Nodes which were not stopped are completely empty. Another node [that was stopped] contains about several tens rows but all their were marked as deleted. Note that the both nodes which weren't stopped before truncate operation has about 5-6 thousands of rows.

Please review the patch.;;;","26/Feb/13 06:43;azotcsit;Is there any progress on review?;;;","27/Feb/13 09:47;vijay2win@yahoo.com;Overall LGTM, 

Do we really need to remove removeTruncationTime? IMHO it might be a useful information for the user to query.
Instead of caching the truncatedAt in CFS can we just cache it in deliverHintsToEndpointInternal? coz it wont be thread safe in CFS anyways.
;;;","25/Mar/13 15:54;jbellis;Ping [~azotcsit];;;","22/May/13 23:24;jbellis;v3 attached.  I've retained removing obsolete truncation information; otherwise, in the case of creating many temporary tables we will cause problems eventually.

Moved caching to HHOM.  Also combined the new hint time with the existing truncated_at blob for simplicity and to make sure both get updated as a group.;;;","23/May/13 02:33;vijay2win@yahoo.com;+1;;;","23/May/13 15:07;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra 1.2 should not accept CQL version ""3.0.0-beta1""",CASSANDRA-4649,12607145,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,11/Sep/12 23:10,16/Apr/19 09:32,14/Jul/23 05:52,25/Sep/12 12:38,1.2.0 beta 2,,,Legacy/CQL,,,0,cql3,,,,,,"During Cassandra 1.1's whole lifecycle, the CREATE KEYSPACE syntax was pretty dramatically and incompatibly different from what is there now for 1.2. That's ok, since we explicitly said there could be breaking changes in the syntax before 3.0.0 final, but at least we should make it clear that 3.0.0 is not compatible with the 3.0.0-beta1 syntax we had out for quite a while.

If we don't want to reject connections asking for 3.0.0-beta1, we should bump the version number to 3.0.1 or something.",,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 13:13;slebresne;4649.txt;https://issues.apache.org/jira/secure/attachment/12544814/4649.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255191,,,Tue Sep 25 12:38:20 UTC 2012,,,,,,,,,,"0|i0epyn:",83968,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"12/Sep/12 13:13;slebresne;Patch attached that
* changes the version to 3.0.0 (we don't have anything in store that would need to break syntax further and we're about to roll 1.2.0 beta1 soon (hopefully) so I think it's indeed time to drop the beta).
* Makes 1.2 refuse 3.0.0-beta1 version (with an hopefully useful error message).

I note however that 1.1 was happily accepting 3.0.0 as a version, and I don't think we have advertised much that people should use 3.0.0-beta1 in 1.1, so most people will be surprised that their CREATE KEYSPACE query is invalid all of a sudden anyway (including user of cqlsh). We do have a NEWS entry to explain that however so not sure there is more we can/need to do. ;;;","20/Sep/12 16:37;thepaul;+1.;;;","25/Sep/12 12:38;slebresne;Oups, forgot that one. Committed now. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start Cassandra with simple authentication enabled,CASSANDRA-4648,12607116,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jsanda,jsanda,11/Sep/12 19:39,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/12 09:03,1.2.0 beta 2,,,,,,0,security,,,,,,"I followed the steps for enabling simple authentication as described here, http://www.datastax.com/docs/1.1/configuration/authentication. I tried starting Cassandra with, 

cassandra -f -Dpasswd.properties=conf/passwd.properties -Daccess.properties=conf/access.properties

Start up failed with this exception in my log:

ERROR [main] 2012-09-11 15:03:04,642 CassandraDaemon.java (line 403) Exception encountered during startup
java.lang.AssertionError: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:136)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:298)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:203)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:386)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:429)
Caused by: org.apache.cassandra.exceptions.InvalidRequestException: You have not logged in
        at org.apache.cassandra.service.ClientState.validateLogin(ClientState.java:254)
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:235)
        at org.apache.cassandra.cql3.statements.SelectStatement.checkAccess(SelectStatement.java:105)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:106)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:124)
        ... 4 more",Mac OS X,aleksey,jsanda,omid,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 12:32;slebresne;4648.txt;https://issues.apache.org/jira/secure/attachment/12544811/4648.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256315,,,Sun Nov 18 12:39:17 UTC 2012,,,,,,,,,,"0|i0gy2v:",96951,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"11/Sep/12 20:02;jbellis;Related: CASSANDRA-4617

I think we need to either not use QP internally, or add a compile-and-local-query only mode that skips StorageProxy and auth.;;;","12/Sep/12 12:32;slebresne;Attaching patch that is so that processInternal skips StorageProxy and authorization (so it also solve CASSANDRA-4617 in particular).

I'm keen on keeping QP here because we use a mix of cf with and without compact storage internally, and not using QP would get annoying and error prone, while QP already deal with that. Also, collections are another thing that might be painful without QP (I don't think we have any in the system tables yet but I'm betting we'll have some soon enough). ;;;","26/Sep/12 17:56;jbellis;Is permissionalteringstatement related or just a refactor that happened to be included in this patch?

Rest LGTM.

NB I removed IOException from IM.apply in b781ee7d52c9f30136ae2ce851c4af6def8df38e so you can drop that catch block.;;;","27/Sep/12 07:32;slebresne;bq. Is permissionalteringstatement related or just a refactor that happened to be included in this patch?

It was meant to make it easier to add executeInternal, i.e. adding it to PermissionAlteringStatement only instead of all its subclasses. And since it factor a few other methods too ...;;;","27/Sep/12 09:03;slebresne;Alright, committed after rebase (and removal of IOException catch). Thanks;;;","17/Nov/12 21:37;aleksey;We still need something like old processInternal (not restricting queries to the local node). Had to reimplement pre-change processInternal in NativeAuthority (#4874), will have to do the same in NativeAuthenticator unless processInternal gets changed (or another method like it is added).

It is my understanding that this patch does more than just skipping authorization (which is achieved easier by simply moving isInternall check in has*Access higher, before validateLogin happens).;;;","18/Nov/12 12:39;slebresne;Yes, authentication was only one small problem, the big one this ticket solvers is that it's not a good idea to use the normal path (basically everythign related to replication) to write to the System tables (that are not replicated anyway).

If there is a need to do normal queries but without authentication checks, I would look into subclassing ClientState with no-op authentication methods.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
(CQL3) Re-allow order by on non-selected columns,CASSANDRA-4645,12607078,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Sep/12 15:07,16/Apr/19 09:32,14/Jul/23 05:52,05/Nov/12 16:06,1.2.0 beta 2,,,,,,0,,,,,,,"CASSANDRA-4612 added a limitation to ORDER BY query in that it requires the columns part of the ORDER BY to be in the select clause, while this wasn't the case previously.

The reason for that is that for ORDER BY with IN queries, the sorting is done post-query, and by the time we do the ordering, we've already cut down the result set to the select clause, so if the column are not in the select clause we cannot sort on them.

We should remove that that limitation however as this is a regression from what we had before. As far as 1.2.0 is concerned, at the very least we should lift the limitation for EQ queries since we don't do any post-query sorting in that case and that was working correctly pre-CASSANDRA-4612. But we should also remove that limitation for IN query, even if it's in a second time.",,gwicke,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/12 11:19;slebresne;4645.txt;https://issues.apache.org/jira/secure/attachment/12552087/4645.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239465,,,Mon Nov 05 16:06:53 UTC 2012,,,,,,,,,,"0|i00hjj:",775,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Nov/12 11:19;slebresne;Attaching trivial patch to fix the limitation introduced by CASSANDRA-4612. I.e. it doesn't force the order by columns to be select, unless this is an IN query. I've opened CASSANDRA-4911 to remove the limitation for IN queries, but that part has never worked anyway so it's more of an improvement.;;;","05/Nov/12 15:28;jbellis;+1;;;","05/Nov/12 16:06;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make CQL3 the default,CASSANDRA-4640,12606864,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Sep/12 13:13,16/Apr/19 09:32,14/Jul/23 05:52,10/Sep/12 16:51,1.2.0 beta 1,,,Legacy/CQL,,,0,,,,,,,"In 1.2, CQL3 will be final and thus I believe we should make it the default for CQL (and thus cqlsh).

Of course CQL2 will still be available, one will just have to call set_cql_version (for thrift) or 'cqlsh -2'.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/12 13:14;slebresne;4640.txt;https://issues.apache.org/jira/secure/attachment/12544464/4640.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256308,,,Mon Sep 10 16:51:23 UTC 2012,,,,,,,,,,"0|i0gxzj:",96936,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Sep/12 13:37;jbellis;Maybe add to NEWS that set_cql_version is available and supported in 1.1?  Otherwise +1.;;;","10/Sep/12 16:51;slebresne;Committed (with the mention that set_cql_version was already supported in 1.1). Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh --color option doesn't allow you to disable colors,CASSANDRA-4634,12606735,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,08/Sep/12 21:53,16/Apr/19 09:32,14/Jul/23 05:52,10/Sep/12 20:07,1.1.6,,,,,,0,,,,,,,"There's no way to disable colors with cqlsh, despite it having a {{--color}} option, because that option can only enable color if present, not disable it, and the default is that color is enabled.

(Incidentally, if the {{--file}} option is used, it will disable color.)",,slebresne,thepaul,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/12 18:37;thobbs;4634-color-option-v2.txt;https://issues.apache.org/jira/secure/attachment/12544500/4634-color-option-v2.txt","08/Sep/12 22:49;thobbs;4634-color-option.txt;https://issues.apache.org/jira/secure/attachment/12544371/4634-color-option.txt",,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256304,,,Mon Sep 10 20:07:42 UTC 2012,,,,,,,,,,"0|i0gxxb:",96926,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"08/Sep/12 22:49;thobbs;Attached patch changes the color option to accept a choice of {{always}}, {{never}}, or {{auto}}, defaulting to {{auto}}, which uses colors if not reading from a file and stdout is connected to a terminal (at least as well as we can detect that).  I based the options off of {{ls}} and {{grep}}, which use the same format.;;;","09/Sep/12 00:13;thepaul;Since I'm not a particular fan of ls/grep's command-line color control (too much typing for cases which are fairly common) and since this isn't backwards compatible (using {{\--color}} alone will give an error), I'm inclined to vote -1 on it. Can we just add a {{\--no-color}} option to force color off for the people who don't want to do {{TERM=dumb}} or {{cqlsh | cat}} or something?;;;","10/Sep/12 08:36;slebresne;I don't have any problem with ls/grep's command-line color control but I would tend to agree that it's probably not worth breaking any backward compatibility and a --no-color would be good enough. I don't care much about the matter though, just my 2 cents.;;;","10/Sep/12 18:37;thobbs;v2 patch adds a {{\-\-no-color}} option instead.  The behavior now is that with {{\-\-color}}, color will always be enabled, with {{\-\-no-color}}, it will always be disabled, and without either one, the behavior is similar to {{auto}}, where color is only enabled if not reading a file and stdout is connected to a terminal.;;;","10/Sep/12 19:12;thepaul;+1;;;","10/Sep/12 20:07;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress:  --enable-cql does not work with COUNTER_ADD,CASSANDRA-4633,12606649,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,cdaw,cdaw,07/Sep/12 19:55,16/Apr/19 09:32,14/Jul/23 05:52,11/Sep/12 19:53,1.2.0,,,Legacy/Tools,,,0,,,,,,,"When I remove --enable-cql the following runs successfully.
Note:  INSERT/READ are fine.

{code}
./cassandra-stress --operation=COUNTER_ADD --enable-cql --replication-factor=3 --consistency-level=ONE --num-keys=10000  --columns=20 

total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [1] retried 10 times - error incrementing key 0001 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

Operation [0] retried 10 times - error incrementing key 0000 ((InvalidRequestException): cannot parse 'C58' as hex bytes)

0,0,0,NaN,0
FAILURE
{code}",,cdaw,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/12 19:46;aleksey;CASSANDRA-4633.patch;https://issues.apache.org/jira/secure/attachment/12544694/CASSANDRA-4633.patch",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256303,,,Tue Sep 11 19:53:51 UTC 2012,,,,,,,,,,"0|i0gxwv:",96924,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"11/Sep/12 19:53;xedin;+1, Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
minimum stack size for u34 and later is 180k,CASSANDRA-4631,12606609,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Sep/12 15:26,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 16:06,1.0.12,1.1.5,,,,,0,,,,,,,"We currently only set the stack to 180k for java 7, but it looks like java 6 u34 and later now need this too.  Let's just set them all to 180k.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/12 15:26;brandon.williams;4631.txt;https://issues.apache.org/jira/secure/attachment/12544228/4631.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256301,,,Fri Sep 07 15:54:29 UTC 2012,,,,,,,,,,"0|i0gxvz:",96920,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Sep/12 15:26;brandon.williams;Trivial patch.;;;","07/Sep/12 15:54;jbellis;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL/JDBC: date vs. timestamp issues,CASSANDRA-4628,12606538,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mkrumpholz,mkrumpholz,mkrumpholz,07/Sep/12 07:25,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 15:23,1.2.0 beta 1,,,,,,0,cql,jdbc,,,,,"Cassandra's datatypes only have one Date/Time type named timestamp containing both date and time. Calling the validator org.apache.cassandra.db.marshal.DateType might be OK in general but can be confusing in the jdbc context where there is a distinction between date, time and timestamp. In terms of jdbc there should be more datatypes for dates and times or the jdbc driver should take one of the following options:
- stick to timestamp
- check if the date has a time part and distinguish by the data between date and timestamp automatically
- use distinct datatypes according to the jdbc spec, the types would need to be in cassandra then too

Now back to my actual problem:
org.apache.cassandra.cql.jdbc.JdbcDate returns Types.DATE in getType(). Even if having inserted a complete date with time (making it a timestamp) the ResultSetMetaData.getColumnType() implementation still returns Types.DATE (source of this is in JdbcDate). If some other java code (where i don't have access to) uses the metadata to get the type and then getDate() to get the value the time is cut off the value and only the date is returned.

But the ResultSet.getObject() implementation returns a complete java.util.Date including the time.",,mkrumpholz,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mkrumpholz,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256298,,,Fri Sep 07 15:27:16 UTC 2012,,,,,,,,,,"0|i0gxun:",96914,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Sep/12 10:26;slebresne;I'm a fairly strong believer that adding the Jdbc* classes was a mistake in the first place. See CASSANDRA-4495 for a bit more argumentation.

So I must admit that I'm not sure I understand what it is you suggest exactly concerning Cassandra itself in this ticket, so I don't want to close this right way, but I have the feeling that whatever needs to be done probably concerns the jdbc driver, not Cassandra.

As a side note, I wouldn't disagree that Cassandra/CQL handling of dates is a tad limited and it might probably be worth beefing it up at some point, but that's largely a different issue.
;;;","07/Sep/12 11:22;mkrumpholz;The JdbcDate class is not part of the jdbc driver package which is a separate project at google (as you mentioned in my other issue). But the getType there returns the JDBC datatype (from java.sql.Types) that is returned in the metadata according to the jdbc spec. The problem is that Types.DATE is the wrong type. It should be Types.TIMESTAMP because the data can be more that just a date, it can be a timestamp. Using ResultSet.getDate() because of the returned type cuts off the time part of the timestamp. JDBC has the date time distinction, cassandra not. in short words, but i assume you are aware of this: date != timestamp, timestamp = date + time;;;","07/Sep/12 14:19;jbellis;bq. the getType there returns the JDBC datatype 

that is an issue with the jdbc driver then, not Cassandra;;;","07/Sep/12 15:15;slebresne;I guess it is true that JdbcDate could have been made to return java.sql.Types.TIMESTAMP rather than DATE for its getJdbcType method. ;;;","07/Sep/12 15:18;jbellis;Oops, I guess that is our code.;;;","07/Sep/12 15:23;mkrumpholz;{quote}
that is an issue with the jdbc driver then, not Cassandra
{quote}
That would be the case only if that class (JdbcDate) would NOT be part of cassandra (apache-cassandra-clientutil-x.x.x.jar) but located in the jdbc lib/jar in the project at google. 

{quote}
Oops, I guess that is our code.
{quote}
Right! For now it's within cassandra code and included as dependency in cassandra-jdbc. If this type class and the meaning of if should change with CASSANDRA-4495 then the code should be moved to the jdbc project or a update there should be done. I don't know how much the casandra dev team is involved with the cassandra-jdbc project...;;;","07/Sep/12 15:23;jbellis;fixed in e1201dff2c9c89ce2ff9a197ac5d99d9288c06e2;;;","07/Sep/12 15:25;jbellis;(I think the confusion arose because it represents a Java Date object, which does include time.  So we had a DateType around, and when we added CQL just named the corresponding class JdbcDate following the convention for the other classes.);;;","07/Sep/12 15:27;mkrumpholz;yeah, as i wrote in the issue description, in the schema_columns table it is a DateType class, data type in the create statement is timestamp as it is and should be in the jdbc context.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support inet data type,CASSANDRA-4627,12606532,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,07/Sep/12 05:12,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/12 19:23,1.2.0 beta 1,,,Legacy/Tools,,,0,cql,cqlsh,,,,,"CASSANDRA-4018 introduced a new cassandra data type with a cql name ""inet"", which is not yet supported in cqlsh. Add support for decoding and formatting.",,omid,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/12 09:34;slebresne;4627.txt;https://issues.apache.org/jira/secure/attachment/12544445/4627.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256297,,,Thu Sep 13 19:23:01 UTC 2012,,,,,,,,,,"0|i0gxu7:",96912,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"09/Sep/12 04:16;thepaul;Turns out this only required an update to the python-cql driver. Once a new release is out I'll update the bundled version in lib/.;;;","10/Sep/12 09:34;slebresne;Since this ticket was open with a generic enough title, attaching a trivial patch that adds ""support"" in a few places where it was missing, namely the binary protocol and the CQL documentation.;;;","10/Sep/12 15:58;thepaul;Oops, I intended this to be just for adding the support to cqlsh. But your patch does add useful support, so I'm in favor of including that as well. I just ask that we not commit it or close the ticket until I can add in the driver update.;;;","11/Sep/12 10:12;slebresne;I know that was for cqlsh :)

Anyway, I've committed the patch but leaving that open until the driver is updated;;;","12/Sep/12 21:01;thepaul;My additional changes committed into the 4627 branch in my github clone:

http://github.com/thepaul/cassandra/tree/4627

Notice that this includes an update of the internal CQL lib, so there's a zipfile to add and one to remove.;;;","13/Sep/12 19:23;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple values for CurrentLocal Node ID,CASSANDRA-4626,12606503,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,amorton,amorton,06/Sep/12 23:25,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 16:46,1.0.12,1.1.5,,,,,0,,,,,,,"From this email thread http://www.mail-archive.com/user@cassandra.apache.org/msg24677.html

There are multiple columns for the CurrentLocal row in NodeIdInfo:

{noformat}

[default@system] list NodeIdInfo ;
Using default limit of 100
...
-------------------
RowKey: 43757272656e744c6f63616c
=> (column=01efa5d0-e133-11e1-0000-51be601cd0ff, value=0a1020d2, timestamp=1344414498989)
=> (column=92109b80-ea0a-11e1-0000-51be601cd0af, value=0a1020d2, timestamp=1345386691897)
{noformat}

SystemTable.getCurrentLocalNodeId() throws an assertion that occurs when the static constructor for o.a.c.utils.NodeId is in the stack.

The impact is a java.lang.NoClassDefFoundError when accessing a particular CF (I assume on with counters) on a particular node.

Cannot see an obvious cause in the code. ",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/12 15:01;slebresne;4626.txt;https://issues.apache.org/jira/secure/attachment/12544224/4626.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256296,,,Fri Sep 07 16:46:11 UTC 2012,,,,,,,,,,"0|i0gxtr:",96910,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Sep/12 15:01;slebresne;I think this can happen because of the commit log. Basically, it's possible that when you restart a node that it doesn't pick the correct current NodeId if he attempts to read the current NodeId before the commit log if fully replayed (and the more recent NodeId is in the log, not yet replayed). This would then lead to having 2 columns in the CurrentLocal row.

However, the main problem is that the way we maintain the CurrentLocal row is fragile and honestly dumb (I wrote it so I'm blaming myself). We store all the generated NodeId sorted by creation time in a separated row, so reading the last column of that row is a much simpler and resilient way to do it. Attaching a patch that does just that.  

The patch also adds a forceFlush in SystemTable.writeCurrentNodeId to avoid the problem of not reading the last NodeId because of log replay.
;;;","07/Sep/12 15:04;slebresne;The patch is against 1.0. However if we're incomfortable messing with the SystemTable with 1.0, there is always the workaround of deleting the NodeIdInfo sstables.;;;","07/Sep/12 15:16;jbellis;Maybe we should name the accessor methods CounterId instead of NodeId?  NodeId is confusingly similar to HostId.;;;","07/Sep/12 16:11;jbellis;+1 on the fix;;;","07/Sep/12 16:26;slebresne;bq. Maybe we should name the accessor methods CounterId instead of NodeId?

That's a good idea, I'll do that.;;;","07/Sep/12 16:46;slebresne;Committed, thanks (I haven't done the rename yet, I'll do it later).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY validation is not restrictive enough,CASSANDRA-4624,12606412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,06/Sep/12 14:51,16/Apr/19 09:32,14/Jul/23 05:52,06/Sep/12 15:57,1.1.5,1.2.0 beta 1,,,,,0,cql3,,,,,,"We're not able to do order by on anything that is a key range. However, we only refuse queries that have an empty where clause, but that doesn't exclude all key ranges at all.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/12 14:53;slebresne;4624.txt;https://issues.apache.org/jira/secure/attachment/12544052/4624.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256294,,,Thu Sep 06 15:57:20 UTC 2012,,,,,,,,,,"0|i0gxsv:",96906,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Sep/12 15:12;jbellis;+1;;;","06/Sep/12 15:57;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
corrupted saved caches,CASSANDRA-4622,12606257,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,05/Sep/12 15:31,16/Apr/19 09:32,14/Jul/23 05:52,11/Sep/12 17:44,1.2.0,,,,,,0,,,,,,,"I'm seeing this fairly frequently on trunk:

{noformat}

 INFO 05:15:23,805 reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
 WARN 05:15:23,808 error reading saved cache /var/lib/cassandra/saved_caches/system-schema_columnfamilies-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
 INFO 05:15:23,858 Opening /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ia-5 (171 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-1 (7983 bytes)
 INFO 05:15:23,870 Opening /var/lib/cassandra/data/system/peers/system-peers-ia-2 (7876 bytes)
 INFO 05:15:23,884 Opening /var/lib/cassandra/data/system/local/system-local-ia-35 (4910 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-33 (75 bytes)
 INFO 05:15:23,885 Opening /var/lib/cassandra/data/system/local/system-local-ia-34 (4676 bytes)
 INFO 05:15:23,912 reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
 WARN 05:15:23,912 error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:151)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:247)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:362)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:334)
        at org.apache.cassandra.db.Table.initCf(Table.java:333)
        at org.apache.cassandra.db.Table.<init>(Table.java:271)
        at org.apache.cassandra.db.Table.open(Table.java:101)
        at org.apache.cassandra.db.Table.open(Table.java:79)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:201)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:349)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:392)
{noformat}",,mauzhang,omid,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/12 17:13;vijay2win@yahoo.com;0001-CASSANDRA-4622.patch;https://issues.apache.org/jira/secure/attachment/12544672/0001-CASSANDRA-4622.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256293,,,Tue Sep 11 17:44:47 UTC 2012,,,,,,,,,,"0|i0gxrz:",96902,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"11/Sep/12 17:13;vijay2win@yahoo.com;Moved the null check to be checked before adding to the future list.;;;","11/Sep/12 17:36;jbellis;+1;;;","11/Sep/12 17:44;vijay2win@yahoo.com;Committed, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in StorageProxy.getRestrictedRange,CASSANDRA-4621,12606245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,pyritschard,pyritschard,05/Sep/12 13:35,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 14:42,1.2.0 beta 1,,,,,,0,,,,,,,"On a freshly built cassandra from trunk, I can create a column family with a composite row key using the syntax:

for instance a standard eventlog CF:

     CREATE TABLE events (
       facility text,
       prio int,
       message text,
       PRIMARY KEY ( (facility, prio) )
     );

A simple query will then generate exceptions:

SELECT * FROM events; will yield:

ERROR 15:33:40,383 Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: [min(0),max(-8021625467324731134)]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:41)
	at org.apache.cassandra.dht.Bounds.split(Bounds.java:59)
	at org.apache.cassandra.service.StorageProxy.getRestrictedRanges(StorageProxy.java:1073)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:879)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:209)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:128)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

","archlinux, openjdk7",mauzhang,pyritschard,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/12 09:28;slebresne;4621-followup.txt;https://issues.apache.org/jira/secure/attachment/12545544/4621-followup.txt","11/Sep/12 09:00;slebresne;4621.txt;https://issues.apache.org/jira/secure/attachment/12544611/4621.txt","10/Sep/12 18:05;slebresne;4621.txt;https://issues.apache.org/jira/secure/attachment/12544496/4621.txt",,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256292,,,Tue Sep 18 14:42:39 UTC 2012,,,,,,,,,,"0|i0gxrr:",96901,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Sep/12 18:05;slebresne;Pretty sure this has nothing to do with CQL. But the trace shows a token that is negative, which should not happen. So my guess is that it's because the new Murmur3Partitioner can generate negative token while it shouldn't. Patch attached to fix (there was 2 places where a negative could be generated, one was because the special case of Long.MIN_VALUE wasn't handled, the other was that we weren't taking the absolute value at all for random token. Both should be fixed though this is probably the latter that produced the error here since it's way more likely to happen).;;;","10/Sep/12 18:10;jbellis;What if we just made the minimum token for M3P Long.MIN_VALUE instead?;;;","10/Sep/12 18:17;slebresne;A priori, I don't think that would be a problem (and if it's fine we should). We could also avoid using BigInteger to generate the middle of two longs :);;;","10/Sep/12 18:18;slebresne;(I'll have a shot at both);;;","10/Sep/12 18:37;slebresne;I also think that it's a problem that Murmur3Partitioner has a MINIMUM token that is a ""valid"" token (one that correspond to actual keys). Indeed (min, X] is suppose to include everything smaller than token X, but if min is a actual token, it is not contained in (min, X]. ;;;","10/Sep/12 18:38;jbellis;Hmm.  Forgot that we changed MINIMUM to < ZERO for RP.;;;","11/Sep/12 09:00;slebresne;Attaching patch that:
* Don't force positive tokens in Murmur3Partitioner (thus using all the bits of the LongToken).
* Exclude Long.MIN_VALUE from the possible tokens of a key (since it's now the MINIMUM token).

I note that this means tokens can be negative and that we'll have to update a number of documentation relating to computing tokens. Though this was already the case before this patch (but maybe to a slightly lesser extend). And hopefully vnodes will make (manual) token computations a thing of the past.;;;","11/Sep/12 15:26;jbellis;LGTM.;;;","11/Sep/12 15:44;slebresne;Committed, thanks;;;","18/Sep/12 09:28;slebresne;Reopening this since the midpoint calculation for murmur3Partitioner is actually broken by having negative tokens (concretely, Murmur3PartitionerTest is failing). Attaching patch to fix it.;;;","18/Sep/12 14:42;jbellis;committed w/ additional comments;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid special characters which might yield to a build fail,CASSANDRA-4620,12606243,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,pyritschard,pyritschard,05/Sep/12 13:29,16/Apr/19 09:32,14/Jul/23 05:52,05/Sep/12 14:43,1.2.0,,,,,,0,,,,,,,on jdk7 ant will fail on StreamingHistogram.java due to the special characters used line 130 and 133.,linux i386,pyritschard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/12 13:30;pyritschard;0001-Avoid-special-characters-which-might-confuse-ant.patch;https://issues.apache.org/jira/secure/attachment/12543850/0001-Avoid-special-characters-which-might-confuse-ant.patch",,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255193,,,Wed Sep 05 14:43:42 UTC 2012,,,,,,,,,,"0|i0epz3:",83970,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"05/Sep/12 13:30;pyritschard;this simple patch removes special characters and fixes the build on all versions of java i tested (jdk7, openjdk7, jdk6 and openjdk6);;;","05/Sep/12 14:43;yukim;+1 and committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql error with ORDER BY when using IN,CASSANDRA-4612,12606123,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/Sep/12 18:20,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 16:01,1.1.5,,,,,,0,,,,,,,"{code}
            CREATE TABLE test(
                my_id varchar, 
                col1 int, 
                value varchar, 
                PRIMARY KEY (my_id, col1)
            );

INSERT INTO test(my_id, col1, value) VALUES ( 'key1', 1, 'a');
INSERT INTO test(my_id, col1, value) VALUES ( 'key2', 3, 'c');
INSERT INTO test(my_id, col1, value) VALUES ( 'key3', 2, 'b');
INSERT INTO test(my_id, col1, value) VALUES ( 'key4', 4, 'd');
SELECT col1 FROM test WHERE my_id in('key1', 'key2', 'key3') ORDER BY col1;
{code}

The following error results: TSocket read 0 bytes
The log gives a traceback:
{code}
ERROR [Thrift:8] 2012-09-04 12:02:15,894 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1356)
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1343)
	at java.util.Arrays.mergeSort(Arrays.java:1270)
	at java.util.Arrays.sort(Arrays.java:1210)
	at java.util.Collections.sort(Collections.java:159)
	at org.apache.cassandra.cql3.statements.SelectStatement.orderResults(SelectStatement.java:821)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:793)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:136)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:107)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:115)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1521)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3618)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3606)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}","ubuntu, cassandra trunk (commit 769fe895a36868c47101f681f5fdd721bee1ad62 )",liqusha,slebresne,tpatterson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/12 20:55;xedin;CASSANDRA-4612-v2.patch;https://issues.apache.org/jira/secure/attachment/12544107/CASSANDRA-4612-v2.patch","06/Sep/12 15:15;xedin;CASSANDRA-4612.patch;https://issues.apache.org/jira/secure/attachment/12544056/CASSANDRA-4612.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,244027,,,Fri Sep 07 16:01:38 UTC 2012,,,,,,,,,,"0|i05ajr:",28806,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"06/Sep/12 11:28;xedin;fixed problem related to determining position when single 'order by' condition is used and added a check to prevent users from ordering by columns which are not included in the select clause.;;;","06/Sep/12 14:32;slebresne;The fix for the case where there is only one ordered column lgtm but this doesn't handle the CompositeComparator case.;;;","06/Sep/12 14:34;slebresne;Also, I believe this affect 1.1 as well, so I would suggest fixing it there.;;;","06/Sep/12 15:15;xedin;ported to 1.1 and added composite support.;;;","06/Sep/12 17:16;slebresne;I believe that removing the ""Order by currently only support the ordering of columns following their declared order in the PRIMARY KEY"" is buggy because it still doesn't work in the case where the key restriction is an EQ (i.e. not an IN). That is, we can remove it but in that case we must change the condition at the top of orderResults() so that we go through the 'CompositeComparator' path if the keyRestriction is an EQ and the requested orderings are not in the same order that in the PK definition. Not a bad idea though.

Some minor nits:
* We can provide the size when allocating the typesWithPositions list. I would have split typesWithPositions in two lists as this reduce the number of allocation needed (because you avoid all the Pair but also because you can use int[] for the positions) and in that case wouldn't really complicate the code anyway. 
* In getColumnPositionInSelect, we could use a by-index for loop and directly return the current index when we find the name. That way we can throw an assertion if we didn't find the name at all since that shouldn't happen anyway (tiny bonus: iteration by index on ArrayList is slightly cheaper as it avoids the iterator allocation).
;;;","06/Sep/12 18:17;xedin;Looks like its's just not my day today, I will fix keyRestriction thing and nits asap.;;;","06/Sep/12 20:55;xedin;v2 with nits fixed and order restriction reverted to original state after discussion with Jonathan.;;;","07/Sep/12 10:04;slebresne;lgtm, +1;;;","07/Sep/12 16:01;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack Size on Sun JVM 1.6.0_33 must be at least 160k,CASSANDRA-4602,12605931,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,amorton,amorton,02/Sep/12 23:34,16/Apr/19 09:32,14/Jul/23 05:52,24/Sep/12 19:50,1.0.12,1.1.5,,,,,1,,,,,,,"I started a fresh Cassandra 1.1.4 install with Sun JVM 1.6.35.

On startup I got this in output.log

{noformat}
The stack size specified is too small, Specify at least 160k
Cannot create Java VM
Service exit with a return value of 1
{noformat}

Remembering CASSANDRA-4275 I monkeyed around and started the JVM with -Xss160k the same as Java 7. I then got

{code:java}
ERROR [WRITE-/192.168.1.12] 2012-08-31 01:43:29,865 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[WRITE-/192.168.1.12,5,main]
java.lang.StackOverflowError
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
	at java.io.BufferedOutputStream.flush(Unknown Source)
	at java.io.DataOutputStream.flush(Unknown Source)
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:156)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

Same as CASSANDRA-4442

At which point I dropped back to Java 6.33. 

CASSANDRA-4457 bumped the stack size to 180 for java 7, should we also do this for Java 6.33+ ?","Ubuntu 10.04 
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01, mixed mode)",chengas123,omid,pmcfadin,rcoli,zenek_kraweznik0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/12 12:08;jbellis;4602.txt;https://issues.apache.org/jira/secure/attachment/12543538/4602.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,251082,,,Thu Sep 27 05:12:02 UTC 2012,,,,,,,,,,"0|i0b41j:",62767,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"03/Sep/12 12:05;jbellis;Were you able to find anything in the release notes on this?  I couldn't.;;;","03/Sep/12 12:08;jbellis;patch against 1.0 to change default to 160;;;","04/Sep/12 19:40;brandon.williams;+1;;;","04/Sep/12 19:55;jbellis;committed;;;","06/Sep/12 10:30;omid;Apparently this is due to HotSpot fix 7059899 [1] on 1.6.0_34 that increased ""StackShadowPages""'s default to 20, since a change in socketWrite's native implementation required more stack space. Increased StackShadowPages might require increased stack size (-Xss) [2] so that upon a call to a native method, there would be at least ""StackShadowPages"" stack space available.

[1] http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7059899
[2] http://www.oracle.com/technetwork/java/javase/crashes-137240.html ;;;","06/Sep/12 13:23;jbellis;Thanks, Omid!;;;","13/Sep/12 07:53;zenek_kraweznik0;cassandra 1.1.5

# java -version
java version ""1.6.0_35""
Java(TM) SE Runtime Environment (build 1.6.0_35-b10)
Java HotSpot(TM) 64-Bit Server VM (build 20.10-b01, mixed mode)
#

ERROR 17:39:53,363 Exception in thread Thread[Thrift:4040,5,main]
java.lang.StackOverflowError
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(Unknown Source)
        at java.io.BufferedInputStream.fill(Unknown Source)
        at java.io.BufferedInputStream.read1(Unknown Source)
        at java.io.BufferedInputStream.read(Unknown Source)
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
        at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

stack shoud be set to minimum 192k;;;","24/Sep/12 19:50;jbellis;it looks like you are sending large-ish Thrift messages.  I'm fine with saying that if you're going to do that, you need to push up the stack size manually.;;;","27/Sep/12 05:12;pmcfadin;+1 on greater than 160. Just fixed a re-occurring StackOverflowError problem by bumping the stack size. Added this line to cassandra-env.sh:

JVM_OPTS=""$JVM_OPTS -Xss194k""

Same JRE as above. build 1.6.0_35-b10

It was set to 160k but still threw exceptions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure unique commit log file names,CASSANDRA-4601,12605927,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,amorton,amorton,amorton,02/Sep/12 22:37,16/Apr/19 09:32,14/Jul/23 05:52,04/Sep/12 20:09,1.1.5,,,,,,0,,,,,,,"The commit log segment name uses System.nanoTime() as part of the file name. There is no guarantee that successive calls to nanoTime() will return different values. And on less than optimal hypervisors this happens a lot. 

I observed the following in the wild:

{code:java}
ERROR [COMMIT-LOG-ALLOCATOR] 2012-08-31 15:56:49,815 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.lang.AssertionError: attempted to delete non-existing file CommitLog-13926764209796414.log
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.discard(CommitLogSegment.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$4.run(CommitLogAllocator.java:223)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
{code}

My _assumption_ is that it was because of duplicate file names. As this is on a hypervisor that is less than optimal. 
 
After a while (about 30 minutes) mutations stopped being processed and the pending count sky rocketed. I _think_ this was because log writing was blocked trying to get a new segment and writers could not submit to the commit log queue. The only way to stop the affected nodes was kill -9. 

Over about 24 hours this happened 5 times. I have deployed a patch that has been running for 12 hours without incident, will attach. 

The affected nodes could still read, and I'm checking logs to see how the other nodes handled the situation.",Sun JVM 1.6.33 / Ubuntu 10.04.4 LTS ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/12 23:13;amorton;cassandra-1.1-4601.patch;https://issues.apache.org/jira/secure/attachment/12543501/cassandra-1.1-4601.patch",,,,,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,250701,,,Tue Sep 04 20:31:23 UTC 2012,,,,,,,,,,"0|i0b09r:",62154,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"03/Sep/12 01:57;jbellis;LGTM overall.

- I'd go ahead and use time-in-millis as base instead of nanotime
- Looks like this diff is reversed from what it ""should"" be?  Old lines are +, new are - instead of the inverse.
- patch 1.0 as well?;;;","04/Sep/12 20:02;jbellis;This actually doesn't affect < 1.1.0; it was introduced by CASSANDRA-3544.;;;","04/Sep/12 20:09;jbellis;committed w/ millis as base.  (in a hurry since we're going to roll 1.1.5 RSN.);;;","04/Sep/12 20:31;amorton;Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event tracing always records thread name as 'TracingStage',CASSANDRA-4599,12605804,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,31/Aug/12 22:19,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/12 23:42,1.2.0 beta 1,,,,,,0,,,,,,,"Since LoggingEvent#getThreadName gets current thread name when accessed, name of tracing thread ('TracingStage') is always logged to events CF.",,dr-alves,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/12 22:20;yukim;4599.txt;https://issues.apache.org/jira/secure/attachment/12543353/4599.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256279,,,Sat Sep 01 00:45:09 UTC 2012,,,,,,,,,,"0|i0gxjj:",96864,,dr-alves,,dr-alves,Low,,,,,,,,,,,,,,,,,"31/Aug/12 22:20;yukim;Trivial fix attached.;;;","31/Aug/12 22:31;dr-alves;lgtm, +1;;;","31/Aug/12 23:42;yukim;Committed.;;;","01/Sep/12 00:45;hudson;Integrated in Cassandra #1988 (See [https://builds.apache.org/job/Cassandra/1988/])
    fix for logging events' correct thread name; patch by yukim reviewed by David Alves for CASSANDRA-4599 (Revision e49d140bb72264a503a634d5603c25766c78e50d)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/tracing/TracingAppender.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describeOwnership() in Murmur3Partitioner.java doesn't work for close tokens,CASSANDRA-4598,12605780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,julienlambert,julienlambert,31/Aug/12 20:50,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/12 21:56,1.2.0 rc2,,,,,,0,,,,,,,"On a 2 node-cluster, if the two tokens are close enough, the ownership information displayed will be 0.00% for each, instead of ~0% for one and ~100% for the other. The number of replicas displayed is then 0, even if you have more.

Reproduce:
- Create a 2-node cluster, using Murmur3Partitioner
- Move the tokens to two consecutive values
- Display ring with nodetool

Problem:
This line causing this problem is in {{describeOwnership()}} of {{Murmur3Partitioner.java}} (lines 117 and 123):

{{float age = ((ti - tim1 + ri) % ri) / ri;}}

If {{ti - tim1}} (the difference of the two consecutive tokens) is too small, then the precision of the float isn't enough to represent the exact numbers (because {{ri}}, the total range of the ring, is a very big number). 

For example, {{(float) (ri + 1) = (float) (ri - 1) = (float) ri = 9.223372E18}}, so that {{((ri+1)%ri)/ri = ((ri-1)%ri)/ri = (ri%ri)/ri = 0}}. Whereas with a correct precision, the exact value for {{(ri-1)%ri}} should be {{ri-1}} and {{(ri-1)/ri ~ 1.0 (100%)}} instead of 0%.

Also, as the number of replica is determined by NodeCmd using the ownership percentages, it is wrong too.

Solution:
We might want to use BigInteger or BigDecimal somewhere?",,julienlambert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5076,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255197,,,Wed Dec 19 21:56:16 UTC 2012,,,,,,,,,,"0|i0epzz:",83974,,,,,Low,,,,,,,,,,,,,,,,,"19/Dec/12 21:56;yukim;We fixed desribeOwnership bug in CASSANDRA-5076, and the above issue was resolved also. Closing this as 'Fixed'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible to set LeveledCompactionStrategy to a column family.,CASSANDRA-4597,12605651,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jal06,jal06,31/Aug/12 07:51,16/Apr/19 09:32,14/Jul/23 05:52,04/Sep/12 11:40,1.1.5,,,,,,0,,,,,,,"CFPropDefs.applyToCFMetadata() does not set the compaction class on CFM

When altering the compaction strategy of a column family to LeveledCompactionStrategy, the compaction strategy is not changed (the describe command shows that the SizeTieredCompactionStrategy is still set to the CF)
When creating a column family WITH compaction_strategy_class='LeveledCompactionStrategy', the compaction strategy class used is  SizeTieredCompactionStrategy

Ex : 
jal@jal-VirtualBox:~/cassandra/apache-cassandra-1.1.1/bin$ ./cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> describe table pns_credentials;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

I want to set the LeveledCompaction strategy for this table, so I execute the following ALTER TABLE :

cqlsh:test1> alter table pns_credentials 
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=10;

In Cassandra logs, I see some informations :
 INFO 10:23:52,532 Enqueuing flush of Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,533 Writing Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,629 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-94-Data.db (1442 bytes) for commitlog position ReplayPosition(segmentId=3556583843054, position=1987)


However, when I look at the description of the table, the table is still with the SizeTieredCompactionStrategy
cqlsh:test1> describe table pns_credentials ;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
 
In the schema_columnfamilies table (in system keyspace), the table pns_credentials is still using the SizeTieredCompactionStrategy
cqlsh:test1> use system;
cqlsh:system> select * from schema_columnfamilies ;
...
         test1 |   pns_credentials |                   null | KEYS_ONLY |                        [] |         | org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy |                          {} |                                                                                                                                                                           org.apache.cassandra.db.marshal.UTF8Type | {""sstable_compression"":""org.apache.cassandra.io.compress.SnappyCompressor""} |          org.apache.cassandra.db.marshal.UTF8Type |           864000 | 1029 |       ise |     org.apache.cassandra.db.marshal.UTF8Type |                        0 |                       32 |                        4 |                0.1 |               True |          null | Standard |        null
... 


Same behaviour using cqlsh or command-cli.
","Ubuntu 12.04
cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0
Cluster with only 1 node",jal06,jeromatron,shamim_ru,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/12 09:20;xedin;CASSANDRA-4597.patch;https://issues.apache.org/jira/secure/attachment/12543524/CASSANDRA-4597.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256277,,,Tue Nov 27 07:44:24 UTC 2012,,,,,,,,,,"0|i0gxin:",96860,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"31/Aug/12 12:16;jbellis;Several schema bugs have been fixed since 1.1.1.  Let us know if you can reproduce in 1.1.4.  You may need to recreate the schema because 1.1.1 used an incorrectly-high timestamp on the original creation.;;;","02/Sep/12 08:23;jal06;I tried to alter the table with the folowing CQL command :
alter table pns_credentials6
WITH compaction_strategy_class='LeveledCompactionStrategy'
AND compaction_strategy_options:sstable_size_in_mb=15;

Using cassandra 1.1.1 : it doesn't work (neither compaction_strategy_class nor compaction_strategy_options is modified)
Using cassandra 1.1.2 : same as 1.1.1
Using cassandra 1.1.3 : neither compaction_strategy_class is not modified, but compaction_strategy_options is modified)
Using cassandra 1.1.4 : same as 1.1.3


Below are the details with cassandra 1.1.4 :
At first, I create a table pns_credentials6 (with compaction_strategy_class='SizeTieredCompactionStrategy')
Then, I describe the table
Then, I alter the table
Then, I describe again the table :  the line compaction_strategy_options:sstable_size_in_mb='15' (since 1.1.3) appears but compaction_strategy_class is still 'SizeTieredCompactionStrategy'

Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.4 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> CREATE TABLE pns_credentials6 (
         ...    ise text PRIMARY KEY,
         ...    isnew int,
         ...    ts timestamp,
         ...    mergestatus int,
         ...    infranetaccount text,
         ...    user_level int,
         ...    msisdn bigint,
         ...    mergeusertype int
         ...  ) WITH
         ...    comment='' AND
         ...    read_repair_chance=0.100000 AND
         ...   gc_grace_seconds=864000 AND
         ...    replicate_on_write='true' AND
         ...    compaction_strategy_class='SizeTieredCompactionStrategy' AND
         ...    compression_parameters:sstable_compression='SnappyCompressor';
cqlsh:test1> describe table pns_credentials6;

CREATE TABLE pns_credentials6 (
  ise text PRIMARY KEY,
  infranetaccount text,
  isnew int,
  mergestatus int,
  mergeusertype int,
  msisdn bigint,
  ts timestamp,
  user_level int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

cqlsh:test1> alter table pns_credentials6
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=15;
cqlsh:test1> describe table pns_credentials6;

CREATE TABLE pns_credentials6 (
  ise text PRIMARY KEY,
  infranetaccount text,
  isnew int,
  mergestatus int,
  mergeusertype int,
  msisdn bigint,
  ts timestamp,
  user_level int
) WITH
  comment='' AND
  caching='KEYS_ONLY' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='15' AND
  compression_parameters:sstable_compression='SnappyCompressor';

As you can see, I did the check with a new table (pns_credentials6), created in 1.1.4.
When you say that I need to recreate the schema, is it what you were expecting ?;;;","02/Sep/12 14:05;jal06;Still happening in 1.1.4;;;","03/Sep/12 01:51;jbellis;Thanks, we'll have a second look at that.;;;","03/Sep/12 08:53;xedin;I have noticed and included a fix for this in patch for CASSANDRA-4497 for 1.2.0, will add patch for 1.1 here soon.;;;","03/Sep/12 11:59;jbellis;+1;;;","04/Sep/12 11:40;xedin;Committed.;;;","26/Nov/12 12:52;shamim_ru;still happening in 1.1.5

Connected to SMEVCLUSTER at 192.168.157.94:9160.
[cqlsh 2.2.0 | Cassandra 1.1.5 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh > describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
  dscn text,
  asid text,
  soapa text,
  sysn text,
  msgs double,
  leid bigint,
  prc text,
  aeid bigint,
  adt timestamp,
  name text,
  asn text,
  msg text,
  msgid text,
  msgt text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';
cqlsh> alter table auditlog_01 with compaction_strategy_class='LeveledCompactionStrategy' AND compaction_strategy_options:sstable_size_in_mb=5;
cqlsh> describe columnfamily auditlog_01;
CREATE TABLE auditlog_01 (
  lid text PRIMARY KEY,
  dscn text,
  asid text,
  soapa text,
  sysn text,
  msgs double,
  leid bigint,
  prc text,
  aeid bigint,
  adt timestamp,
  name text,
  asn text,
  msg text,
  msgid text,
  msgt text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='5' AND
  compression_parameters:sstable_compression='SnappyCompressor';
;;;","26/Nov/12 17:42;jeromatron;Shamim,

Your case may be a product of CASSANDRA-4965.  Can you try to update the column family metadata in the cassandra-cli such as: 
{code}update column family auditlog_01 with compaction_strategy=LeveledCompactionStrategy;{code}

See if that works for you.;;;","27/Nov/12 07:44;shamim_ru;Thank'x Jeremy,
  yes it's works through cli. It's bug on cql 2.* which defined on issue CASSANDRA-4965;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COPY TO and COPY FROM don't default to consistent ordering of columns,CASSANDRA-4594,12605617,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,30/Aug/12 23:13,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 19:03,1.1.6,,,,,,0,cqlsh,,,,,,"Here is the input:
{code}                                                         
CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
USE test;                                                                       
                                                                                
CREATE TABLE airplanes (                                                        
                name text PRIMARY KEY,                                          
                manufacturer ascii,                                             
                year int,                                                       
                mach float                                                      
            );                                                                  
                                                                                
INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
                                                                                
COPY airplanes TO 'temp.cfg' WITH HEADER=true;                                  
                                                                                
TRUNCATE airplanes;                                                                
                                                                                   
COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                                
                                                                                   
SELECT * FROM airplanes;
{code}

Here is what happens when executed. Note how it tried to import the float into the int column:
{code}
cqlsh:test> DROP KEYSPACE test;                                                                
cqlsh:test> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;
cqlsh:test> USE test;                                                                       
cqlsh:test>                                                                                    
cqlsh:test> CREATE TABLE airplanes (                                            
        ...                 name text PRIMARY KEY,                              
        ...                 manufacturer ascii,                                 
        ...                 year int,                                           
        ...                 mach float                                          
        ...             );                                                      
cqlsh:test>                                                                     
cqlsh:test> INSERT INTO airplanes (name, manufacturer, year, mach) VALUES ('P38-Lightning', 'Lockheed', 1937, '.7');
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes TO 'temp.cfg' WITH HEADER=true;                      
1 rows exported in 0.003 seconds.                                               
cqlsh:test> TRUNCATE airplanes;                                                 
cqlsh:test>                                                                     
cqlsh:test> COPY airplanes FROM 'temp.cfg' WITH HEADER=true;                    
Bad Request: unable to make int from '0.7'                                      
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.002 seconds.
{code}","Happens in CQLSH 2, may or may not happen in CQLSH 3",aleksey,slebresne,thepaul,tpatterson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256275,,,Tue Sep 18 18:58:19 UTC 2012,,,,,,,,,,"0|i0gxhj:",96855,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"31/Aug/12 02:33;thepaul;This is because a {{select * from airplanes;}} does not give the columns in the order they were defined. I'm not sure why not; if that's a bug in C*, then we should fix that. If there isn't supposed to be any expectation of order, then cqlsh should be inspecting the columns and specifying them explicitly.;;;","04/Sep/12 07:21;slebresne;bq. This is because a select * from airplanes; does not give the columns in the order they were defined.

No it doesn't. The vaguely technical reason is that we don't keep internally the order of definition of columns, so we would have to start keeping that information (which honestly wouldn't be that much of a burden now that we have the list support, but was a tad more annoying to do at the time I wrote the code for select *). And I think that it's one place where it's not worth committing to any specific order but rather left it unspecified, so I don't see the point in bothering to record said definition order.

For the record though, the order of * is not completely random. It returns first the columns composing the PK (in the order of the PK) and then the rest of the columns in lexicographic order. But really it's mostly because it's convenient for the implementation to do it that way.;;;","07/Sep/12 04:01;thepaul;Seems worth it to fix this in the 1.1 branch too.;;;","12/Sep/12 19:58;thepaul;Fixed in my github clone, in the 4594-1.1 branch:

http://github.com/thepaul/cassandra/tree/4594-1.1

Current version is tagged pending/4594-1.1.

And since the merge forward of this change will conflict with CASSANDRA-4491, and the proper resolution isn't obvious, I've also made a branch with this change for 1.2 on top of 4491:

http://github.com/thepaul/cassandra/tree/4594-1.2;;;","13/Sep/12 18:58;brandon.williams;Committed your 1.1 fix, and committed 4491 to 1.2 but I can't get your 1.2 branch to apply to cql3handling.py.;;;","18/Sep/12 04:41;thepaul;Ah, sorry, I didn't make it clear on 4491 that the github branch had multiple commits from off of trunk. Looks like you only cherry-picked the last. I'll make a note there.;;;","18/Sep/12 16:20;aleksey;trunk, cql3, python 2.7:

{quote}
cqlsh:test> COPY airplanes TO 'temp.cfg' WITH HEADER=true;                                  
CQL query must be bytes, not unicode
{quote}
and
{quote}
cqlsh:test> COPY airplanes FROM 'temp.cfg' WITH HEADER=true; 
Traceback (most recent call last):
  File ""bin/cqlsh"", line 914, in perform_statement
    self.cursor.execute(statement, decoder=decoder)
  File ""bin/../lib/cql-internal-only-1.2.0.zip/cql-1.2.0/cql/cursor.py"", line 75, in execute
    raise ValueError(""CQL query must be bytes, not unicode"")
ValueError: CQL query must be bytes, not unicode
{quote}

I've attached a patch that fixes the issue but probably isn't the best possible solution.;;;","18/Sep/12 17:56;thepaul;Aleksey: you must be missing the patch for this ticket. It includes a change to the cql_protect_name function to encode unicode names in utf8 when necessary. Using {{str}} won't work when there are non-ascii characters and the default encoding is not utf8.;;;","18/Sep/12 18:18;aleksey;Paul: if it's in the trunk then I'm not missing it. Just double-checked it, still got exact same exceptions.;;;","18/Sep/12 18:23;thepaul;It's not yet in trunk. See above; Brandon couldn't get the commit to apply, because it was missing two of the commits from 4491.;;;","18/Sep/12 18:26;aleksey;Ah. Then I misunderstood you. Sorry.;;;","18/Sep/12 18:28;brandon.williams;Sylvain said he committed those and I think I see them there, maybe you just need to pull.;;;","18/Sep/12 18:31;aleksey;I pulled, of course. So unless git is lying to me, I should have the latest trunk.;;;","18/Sep/12 18:40;thepaul;Yes, the commits from 4491 are there. The patch for _this commit_ is not.;;;","18/Sep/12 18:48;brandon.williams;Ok, I got it to merge cleanly and committed it to trunk.;;;","18/Sep/12 18:58;aleksey;The problem is now gone.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""The system cannot find the path specified"" when creating hard link on Windows",CASSANDRA-4590,12605562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,allenservedio,allenservedio,30/Aug/12 18:44,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 16:58,1.1.5,,,,,,0,,,,,,,"When upgrading from Cassandra 1.0.5 to 1.1.3, we have a test case (uses embedded Cassandra) that started failing as shown below. Other than the upgrade, no changes were made to the code or config. I believe this MAY be related to the change made in CASSANDRA-3101.

We verified that the file it is trying to create the hard link to does exist - so it is purely the creation of the link that is failing.

Here is the basic failure:

# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.


Here is a more complete log output:


# [11:30:59.975] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.976] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Digest.sha1]
# [11:30:59.977] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Index.db]
# [11:30:59.978] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Filter.db]
# [11:30:59.978] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Statistics.db]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.i.s.SSTable] [delete] [Deleted target\test\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-he-4]
# [11:30:59.981] [INFO ] [o.a.c.d.ColumnFamilyStore] [maybeSwitchMemtable] [Enqueuing flush of Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.981] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Writing Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.992] [DEBUG] [o.a.c.d.Directories] [getLocationWithMaximumAvailableSpace] [expected data files size is 134; largest free partition (target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts) has 82645161984 bytes free]
# [11:31:00.012] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Completed flushing target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db (123 bytes) for commitlog position ReplayPosition(segmentId=592725621297887, position=6701)]
# [11:31:00.013] [DEBUG] [o.a.c.u.I.IntervalNode] [<init>] [Creating IntervalNode from [Interval(DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334), DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334))]]
# [11:31:00.013] [DEBUG] [o.a.c.d.DataTracker] [addNewSSTablesSize] [adding target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1 to list of files tracked for RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [submitBackground] [Scheduling a background task check for RevKeyspace.PropertyProductDefaultInventoryCounts with SizeTieredCompactionStrategy]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [Checking RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [discard completed log segments for ReplayPosition(segmentId=592725621297887, position=6701), column family 1001]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.SizeTieredCompactionStrategy] [getNextBackgroundTask] [Compaction buckets are [[SSTableReader(path='target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db')]]]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [Not safe to delete commit log CommitLogSegment(target\test\cassandra\commitlog\CommitLog-592725621297887.log); dirty is Versions (7), ; hasNext: false]
# [11:31:00.015] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [No tasks available]
# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) [cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.308] [ERROR] [o.a.c.s.AbstractCassandraDaemon] [uncaughtException] [Exception in thread Thread[MigrationStage:1,5,main]]
java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [ERROR] [o.a.c.t.CustomTThreadPoolServer] [run] [Error occurred during processing of message.]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:170) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.CassandraServer.system_drop_keyspace(CassandraServer.java:1008) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3476) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3464) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.get(FutureTask.java:83) ~[na:1.6.0_33]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 11 common frames omitted
Caused by: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	... 3 common frames omitted
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [DEBUG] [o.a.c.s.ClientState] [logout] [logged out: #<User allow_all groups=[]>]
# [11:31:00.310] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-5>]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [MARK HOST AS DOWN TRIGGERED for host 127.0.0.1(127.0.0.1):9162]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [Pool state on shutdown: <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}; IsActive?: true; Active: 1; Blocked: 0; Idle: 15; NumBeforeExhausted: 49]
# [11:31:00.311] [INFO ] [m.p.c.c.ConcurrentHClientPool] [shutdown] [Shutdown triggered on <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-6>]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-15>]
# [11:31:00.311] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-14>]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-13>]
",Windows 7 - 64 bit,allenservedio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256272,,,Fri Sep 07 16:58:53 UTC 2012,,,,,,,,,,"0|i0gxfz:",96848,,,,,Low,,,,,,,,,,,,,,,,,"31/Aug/12 12:31;jbellis;Does the directory exist that it's trying to create the link in?;;;","31/Aug/12 13:57;allenservedio;We just went back and double checked - yes the directory does exist. Also, before even submitting this defect we verified that: a) the developers have plenty of space on their drive and b) we were able to verify the same behavior on another developer's box (in case it was environmental).

I forgot to mention... We have other tests that also use Embedded Cassandra and work fine. So, either they are not triggering this functionality OR there is something about the path that is messing things up (my guess).;;;","31/Aug/12 14:41;jbellis;What happens if you run the mklink command manually?;;;","31/Aug/12 14:46;jbellis;Also: it's worth trying a shorter path to put your data in.  According to http://msdn.microsoft.com/en-us/library/aa365247.aspx the maximum path length should be 256 and you're only at ~200 but I don't know how far people have successfully pushed it.;;;","31/Aug/12 14:59;allenservedio;Yep, it looks like a path length problem. 

Trying with existing path:

c:\ABC\source_code\s7-t1>cmd /c mklink /H C:\ABC\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Compressio
nInfo.db C:\ABC\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db
The system cannot find the path specified.

Shortening path (IT WORKS):

C:\>cmd /c mklink /H C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db 

C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCoun
ts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db
Hardlink created for C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db <<===>> C:\test\cassandra\data\RevKeyspace\PropertyProductDefaultInven
toryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db

Probably worth just updating the failure message to let folks know of the likely problem. My apologies for opening this before doing better due diligence on tracking down the problem (esp. since I had suspected what it was...). Anyway, it is documented now :-)
;;;","07/Sep/12 16:58;jbellis;Updated error message as follows:

{code}
            String st = osname.startsWith(""Windows"")
                      ? ""Unable to create hard link.  This probably means your data directory path is too long.  Exception follows:""
                      : ""Unable to create hard link with exec.  Suggest installing JNA to avoid the need to exec entirely.  Exception follows: "";
            logger.error(st, ex);
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError in LeveledCompactionStrategy$LeveledScanner.computeNext,CASSANDRA-4587,12605495,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,schnidrig,schnidrig,30/Aug/12 09:20,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/12 18:46,1.1.5,,,,,,0,compaction,,,,,,"while running nodetool repair, the following was logged in system.log:


ERROR [ValidationExecutor:2] 2012-08-30 10:58:19,490 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.StackOverflowError
        at sun.nio.cs.UTF_8.updatePositions(UTF_8.java:76)
        at sun.nio.cs.UTF_8$Encoder.encodeArrayLoop(UTF_8.java:411)
        at sun.nio.cs.UTF_8$Encoder.encodeLoop(UTF_8.java:466)
        at java.nio.charset.CharsetEncoder.encode(CharsetEncoder.java:561)
        at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:258)
        at java.lang.StringCoding.encode(StringCoding.java:290)
        at java.lang.String.getBytes(String.java:954)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:64)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:46)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1007)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
        at org.apache.cassandra.io.sstable.SSTableBoundedScanner.<init>(SSTableBoundedScanner.java:41)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:869)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:247)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
.

(about 900 lines deleted)
.


        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:240)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:248)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy$LeveledScanner.computeNext(LeveledCompactionStrategy.java:202)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:90)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:47)
        at org.apache.cassandra.db.compaction.CompactionIterable.iterator(CompactionIterable.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:703)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:442)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","debian
OpenJDK 64-Bit Server VM/1.6.0_18
Heap size: 8341422080/8342470656",schnidrig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/12 16:09;jbellis;4587.txt;https://issues.apache.org/jira/secure/attachment/12543115/4587.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256269,,,Thu Aug 30 18:46:59 UTC 2012,,,,,,,,,,"0|i0gxfb:",96845,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"30/Aug/12 16:09;jbellis;patch to simplify LeveledScanner.computeNext and avoid recursion;;;","30/Aug/12 18:34;yukim;+1;;;","30/Aug/12 18:46;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL queries using LIMIT sometimes missing results,CASSANDRA-4579,12605133,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,27/Aug/12 20:51,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 06:35,1.2.0 beta 1,,,,,,0,cql,cql3,,,,,"In certain conditions, CQL queries using LIMIT clauses are not being given all of the expected results (whether unset column values or missing rows).

Here are the condition sets I've been able to identify:

First mode: all rows are returned, but in the last row of results, all columns which are not part of the primary key receive no values, except for the first non-primary-key column. Conditions:

 * Table has a multi-component primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

Second mode: result has fewer rows than it should, lower than both the LIMIT and the actual number of matching rows. Conditions:

 * Table has a single-column primary key
 * Table has more than one column which is not a component of the primary key
 * The number of results which would be returned by a query is equal to or more than the specified LIMIT

It would make sense to me that this would have started with CASSANDRA-4329, but bisecting indicates that this behavior started with commit 91bdf7fb4220b27e9566c6673bf5dbd14153017c, implementing CASSANDRA-3647.

Test case for the first failure mode:

{noformat}
DROP KEYSPACE test;

CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int,
    b int,
    c int,
    d int,
    e int,
    PRIMARY KEY (a, b)
);

INSERT INTO testcf (a, b, c, d, e) VALUES (1, 11, 111, 1111, 11111);
INSERT INTO testcf (a, b, c, d, e) VALUES (2, 22, 222, 2222, 22222);
INSERT INTO testcf (a, b, c, d, e) VALUES (3, 33, 333, 3333, 33333);
INSERT INTO testcf (a, b, c, d, e) VALUES (4, 44, 444, 4444, 44444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- columns d and e in result row are null
SELECT * FROM testcf LIMIT 2; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 3; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 4; -- columns d and e in last result row are null
SELECT * FROM testcf LIMIT 5; -- results are correct (4 rows returned)
{noformat}

Test case for the second failure mode:

{noformat}
CREATE KEYSPACE test
    WITH strategy_class = 'SimpleStrategy'
    AND strategy_options:replication_factor = 1;

USE test;

CREATE TABLE testcf (
    a int primary key,
    b int,
    c int,
);

INSERT INTO testcf (a, b, c) VALUES (1, 11, 111);
INSERT INTO testcf (a, b, c) VALUES (2, 22, 222);
INSERT INTO testcf (a, b, c) VALUES (3, 33, 333);
INSERT INTO testcf (a, b, c) VALUES (4, 44, 444);

SELECT * FROM testcf;

SELECT * FROM testcf LIMIT 1; -- gives 1 row
SELECT * FROM testcf LIMIT 2; -- gives 1 row
SELECT * FROM testcf LIMIT 3; -- gives 2 rows
SELECT * FROM testcf LIMIT 4; -- gives 2 rows
SELECT * FROM testcf LIMIT 5; -- gives 3 rows
{noformat}",,slebresne,thepaul,tux21b,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/12 07:35;slebresne;0001-Add-all-columns-from-a-prefix-group-before-stopping.txt;https://issues.apache.org/jira/secure/attachment/12545373/0001-Add-all-columns-from-a-prefix-group-before-stopping.txt","17/Sep/12 07:35;slebresne;0002-Fix-LIMIT-for-NamesQueryFilter.txt;https://issues.apache.org/jira/secure/attachment/12545374/0002-Fix-LIMIT-for-NamesQueryFilter.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256263,,,Tue Sep 18 06:35:47 UTC 2012,,,,,,,,,,"0|i0gxc7:",96831,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"27/Aug/12 20:51;thepaul;Thanks to Christoph Hack for identifying the problem.;;;","04/Sep/12 13:40;slebresne;There is indeed 2 bugs when counting columns with composites (introduced by the change made for collections, so 1.1 is not affected in particular).

The first one is that to count the number of CQL row to return, SliceQueryFilter groups columns having the same composite prefix (i.e. all the columns belonging to the same CQL row) and count that as 1. However the code was stopping collecting columns as sound as the requested count was reached, without waiting having seen all the columns of the last ""group"".

The second one is that for NamesQueryFilter, each internal Cassandra row will yield exactly one CQL row, so we must use the ""count keys"" rather than ""count columns"" argument for getRangeSlice in that case.

Attached fix for both (I've pushed a dtest with the two examples from that ticket).
;;;","17/Sep/12 07:35;slebresne;Rebased patches attached.;;;","17/Sep/12 21:23;xedin;+1;;;","18/Sep/12 06:35;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dead lock in mutation stage when many concurrent writes to few columns,CASSANDRA-4578,12605040,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,suguru,suguru,27/Aug/12 04:46,16/Apr/19 09:32,14/Jul/23 05:52,01/Nov/12 19:34,1.1.5,,,,,,0,,,,,,,"When I send many request to increment counters to few counter columns, sometimes mutation stage cause dead lock. When it happened, all of mutation threads are locked and do not accept updates any more.

{noformat}
""MutationStage:432"" - Thread t@1389
   java.lang.Thread.State: TIMED_WAITING
	at java.lang.Object.wait(Native Method)
	- waiting on <b90b45b> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:443)
	at java.util.concurrent.TimeUnit.timedWait(TimeUnit.java:292)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:54)
	at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:55)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- locked <4b1b0a6f> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
{noformat}","15 cassandra instances
CentOS5
8 Core 64GB Memory

java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)
",cywjackson,slebresne,suguru,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/12 16:56;zznate;4578-1.0-backport.txt;https://issues.apache.org/jira/secure/attachment/12551577/4578-1.0-backport.txt","06/Sep/12 13:02;slebresne;4578.txt;https://issues.apache.org/jira/secure/attachment/12544039/4578.txt","27/Aug/12 04:46;suguru;threaddump-1344957574788.tdump;https://issues.apache.org/jira/secure/attachment/12542575/threaddump-1344957574788.tdump",,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,253438,,,Thu Nov 01 19:34:07 UTC 2012,,,,,,,,,,"0|i0dnin:",77739,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Aug/12 04:46;suguru;Attached thread dump;;;","27/Aug/12 14:34;jbellis;You're right, since CMVH grabs a writer thread until it gets replies from the other replicas, you can have two replicas deadlock with A waiting for a reply from B, and B waiting for a reply from A.

One fix would be to move the local write into CMVH and the remote part into a separate stage (or maybe just a custom callback).

As a workaround, use CL.ONE with counters.;;;","06/Sep/12 13:02;slebresne;Attaching patch to use a callback (as it avoids creating lots of thread that just spend time waiting on a condition) to send back the response from CMVH.;;;","06/Sep/12 19:59;jbellis;Would prefer to have the callback final in constructor to make it more clear that it doesn't get changed during processing, otherwise +1;;;","07/Sep/12 08:52;slebresne;bq. Would prefer to have the callback final in constructor

I initially feared pushing the callback to the constructor would artifically require too much code changes but looking closer it doesn't really, so committed with that changed.
;;;","31/Oct/12 16:54;zznate;Re-open for backport to 1.0.x;;;","31/Oct/12 16:56;zznate;Against cassandra-1.0 latest. Only differs in line numbers, otherwise no issues. All tests pass.;;;","01/Nov/12 19:34;jbellis;Committed the backport to the 1.0 branch.

Not tagging with a 1.0 version since there are no plans for an official 1.0.13 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow preparing queries without parameters,CASSANDRA-4577,12604982,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tux21b,tux21b,tux21b,25/Aug/12 21:51,16/Apr/19 09:32,14/Jul/23 05:52,05/Sep/12 14:10,1.2.0 beta 1,,,Legacy/CQL,,,0,cql3,,,,,,"Currently it's not possible to prepare any queries that do not take any parameters using Cassandra's new native protocol because of an assertion error.

This makes client development rather difficult (you need to parse CQL queries to detect the number of parameters and skip the preparation of those) and there is probably no reason to handle queries with no parameters separately.",,slebresne,thepaul,tux21b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/12 21:55;tux21b;trunk-4577.txt;https://issues.apache.org/jira/secure/attachment/12542426/trunk-4577.txt",,,,,,,,,,,,,,,,,1.0,tux21b,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256262,,,Wed Sep 05 14:10:54 UTC 2012,,,,,,,,,,"0|i0gxbj:",96828,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"25/Aug/12 21:55;tux21b;This simple patch seems to work fine for me, but please review carefully.;;;","05/Sep/12 14:10;slebresne;lgtm, +1. Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in non-upgraded node's log when upgrading another node to trunk,CASSANDRA-4576,12604898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,24/Aug/12 18:17,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 22:25,1.2.0,,,,,,0,,,,,,,"I'm upgrading a 2-node cluster from cassandra-1.1 to trunk. On node1 I flush, stop the node, upgrade it to trunk, and start it. The following error gets written once a second in the log for node2:

{code}
ERROR [GossipStage:10] 2012-08-24 11:03:36,293 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[GossipStage:10,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.UnknownHostException: addr is of illegal length
	at java.net.InetAddress.getByAddress(InetAddress.java:935)
	at java.net.InetAddress.getByAddress(InetAddress.java:1318)
	at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
	at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
	... 4 more
{code}


Here is the exact code I ran to produce the error:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.partitioner = 'org.apache.cassandra.dht.RandomPartitioner'
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        (node1, node2) = cluster.nodelist()
        node1.watch_log_for('Listening for thrift clients...')
        node2.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
        import pdb; pdb.set_trace()  # <-- pause here and tail -f the node2.logfilename()
{code}","ubuntu, ccm cluster with dtests",tpatterson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/12 15:35;xedin;CASSANDRA-4576.patch;https://issues.apache.org/jira/secure/attachment/12545576/CASSANDRA-4576.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256261,,,Tue Sep 18 22:25:44 UTC 2012,,,,,,,,,,"0|i0gxb3:",96826,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"24/Aug/12 20:58;jbellis;Brandon says, ""Something is wrong w/ the upgrade of the system KS, not gossip."";;;","18/Sep/12 15:35;xedin;Actually it's a problem with Gossiper :);;;","18/Sep/12 16:47;brandon.williams;Can you explain what happened here?  I'm confused by the version 11 change, given that trunk is version 12.;;;","18/Sep/12 18:48;xedin;Sure, GossipDigestAckSerializer always expects boolean on deserialization, that would only changed in 1.2 which doesn't write that boolean.;;;","18/Sep/12 22:19;brandon.williams;I see, thanks. +1;;;","18/Sep/12 22:25;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing String.format() in AntiEntropyService.java logs,CASSANDRA-4574,12604687,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,julienlambert,julienlambert,23/Aug/12 21:55,16/Apr/19 09:32,14/Jul/23 05:52,24/Aug/12 03:56,1.0.12,1.1.5,,,,,0,,,,,,,"A String.format() is missing in AntiEntropyService.java (line 625 in 1.2). 
This is what is written to the logs: AntiEntropyService.java (line 625) \[repair #%s] No neighbors to repair with on range %s: session completed.",,dbrosius@apache.org,julienlambert,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256259,,,Fri Aug 24 03:55:54 UTC 2012,,,,,,,,,,"0|i0gxa7:",96822,,,,,Low,,,,,,,,,,,,,,,,,"24/Aug/12 03:55;dbrosius@apache.org;thanks
committed to cassandra-1.0 as ec76baf0dd2a976542106cd5e58652c3d36ffd23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSHA doesn't handle large messages gracefully,CASSANDRA-4573,12604662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,thobbs,thobbs,23/Aug/12 18:44,16/Apr/19 09:32,14/Jul/23 05:52,06/Aug/13 22:22,2.0.0,,,,,,0,,,,,,,"HSHA doesn't seem to enforce any kind of max message length, and when messages are too large, it doesn't fail gracefully.

With debug logs enabled, you'll see this:

{{DEBUG 13:13:31,805 Unexpected state 16}}

Which seems to mean that there's a SelectionKey that's valid, but isn't ready for reading, writing, or accepting.

Client-side, you'll get this thrift error (while trying to read a frame as part of {{recv_batch_mutate}}):

{{TTransportException: TSocket read 0 bytes}}",,peter-librato,thobbs,vijay2win@yahoo.com,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/13 04:14;xedin;CASSANDRA-4573.patch;https://issues.apache.org/jira/secure/attachment/12595548/CASSANDRA-4573.patch","02/Aug/13 04:14;xedin;disruptor-thrift-server-0.2.2-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12595549/disruptor-thrift-server-0.2.2-SNAPSHOT.jar","23/Aug/12 18:46;thobbs;repro.py;https://issues.apache.org/jira/secure/attachment/12542170/repro.py",,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256258,,,Thu Aug 08 18:31:45 UTC 2013,,,,,,,,,,"0|i0gx9r:",96820,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,thobbs,,,"23/Aug/12 18:46;thobbs;The attached repro.py reproduces the issue using pycassa, but I can also reproduce with phpcassa.  I'm testing against 1.1.4, with the only change to cassandra.yaml being a switch to hsha.;;;","24/Aug/12 00:56;vijay2win@yahoo.com;{quote}
With debug logs enabled, you'll see this:
DEBUG 13:13:31,805 Unexpected state 16
{quote}
Unexpected state 16 means the intrest operation is accept. 
If you look at the code it is a while loop (the intrestOps can change between the if conditions) and will be handled in the next iteration.

{quote}
HSHA doesn't seem to enforce any kind of max message length
{quote}
neither does sync :) i can reproduce this error both in Sync and hsha, still trying to see where the timeout comes from though.;;;","24/Aug/12 03:37;jbellis;bq. neither does sync 

sync does enforce frame size;;;","24/Aug/12 04:32;vijay2win@yahoo.com;but Frame size is set for both HSHA and Sync

{code}
TNonblockingServer.Args serverArgs = new TNonblockingServer.Args(serverTransport).inputTransportFactory(inTransportFactory)
                                                                                       .outputTransportFactory(outTransportFactory)
{code};;;","27/Aug/12 02:24;vijay2win@yahoo.com;Hi Tyler, I think the issue is related to GC pause can you conform you see the same?

Run: tail -f /var/log/cassandra/system.log |grep GCInspector
Enable GC logging.
Run: python repro.py

You would see timeout when you see ""threads were stopped:"" to be > 1.5 Seconds or so.;;;","27/Aug/12 17:18;thobbs;Vijay, I'm actually not seeing very long garbage collections, if I'm reading the logs correctly.  These are the relevant logs, running with a heap of 2GB and young gen size of 400MB:

{noformat}
{Heap before GC invocations=0 (full 0):
 par new generation   total 368640K, used 327680K [0x2f200000, 0x48200000, 0x48200000)
  eden space 327680K, 100% used [0x2f200000, 0x43200000, 0x43200000)
  from space 40960K,   0% used [0x43200000, 0x43200000, 0x45a00000)
  to   space 40960K,   0% used [0x45a00000, 0x45a00000, 0x48200000)
 concurrent mark-sweep generation total 1687552K, used 0K [0x48200000, 0xaf200000, 0xaf200000)
 concurrent-mark-sweep perm gen total 16384K, used 14333K [0xaf200000, 0xb0200000, 0xb3200000)
2012-08-27T12:03:56.096-0500: [GC Before GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 432013312
Max   Chunk Size: 432013312
Number of Blocks: 1
Av.  Block  Size: 432013312
Tree      Height: 1
Before GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 0
Max   Chunk Size: 0
Number of Blocks: 0
Tree      Height: 0
[ParNew
Desired survivor size 20971520 bytes, new threshold 1 (max 1)
- age   1:    2692712 bytes,    2692712 total
: 327680K->2642K(368640K), 0.0564410 secs] 327680K->2642K(2056192K)After GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 431996928
Max   Chunk Size: 431996928
Number of Blocks: 1
Av.  Block  Size: 431996928
Tree      Height: 1
After GC:
Statistics for BinaryTreeDictionary:
------------------------------------
Total Free Space: 0
Max   Chunk Size: 0
Number of Blocks: 0
Tree      Height: 0
, 0.0567720 secs] [Times: user=0.03 sys=0.00, real=0.06 secs] 
Heap after GC invocations=1 (full 0):
 par new generation   total 368640K, used 2642K [0x2f200000, 0x48200000, 0x48200000)
  eden space 327680K,   0% used [0x2f200000, 0x2f200000, 0x43200000)
  from space 40960K,   6% used [0x45a00000, 0x45c94998, 0x48200000)
  to   space 40960K,   0% used [0x43200000, 0x43200000, 0x45a00000)
 concurrent mark-sweep generation total 1687552K, used 0K [0x48200000, 0xaf200000, 0xaf200000)
 concurrent-mark-sweep perm gen total 16384K, used 14333K [0xaf200000, 0xb0200000, 0xb3200000)
}
Total time for which application threads were stopped: 0.0576140 seconds
Total time for which application threads were stopped: 0.0080490 seconds
Total time for which application threads were stopped: 0.0000810 seconds
Total time for which application threads were stopped: 0.0000410 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000370 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000360 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000340 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000320 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000760 seconds
Total time for which application threads were stopped: 0.0000490 seconds
Total time for which application threads were stopped: 0.0000330 seconds
Total time for which application threads were stopped: 0.0000370 seconds
Total time for which application threads were stopped: 0.0000460 seconds
Total time for which application threads were stopped: 0.0000350 seconds
Total time for which application threads were stopped: 0.0004150 seconds
Total time for which application threads were stopped: 0.0001230 seconds
Total time for which application threads were stopped: 0.0035150 seconds
{noformat}

The client-side socket timeout is set to 3 seconds, so it's not hitting that timeout due to garbage collections.  I should also note that the client-side error is different when there is a client socket timeout (something like {{TTransportException: timed out reading 4 bytes}}).;;;","13/Sep/12 03:10;vijay2win@yahoo.com;Hi Tyler, I am not able to re-produce it so far. I am running 2GB/400MB on AWS M4XL....

[ec2-user@ip-10-82-21-221 ~]$ grep -i ThriftServer.java /mnt/log/cassandra/system.log 
 INFO [main] 2012-09-11 21:52:43,702 ThriftServer.java (line 112) Binding thrift service to localhost/127.0.0.1:9160
 INFO [main] 2012-09-11 21:52:43,704 ThriftServer.java (line 121) Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO [main] 2012-09-11 21:52:43,710 ThriftServer.java (line 191) Using custom half-sync/half-async thrift server on localhost/127.0.0.1 : 9160
 INFO [Thread-2] 2012-09-11 21:52:43,720 ThriftServer.java (line 200) Listening for thrift clients...
[ec2-user@ip-10-82-21-221 ~]$ 


The Timeout happens both in Sync and HSHA servers (randomly and i am not able to reproduce both cases reliably) and the only thing which i can notice is that the client (pycassa) runs 100% CPU most of the time... other than that everything else looks normal.;;;","20/Jul/13 06:42;peter-librato;We may be seeing this behavior in 1.2.6. I haven't enabled debug but we are definitely seeing a correlation between groups of 'Read an invalid frame size of 0' messages (dozens at a time) during the same second that we're seeing ""large"" (10 seconds or more) 'GC for ConcurrentMarkSweep' events.

On a 9 node cluster we see this anywhere from 1 to 9 times a day.

;;;","20/Jul/13 21:26;vijay2win@yahoo.com;Peter, Looks like your issue is because of the client timeout when you didn't receive a response for 10 sec. Time to tune the heap or add more nodes.

Tyler, is this ticket still valid?;;;","23/Jul/13 19:00;thobbs;bq. Tyler, is this ticket still valid?

The changes for CASSANDRA-5582 probably make this invalid for 2.0, which I'm fine with.  I'll assign to myself, test against 2.0, close this ticket and open a new one for the 2.0 implementation if needed.;;;","01/Aug/13 22:56;thobbs;After a quick check, it looks like the CASSANDRA-5582 implementation also doesn't enforce the max frame size.;;;","01/Aug/13 23:15;jbellis;[~xedin], can you have a look?;;;","02/Aug/13 00:14;xedin;It looks like strict read option was removed from TBinaryProtocol which has actually responsible for graceful failure, I also notice that Config.thrift_max_message_length_in_mb is marked as Deprecated, starting from 1.1 I can bring that back but curious is there any good reason behind that? ;;;","02/Aug/13 00:17;jbellis;Those are not the same thing as frame length, see THRIFT-820 and CASSANDRA-5529.;;;","02/Aug/13 00:40;xedin;I actually this I know what is the problem, I will work on solution for distruptor server asap.;;;","02/Aug/13 04:14;xedin;[~thobbs] Please try trunk with attached patch and replace thrift-server-0.2.1.jar in lib/ with the one attached. This would detect frame size violation right when it's read from the socket and report an error as well as close connection.;;;","02/Aug/13 16:38;thobbs;[~xedin] that looks good to me.  There's one problem with the patch: you changed the cassandra.yaml default to {{hsha}}; other than that, +1.;;;","02/Aug/13 16:55;xedin;Thanks! Yaml change wasn't intentional :) I will release 0.2.2 version of thrift-server and commit changes to cassandra once it's in maven central. ;;;","06/Aug/13 22:22;xedin;Committed to 2.0.0 branch (with updated disruptor hsha server to 0.3.0).;;;","08/Aug/13 15:43;jbellis;[~xedin], can you add the license file for thrift-server to lib/licences?;;;","08/Aug/13 18:31;xedin;[~jbellis] Done, licences committed to cassandra-2.0.0 for thrift-server and LMAX disruptor.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
lost+found directory in the data dir causes problems again,CASSANDRA-4572,12604655,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,23/Aug/12 18:08,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/12 19:02,1.1.5,,,,,,0,,,,,,,"Looks like we've regressed from CASSANDRA-1547 and mounting a fs directly on the data dir is a problem again.

{noformat}
INFO [main] 2012-08-22 23:30:03,710 Directories.java (line 475) Upgrade from pre-1.1 version detected: migrating sstables to new directory layout ERROR [main] 2012-08-22 23:30:03,712 AbstractCassandraDaemon.java (line 370) Exception encountered during startup 
                java.lang.NullPointerException         at org.apache.cassandra.db.Directories.migrateSSTables(Directories.java:487)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/12 16:20;yukim;4572-1.1.txt;https://issues.apache.org/jira/secure/attachment/12542935/4572-1.1.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256257,,,Thu Aug 30 19:02:06 UTC 2012,,,,,,,,,,"0|i0gx9b:",96818,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"29/Aug/12 16:20;yukim;When you do File#listFiles on lost+found directory, it returns null. I believe there are other cases that it returns null, so attached patch just checks null after File#listFiles is performed.;;;","30/Aug/12 17:44;jbellis;+1;;;","30/Aug/12 19:02;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Strange permament socket descriptors increasing leads to ""Too many open files""",CASSANDRA-4571,12604647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,sergshne,sergshne,23/Aug/12 17:20,16/Apr/19 09:32,14/Jul/23 05:52,04/Sep/12 19:09,1.1.5,,,,,,0,,,,,,,"On the two-node cluster there was found strange socket descriptors increasing. lsof -n | grep java shows many rows like""

java       8380 cassandra  113r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  114r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  115r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  116r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  117r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  118r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  119r     unix 0xffff8101a374a080            938348482 socket
java       8380 cassandra  120r     unix 0xffff8101a374a080            938348482 socket
"" And number of this rows constantly increasing. After about 24 hours this situation leads to error.
We use PHPCassa client. Load is not so high (aroud ~50kb/s on write). ","CentOS 5.8 Linux 2.6.18-308.13.1.el5 #1 SMP Tue Aug 21 17:10:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux. 

java version ""1.6.0_33""
Java(TM) SE Runtime Environment (build 1.6.0_33-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.8-b03, mixed mode)
",brandon.williams,cherro,christianmovi,eperott,etobgra,j.casares,jeromatron,joe.miller,samt,sergshne,swillcox,tmeighen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/12 16:19;jbellis;4571.txt;https://issues.apache.org/jira/secure/attachment/12543697/4571.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248022,,,Wed Oct 17 14:51:32 UTC 2012,,,,,,,,,,"0|i09ai7:",52135,,samt,,samt,Critical,,,,,,,,,,,,,,,,,"23/Aug/12 18:21;brandon.williams;I've seen this a few times, but never found a cause/resolution, so I'll go ahead and dump what I know:

* All cases thus far seem to be upgrades, not new installations.

* 1.1 but less than 1.1.2 doesn't seem to exhibit

* Cassandra doesn't use unix sockets, at all

* This is fairly rare and only hits a handful of users

* some people have this happen on all nodes, some have it happen on only a portion

* going to such lengths as trying all kinds of different JVM versions and completely switching OSes has not helped

One user wrote a simple app to track the lost FDs here: http://pastebin.com/faBkJueB and it seemed to correlate with opening one sstable, and another user has corroborated that.  Both report heavy reads on that CF.

No way to reproduce is yet known, I've failed in all my attempts.;;;","23/Aug/12 18:26;etobgra;I have seen the problem on cassandra 1.1.3 as well. Our 3 node cluster has the same issue. It's a blocker.
We are using Hector as client and FD increases up to 100K and keeps growing...

Java (build 1.6.0_32-b05)
Linux 2.6.32-220.el6.x86_64 #1 SMP Wed Nov 9 08:03:13 EST 2011 x86_64 x86_64 x86_64 GNU/Linux

After a fresh start cassandra uses one unix FD then we put some load on and it keeps growing.

lsof -p 14597 | grep -i unix
java    14597 root   43u  unix 0xffff88082a3acc80        0t0 42443166 socket

Put load on cassandra and then it increases

lsof -p 14597 | grep -i unix | wc -l
5678
7654
.....
98403










;;;","23/Aug/12 18:29;etobgra;I can reproduce it every time we simulate a specific test case with load using many reads.
We have a new installation of cassandra 1.1.3.

So if you want some trace or dump whatever I can give it to you.;;;","23/Aug/12 18:54;sergshne;It seems that bug is related to Java NIO internals (May be to Thrift framework). Please, read https://forums.oracle.com/forums/thread.jspa?threadID=1146235 for more details and give your thoughts about.
From topic: ""I am submitting this post to highlight a possible NIO ""gotcha"" in multithreaded applications and pose a couple of questions. We have observed file descriptor resource leakage (eventually leading to server failure) in a server process using NIO within the excellent framework written by Ronny Standtke (http://nioframework.sourceforge.net). Platform is JDK1.6.0_05 on RHEL4. I don't think that this is the same issue as that in connection with TCP CLOSED sockets reported elsewhere - What leaks here are descriptors connected to Unix domain sockets.

In the framework, SelectableChannels registered in a selector are select()-ed in a single thread that handles data transfer to clients of the selector channels, executing in different threads. When a client shuts down its connection (invoking key.cancel() and key.channel.close()) eventually we get to JRE AbstractInterruptibleChannel::close() and SocketChannelImpl::implCloseSelectableChannel() which does the preClose() - via JNI this dup2()s a statically maintained descriptor (attached to a dummy Unix domain socket) onto the underlying file descriptor (as discussed by Alan Bateman (http://mail.openjdk.java.net/pipermail/core-libs-dev/2008-January/000219.html)). The problem occurs when the select() thread runs at the same time and the cancelled key is seen by SelectorImpl::processDeregisterQueue(). Eventually (in our case) EPollSelectorImpl::implDereg() tests the ""channel closed"" flag set by AbstractInterruptibleChannel::close() (this is not read-protected by a lock) and executes channel.kill() which closes the underlying file descriptor. If this happens before the preClose() in the other thread, the out-of-sequence dup2() leaks the file descriptor, attached to the UNIX domain socket.

In the framework mentioned, we don't particularly want to add locking in the select() thread as this would impact other clients of the selector - alternatively a fix is to simply comment out the key.cancel(). channel.close() does the cancel() for us anyway, but after the close()/preClose() has completed, so the select() processing then occurs in the right sequence. (I am notifying Ronny Standtke of this issue independently).""

See also following links for more information:
http://stackoverflow.com/questions/7038688/java-nio-causes-file-descriptor-leak
http://mail-archives.apache.org/mod_mbox/tomcat-users/201201.mbox/%3CCAJkSUv-DDKTCQ-pD7W=QOVmPH1dXeXOvcr+3mCgu05cqpT7Zjg@mail.gmail.com%3E
http://www.apacheserver.net/HBase-Thrift-for-CDH3U3-leaking-file-descriptors-socket-at1580921.htm
;;;","23/Aug/12 19:28;jeromatron;Tobias: is it possible to get the test case and the server setup to try to reproduce?  Heap dumps haven't proven very useful thus far.;;;","23/Aug/12 21:49;sergshne;Bug is not recreating with one node cluster;;;","24/Aug/12 13:26;eperott;Adding few more observation from the 3-node cluster mentioned by Tobias above...

We have tried to reproduce the problem in a more isolated scenario without success.

I don't know whether this is of any use or not but we used strace to get a rough view on whats causing the leaked file descriptors. During a session with strace we placed a constant load of reads on the cluster. During this session strace collected ~2.400.000 rows of system calls. The first 2.000.000 have the following pattern:

# cat strace.cassandra.1* | sort | head -2000000 | grep open | wc -l
47
# cat strace.cassandra.1* | sort | head -2000000 | grep close | wc -l
48
# cat strace.cassandra.1* | sort | head -2000000 | grep dup2 | wc -l
20
# cat strace.cassandra.1* | sort | head -2000000 | grep fcntl | wc -l
86

The last 400.000 collected system calls reflect the time when FD leakage started:

# cat strace.cassandra.1* | sort | tail -390000 | grep open | wc -l
11344
# cat strace.cassandra.1* | sort | tail -390000 | grep close | wc -l
5718
# cat strace.cassandra.1* | sort | tail -390000 | grep dup2 | wc -l
16280
# cat strace.cassandra.1* | sort | tail -390000 | grep fcntl | wc -l
22670

As you can see, just a small part of the calls to open() and dup2() have a corresponding call to close().

A glimps of the system calls at the time when the problem starts:
# cat strace.cassandra.1* | sort -s -n -k 1 | less
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY) = 2381
14:59:38 fstat(2381, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2381, F_GETFD) = 0
14:59:38 fcntl(2381, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2381, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2381, 0, SEEK_CUR) = 0
14:59:38 lseek(2381, 53072226, SEEK_SET) = 53072226
14:59:38 read(2381, ""\200\200\4tPREPARE\1\0\4\307\352\fi|J\0\0\0\4P5\337\257\0\3XI""..., 15790) = 15790
14:59:38 lseek(2381, 0, SEEK_CUR) = 53088016
14:59:38 read(2381, ""\330`\243\311"", 4) = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY)                  = 2387
14:59:38 fstat(2387, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2387, F_GETFD) = 0
14:59:38 fcntl(2387, F_SETFD, FD_CLOEXEC)                   = 0
14:59:38 fstat(2387, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2387, 0, SEEK_CUR) = 0
14:59:38 lseek(2387, 1073276, SEEK_SET) = 1073276
14:59:38 read(2387, ""\200\200\4\360>nd\0\7batchId\0\0\4\307\3528\3760\240\0\0\0\34VOU""..., 18284) = 18284
14:59:38 lseek(2387, 0, SEEK_CUR)           = 1091560
14:59:38 read(2387, ""\301\320\321r"", 4) = 4
14:59:38 dup2(43, 2387)                  = 2387
14:59:38 close(2387)       = 0
14:59:38 futex(0x7fa968000eb4, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x7fa968000eb0, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY) = 2389
14:59:38 fstat(2389, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2389, F_GETFD) = 0
14:59:38 fcntl(2389, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2389, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2389, 0, SEEK_CUR) = 0
14:59:38 lseek(2389, 16713707, SEEK_SET) = 16713707
14:59:38 read(2389, ""\200\200\4tgent\0\0\4\307\352\3\25\335\323\0\0\0\4bond\0\7batch""..., 15853) = 15853
14:59:38 lseek(2389, 0, SEEK_CUR) = 16729560
14:59:38 read(2389, ""pS\r\235"", 4)     = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY) = 2401
14:59:38 fstat(2401, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2401, F_GETFD) = 0
14:59:38 fcntl(2401, F_SETFD, FD_CLOEXEC)       = 0
14:59:38 fstat(2401, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2401, 0, SEEK_CUR)                  = 0
14:59:38 lseek(2401, 325664, SEEK_SET) = 325664
14:59:38 read(2401, ""\200\200\4\01066\0\t\0010(\0\0\0\20\0\0\0\3\0\0\0\1\t\23\t\1(P5\353\317""..., 18359) = 18359
14:59:38 lseek(2401, 0, SEEK_CUR) = 344023
14:59:38 read(2401, ""\265.\321p"", 4)   = 4
14:59:38 dup2(43, 2401)       = 2401
14:59:38 close(2401) = 0
14:59:38 futex(0x41e1f804, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0x41e1f800, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1})                   = 1
14:59:38 futex(0x7fae0c3278d0, FUTEX_WAKE_PRIVATE, 1) = 0
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1260-Data.db"", O_RDONLY)                   = 2416
14:59:38 fstat(2416, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 fcntl(2416, F_GETFD) = 0
14:59:38 fcntl(2416, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2416, {st_mode=S_IFREG|0644, st_size=448850532, ...}) = 0
14:59:38 lseek(2416, 0, SEEK_CUR) = 0
14:59:38 lseek(2416, 275573213, SEEK_SET) = 275573213
14:59:38 read(2416, ""\200\200\4\0000n\1\0\310\0\16activationCode\0\0\4\307\351\361#""..., 15829) = 15829
14:59:38 lseek(2416, 0, SEEK_CUR)                  = 275589042
14:59:38 read(2416, ""\316\223j\235"", 4) = 4
14:59:38 open(""/ssd_disks_1/cassandra/data/cake/vouchers/cake-vouchers-he-1275-Data.db"", O_RDONLY) = 2438
14:59:38 fstat(2438, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 fcntl(2438, F_GETFD) = 0
14:59:38 fcntl(2438, F_SETFD, FD_CLOEXEC) = 0
14:59:38 fstat(2438, {st_mode=S_IFREG|0644, st_size=9056852, ...}) = 0
14:59:38 lseek(2438, 0, SEEK_CUR) = 0
14:59:38 lseek(2438, 5538661, SEEK_SET) = 5538661
14:59:38 read(2438, ""\200\200\4\0\0\r\1,P5\354\""\0\4\307\352\312^\3450\5\23<\01755643264""..., 18465) = 18465
14:59:38 lseek(2438, 0, SEEK_CUR)                  = 5557126
14:59:38 read(2438, ""\232\327\216q"", 4) = 4
14:59:38 dup2(43, 2438) = 2438
14:59:38 close(2438)                   = 0

;;;","24/Aug/12 16:23;jbellis;bq. https://forums.oracle.com/forums/thread.jspa?threadID=1146235

That thread says, ""the race condition with cancel is something we are currently working on  in http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6693490,"" which is marked fixed in java6u18.;;;","24/Aug/12 16:25;jbellis;Tobias/Per, can you try your test case against 1.1.2 to verify that this was indeed introduce in 1.1.3 somehow?;;;","24/Aug/12 18:15;joe.miller;We are seeing this fd socket leak issue on 1.1.2 (after upgrade from 1.0.10);;;","24/Aug/12 20:19;jbellis;Thanks Joe.  Can anyone confirm/reject 1.1.1 then?;;;","24/Aug/12 22:33;tmeighen;Just wanted to add my experience with this. In our case we saw the issue when we upgraded from 1.0.x to 1.1.x. We are currently running 1.1.2 on Ubuntu 12.04 with Sun JDK 1.6.0_34. I'm pretty sure that we had the issue with 1.1.1 as well.

With the help of some kind folks on the irc channel I was able to track the vast majority of the leaks to one particular column family. Some of the characteristics of the CF:

- Heavy read load
- Data is deleted
- Accessed using Hector Object Mapping (the only CF in our system that uses this)
- Relatively small Column Family
- Row caching enabled *

Once I was able to pinpoint this table I started reviewing the schema and noticed that in the migration from 1.0.x to 1.1.x we had lost our row cache settings. The original schema defined row_cache_size=20000 but when we upgraded I found that caching=""KEYS_ONLY"" instead of ""ALL"". When I re-enabled the row cache we stopped leaking file descriptors. I'm assuming this is because the data was being pulled from memory instead of disk and so the underlying issue was no longer exposed.

Another thing to note is that our ring (4 nodes at the time, I've upped it to 6 since) was under fairly heavy load and doing a lot of garbage collection.;;;","25/Aug/12 02:42;sergshne;I have checked 1.1.1 and have found the same issue.;;;","29/Aug/12 16:21;swillcox;We are also seeing this bug and all nodes eventually run out of file descriptors and crash. It is a blocker for us.;;;","29/Aug/12 16:46;eperott;To verify, we started from scratch. A new installation on 3 servers. And the FD leak is still there. So, with our particular setup we are able to reproduce the bug.

These are the characteristics of our setup:
- We have one single CF.
- Rows are inserted in batches.
- Rows are red, updated and deleted in a random like pattern.
- The FD leak seem to start during heavy read load (but can appear during mixed read/write/delete operations as well).
- We are using Hector to access this single CF.
- Cassandra configuration is basically standard.

The FD leaks does not show immediately. It appears once there is ~60M rows in CF.
;;;","29/Aug/12 16:53;jbellis;Are you sure you can't reproduce on a single-node cluster?

Because we're getting conflicting evidence here; on the one hand, strace indicates that the fd leakage is related to file i/o, but if so, you shouldn't need multiple nodes in the cluster to repro.;;;","30/Aug/12 00:07;sergshne;bq.Are you sure you can't reproduce on a single-node cluster?

My mistake. I've checked it again. Bug also was reproduced with one-node cluster.;;;","31/Aug/12 13:14;jbellis;bq. The FD leak seem to start during heavy read load (but can appear during mixed read/write/delete operations as well)

Does this mean that you can reproduce the leak if you stop doing inserts/updates entirely and just do reads?

What kind of reads are you doing?  index lookups?  seq scan?  named-columns-from-row?  slice-from-row?;;;","31/Aug/12 14:25;etobgra;Yes. I have tried to run traffic for a couple of days which does mixed operations and cassandra is still running.
However, i just started to run a traffic testcase which does only reads and the issue is back directly.
I don't know if it matters but all my reads returns empty responses which is expected since these keys should be deleted :) 
The testcase does a lookup on the key only.

E.g I have file with a number of keys and then my testcase try to lookup a row using that key and the response is empty since these keys does not exists.





;;;","31/Aug/12 14:52;swillcox;We can reliably reproduce this issue in our test environment every day. Start the servers up in the morning and by the end of the workday the number of open file descriptors reaches from 40-60K and the nodes stop responding. We have turned row caching off and it still has this problem. You can contact me if you think remotely debugging this issue will help in determining what is causing this.

We have reproduced this using just one node.;;;","04/Sep/12 15:12;brandon.williams;Thanks to a reproducible example from Viktor Kuzmin, I've bisected this down to CASSANDRA-4116.;;;","04/Sep/12 16:13;jbellis;Patch to fix 4116 sstable iterator leak.;;;","04/Sep/12 16:25;brandon.williams;Issue does not repro with this patch.;;;","04/Sep/12 18:49;samt;LGTM +1;;;","04/Sep/12 19:09;jbellis;committed!  thanks everyone for the help tracking this down.;;;","15/Oct/12 17:24;j.casares;This can still be seen in 1.1.5 if the user is running Java 1.6.0_29. The current solution is to upgrade to 1.6.0_35.;;;","15/Oct/12 18:52;cherro;For anybody else encountering this unbounded socket growth problem on 1.1.5, note that while upgrading 1.6.0_35 seemed to help, a longer load test still reproduced the symptom. FWIW, upgradesstables ran for a period during this particular test - unclear if the increased compaction activity contributed.;;;","16/Oct/12 01:57;jbellis;Related to CASSANDRA-4740?;;;","16/Oct/12 04:13;cherro;FYI was able to reproduce the symptom on Cassandra 1.1.6.
@[~jbellis] Re: CASSANDRA-4740 and whether it relates to this: 
* Haven't looked across all nodes for phantom connections yet
* Have searched across all logs - found a single instance of ""Timed out replaying hints"".
* Mina mentioned that ""Nodes running earlier kernels (2.6.39, 3.0, 3.1) haven't exhibited this"". We are seeing this on Linux kernel 2.6.35 with Java 1.6.0_35.
;;;","16/Oct/12 16:15;cherro;We are also seeing errors similar to those reported in CASSANDRA-4687.
Could this be a side-effect of that problem? In {{SSTableSliceIterator}} as of commit {{e1b10590e84189b92af168e33a63c14c3ca1f5fa}}, if the constructor key equality assertion fails, {{fileToClose}} does not get closed.;;;","16/Oct/12 16:45;jbellis;Are you then seeing that assertion failure logged?;;;","16/Oct/12 17:19;cherro;Yes, seeing the key equality AssertionErrors from two SSTable iterators: SSTableSliceIterator:60 and SSTableNamesIterator:72.
Also seeing same EOF error reported by [~tjake] in CASSANDRA-4687:
{code}
java.io.IOError: java.io.EOFException: unable to seek to position 61291844 in /redacted/cassandra/data/test1/redacted/test1-redacted-hf-1-Data.db (59874704 bytes) in read-only mode
        at org.apache.cassandra.io.util.CompressedSegmentedFile.getSegment(CompressedSegmentedFile.java:69)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:898)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:50)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:67)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:79)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:256)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:64)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1345)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1207)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1142)
        at org.apache.cassandra.db.Table.getRow(Table.java:378)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:69)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: unable to seek to position 61291844 in /redacted/cassandra/data/test1/redacted/test1-redacted-hf-1-Data.db (59874704 bytes) in read-only mode
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:253)
        at org.apache.cassandra.io.util.CompressedSegmentedFile.getSegment(CompressedSegmentedFile.java:64)
        ... 16 more
{code}
;;;","17/Oct/12 03:36;cherro;Tested this patch: https://gist.github.com/2f10efd3922fab9a095e applied to a build from branch cassandra-1.1 at commit 4d2e5e73b127dc0b335176ddc1dec1f0244e7f6d.

This definitely reduced the growth of socket FD handles, but there must be other scenarios like this in the codebase because it did grow beyond 2 which is where I've seen it at steady state under normal conditions.

The AssertionErrors from CASSANDRA-4687 were so spurious that they were pegging disk IO. When I ran the same test again with assertions disabled for the org.apache.cassandra.db.columniterator package, I saw many errors like those described in CASSANDRA-4417 (""invalid counter shard detected""). See my comments in that issue.

Shouldn't CASSANDRA-4571 be re-opened?




;;;","17/Oct/12 14:51;jbellis;If it's caused by 4687 assertion errors?  No.

Disabling key cache is a workaround for 4687 btw.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
countPendingHints JMX operation is returning garbage for the key,CASSANDRA-4568,12604476,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jblangston@datastax.com,jblangston@datastax.com,22/Aug/12 19:29,16/Apr/19 09:32,14/Jul/23 05:52,28/Aug/12 14:32,1.0.12,1.1.5,,,,,0,datastax_qa,,,,,,"countPendingHints JMX operation should return a map from key: endpoint IP address to value: number of pending hints. It is returning garbage for the key (looks like binary data concerning the hint itself). The value looks correct.

Steps to reproduce:

1) Set up a two-node cluster. 

2) Disable gossip on the second node.  

`nodetool ring` output from node 1:

Address         DC          Rack        Status State   Load            Effective-Ownership Token                                       
                                                                                           85070591730234615865843651857942052864      
192.168.1.162   datacenter1 rack1       Up     Normal  21.46 KB        100.00%             0                                           
192.168.1.130   datacenter1 rack1       Down   Normal  6.67 KB         100.00%             85070591730234615865843651857942052864      


3) While the second node is still down, create a keyspace with RF=2 and a CF within this keyspace. Then insert two records into the CF:

Connected to Test Cluster at 192.168.1.162:9160.
[cqlsh 2.2.0 | Cassandra 1.1.2 | CQL spec 2.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> create KEYSPACE demo WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 2;
cqlsh> use demo;
cqlsh:demo> create table users (username varchar primary key, password varchar);
cqlsh:demo> insert into users (username, password) values (scott, tiger);
cqlsh:demo> insert into users (username, password) values (root, password);

4) Use a JMX client to execute the countPendingHints operation:

jblangston:~ jblangston$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7199 org.apache.cassandra.db:type=HintedHandoffManager countPendingHints

08/22/2012 14:21:37 -0500 org.archive.jmx.Client countPendingHints: {@B^h ??	?[b??scottdemoscott?????password?ߞHtigerdemoF
?P??	?[b??rootdemoroot?????password?ߞ?Wpassworddemo=2}

5) Notice the output.  The value (2) is correct but the key is garbage instead of an endpoint IP address.",,jblangston@datastax.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/12 18:32;brandon.williams;4568.txt;https://issues.apache.org/jira/secure/attachment/12542310/4568.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256254,,,Tue Aug 28 14:32:42 UTC 2012,,,,,,,,,,"0|i0gx7z:",96812,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Aug/12 18:32;brandon.williams;Patch to correctly display the token.;;;","24/Aug/12 20:56;jbellis;why not just use r.key.token?

+1 otherwise.;;;","28/Aug/12 14:32;jbellis;Went ahead and committed w/ that change because open issues for 1.0.12 damage my calm.

Note on the issue description: Since 1.0.0 (CASSANDRA-2045) the row key for hints is the token of the [v]node owning the hints, not the ip address.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in log related to Murmur3Partitioner,CASSANDRA-4567,12604460,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tpatterson,tpatterson,22/Aug/12 17:41,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/12 16:56,1.2.0 beta 1,,,,,,0,,,,,,,"Start a 2-node cluster on cassandra-1.1. Bring down one node, upgrade it to trunk, start it back up. The following error shows up in the log:
{code}
...
 INFO [main] 2012-08-22 10:44:40,012 CacheService.java (line 170) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 (148 bytes)
 INFO [SSTableBatchOpen:2] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 (226 bytes)
 INFO [SSTableBatchOpen:3] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 (89 bytes)
ERROR [SSTableBatchOpen:3] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:2] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:2,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:1] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [main] 2012-08-22 10:44:40,486 DatabaseDescriptor.java (line 522) Couldn't detect any schema definitions in local storage.
 INFO [main] 2012-08-22 10:44:40,487 DatabaseDescriptor.java (line 525) Found table data in data directories. Consider using the CLI to define your schema.
...
{code}

Note that the error does not happen when upgrading from cassandra-1.0 to cassandra-1.1, or when ""upgrading"" from trunk to trunk.

This is the exact dtest I used:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        node1.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
{code}",Using ccm on ubuntu,tpatterson,vijay2win@yahoo.com,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/12 21:17;vijay2win@yahoo.com;0001-CASSANDRA-4567-v2.patch;https://issues.apache.org/jira/secure/attachment/12542830/0001-CASSANDRA-4567-v2.patch","28/Aug/12 21:39;vijay2win@yahoo.com;0001-CASSANDRA-4567-v3.patch;https://issues.apache.org/jira/secure/attachment/12542835/0001-CASSANDRA-4567-v3.patch","28/Aug/12 16:50;vijay2win@yahoo.com;0001-CASSANDRA-4567.patch;https://issues.apache.org/jira/secure/attachment/12542785/0001-CASSANDRA-4567.patch",,,,,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256253,,,Thu Aug 30 16:56:31 UTC 2012,,,,,,,,,,"0|i0gx7j:",96810,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"22/Aug/12 22:44;vijay2win@yahoo.com;CASSANDRA-3772 modified the default Paritioner to M3P (was RandomP). Users who upgrade the existing clusters need to change the yaml settings. 
May be updating NEWS.txt will help?;;;","22/Aug/12 23:07;xedin;Agreed, I will update the NEWS as part of CASSANDRA-3772 when finished, I don't really see any user-friendly way to change default partitioner, users who use packages shouldn't be affected as configuration is separate but automated tools probably would have to add a check to reset partitioner when testing 1.1 -> 1.2 migration...;;;","23/Aug/12 16:35;tpatterson;Would it be feasible to provide a more helpful error message for people that didn't read NEWS.txt?;;;","24/Aug/12 03:36;jbellis;Should we just ignore the partitioner after first startup, the way we do with initial_token?;;;","24/Aug/12 16:21;vijay2win@yahoo.com;We can implement it like initial token, but the problem is that the User will not notice/fix it…. When a new node is bootstrapped then he/she will run into issues.;;;","27/Aug/12 17:03;jbellis;We could check partitioner on bootstrap the way we do cluster name...;;;","28/Aug/12 16:50;vijay2win@yahoo.com;Didn't realize that System table requires SS.getPartitioner (Some how thought it uses local partitioner). 

Looks like we cannot store the partitioner information in system table like initial token (because to open the Table/CF we need a partitioner) and we have to store it as a cluster metadata (new file or something). IMHO, We can make the log messages pretty print and live with it?

BTW: checking the partitioner to be consistent with the cluster is a safe thing which we have to do, hence the attached patch.;;;","28/Aug/12 18:38;jbellis;bq. We can make the log messages pretty print and live with it

+1;;;","28/Aug/12 20:48;xedin;Two things: 

 - I think we can cache canonical name instead of calling StorageService.getPartitioner().getClass().getCanonicalName() all the time
 - In ""if (gDigestMessage.partioner != null && !gDigestMessage.partioner.equals(DatabaseDescriptor.getClusterName()))"" the second condition tries to match partitioner to cluster_name.;;;","28/Aug/12 21:12;vijay2win@yahoo.com;Done! (not sure how i missed this with the tests which i did :( )

Attached patch also exits the VM if it detects the change in partitioner, earlier we where skiping the SST's and start the node without any data.;;;","28/Aug/12 21:25;xedin;I see you added partitionerName to GossipDigestSynVerbHandler but I think better would be to add it to DatabaseDescriptor which would allow us to use it in GossipDigestSynVerbHandler and well as Gossiper :);;;","28/Aug/12 21:39;vijay2win@yahoo.com;Done!;;;","29/Aug/12 20:54;xedin;+1;;;","30/Aug/12 01:22;vijay2win@yahoo.com;Committed with regenerated test/data/serialization/1.2/gms.Gossip.bin, Thanks!;;;","30/Aug/12 13:54;jbellis;looks like this broke SSTableReaderTest.testPersistantStatisticsWithSecondaryIndex;;;","30/Aug/12 16:56;vijay2win@yahoo.com;Fixed in 9cd53fba648ae5a30a181f8a06786f33db95a0fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test-clientutil target is failing,CASSANDRA-4566,12604450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,dbrosius@apache.org,urandom,urandom,22/Aug/12 16:00,16/Apr/19 09:32,14/Jul/23 05:52,06/Sep/12 04:05,1.2.0,,,Legacy/Tools,Packaging,,0,lhf,,,,,,"The {{test-clientutil}} target is failing (on trunk at least):

{noformat}
    [junit] java.lang.NoClassDefFoundError: org/apache/cassandra/io/IVersionedSerializer
    [junit] 	at java.lang.ClassLoader.defineClass1(Native Method)
    [junit] 	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:631)
    [junit] 	at java.lang.ClassLoader.defineClass(ClassLoader.java:615)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:38)
    [junit] 	at org.apache.cassandra.cql.jdbc.ClientUtilsTest.test(ClientUtilsTest.java:59)
    [junit] Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.io.IVersionedSerializer
    [junit] 	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
    [junit] 	at java.security.AccessController.doPrivileged(Native Method)
    [junit] 	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
    [junit] 	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
{noformat}

This target is to ensure that the clientutil jar has no unsatisfied dependencies.  In this case it looks like static initialization is pulling in {{o.a.c.io.IVersionedSerializer}}, (probably transitively).",,dbrosius@apache.org,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/12 15:04;urandom;0001-4566-v2.txt.patch;https://issues.apache.org/jira/secure/attachment/12543862/0001-4566-v2.txt.patch","25/Aug/12 22:20;dbrosius@apache.org;4566.txt;https://issues.apache.org/jira/secure/attachment/12542430/4566.txt",,,,,,,,,,,,,,,,2.0,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256252,,,Thu Sep 06 04:05:37 UTC 2012,,,,,,,,,,"0|i0gx73:",96808,,urandom,,urandom,Critical,,,,,,,,,,,,,,,,,"25/Aug/12 22:20;dbrosius@apache.org;pull out UUIDSerializer to a top level class, so that it isn't referenced from UUIDGen.

Also remove dependency on guava just for Charsets in the clientutil.jar;;;","05/Sep/12 15:07;urandom;+1 for the approach, but please remove Guava from the classpath of the {{test-clientutil-jar}} target in build.xml, and the unused Guava imports in {{o.a.c.cql.jdbc.Jdbc\{Ascii,UTF8\}}}, (0001-4566-v2.txt.patch does all of this).;;;","05/Sep/12 20:34;dbrosius@apache.org;looks good.;;;","06/Sep/12 04:05;dbrosius@apache.org;committed as 978d7bb7d59492129aee2747308d55f949003b02;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MoveTest madness,CASSANDRA-4564,12604290,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,swordheart,urandom,urandom,21/Aug/12 22:17,16/Apr/19 09:32,14/Jul/23 05:52,04/Jan/13 13:39,1.1.9,1.2.1,,Legacy/Testing,,,0,lhf,,,,,,"I encountered what looks like bugs in {{o.a.c.service.MoveTest.newTestWriteEndpointsDuringMove()}} while doing something else; Here is a (poorly researched )ticket before I forget :)

* There are two loops over non-system tables, and the first is a NOOP
* In the second loop, a set exactly {{replicationFactor}} in size is compared against {{tmd.getWriteEndpoints()}}, which should produce greater than {{replicationFactor}} endpoints during a move (shouldn't it?); How does this pass?",,swordheart,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/12 06:26;swordheart;cassandra-1.1-4564.txt;https://issues.apache.org/jira/secure/attachment/12561849/cassandra-1.1-4564.txt",,,,,,,,,,,,,,,,,1.0,swordheart,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256250,,,Fri Jan 04 13:39:56 UTC 2013,,,,,,,,,,"0|i0gx67:",96804,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"20/Dec/12 06:25;swordheart;For the second issue, the root cause is: the key token is endpointToken+5, and the endpoint token is moved to endpointToken+2, so there is no key need to be moved. I have changed the move token to enpointToken+7 and updated the test cases.;;;","22/Dec/12 15:21;jbellis;Thanks, Liu.  Review may proceed a bit slowly due to the US holidays, but we'll get to it eventually!;;;","04/Jan/13 13:39;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update column family fails,CASSANDRA-4561,12604193,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,zenek_kraweznik0,zenek_kraweznik0,21/Aug/12 09:24,16/Apr/19 09:32,14/Jul/23 05:52,04/Sep/12 21:17,1.1.5,,,,,,2,,,,,,,"[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};


[default@test] update column family Messages with min_compaction_threshold = 4 and  max_compaction_threshold = 32;
a5b7544e-1ef5-3bfd-8770-c09594e37ec2
Waiting for schema agreement...
... schemas agree across the cluster

[default@test] show schema;
create column family Messages
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 2
  and max_compaction_threshold = 4
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compaction_strategy_options = {'sstable_size_in_mb' : '1024'}
  and compression_options = {'chunk_length_kb' : '64', 'sstable_compression' : 'org.apache.cassandra.io.compress.DeflateCompressor'};",,arya,awinter,hudson,slebresne,suguru,xedin,zenek_kraweznik0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/12 11:45;xedin;CASSANDRA-4561-CS.patch;https://issues.apache.org/jira/secure/attachment/12544204/CASSANDRA-4561-CS.patch","29/Aug/12 13:22;xedin;CASSANDRA-4561.patch;https://issues.apache.org/jira/secure/attachment/12542918/CASSANDRA-4561.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256247,,,Tue Sep 11 00:32:47 UTC 2012,,,,,,,,,,"0|i0gx4v:",96798,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Aug/12 09:28;zenek_kraweznik0;in logfile I see only this:
 INFO [MigrationStage:1] 2012-08-21 11:27:55,560 ColumnFamilyStore.java (line 659) Enqueuing flush of Memtable-schema_columnfamilies@970905946(1266/1582 serialized/live bytes, 20 ops)
 INFO [FlushWriter:5] 2012-08-21 11:27:55,561 Memtable.java (line 264) Writing Memtable-schema_columnfamilies@970905946(1266/1582 serialized/live bytes, 20 ops)
 INFO [FlushWriter:5] 2012-08-21 11:27:55,587 Memtable.java (line 305) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-he-196-Data.db (1336 bytes) for commitlog position ReplayPosition(segmentId=4914817711083622, position=333055);;;","21/Aug/12 12:58;zenek_kraweznik0;oh, and it obviously happens when I'm trying to change anything else, not only compaction treshhold, ex: sstable_size_in_mb;;;","22/Aug/12 03:04;suguru;I saw a similar case. In my case, columns of schema_keyspaces and schema_columnfamilies in the system keyspace had future timestamp such as 2052-08-01 11:22:33. All operations of changing schemas were not affected. I do not know why timestamps had future values. I fixed it with rebuilding system keyspace.;;;","22/Aug/12 07:12;zenek_kraweznik0;Hmmm, it's possible. How did you rebuilt system keyspace?;;;","22/Aug/12 09:16;xedin;Zenek, what version are you using? It's possible if you are on version less than 1.1.3 or if you have created your schema before 1.1.3 because of CASSANDRA-4432...;;;","22/Aug/12 10:21;zenek_kraweznik0;I'm using 1.1.4, but schema was created in 0.8.5

Could you help me solve my problem step by step?;;;","22/Aug/12 10:27;xedin;Did you upgrade directly to 1.1.4 or one of the previous 1.1 versions first? Please check timestamps in your schema_* ColumnFamilies and see if they have any future dates. The easiest way I see to fix this would be to re-create your schema if you have timestamp problems from CASSANDRA-4432 ;;;","22/Aug/12 12:40;zenek_kraweznik0;I've upgraded cassandra from 8.7 to 1.0, then to 1.0.5 -> 1.0.6 -> 1.1.0 -> 1.1.1 -> 1.1.2 -> 1.1.0 -> 1.1.3 -> 1.1.4.
;;;","22/Aug/12 15:16;xedin;aha, so check system.schema_* CFs column's timestamp values I bet they are from the future. The simplest fix for you would be to delete system/schema_* SSTables and re-create a schema after restart.;;;","22/Aug/12 15:20;jbellis;bq. check system.schema_* CFs column's timestamp values I bet they are from the future

Can we fix that on startup for other upgraders?;;;","22/Aug/12 15:23;xedin;Yes, I was about too :);;;","23/Aug/12 23:40;arya;+1 I just ended up seeing this problem in our cluster which was upgraded from 1.1.2 to 1.1.3. Will probably have to find a workaround since I have to change schema now.;;;","27/Aug/12 18:17;arya;I took my cluster down for 10 minutes. I took a snapshot of 'show schema' into a text file and removed system KS from it so I will only have my KS schema definition in it. Then I stop cassandra on all nodes. On each node, I removed system/schema_* folders from system's keyspace folder in cassandra data dir. I started all cassandra nodes. When I tried to reload the schema file using cli to recreate my CFs, I kept getting the message that CFs already exist. When I listed schema_columnfamilies in one of the node, I saws the same long timestamps like 2705487066780774 on columns of that CF. So, the procedure didn't quiet work out for me. What could have gone wrong here?

Please advice.;;;","27/Aug/12 18:35;xedin;What cassandra version are you running? Did you try to recreate a schema on all of the nodes or just one of them?;;;","27/Aug/12 21:07;arya;I am running 1.1.3 which was upgraded from 1.1.2. When nodes came up, I created the schemas in one node only.;;;","27/Aug/12 21:08;arya;As part of my process, I also removed saved_caches/system-*.;;;","27/Aug/12 21:14;xedin;if you still have such timestamps and messages that CF already exists means that schema wasn't empty on restart and was reloaded from one of the nodes. I'm currently working on the patch which would fix timestamp situation on node's start up.;;;","27/Aug/12 21:44;arya;Do you have an ETA? I'll be happy to monkey apply it on my cluster and be the guinea pig for it.;;;","27/Aug/12 23:40;xedin;Not yet but I'm working on it and soon as it's ready I will attach it here.;;;","04/Sep/12 19:38;jbellis;Don't you want to call cfs.truncate().get() to block for the truncate to finish?  Rest LGTM.;;;","04/Sep/12 19:59;xedin;good point, I would add that and commit, thanks!;;;","04/Sep/12 21:17;xedin;Committed.;;;","04/Sep/12 22:25;hudson;Integrated in Cassandra #2035 (See [https://builds.apache.org/job/Cassandra/2035/])
    change SYSTEM_TABLE to SYSTEM_KS related to CASSANDRA-4561 merge (Revision cf23bd0a0fd192d991e971bcae94c2447126f873)

     Result = FAILURE
xedin : 
Files : 
* src/java/org/apache/cassandra/db/DefsTable.java
;;;","07/Sep/12 02:49;awinter;After applying this patch (1.1.5-tentative) I see the future timestamps being detected and fixed:

{code}
INFO [main] 2012-09-07 02:24:00,090 DefsTable.java (line 202) Fixing timestamps of schema ColumnFamily schema_keyspaces...
INFO [main] 2012-09-07 02:24:00,168 DefsTable.java (line 202) Fixing timestamps of schema ColumnFamily schema_columnfamilies...
{code}

If I list schema_keyspaces I still see the old (future) timestamps.

Given the timestamps weren't actually updated I did go back and found the following error shortly after the restarted node joined the ring (this may or not be related):

{code}ERROR [InternalResponseStage:1] 2012-09-07 02:24:16,555 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:468)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:346)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:324)
        at org.apache.cassandra.service.MigrationManager$MigrationTask$1.response(MigrationManager.java:416)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{code}

I have however assumed that the patch automatically fixes this across all servers in a multi dc ring once this patch is applied.  If there is a specific series of steps required to fix this, such as the full ring outage, deletion of system/schema_* etc can it please be documented here, the wiki or in the NEWS.txt file under upgrading notes?;;;","07/Sep/12 09:01;xedin;Yes it does, but it assumes that you have schema in agreement across the nodes before it tries to fix timestamps. I can put that into NEWS.txt;;;","07/Sep/12 09:09;awinter;My schema is in agreement, but the timestamps aren't fixed, even though the log suggests otherwise (is that previously mentioned NPE related?).  

{code}
[default@unknown] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.PropertyFileSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
        89b22434-5e34-381d-83d1-2a3cde1482fe: [x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x]

[default@unknown]
{code}

Schema updates continue to silently fail.;;;","07/Sep/12 09:23;xedin;Interesting, why does it try to merge remote schema if all of your nodes are in agreement, can you run in debug mode as attach the log? The thing is timestamps are fixed long before the storage server is initialized and the process involves truncate of the system.schema_* CFs so I don't see any other reason why your schema still has old timestamps except some other node sent it migration request... 

NPE that you experience is also interesting, cfMetaData() is always not null as CFs are copied from one map to another on KS initialization, and KS itself could not be null or you would have the assertion error instead of NPE.;;;","07/Sep/12 10:36;slebresne;Maybe we should reopen this if that didn't fully fixed the problem? I'd better hold on a bit on 1.1.5 if this is not fixed yet as this seem to hit quite a few people.;;;","07/Sep/12 11:10;xedin;We are not yet sure what is the problem so I think we should hold on reopening for a bit.;;;","07/Sep/12 11:19;xedin;I just thought that a good addition to existing patch could be change in ColumnSerializer.deserialize to fix timestamps from the future, that way migrations from the remote locations would be deserialized with correct timestamp even if they were sent with the wrong one...;;;","07/Sep/12 11:21;slebresne;Well, Anton still sees timestamp in the future and is still unable to do schema updates, it does sound a lot like there is a problem and it's related to this ticket. I just meant that I prefer keeping that in mind before releasing 1.1.5 into the wild too quickly.;;;","07/Sep/12 11:27;xedin;I understand, but we don't know if that is caused by the fix not working or by something else as exception indicates that something was send to the node, this could be a separate issue. I will work on the ColumnSerializer change I mentioned and we'll see if it helps.  ;;;","07/Sep/12 11:35;awinter;Debug log sent to [~xedin] privately.;;;","07/Sep/12 11:45;xedin;adding a patch that fixes column timestamp on deserialization. Anton, can you please apply and see if that fixes the problem?;;;","07/Sep/12 11:59;xedin;As I see from the log - timestamps were actually fixed to ""7 september 2012 cl 10:58 GMT"" but then overriten by remote migration, latest patch should help with that.;;;","07/Sep/12 12:08;zenek_kraweznik0;When cassandra 1.1.5 will be available?;;;","07/Sep/12 13:16;awinter;I've patched & deployed to a couple of nodes where I now see the corrected timestamps.  The NPE appears to also be gone.

It will take me some time to deploy to the entire ring but it looks promising so far.;;;","07/Sep/12 14:42;awinter;Ring upgraded with the second patch and I am now able to perform schema updates.  Thanks!;;;","07/Sep/12 15:52;jbellis;+1 lgtm;;;","07/Sep/12 16:08;xedin;Committed to cassandra-1.1 branch.;;;","11/Sep/12 00:32;hudson;Integrated in Cassandra #2084 (See [https://builds.apache.org/job/Cassandra/2084/])
    change ColumnSerializer.deserialize to be able fix timestamps from future, related to CASSANDRA-4561 (Revision 2c69e2ea757be9492a095aa22b5d51234c4b4102)
change ColumnSerializer.deserialize to be able fix timestamps from future, related to CASSANDRA-4561 (Revision 429fa7a80e22757a55c03e99c27c157824a666af)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/db/ColumnSerializer.java

xedin : 
Files : 
* src/java/org/apache/cassandra/db/ColumnSerializer.java
;;;",,,,,,,,,,,,,,,,,,
select statement with indexed column causes node to OOM,CASSANDRA-4555,12603936,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dr-alves,dr-alves,18/Aug/12 01:02,16/Apr/19 09:32,14/Jul/23 05:52,18/Aug/12 04:49,1.2.0 beta 1,,,,,,0,,,,,,,"After creating a keyspace, table and index on a clean ccm 3 node cluster based on trunk, when a select statement with an index expression is executed in cqlsh one of the nodes OOM's and goes down.

The steps to reproduce the problem are:

create a 3 node cluster from trunk (I used ccm)

execute the following statements in cqlsh:

{noformat}
CREATE KEYSPACE trace WITH strategy_class = 'SimpleStrategy'
  AND strategy_options:replication_factor = '1';

CREATE TABLE trace.trace_events(sessionId  timeuuid,
  coordinator       inet,
  eventId           timeuuid,
  description       text,
  duration          bigint,
  happened_at       timestamp,
  name              text,
  payload_types     map<text, text>,
  payload           map<text, blob>,
  source            inet,
  type              text,
  PRIMARY KEY (sessionId, coordinator, eventId));

CREATE INDEX idx_name ON trace.trace_events (name);
{noformat}

Executing the following statement causes node2 to OOM:

select * from trace_events where name = 'batch_mutate';

In my case node2 goes down with:

{noformat}
ERROR [Thread-9] 2012-08-17 19:42:55,741 CassandraDaemon.java (line 131) Exception in thread Thread[Thread-9,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.dht.Token$TokenSerializer.deserialize(Token.java:97)
	at org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer.deserialize(AbstractBounds.java:161)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:299)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:181)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:94)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:181)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:69)
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,746 ThriftServer.java (line 221) Stop listening to thrift clients
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:55,748 Gossiper.java (line 1054) Announcing shutdown
 INFO [StorageServiceShutdownHook] 2012-08-17 19:42:56,749 MessagingService.java (line 657) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.2] 2012-08-17 19:42:56,751 MessagingService.java (line 849) MessagingService shutting down server thread.
{noformat}
",MAC OSx,dr-alves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/12 04:16;jbellis;4555.txt;https://issues.apache.org/jira/secure/attachment/12541468/4555.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256241,,,Sat Aug 18 04:49:55 UTC 2012,,,,,,,,,,"0|i0gx2f:",96787,,dr-alves,,dr-alves,Normal,,,,,,,,,,,,,,,,,"18/Aug/12 03:33;jbellis;It OOMs with no data in it at all?;;;","18/Aug/12 03:55;dr-alves;With and without data. It OOMs because it tries to allocate a huge byte[] on TokenSerializer.deserialize(). ;;;","18/Aug/12 04:16;jbellis;patch to make de/serializers agree to use a short length for index expression value, and to add validation that client-provided expressions do conform to this limitation;;;","18/Aug/12 04:30;dr-alves;+1 applies cleanly and solves the problem.;;;","18/Aug/12 04:49;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while loading Saved KeyCache,CASSANDRA-4553,12603795,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,16/Aug/12 22:35,16/Apr/19 09:32,14/Jul/23 05:52,17/Aug/12 02:30,1.2.0 beta 1,,,,,,0,,,,,,,"WARN [main] 2012-08-16 15:31:13,896 AutoSavingCache.java (line 146) error reading saved cache /var/lib/cassandra/saved_caches/system-local-KeyCache-b.db
java.lang.NullPointerException
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:140)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:251)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:354)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:326)
	at org.apache.cassandra.db.Table.initCf(Table.java:312)
	at org.apache.cassandra.db.Table.<init>(Table.java:252)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.db.Table.open(Table.java:75)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:285)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:168)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
",,vijay2win@yahoo.com,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/12 22:36;vijay2win@yahoo.com;0001-CASSANDRA-4553.patch;https://issues.apache.org/jira/secure/attachment/12541289/0001-CASSANDRA-4553.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256240,,,Fri Aug 17 02:30:00 UTC 2012,,,,,,,,,,"0|i0gx1r:",96784,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"16/Aug/12 22:36;vijay2win@yahoo.com;Simple fix to handle null in ASC;;;","16/Aug/12 22:47;jbellis;+1, although would be even better w/ comment as to why we expect keycache deserialize to return nulls sometimes (but not rowcache);;;","16/Aug/12 22:49;xedin;+1 with Jonathan;;;","17/Aug/12 02:30;vijay2win@yahoo.com;Committed with comments, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodetool getendpoints keys do not work with spaces, key_validation=ascii value of key => ""a test""  no delimiter",CASSANDRA-4551,12603783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdeangel,mvaldez,mvaldez,16/Aug/12 20:36,16/Apr/19 09:32,14/Jul/23 05:52,23/Aug/13 01:16,1.2.9,,,Tool/nodetool,,,0,datastax_qa,lhf,,,,,"Nodetool getendpoints keys do not work with embedded spaces, key_validation=ascii value of key => ""a test""  no delimiter tried to escape key => ""a test"" with double and single quotes. It doesn't work. It just reiterates the format of the tool's command: getendpoints requires ks, cf and key args",,dbrosius,gdeangel,mvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/13 05:50;gdeangel;CASSANDRA-4551.txt;https://issues.apache.org/jira/secure/attachment/12599119/CASSANDRA-4551.txt",,,,,,,,,,,,,,,,,1.0,gdeangel,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256238,,,Fri Aug 23 01:16:22 UTC 2013,,,,,,,,,,"0|i0gx0n:",96779,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,"21/Aug/13 03:27;gdeangel;The documentation for nodetool getendpoints says that the key must be in hex format. Is the key being entered as the string ""a test"" or the hex representation?;;;","21/Aug/13 04:35;jbellis;That's outdated -- we do call keyValidator.fromString, so human-readable keys are okay.

The goal here is to get {{getendpoints myks mytable ""a test""}} to parse the key as {{a test}} rather than the key {{a}} followed by an unparsed argument {{test}}.;;;","22/Aug/13 01:11;dbrosius;Is this a problem in nodetool.bat as well?;;;","22/Aug/13 05:17;gdeangel;I'm not sure about that but I can try to double check tomorrow if I can get access to a Windows machine.;;;","22/Aug/13 17:54;gdeangel;I did not see the same issue with nodetool.bat. Seems to work fine.;;;","23/Aug/13 01:16;dbrosius;thanks for checking, +1

committed to cassandra-1.2 as commit ddb501df408af59e213380263f3c519d11b89977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: handle when full cassandra type class names are given,CASSANDRA-4546,12603535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,14/Aug/12 23:28,16/Apr/19 09:32,14/Jul/23 05:52,16/Aug/12 22:27,1.2.0 beta 1,,,Legacy/Tools,,,0,cqlsh,,,,,,"When a builtin Cassandra type was being used for data in previous versions of Cassandra, only the short name was sent: ""UTF8Type"", ""TimeUUIDType"", etc. Starting with 1.2, as of CASSANDRA-4453, the full class names are sent.

Cqlsh doesn't know how to handle this, and is currently treating all data as if it were an unknown type. This goes as far as to cause an exception when the type is actually a number, because the driver deserializes it right, and then cqlsh tries to use it as a string.

Here for googlage:

{noformat}
AttributeError: 'int' object has no attribute 'replace'
{noformat}

Fixeries are in order.",,kirktrue,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/12 23:39;thepaul;4546.patch.txt;https://issues.apache.org/jira/secure/attachment/12540977/4546.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256234,,,Thu Aug 16 22:27:35 UTC 2012,,,,,,,,,,"0|i0gwyn:",96770,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"14/Aug/12 23:39;thepaul;Fix attached. Also available on the 4546 branch on my github- this version tagged pending/4546:

https://github.com/thepaul/cassandra/tree/4546;;;","16/Aug/12 22:10;kirktrue;+1 on the first part of the patch.

The third change in the patch _appears_ unrelated to me. Please clarify for my own edification.

Thanks.;;;","16/Aug/12 22:17;thepaul;It's not directly related. Just a minor problem with error reporting that came up while I was testing this.;;;","16/Aug/12 22:27;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should emit number of sstables in each level from JMX,CASSANDRA-4537,12603335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,kohlisankalp,kohlisankalp,13/Aug/12 21:32,16/Apr/19 09:32,14/Jul/23 05:52,11/Sep/12 22:24,1.2.0 beta 1,,,,,,0,compaction,leveled,,,,,"We should add methods to this Mbean org.apache.cassandra.db.ColumnFamilyStoreMBean

These metrics will be helpful to see how sstables are distributed in different levels and how they move to higher level with time. 
Currently we can see this by looking at the json file but with JMX, we can monitor the historic values over time using any monitoring tool.  ",,cscetbon,hudson,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,,,,,,43200,43200,,0%,43200,43200,,,,,,,,,,,,,,,,,,,,"10/Sep/12 20:09;yukim;4537-v2.txt;https://issues.apache.org/jira/secure/attachment/12544519/4537-v2.txt","07/Sep/12 20:57;yukim;4537.txt;https://issues.apache.org/jira/secure/attachment/12544289/4537.txt",,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256226,,,Tue Mar 12 16:11:11 UTC 2013,,,,,,,,,,"0|i0gwv3:",96754,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Aug/12 02:08;brandon.williams;I would go one further and say it should be available via nodetool compactionstats.  I can't count the number of times I've had to paste a python script that prints level counts.;;;","07/Sep/12 20:57;yukim;Attaching patch that adds SSTableCountPerLevel to ColulmnFamilyStoreMBean, which returns number of SSTable for each level in _int[]_ and prints it with nodetool cfstats.

Output looks like below:
{code}
...
    Column Family: Standard1
    SSTable count: 19
    SSTables in each level: [0, 8, 11, 0, 0, 0, 0, 0]
    Space used (live): 100923462
    Space used (total): 101012612
...
{code}

compactionstats displays info only when compaction is happening, so I chose cfstats to show info.

cfstats output is long, but let CASSANDRA-4191 handle it.;;;","07/Sep/12 21:11;jbellis;Idea: show number of sstables / desired max, when we exceed the desired threshold?  This would give you a quick eyeball of ""here's where compaction is behind.""

SSTables in each level: [43/0, 8, 102/100, 123, 0, 0, 0, 0]
;;;","10/Sep/12 20:09;yukim;v2 attached. It displays max threshold of level when exceeded.;;;","11/Sep/12 19:31;jbellis;+1, lgtm

nit: getLevelSize call is a little confusing since it handles i > generations.length, inlining to generations[i].size might be more clear;;;","11/Sep/12 22:24;yukim;Committed with above nit fixed.;;;","11/Sep/12 23:18;hudson;Integrated in Cassandra #2090 (See [https://builds.apache.org/job/Cassandra/2090/])
    Add SSTable count per level to cfstats patch by yukim; reviewed by jbellis for CASSANDRA-4537 (Revision c64d975cdf9eebddb04801573035a7272f779fed)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
* src/java/org/apache/cassandra/tools/NodeCmd.java
* src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java
* CHANGES.txt
* src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java
;;;","07/Mar/13 15:57;cscetbon;Whenever I try to access this attribute I get an empty string in version 1.2.2 I was expecting that even if I have only stables in level 0 I would get an array like [x,0,0,0,..];;;","07/Mar/13 16:09;yukim;[~cscetbon] Do you mean SSTableCountPerLevel attribute in ColumnFamilyStoreMBean? It should return int[] even if there are only level 0s or no sstables at all when ColumnFamily is configured to use leveled compaction. When you use size tiered compaction, then it returns null.;;;","12/Mar/13 16:11;cscetbon;why is this information not available when we are using size tiered compaction ? It would be interesting to know how much different similar-sized stables exist.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
old-style mapred interface doesn't set key limit correctly,CASSANDRA-4534,12603250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,13/Aug/12 14:54,16/Apr/19 09:32,14/Jul/23 05:52,14/Aug/12 21:02,1.0.12,1.1.4,,,,,0,,,,,,,"{{next(ByteBuffer key, SortedMap<ByteBuffer, IColumn> value)}} calls clear/put/rewind, but not flip or limit.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/12 14:59;jbellis;4534.txt;https://issues.apache.org/jira/secure/attachment/12540666/4534.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256224,,,Tue Aug 14 21:02:38 UTC 2012,,,,,,,,,,"0|i0gwtz:",96749,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Aug/12 14:59;jbellis;fix attached;;;","13/Aug/12 15:01;brandon.williams;+1;;;","14/Aug/12 21:02;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multithreaded cache saving can skip caches,CASSANDRA-4533,12603184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,hanzhu,hanzhu,13/Aug/12 06:40,16/Apr/19 09:32,14/Jul/23 05:52,22/Aug/12 14:01,1.1.5,,,,,,0,,,,,,,"Cassandra flushes the key and row cache to disk periodically. It also uses a atomic flag in flushInProgress to enforce single cache writer at any time.

However, the cache saving task could be submitted to CompactionManager concurrently, as long as the number of worker thread in CompactionManager is larger than 1. 

Due to the effect of above atomic flag, only one cache will be written out to disk. Other writer are cancelled when the flag is true.

I observe the situation in Cassandra 1.0. If nothing is changed, the problem should remain in Cassandra 1.1, either.",,hanzhu,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/12 20:33;yukim;4533-1.1.txt;https://issues.apache.org/jira/secure/attachment/12541641/4533-1.1.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256223,,,Wed Aug 22 14:01:47 UTC 2012,,,,,,,,,,"0|i0gwtj:",96747,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Aug/12 20:15;jbellis;Looks like we should switch to a ConcurrentSet like we did in Memtable.meteringInProgress.;;;","15/Aug/12 06:15;hanzhu;Is it possible to fix it also in 1.0?;;;","15/Aug/12 13:50;jbellis;No, it's not worth risking regressions over in 1.0.x. 

The good news is that 1.1.4+ look pretty stable.;;;","15/Aug/12 15:19;hanzhu;Looks like we have to maintain our own fork, as we can not keep up with the upgrade cycle... ;;;","20/Aug/12 20:33;yukim;Attaching patch against 1.1 branch.
Caches are grobal since 1.1, so I used CacheType as key for flushInProgerss concurrent set.;;;","21/Aug/12 20:08;jbellis;Hmm, I don't think this quite works because it still means we can skip saving cache for CF X when CF Y is being flushed.

I think the problem this code is trying to solve, over a basic executor + queue, is multiple tasks for X getting queued up while (say) compaction is sucking a lot of i/o, then firing off those cache-save tasks for X faster than the defined saving period when it speeds up.

I guess we could make it a Pair<CF, CacheType>?

TBH this is probably premature optimization, if your cache period is so frequent that multiple queued tasks is a problem, then you should just fix that.  I'd be okay with just ripping this out.  Alternatively, we could have the task check to see if the last-saved cache is older than M minutes before overwriting it, similar to how normal background compaction submissions are a no-op if it turns out there's nothing to do by the time we execute the task.;;;","22/Aug/12 01:05;yukim;bq. Hmm, I don't think this quite works because it still means we can skip saving cache for CF X when CF Y is being flushed.

In my understanding, since 1.1, C* stores key and row caches globally, those are saved at once for every CF for each cache type.
AutoSavingCache$Writer writes all CF for certain CacheType in one execution.;;;","22/Aug/12 02:24;jbellis;You're right, my mistake.  +1;;;","22/Aug/12 14:01;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when trying to select a slice from a composite table,CASSANDRA-4532,12603138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,basu76,basu76,12/Aug/12 19:41,16/Apr/19 09:32,14/Jul/23 05:52,06/Sep/12 14:57,1.1.5,,,Legacy/CQL,,,1,cql,cql3,Slice,,,,"I posted this question on StackOverflow, because i need a solution. 

Created a table with :

{noformat}
create table compositetest(m_id ascii,i_id int,l_id ascii,body ascii, PRIMARY KEY(m_id,i_id,l_id));
{noformat}

wanted to slice the results returned, so did something like below, not sure if its the right way. The first one returns data perfectly as expected, second one to get the next 3 columns closes the transport of my cqlsh

{noformat}
cqlsh:testkeyspace1> select * from compositetest where i_id<=3 limit 3;
 m_id | i_id | l_id | body
------+------+------+------
   m1 |    1 |   l1 |   b1
   m1 |    2 |   l2 |   b2
   m2 |    1 |   l1 |   b1

cqlsh:testkeyspace1> Was trying to write something for slice range.

TSocket read 0 bytes
{noformat}

Is there a way to achieve what I am doing here, it would be good if some meaning ful error is sent back, instead of cqlsh closing the transport.

On the server side I see the following error.

{noformat}
ERROR [Thrift:3] 2012-08-12 15:15:24,414 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.SelectStatement$Restriction.setBound(SelectStatement.java:1277)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1151)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:1001)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:215)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{noformat}

With ThriftClient I get :

{noformat}
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql_query(Cassandra.java:1402)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_cql_query(Cassandra.java:1388)
{noformat}",Cassandra 1.1.3 (2 nodes) on a single host - mac osx,ardot,basu76,omid,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/12 13:41;slebresne;4532.txt;https://issues.apache.org/jira/secure/attachment/12544045/4532.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256222,,,Thu Sep 06 16:48:06 UTC 2012,,,,,,,,,,"0|i0gwt3:",96745,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Aug/12 19:15;thepaul;Please give the exact queries you used which reproduce the problem. ""{{Was trying to write something for slice range.}}"" isn't a valid query.;;;","29/Aug/12 13:44;basu76;I was trying to get a slice range, like you could do in thrift.

table defn :

create tables schedules(status ascii, timecreated bigint, key ascii, nil ascii, PRIMARY KEY(status,timecreated,key));

for the same time there can be a lot of entries.

Lets suppose there are 50 entries that match where timecreated is < Ln

1st query : select * from schedules where timecreated <= <Ln> limit 10;


2nd Query : select * from schedules where timecreated>=L10 AND key=K10 and timecreated<Ln.

In CQL terms this is a wrong query I know, basically not sure how to represent Between in CQL


In Hector I would do, get slice range limiting 10 first time, 

for the next query (until no more are returned) I would use the time returned by last query and key returned by last query as the start range. This is in production and works perfectly fine;;;","29/Aug/12 13:54;jbellis;can you test against trunk?;;;","29/Aug/12 14:21;basu76;No luck. See the last query closed the socket. I took the latest from git and compiled

Here are the steps to reproduce :

cqlsh:testkeyspace1> create table compositetest(status ascii,ctime bigint,key ascii,nil ascii,PRIMARY KEY(status,ctime,key));

cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key1','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345678,'key2','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key3','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key4','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345679,'key5','');
cqlsh:testkeyspace1> insert into compositetest(status,ctime,key,nil) VALUES ('C',12345680,'key6','');
cqlsh:testkeyspace1> select * from compositetest;
 status | ctime    | key  | nil
--------+----------+------+-----
      C | 12345678 | key1 |    
      C | 12345678 | key2 |    
      C | 12345679 | key3 |    
      C | 12345679 | key4 |    
      C | 12345679 | key5 |    
      C | 12345680 | key6 |    

1st query of slice :

cqlsh:testkeyspace1> select * from compositetest where ctime<=12345680 limit 3;
 status | ctime    | key  | nil
--------+----------+------+------
      C | 12345678 | key1 |     
      C | 12345678 | key2 |     
      C | 12345679 | key3 | null

Second Query : I want to get values where first one left off (Yes you could do this with hector) [Try 1]

cqlsh:testkeyspace1> select * from compositetest where ctime>=12345679 and key='key3' and ctime<=12345680 limit 3;
Bad Request: PRIMARY KEY part key cannot be restricted (preceding part ctime is either not restricted or by a non-EQ relation) [Try 2]
cqlsh:testkeyspace1> select * from compositetest where ctime=12345679 and key='key3' and ctime<=12345680 limit 3;
TSocket read 0 bytes
cqlsh:testkeyspace1>;;;","06/Sep/12 13:41;slebresne;Attaching (trivial) patch to fix the validation issue (the request is invalid because it mixes an equal and inequal on the same ctime column).

For the record, a valid request that does the equivalent of a thrift sliceRange would be:
{noformat}
SELECT * FROM compositetest WHERE ctime>=12345679 and ctime<=12345680 limit 3;
{noformat}
If you further want to restrict the result to those column where key='key3', you'd have to do it client side (but that would be the same with thrift).
;;;","06/Sep/12 13:53;jbellis;+1;;;","06/Sep/12 14:57;slebresne;Committed, thanks;;;","06/Sep/12 15:59;basu76;Sylvain,
Thanks for the fix, May be I should have opened the ticket differently. My main issue is not that the connection on cqlsh was getting closed (because of NPE) or exception being thrown in thrift client. To adopt CQL3 mainstream slice with paging is need, I saw some other ticket, where this proposal was there. 

Main reason I opened the ticket was for not being able to slice (rather no syntax support for that, to continue where the last query left off). This is a piece of code that was written using Hector and my assumption is the filtering was done on server side. We are using this in production. I modified attributes (to get rid of proprietary stuff, so may be broken, but should give an idea of what I was trying to do with CQL3)

            long cStartTime = 0L;
            long startTime = System.nanoTime();
            boolean fetchNextBatch = true;
            int totalKeysFetched = 0;
            String lastKeyFetched = null;
            long cEndTime = <Some Time in Millis>;
            while(fetchNextBatch) {
                fetchNextBatch = false;
                int pageSize = 3; //Just for demonstration
                SliceQuery<Object,DynamicComposite,String> sliceQuery =   HFactory.createSliceQuery(keyspace,<KEY SERIALIZER>,DynamicCompositeSerializer.get(),StringSerializer.get());
                sliceQuery.setKey(""C"");
                sliceQuery.setColumnFamily(""<SOME CF NAME>"");
                DynamicComposite startRange = new DynamicComposite();
  
                startRange.addComponent(cStartTime,LongSerializer.get()); //For the first fetch - this will be 0L
                startRange.addComponent(lastKeyFetched,StringSerializer.get()); // this will be null for first fetch

                DynamicComposite endRange = new DynamicComposite();
                endRange.addComponent(new Long(cEndTime), LongSerializer.get(), ""LongType"", AbstractComposite.ComponentEquality.LESS_THAN_EQUAL);
                //Add another config if we need columnPageSize
                sliceQuery.setRange(startRange,endRange,false,pageSize);

                long start = System.nanoTime();
                QueryResult<ColumnSlice<DynamicComposite, String>> result = sliceQuery.execute();
                float t =  (float)((System.nanoTime() - start)/1000000);
                System.out.println(""TIME FOR QUERY :"" + t  + "" MILLI SECONDS"");

                ColumnSlice<DynamicComposite, String> cs = result.get();
                List<HColumn<DynamicComposite,String>> compositeList = cs.getColumns();

                for(int i =0;i<compositeList.size();i++) {
                    HColumn<DynamicComposite, String> col = compositeList.get(i);
                    cStartTime = col.getName().get(0,LongSerializer.get()); //This will be the cTime for the start range of the next query
                    lastKeyFetched = col.getName().get(1,StringSerializer.get()); //In the start range for the next query, this key will be used.
                    keyTimeMap.put(lastKeyFetched,scheduleStartTime);
                    totalKeysFetched ++;
                }

                //Process Fetched Data 

                fetchNextBatch = compositeList.size() == pageSize; // If the number of records retrieved is equal to the page size, then there are probably more records left 
            };;;","06/Sep/12 16:48;slebresne;@basanth Slices are definitely supported in CQL3, the query I gave you earlier involves a slice for instance. In a fair amount of cases, paging within a Cassandra row is also pretty simple to do in CQL3 too. It is true that in some cases it's not completely as easy as in thrift. For those last cases, CASSANDRA-4415 should provide a solution.

If you questions on how to translate some piece thrift code to CQL3, you're definitively welcome to ask them but please use the user mailing list for such inquiries.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage allow_deletes doesn't work in Hadoop cluster,CASSANDRA-4499,12602099,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mrtidy,mrtidy,07/Aug/12 23:21,16/Apr/19 09:32,14/Jul/23 05:52,21/Sep/12 20:15,1.1.6,,,,,,0,hadoop,pig,,,,,"When using CassandraStorage in a Pig script that runs in a Hadoop cluster, the environment variable configuration option for allow_deletes doesn't work.  I'd like to see allow_deletes added as a URL parameter.

For example, I'd like to do
STORE storable_events INTO 'cassandra://drlcrs/event?allow_deletes=true' USING org.apache.cassandra.hadoop.pig.CassandraStorage();",,jeromatron,mrtidy,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/12 12:15;brandon.williams;4499-v2.txt;https://issues.apache.org/jira/secure/attachment/12545717/4499-v2.txt","07/Aug/12 23:38;mrtidy;trunk-4499.txt;https://issues.apache.org/jira/secure/attachment/12539741/trunk-4499.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256202,,,Fri Sep 21 20:15:47 UTC 2012,,,,,,,,,,"0|i0gwhr:",96694,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"07/Aug/12 23:38;mrtidy;No unit tests available yet - I'm not sure how to test the URL or failure cases by looking at the existing tests.  I'll have to dig a little more.;;;","07/Aug/12 23:46;mrtidy;I've submitted trunk-4499.txt which is missing unit tests but is what I'm looking for so that allow_deletes can be set without using environment variables.;;;","07/Aug/12 23:54;brandon.williams;bq. No unit tests available yet - I'm not sure how to test the URL or failure cases by looking at the existing tests. I'll have to dig a little more.

We should be able to just insert empty tuples in the tests in examples/pig.

While we're at it, maybe we should move the wide row flag to the url too.;;;","19/Sep/12 12:15;brandon.williams;v2 also allows setting widerow and secondary index mode with url params.;;;","21/Sep/12 10:58;xedin;+1;;;","21/Sep/12 20:15;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CQL pseudo-maps to real map syntax,CASSANDRA-4497,12602033,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,07/Aug/12 16:23,16/Apr/19 09:32,14/Jul/23 05:52,04/Sep/12 17:43,1.2.0 beta 1,,,,,,0,cql3,,,,,,"- compression_parameters
- replication_parameters (combine strategy + options like we did compression)
- anything else?",,slebresne,thepaul,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/12 16:26;slebresne;4497-v2.txt;https://issues.apache.org/jira/secure/attachment/12543699/4497-v2.txt","04/Sep/12 11:46;xedin;CASSANDRA-4497.patch;https://issues.apache.org/jira/secure/attachment/12543657/CASSANDRA-4497.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256200,,,Tue Sep 04 17:43:32 UTC 2012,,,,,,,,,,"0|i0gwgv:",96690,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"07/Aug/12 16:36;slebresne;There's also compaction (same than replication, we want to merge the class and the options).

I would also be in favor of using shortish name, something like:
{noformat}
CREATE KEYSPACE foo WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }
{noformat}
(we could even shorten replication_factor to factor I guess, though I'm personally good like that.

Talking of the replication strategy, I'd be in favor of having some default for the replication strategy as we do when using the cassandra-cli, as this make it easier to do quick tests.;;;","07/Aug/12 18:35;jbellis;bq. I would also be in favor of using shortish name


+1, although we already accept short classnames everywhere (?) we accept long ones, for built-in classes.  Unsure why docs continue to use fully-qualified names.

bq. I'd be in favor of having some default for the replication strategy as we do when using the cassandra-cli

I'd like to, but I think the reasoning in https://issues.apache.org/jira/browse/CASSANDRA-2529?focusedCommentId=13022925&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13022925 still applies.;;;","07/Aug/12 19:16;slebresne;bq. although we already accept short classnames everywhere

Yes, but I was mostly taking about using 'replication' instead of 'replication_parameters'. That's a detail of course.

bq. I'd like to, but I think the reasoning in https://issues.apache.org/jira/browse/CASSANDRA-2529?focusedCommentId=13022925&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13022925 still applies

That's true. Maybe we could at least make it so that ""replication = { 'replication_factor' : 1 }"" infers SimpleStrategy? ;;;","07/Aug/12 19:33;jbellis;At that point, I don't think the characters saved justify the ambiguity over what's happening.;;;","07/Aug/12 20:20;slebresne;I guess it's not so much about saving keystrokes (though as someone that does a lot a quick, one-shot CQL tests, I wouldn't mind the shortcut), but the fact that the create keyspace statement is the first statement beginner will see. At that time, it feels like even having the strategy class means you need to either say ""ignore that for now"" (which is always bad and frustrate people) or have to explain that it's because there is multiple strategies and yada yada yada. And given we don't allow 'replication_factor' for NTS, I don't think this adds much ambiguity. Not a huge deal though, but I do feel this minimal create keyspace statement is not minimal enough.;;;","07/Aug/12 20:31;jbellis;I agree with your problem statement, but I haven't seen a sufficiently attractive solution yet. :);;;","10/Aug/12 12:01;slebresne;Actually, while we're at using collections in system table, it would be to also
use collections for the columns_aliases, key_aliases and the index_options for
in ColumnDefinition. For the column_aliases and key_aliases, it's indirectly
visible to user if they query System.schema_columnfamilies, and since that will
be the new way to do a describe, it's worth removing the use of json.;;;","30/Aug/12 16:07;xedin;{replication, compression, compaction}_parameters are made to be set using <key> = { <k> : <v>, ... } syntax.;;;","31/Aug/12 17:55;thepaul;From the patch, it looks like we won't be supporting the old syntax anymore. This, and the option name changes, is going to make all our docs more complicated, and also non-trivially complicate the smarter clients (in particular, cqlsh, opscenter, etc). Also, it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2"", which it seems like ought to be implied by the use of the map syntax. The whole thing doesn't seem much cleaner than before. What's the main motivation?;;;","31/Aug/12 23:51;jbellis;How is it more complicated to document that replication_parameters is a map like any other, than replication_parameters has its own one-off syntax that only exists for historical reasons?  I don't buy it.

bq. Also, it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2"", which it seems like ought to be implied by the use of the map syntax

Agreed, if we can fix this easily we should.

bq. What's the main motivation

Two motivations:
- reduce the special case weirdnesses exposed to users, as above
- reduce special case weirdnesses in our internal implementation (which is also exposed to ""power users"" in the system KS).  so much more pleasant to ""select replication_options from schema_keyspaces where keyspace_name = 'foo'"" and get a Map back.;;;","01/Sep/12 03:12;thepaul;bq. How is it more complicated to document that replication_parameters is a map like any other, than replication_parameters has its own one-off syntax that only exists for historical reasons? I don't buy it.

It's not, but it is more complicated when we need to say ""if you're using CQL 3 with Cassandra 1.0 or 1.1, then you need to do this. If you're using CQL 3 with 1.2 or higher, then you need to do this totally different thing. If you're using CQL 2, then you do something else."" I know, ideally we would have fully separate documentation sets and fully separate cqlsh versions per major Cassandra version, but people don't seem to be very good at referring only to the right one.

bq. reduce the special case weirdnesses exposed to users, as above

I'm admittedly thick, but which weirdnesses are reduced here?

bq. reduce special case weirdnesses in our internal implementation (which is also exposed to ""power users"" in the system KS). so much more pleasant to ""select replication_options from schema_keyspaces where keyspace_name = 'foo'"" and get a Map back.

I agree with that 100%, but this patch doesn't do that. It only changes the dedicated syntax for setting keyspace and columnfamily options. I'd much prefer getting real maps inside the system.schema* tables and leaving the cql syntax alone when we can. It has been a real moving target, and swiping out the ""first command you do"" from under everyone's feet seems like it might be discouraging.;;;","01/Sep/12 15:16;xedin;Ok, let make it clear what do me want to be the out come of the ticket: I thought that we want to change CQL3 parameter behavior from pseudo-maps to real maps for commands like ""create keyspace/table"" and ""alter table""? I'm not a fan of keeping old syntax because that could lead to confusing mixing problems where users would use old syntax in one place and new one later in the same statement... If we also want to modify how schema handles maps or namely change json to native maps that would require a separate ticket because of work front which would have to be done. ;;;","03/Sep/12 01:49;jbellis;bq.  I'm not a fan of keeping old syntax because that could lead to confusing mixing problems where users would use old syntax in one place and new one later in the same statement

+1

bq. If we also want to modify how schema handles maps or namely change json to native maps that would require a separate ticket 

Fine, CASSANDRA-4603.;;;","04/Sep/12 11:46;xedin;rebased patch after changes in CASSANDRA-4597;;;","04/Sep/12 16:26;slebresne;
Since we're breaking from earlier version of CQL3 anyway (and we've been clear that CQL3 has been beta so far), I would be in favor of shortening some options names (some were in Pavel's patch already but not all) as said previous.

I'm attaching a v2 based on Pavel's patch that does that. So that the exact syntax is:
{noformat}
CREATE KEYSPACE foo WITH replication = { 'class' : 'SimpleStrategy',  'replication_factor' : 1 }
CREATE TABLE .... WITH compression = { ... }
                   AND compaction = { 'class' : 'SizeTieredCompactionStrategy', 'min_threshold' : 2, 'max_threshold' : 4 }
{noformat}

v2 also slightly update the grammar to better reuse code between create and alter table, and it fixes a potential NPE in the grammar convertMap method (due to antlr being a smart-ass). It also throw a ConfiguarionException in the case were compaction options are given but not the 'class' itself (since that feels more consistent with the semantic of having a map literal).

bq. it looks like there isn't any way to change one option and leave the other intact (like ""WITH replication['replication_factor'] = 2""

I agree, but that's a bit of a pain to add (in particular things like ""WITH replication = { 'class' : 'SimpleStrategy} AND replication['replication_factor'] = 2"" are a bit annoying to handle) so I would suggest leaving that to later.
;;;","04/Sep/12 16:51;xedin;+1, with nit: I think we would be should make naming consistent and change COMPACTION_OPTIONS to COMPACTION_PARAMETERS.;;;","04/Sep/12 17:43;slebresne;Committed with nits fixed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool can't work at all !,CASSANDRA-4494,12601526,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,thepaul,sun74533,sun74533,06/Aug/12 03:28,16/Apr/19 09:32,14/Jul/23 05:52,13/Aug/12 14:29,1.1.4,,,Tool/nodetool,,,0,,,,,,,"1. download cassandra 1.1.3 , then start with ""{cassandra}/bin/cassandra -pf &""
2. cd to bin , call nodetool as ""./nodetool -h localhost ring""
3. console returned : failed to connect to 'localhost:7199' : connection refused


BUT ,

at the same centos , all was ok before (1.1.2) .


PS: 

cassandra-cli/cqlsh works well (1.1.3)


--------------

update:

even if add the following in cassandra-env.sh , connection refused as well :
JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=10.10.30.11
",centos 64bit,jdilloyd,jeromatron,pvelas,slebresne,sun74533,thepaul,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/12 22:04;thepaul;4494-sh-error.patch.txt;https://issues.apache.org/jira/secure/attachment/12539721/4494-sh-error.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256198,,,Mon Aug 13 14:29:05 UTC 2012,,,,,,,,,,"0|i0gwfz:",96686,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,"06/Aug/12 10:50;jdilloyd;Confirmed.

1.1.2 nodetool works perfectly
1.1.3 nodetool throws back 'connection refused' error

setting the JVM_OPTS hostname in cassandra-env.sh has no effect.

connection via 'localhost', server hostname, or server IP address all return the same result.

connection to remote 1.1.3 host from 1.1.3 host fails.
Connection to remote 1.1.2 host from 1.1.3 host works fine.
Connection from 1.1.2 host to remote 1.1.3 host fails.

restarting the node has no effect.
replacing nodetool with file from previous version (1.1.2) has no effect.



CentOS release 5.8 (Final) x86_64
;;;","06/Aug/12 13:26;sun74533;there should be some official tips to tell us is it a bug or not ? if a bug , it should be fixed immediately , or show some resolutions officially . ;;;","06/Aug/12 14:12;slebresne;I don't have a centos to test, but I do get the same on macosx (but not on ubuntu for instance). More precisely, the error is:
{noformat}
./bin/../conf/cassandra-env.sh: line 180: syntax error near unexpected token `['
./bin/../conf/cassandra-env.sh: line 180: `startswith () [ ""${1#$2}"" != ""$1"" ]'
{noformat}

The reason this breaks JMX is just that the JMX port is not set at that point of the script and the script likely exit on the error. It seems there is some shell incompatibility again.

While we fix that, a workaround could be to edit the cassandra-env.sh to comment the incriminated line.;;;","07/Aug/12 16:06;pvelas;Version 1.1.3 don't listen on port 7199 .
Previous version 1.1.2 is OK. ;;;","07/Aug/12 18:28;jbellis;""bash bin/nodetool"" should work.;;;","07/Aug/12 18:53;jdilloyd;*EDIT*

Correction: that doesn't work either.;;;","07/Aug/12 19:03;pvelas;When I try to use cassandra-env.sh from 1.1.2 version and restarted cassandra process then I can use nodetool without problems.
I can now see service sucessfully binded to port 7199.

{code}
[root@cass1 conf]# netstat -tunap | grep 7199
tcp        0      0 0.0.0.0:7199                0.0.0.0:*                   LISTEN      10904/java 
{code};;;","07/Aug/12 22:04;thepaul;Guess POSIX sh doesn't allow omission of curly braces around a function body of a single command. Dash fails at being a plain posix sh again :(;;;","08/Aug/12 03:48;sun74533;edit the cassandra-env.sh on line 180 :

original : startswith ()  [ ""${1#$2}"" != ""$1"" ] 

change to : startswith () { [ ""${1#$2}"" != ""$1"" ] ; }

then restart cassandra , every thing goes OK !;;;","13/Aug/12 14:29;urandom;Sorry, I committed an (identical )fix to trunk for this on Friday when Jira was down.  That's been applied to 1.1.  Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix update of CF comparator (including adding new collections),CASSANDRA-4493,12601508,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Aug/12 19:22,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/12 09:59,1.2.0 beta 1,,,,,,0,,,,,,,"Updating the comparator of a column family (which is authorized if the new comparator is ""compatible"" with the old one, and that includes adding a new component to a CompositeType, or adding a new collection to a CQL3 table) doesn't completely work. The problem is that even if we change the comparator in CFMetada, the old comparator will still be aliased by the current memtable. This means updates (that expect the new comparator) will fail until a new memtable is created.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/12 19:29;slebresne;0001-Flush-after-CFStore-reload.txt;https://issues.apache.org/jira/secure/attachment/12539217/0001-Flush-after-CFStore-reload.txt","05/Aug/12 19:29;slebresne;0002-Fix-ALTER-logic.txt;https://issues.apache.org/jira/secure/attachment/12539218/0002-Fix-ALTER-logic.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256197,,,Fri Sep 07 09:59:09 UTC 2012,,,,,,,,,,"0|i0gwfr:",96685,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Aug/12 19:29;slebresne;Attached a fix (first patch) that adds a flush to the reload of CFS (since it's called when the CFMetadata a the cf is modified).

I note that this affect at least 1.1 and maybe some of 1.0 (I don't remember exactly when we started allowing to change the comparator). However, there was little reason to use that feature so far. This change however in 1.2 since adding a new collection actually change the comparator.

I'm also attaching a 2nd patch that fixes a bit of broken logic in ALTER TABLE for collections. This is not directly related expect that I discovered both bug at the same time, and that second one is fairly simple so I figured this might not warrant a specific ticket.;;;","06/Sep/12 20:11;jbellis;+1;;;","07/Sep/12 09:59;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintsColumnFamily compactions hang when using multithreaded compaction,CASSANDRA-4492,12601486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,carlyeks,alienth,alienth,05/Aug/12 04:12,16/Apr/19 09:32,14/Jul/23 05:52,18/Dec/12 21:15,1.1.9,1.2.0,,,,,0,,,,,,,"Running into an issue on a 6 node ring running 1.0.11 where HintsColumnFamily compactions often hang indefinitely when using multithreaded compaction. Nothing of note in the logs. In some cases, the compaction hangs before a tmp sstable is even created.

I've wiped out every hints sstable and restarted several times. The issue always comes back rather quickly and predictably. The compactions sometimes complete if the hint sstables are very small. Disabling multithreaded compaction stops this issue from occurring.

Compactions of all other CFs seem to work just fine.

This ring was upgraded from 1.0.7. I didn't keep any hints from the upgrade.

I should note that the ring gets a huge amount of writes, and as a result the HintedHandoff rows get be quite wide. I didn't see any large-row compaction notices when the compaction was hanging (perhaps the bug was triggered by incremental compaction?). After disabling multithreaded compaction, several of the rows that were successfully compacted were over 1GB.

Here is the output I see from compactionstats where a compaction has hung. The 'bytes compacted' column never changes.

{code}
pending tasks: 1
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction          systemHintsColumnFamily          268082       464784758     0.06%
{code}


The hung thread stack is as follows: (full jstack attached, as well)

{code}
""CompactionExecutor:37"" daemon prio=10 tid=0x00000000063df800 nid=0x49d9 waiting on condition [0x00007eb8c6ffa000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000050f2e0e58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:329)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Deserializer.computeNext(ParallelCompactionIterable.java:281)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:147)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:126)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:100)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:101)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:88)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:141)
        at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:395)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}",,alienth,carlyeks,christianmovi,mkjellman,rcoli,stefan.fleiter@web.de,tjake,tvachon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4673,,,,,,,,,,,"19/Dec/12 00:26;carlyeks;4492-patch2.patch;https://issues.apache.org/jira/secure/attachment/12561612/4492-patch2.patch","18/Dec/12 23:49;carlyeks;4492-patch2.patch;https://issues.apache.org/jira/secure/attachment/12561608/4492-patch2.patch","18/Dec/12 19:21;carlyeks;4492.patch;https://issues.apache.org/jira/secure/attachment/12561544/4492.patch","05/Aug/12 04:59;alienth;jstack.txt;https://issues.apache.org/jira/secure/attachment/12539176/jstack.txt",,,,,,,,,,,,,,4.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249088,,,Wed Dec 19 06:10:53 UTC 2012,,,,,,,,,,"0|i0a4on:",57032,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Aug/12 14:02;brandon.williams;MT compaction is unfortunately a) not highly used and b) known to be suspect to issues.  The best course of action right now is to just not use it. :(;;;","16/Oct/12 18:01;tjake;I hit this recently as well.

Though mine I was able to reproduce.  If you call truncate while a compaction is currently going on it hangs both the truncate and the parallel compaction iterator.  

;;;","17/Oct/12 00:45;mkjellman;hit this as well on both 1.1.5 and 1.1.6. Turned multhreaded compaction off and HintsColumnFamily finished very quickly. Nothing in the logs of interest and not reproducible.;;;","29/Nov/12 21:16;tvachon;This actually is severe.  Since they hang, it blocks all schema changes.;;;","11/Dec/12 16:48;jbellis;If you can give us a set of sstables that reliably cause the hang (snapshot before compaction option should be useful here) then we can troubleshoot.  Nothing is obviously wrong from just eyeballing things.;;;","11/Dec/12 17:03;jbellis;bq.  If you call truncate while a compaction is currently going on it hangs both the truncate and the parallel compaction iterator. 

Truncate took some big changes for 1.2 (CASSANDRA-4096) so I doubt this is still the case.  (If it is, I'm not sure it's related to compaction alone causing the hang.);;;","11/Dec/12 19:26;tjake;Using 1.2 with LeveledCompaction I have not hit any deadlocks;;;","11/Dec/12 19:34;tvachon;[~jbellis] no I can't.  We turned off MT compaction and they have been compacted away.;;;","11/Dec/12 19:50;jbellis;Does your workload also involve truncates [~tvachon] [~mkjellman] [~alienth]?;;;","11/Dec/12 20:01;tvachon;No, update heavy though ;;;","11/Dec/12 20:05;mkjellman;no, minimal (if any) deletes.;;;","18/Dec/12 15:34;tjake;This is still happening:

Looking at the code it seems there are two places where HintedHandoffManager calls a user defined compact() for all sstables.

Multithreaded compaction would allow this to race since I see no check to avoid multiple calls to user defined compaction for the same sstables

;;;","18/Dec/12 15:35;tvachon;{quote}Looking at the code it seems there are two places where HintedHandoffManager calls a user defined compact() for all sstable{quote}

Well that would explain why everytime I restart and I get hints, I get every sstable compacted;;;","18/Dec/12 15:40;jbellis;Jake: I think you're missing how markCompacting works in submitUserDefined;;;","18/Dec/12 15:48;tjake;Ah, I needed to scroll down more.  Well I can confirm this is sill happening in 1.2 but only for hints CF  ;;;","18/Dec/12 19:06;carlyeks;Here is what I think is happening (with help from Jake):

For simplicity, we are compacting two SSTables, sstable-1 and sstable-2.

- Read a row from sstable-1, which is empty
  - We don't call close on the LazilyCompactedRow because only Write or Update calls close; this means that the NotifyingSSTableIdentityIterator never signals the condition.
- Read a row from sstable-2, which is not empty
- Call hasNext() in CompactionTask's runWith() on the iterator for sstable-1, which was never triggered

This means that we are now deadlocked in ParallelCompactionIterable.Deserializer's as waiting for the signal and waiting for another row. We never return because we have no way of closing sstable-1's NotifyingSSTableIdentityIterator, and moving to the next row.;;;","18/Dec/12 19:22;carlyeks;This patch add the Closeable interface to AbstractCompactedRow, and calls row.close() if the row is empty.;;;","18/Dec/12 21:15;jbellis;I think you've nailed it.  Committed to 1.1 and 1.2.0.  Nice work!;;;","18/Dec/12 23:49;carlyeks;One more place that wasn't handling the close on empty - was hitting this case as well.

I've only included the additional patch.;;;","19/Dec/12 02:48;jbellis;fixed formatting and committed;;;","19/Dec/12 06:10;mkjellman;[~carlyeks] thanks for your work on this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh needs to use system.local instead of system.Versions,CASSANDRA-4491,12601482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,05/Aug/12 03:27,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 07:32,1.2.0 beta 1,,,Legacy/Tools,,,0,cqlsh,,,,,,"Apparently the system.Versions table was removed as part of CASSANDRA-4018. cqlsh in 1.2 should use system.local preferentially, and fall back on system.Versions to keep backwards compatibility with older c*.

Also changed in 4018: all the system.schema_* CFs now use columns named ""keyspace_name"", ""columnfamily_name"", and ""column_name"" instead of ""keyspace"", ""columnfamily"", and ""column"".

While we're at it, let's update the cql3 table structure parsing and the DESCRIBE command for the recent Cassandra changes too.",,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256196,,,Tue Sep 18 07:32:24 UTC 2012,,,,,,,,,,"0|i0gwf3:",96682,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"12/Sep/12 19:12;thepaul;Changes made in my github clone:

https://github.com/thepaul/cassandra/tree/4491

Current version tagged pending/4491.;;;","13/Sep/12 18:32;brandon.williams;Committed.;;;","18/Sep/12 04:42;thepaul;Sorry, my fault for not being clear: this change involved multiple commits. Three, in this case. The last one got in right.;;;","18/Sep/12 07:32;slebresne;I've committed the first 2 patches then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove uses of SchemaDisagreementException,CASSANDRA-4487,12601434,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,03/Aug/12 22:21,16/Apr/19 09:32,14/Jul/23 05:52,16/Aug/12 22:56,1.2.0 beta 1,,,Legacy/CQL,,,0,,,,,,,"Since we can handle concurrent schema changes now, there's no need to validateSchemaAgreement before modification now.",,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/12 13:47;xedin;0001-code-changes.patch;https://issues.apache.org/jira/secure/attachment/12539572/0001-code-changes.patch","07/Aug/12 13:47;xedin;0002-re-generated-thrift.patch;https://issues.apache.org/jira/secure/attachment/12539573/0002-re-generated-thrift.patch","15/Aug/12 11:08;xedin;CASSANDRA-4487-v2.patch;https://issues.apache.org/jira/secure/attachment/12541035/CASSANDRA-4487-v2.patch",,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256194,,,Thu Aug 16 22:56:30 UTC 2012,,,,,,,,,,"0|i0gwdr:",96676,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Aug/12 20:32;jbellis;We actually want to leave SDE in the thrift interface (but remove throws declarations), so clients can support old C* versions easily.;;;","07/Aug/12 20:48;slebresne;We might also want to keep the validateSchemaIsSettled for the thrift side, otherwise we're changing the semantic of the operation. I mean, it's fine for CQL3, but if we really want to be conservative on the thrift side .... (typically waiting on validateSchemaIsSettled means that you know you can start inserting data in your newly created CF). ;;;","07/Aug/12 21:15;xedin;So we want to limit the scope of the to CQL3?;;;","07/Aug/12 21:32;jbellis;bq. typically waiting on validateSchemaIsSettled means that you know you can start inserting data in your newly created CF

I don't understand -- we don't do that on the normal request path even in thrift, just for schema mutations.  ;;;","07/Aug/12 21:38;xedin;The thing I don't understand is how it's different for CQL3 comparing to Thrift/CQL, in either case if we to remove the check users won't be able to run any operations safely except to the localhost...;;;","07/Aug/12 21:54;slebresne;bq. we don't do that on the normal request path even in thrift, just for schema mutations

Hum, I could have swear we did that for thrift but apparently not. However we do do it for CQL (that is both CQL2 and for CQL3). Maybe that just a relic of old time, I don't know.

bq. how it's different for CQL3 comparing to Thrift/CQL, in either case if we to remove the check users won't be able to run any operations safely except to the localhost

I think the waiting for schema settled shouldn't be part of the request path, but should be done separately by clients if they want to. But if we were doing the wait in thrift, I would have suggested to keep it just for the sake of not changing the behavior. But anyway, if we don't do it, I guess there is no problem.;;;","07/Aug/12 22:04;xedin;bq. Hum, I could have swear we did that for thrift but apparently not. However we do do it for CQL (that is both CQL2 and for CQL3). Maybe that just a relic of old time, I don't know.

It should be, because it only makes sense to check it for schema migrations.

bq. I think the waiting for schema settled shouldn't be part of the request path, but should be done separately by clients if they want to. But if we were doing the wait in thrift, I would have suggested to keep it just for the sake of not changing the behavior. But anyway, if we don't do it, I guess there is no problem.

I'm not sure if that hurts that much to leave such validation on schema migration patch, definitely would avoid problems related to change contention or errors on the read/write patch with CL > ONE if things weren't settled properly. But I think if we are to remove checks we probably should remove all of them to keep behavior consistent.

;;;","15/Aug/12 11:08;xedin;Attaching v2 which removes all schema agreement validation but leafs SDE in place for Thrift API backward compatibility.;;;","16/Aug/12 16:45;jbellis;LGTM;;;","16/Aug/12 22:56;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Drain causes incorrect error messages: ""Stream took more than 24H to complete; skipping""",CASSANDRA-4484,12601241,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,canadianveggie,canadianveggie,canadianveggie,02/Aug/12 16:41,16/Apr/19 09:32,14/Jul/23 05:52,02/Aug/12 17:57,1.1.4,,,,,,0,,,,,,,"After calling drain on a node, there are a bunch of incorrect error messages in the cassandra log file: ""Stream took more than 24H to complete; skipping"".

The problem is in MessagingService.waitForStreaming. It is logging an error if ThreadPoolExecutor.awaitTermination returns true, but if a timeout happens it returns false. See http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#awaitTermination%28long,%20java.util.concurrent.TimeUnit%29",,canadianveggie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,canadianveggie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256191,,,Thu Aug 02 17:57:01 UTC 2012,,,,,,,,,,"0|i0gwcf:",96670,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Aug/12 16:43;canadianveggie;Fixed with this pull request: https://github.com/apache/cassandra/pull/12;;;","02/Aug/12 17:56;jbellis;(Introduced in CASSANDRA-3679 for 1.1.0.);;;","02/Aug/12 17:57;jbellis;patch lgtm, committed.  thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMX attribute setters not consistent with cassandra.yaml,CASSANDRA-4479,12601104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cmerrill,edong,edong,01/Aug/12 18:51,16/Apr/19 09:32,14/Jul/23 05:52,23/Aug/12 16:01,1.2.0 beta 1,,,,,,0,,,,,,,"If a setting is configurable both via cassandra.yaml and JMX, the two should be consistent. If that doesn't hold, then the JMX setter can't be trusted. 

Here I present the example of phi_convict_threshold.

I'm trying to set phi_convict_threshold via JMX, which sets FailureDetector.phiConvictThreshold_, but this doesn't update Config.phi_convict_threshold, which gets its value from cassandra.yaml when starting up.

Some places, such as FailureDetector.interpret(InetAddress), use FailureDetector.phiConvictThreshold_; others, such as AntiEntropyService.line 813 in cassandra-1.1.2, use Config.phi_convict_threshold:
{code}
            // We want a higher confidence in the failure detection than usual because failing a repair wrongly has a high cost.
            if (phi < 2 * DatabaseDescriptor.getPhiConvictThreshold())
                return;
{code}

where DatabaseDescriptor.getPhiConvictThreshold() returns Conf.phi_convict_threshold.


So, it looks like there are cases where a value is stored in multiple places, and setting the value via JMX doesn't set all of them. I'd say there should only be a single place where a configuration parameter is stored, and that single field:
* should read in the value from cassandra.yaml, optionally falling back to a sane default
* should be the field that the JMX attribute reads and sets, and
* any place that needs the current global setting should get it from that field. However, there could be cases where you read in a global value at the start of a task and keep that value locally until the end of the task.

Also, anything settable via JMX should be volatile or set via a synchronized setter, or else according to the Java memory model other threads may be stuck with the old setting.


So, I'm requesting the following:
* Setting up guidelines for how to expose a configuration parameter both via cassandra.yaml and JMX, based on what I've mentioned above
* Going through the list of configuration parameters and fixing any that don't match those guidelines


I'd also recommend logging any changes to configuration parameters.",,appodictic,cmerrill,edong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/12 17:59;cmerrill;trunk-4479.txt;https://issues.apache.org/jira/secure/attachment/12541616/trunk-4479.txt",,,,,,,,,,,,,,,,,1.0,cmerrill,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256189,,,Thu Aug 23 16:01:41 UTC 2012,,,,,,,,,,"0|i0gwav:",96663,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"01/Aug/12 19:02;edong;For comparison, rpc_timeout_in_ms is settable through JMX via StorageProxy[MBean], but StorageProxy doesn't have its own rpc_timeout_in_ms field, it calls DatabaseDescriptor.setRpcTimeout(Long), which sets Conf.rpc_timeout_in_ms. However, Conf.rpc_timeout_in_ms is neither volatile nor set via a synchronized method, which is still bad.;;;","01/Aug/12 23:48;edong;A quick-and-dirty laundry list of MBean setters:

{noformat}
$ find . -name '*MBean.java' -exec grep 'void set' {} +
./src/java/org/apache/cassandra/concurrent/JMXConfigurableThreadPoolExecutorMBean.java:    void setCorePoolSize(int n);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setMinimumCompactionThreshold(int threshold);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setMaximumCompactionThreshold(int threshold);
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setCompactionStrategyClass(String className) throws ConfigurationException;
./src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java:    public void setCompressionParameters(Map<String,String> opts) throws ConfigurationException;
./src/java/org/apache/cassandra/gms/FailureDetectorMBean.java:    public void setPhiConvictThreshold(int phi);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setRowCacheSavePeriodInSeconds(int rcspis);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setKeyCacheSavePeriodInSeconds(int kcspis);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setRowCacheCapacityInMB(long capacity);
./src/java/org/apache/cassandra/service/CacheServiceMBean.java:    public void setKeyCacheCapacityInMB(long capacity);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setHintedHandoffEnabled(boolean b);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setMaxHintWindow(int ms);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setMaxHintsInProgress(int qs);
./src/java/org/apache/cassandra/service/StorageProxyMBean.java:    public void setRpcTimeout(Long timeoutInMillis);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setLog4jLevel(String classQualifier, String level);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setStreamThroughputMbPerSec(int value);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setCompactionThroughputMbPerSec(int value);
./src/java/org/apache/cassandra/service/StorageServiceMBean.java:    public void setIncrementalBackupsEnabled(boolean value);
{noformat}

DatabaseDescriptor setters; according to [ArchitectureInternals|http://wiki.apache.org/cassandra/ArchitectureInternals], all node configuration parameters should be in here:
{noformat}
$ grep 'void set' src/java/org/apache/cassandra/config/DatabaseDescriptor.java 
    public static void setPartitioner(IPartitioner newPartitioner)
    public static void setEndpointSnitch(IEndpointSnitch eps)
    public static void setRpcTimeout(Long timeOutInMillis)
    public static void setInMemoryCompactionLimit(int sizeInMB)
    public static void setCompactionThroughputMbPerSec(int value)
    public static void setStreamThroughputOutboundMegabitsPerSec(int value)
    public static void setBroadcastAddress(InetAddress broadcastAdd)
    public static void setDynamicUpdateInterval(Integer dynamicUpdateInterval)
    public static void setDynamicResetInterval(Integer dynamicResetInterval)
    public static void setDynamicBadnessThreshold(Double dynamicBadnessThreshold)
    public static void setIncrementalBackupsEnabled(boolean value)
{noformat};;;","03/Aug/12 21:09;cmerrill;I'm looking into this and will submit a patch for consideration.;;;","23/Aug/12 16:01;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3: defining more than one pk should be invalid,CASSANDRA-4477,12600923,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,31/Jul/12 22:17,16/Apr/19 09:32,14/Jul/23 05:52,15/Aug/12 10:13,1.2.0 beta 1,,,,,,0,cql3,,,,,,"dtests caught this on trunk:

{noformat}
  File ""/var/lib/buildbot/cassandra-dtest/cql_tests.py"", line 277, in create_invalid_test
    assert_invalid(cursor, ""CREATE TABLE test (key1 text PRIMARY KEY, key2 text PRIMARY KEY)"")
  File ""/var/lib/buildbot/cassandra-dtest/assertions.py"", line 31, in assert_invalid
    assert False, ""Expecting query to be invalid""
AssertionError: Expecting query to be invalid
{noformat}",,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/12 07:31;slebresne;4477.txt;https://issues.apache.org/jira/secure/attachment/12538653/4477.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256188,,,Wed Aug 15 10:13:47 UTC 2012,,,,,,,,,,"0|i0gwaf:",96661,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"01/Aug/12 07:31;slebresne;Attached patch to fix the issue and correctly refuse invalid PK definitions.;;;","15/Aug/12 10:13;xedin;+1, Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using 'key' as primary key throws exception when using CQL2,CASSANDRA-4475,12600870,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,yukim,yukim,31/Jul/12 15:01,16/Apr/19 09:32,14/Jul/23 05:52,31/Jul/12 15:37,1.2.0 beta 1,,,,,,0,cql,,,,,,"When I run following CQL on trunk, throws exception (only in CQL2). This statement used to work and I think something is broken after CASSANDRA-4179.

{code}
CREATE TABLE Standard1 (key ascii PRIMARY KEY, c0 ascii);
{code}

Exception is:

{code}
ERROR [Thrift:1] 2012-07-31 09:54:02,585 CustomTThreadPoolServer.java (line 202) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:166)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:123)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
        at org.apache.cassandra.db.marshal.UTF8Type.getString(UTF8Type.java:49)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:45)
        at org.apache.cassandra.cql3.CFDefinition.getKeyId(CFDefinition.java:167)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:81)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1382)
        at org.apache.cassandra.config.CFMetaData.keyAliases(CFMetaData.java:235)
        at org.apache.cassandra.cql.CreateColumnFamilyStatement.getCFMetaData(CreateColumnFamilyStatement.java:170)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:692)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:846)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/12 15:09;slebresne;4445.txt;https://issues.apache.org/jira/secure/attachment/12538560/4445.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256187,,,Tue Jul 31 15:37:06 UTC 2012,,,,,,,,,,"0|i0gw9r:",96658,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"31/Jul/12 15:09;slebresne;Right, forgot that CQL2 can have a null keyAlias. Trivial fix attached.;;;","31/Jul/12 15:31;yukim;+1;;;","31/Jul/12 15:37;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh COPY FROM without explicit column names is broken,CASSANDRA-4470,12600572,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,thepaul,thepaul,28/Jul/12 20:32,16/Apr/19 09:32,14/Jul/23 05:52,29/Jul/12 16:50,1.1.3,,,Legacy/Tools,,,0,cqlsh,,,,,,"When trying to do a COPY FROM command in cqlsh without an explicit list of column names, an error results:

{noformat}
cqlsh:a> copy blah from stdin;
[Use \. on a line by itself to end input]
[copy] a,b,c

object of type 'NoneType' has no len()
{noformat}

Broken by the fix for CASSANDRA-4434.",,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/12 20:35;thepaul;4470.txt;https://issues.apache.org/jira/secure/attachment/12538253/4470.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256182,,,Sun Jul 29 16:50:31 UTC 2012,,,,,,,,,,"0|i0gw7j:",96648,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Jul/12 20:35;thepaul;Fixed in attached patch, and in my github, in the 4470 branch. Current version tagged pending/4470:

https://github.com/thepaul/cassandra/tree/4470;;;","29/Jul/12 16:50;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix online help in cqlsh for COPY commands,CASSANDRA-4469,12600569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,28/Jul/12 18:55,16/Apr/19 09:32,14/Jul/23 05:52,28/Jul/12 21:03,1.1.3,,,Legacy/Tools,,,0,cqlsh,,,,,,"the HELP COPY output from cqlsh says:

{noformat}
COPY [cqlsh only]

  Imports CSV data into a Cassandra table.

COPY <table_name> [ ( column [, ...] ) ]
     FROM ( '<filename>' | STDIN )
     [ WITH <option>='value' [AND ...] ];
COPY <table_name> [ ( column [, ...] ) ]
     TO ( '<filename>' | STDOUT )
     [ WITH <option>='value' [AND ...] ];
{noformat}

It's confusing cause COPY is now for both export and import, since CASSANDRA-4434.",,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/12 18:56;thepaul;4469.txt;https://issues.apache.org/jira/secure/attachment/12538251/4469.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256181,,,Sat Jul 28 21:03:51 UTC 2012,,,,,,,,,,"0|i0gw73:",96646,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"28/Jul/12 18:56;thepaul;fix the docstring/help text for COPY;;;","28/Jul/12 21:03;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader hadoop integration fails with ghost keys,CASSANDRA-4466,12600449,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ndrummond@rim.com,ndrummond@rim.com,27/Jul/12 10:25,16/Apr/19 09:32,14/Jul/23 05:52,31/Jul/12 15:52,1.0.12,1.1.4,,,,,0,,,,,,,"When running hadoop-cassandra jobs with range queries over ghost keys, the ColumnFamilyRecordReader throws an exception if the last key in a slice_range query is a ghost key. 

This seems to be related to changes made in CASSANDRA-2855 to prevent ghost keys appearing in a hadoop map. 

The call stack trace is attached.

I made a one-line change to ColumnFamilyRecordReader.java, which seems to solve this issue for us.
",,jeromatron,ndrummond@rim.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/12 10:31;ndrummond@rim.com;4466-stack;https://issues.apache.org/jira/secure/attachment/12538146/4466-stack","27/Jul/12 10:29;ndrummond@rim.com;4466-v0.patch;https://issues.apache.org/jira/secure/attachment/12538145/4466-v0.patch","30/Jul/12 17:46;jbellis;4466.txt;https://issues.apache.org/jira/secure/attachment/12538378/4466.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256180,,,Tue Jul 31 15:52:40 UTC 2012,,,,,,,,,,"0|i0gw6f:",96643,,ndrummond@rim.com,,ndrummond@rim.com,Low,,,,,,,,,,,,,,,,,"27/Jul/12 10:31;ndrummond@rim.com;exception call stack;;;","27/Jul/12 19:21;jbellis;setting rows=null is the signal to stop looping over that inputsplit, so that's not going to work very well in the general case.;;;","30/Jul/12 08:35;ndrummond@rim.com;Ah ok, thanks for the input. What's the best approach to solving this ?;;;","30/Jul/12 17:46;jbellis;I think we can just add back the last-seen row.  Patch attached.;;;","31/Jul/12 10:32;ndrummond@rim.com;I can confirm the patch works for our dataset. Thanks for this.;;;","31/Jul/12 14:29;brandon.williams;+1;;;","31/Jul/12 15:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Index fails to be created on all nodes in cluster, restart resolves",CASSANDRA-4465,12600253,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,heffergm,heffergm,26/Jul/12 01:15,16/Apr/19 09:32,14/Jul/23 05:52,24/Mar/13 06:27,,,,,,,2,,,,,,,"On a production cluster, under load, creating an index on a column resulted in the index being successfully created on 4 of 21 nodes. All nodes received the schema agreement and were in concert. There were no errors logged on any of the nodes that failed to build the index.

A rolling restart of the cluster resulted in the nodes which had previously failed to build the index doing so when coming back up from a restart.","21 node cluster, Ubuntu Linux 11.10 in a virtualized environment, Apache cassandra community release, binary distribution",cnlwsu,heffergm,mvaldez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256179,,,Sun Mar 24 06:27:32 UTC 2013,,,,,,,,,,"0|i0gw67:",96642,,,,,Low,,,,,,,,,,,,,,,,,"01/Aug/12 16:26;cnlwsu;I have also seen this on a 7 node (Ubuntu 10.04 in Rackspace, and 10.04 in ESX VM) cluster.  In fact this has happened to me often.  I have seen it when I have created a new column family (wait for schema agreement) then add the index (via solr schema.xml http post) but I cannot reproduce on single node cluster.;;;","24/Mar/13 06:27;jbellis;Should be fixed by new schema propagation code in 1.1 and 1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes Don't Restart: Assertion Error on Serializing Cache provider,CASSANDRA-4463,12600072,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,arya,arya,24/Jul/12 21:47,16/Apr/19 09:32,14/Jul/23 05:52,18/Apr/13 18:32,1.1.11,1.2.5,,,,,2,,,,,,,"I stopped Cassandra on one of our 1.1.2 nodes and I couldn't start it any more. System.log didn't have much useful info but output.log had this:

java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:43)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:39)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:116)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:174)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:45)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:430)
        at org.apache.cassandra.db.Table.open(Table.java:124)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        at com.netflix.priam.cassandra.NFThinCassandraDaemon.init(NFThinCassandraDaemon.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3


Deleting the stuff in saved_caches folder fixed the problem.","Ubuntu 12.04 Precise
Cassandra 1.1.5
Oracle Java 6",arya,dbrosius,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 15:01;jbellis;4463.txt;https://issues.apache.org/jira/secure/attachment/12577809/4463.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,248197,,,Wed Apr 10 02:26:38 UTC 2013,,,,,,,,,,"0|i09men:",54063,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,"13/Oct/12 00:43;arya;This is still happening in 1.1.5. I just had to perform some maintenance and restart service and I got this again:

java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:43)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:39)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:116)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:174)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:45)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:446)
        at org.apache.cassandra.db.Table.open(Table.java:124)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:206)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:290)
        at com.netflix.priam.cassandra.NFThinCassandraDaemon.init(NFThinCassandraDaemon.java:41)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Cannot load daemon
Service exit with a return value of 3;;;","09/Apr/13 15:01;jbellis;Null CFs are not allowed in the row cache.  The normal read path checks this as follows:

{code}
.           ColumnFamily data = getTopLevelColumns(QueryFilter.getIdentityFilter(filter.key, new QueryPath(columnFamily)),
                                                   Integer.MIN_VALUE,
                                                   true);
            if (sentinelSuccess && data != null)
                CacheService.instance.rowCache.replace(key, sentinel, data);
{code}

We need to check this during cache population at startup as well, since the cache is only saved intermittently -- a row that contained data when the cache was saved, and subsequently invalidated, will read as null on the following startup.;;;","10/Apr/13 00:55;dbrosius;extraneous semi at end of line.

do you want to increment cachedRowsRead if the data is null?

otherwise +1;;;","10/Apr/13 02:26;jbellis;I'm not 100% sure why we track cachedRowsRead to be honest.  If I were to guess it might be some measure of how much work cache loading cost us at startup, in which case I think we do want to increment.  (This is more explicit in 1.2, where we log the time taken if rows read > 0.)

Fixed EOL and committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgradesstables strips active data from sstables,CASSANDRA-4462,12600069,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mheffner,mheffner,24/Jul/12 21:23,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/12 08:46,1.0.11,1.1.3,,,,,0,,,,,,,"From the discussion here: http://mail-archives.apache.org/mod_mbox/cassandra-user/201207.mbox/%3CCAOac0GCtyDqS6ocuHOuQqre4re5wKj3o-ZpUZGkGsjCHzDVbTA%40mail.gmail.com%3E

We are trying to migrate a 0.8.8 cluster to 1.1.2 by migrating the sstables from the 0.8.8 ring to a parallel 1.1.2 ring. However, every time we run the `nodetool upgradesstables` step we find it removes active data from our CFs -- leading to lost data in our application.

The steps we took were:


1. Bring up a 1.1.2 ring in the same AZ/data center configuration with
tokens matching the corresponding nodes in the 0.8.8 ring.
2. Create the same keyspace on 1.1.2.
3. Create each CF in the keyspace on 1.1.2.
4. Flush each node of the 0.8.8 ring.
5. Rsync each non-compacted sstable from 0.8.8 to the corresponding node in
1.1.2.
6. Move each 0.8.8 sstable into the 1.1.2 directory structure by renaming the file to the  /cassandra/data/<keyspace>/<cf>/<keyspace>-<cf>... format. For example, for the keyspace ""Metrics"" and CF ""epochs_60"" we get:
""cassandra/data/Metrics/epochs_60/Metrics-epochs_60-g-941-Data.db"".
7. On each 1.1.2 node run `nodetool -h localhost refresh Metrics <CF>` for each CF in the keyspace. We notice that storage load jumps accordingly.
8. On each 1.1.2 node run `nodetool -h localhost upgradesstables`.

Afterwards we would test the validity of the data by comparing it with data from the original 0.8.8 ring. After an upgradesstables command the data was always incorrect.

With further testing we found that we could successfully use scrub to convert our sstables without data loss. However, any invocation of upgradesstables causes active data to be culled from the sstables:

 INFO [CompactionExecutor:4] 2012-07-24 04:27:36,837 CompactionTask.java (line 109) Compacting [SSTableReader(path='/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-51-Data.db')]
 INFO [CompactionExecutor:4] 2012-07-24 04:27:51,090 CompactionTask.java (line 221) Compacted to [/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-58-Data.db,].  60,449,155 to 2,578,102 (~4% of original) bytes for 4,002 keys at 0.172562MB/s.  Time: 14,248ms.

These are the steps we've tried:

WORKS		refresh -> scrub
WORKS		refresh -> scrub -> major compaction
WORKS		refresh -> scrub -> cleanup
WORKS		refresh -> scrub -> repair

FAILS		refresh -> upgradesstables
FAILS		refresh -> scrub -> upgradesstables
FAILS		refresh -> scrub -> repair -> upgradesstables
FAILS		refresh -> scrub -> major compaction -> upgradesstables

We have fewer than 143 million row keys in the CFs we're testing and none
of the *-Filter.db files are > 10MB, so I don't believe this is our
problem: https://issues.apache.org/jira/browse/CASSANDRA-3820

The keyspace is defined as:

Keyspace: Metrics:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:3]

And the column family that we tested with is defined as:

    ColumnFamily: metrics_900
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.1
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor

All rows have a TTL of 30 days and a gc_grace=0 so it's possible that a small number of older columns would be removed during a compaction/scrub/upgradesstables step. However, the majority should still be kept as their TTL's have not expired yet.",Ubuntu 11.04 64-bit,mheffner,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 07:49;slebresne;4462.txt;https://issues.apache.org/jira/secure/attachment/12537812/4462.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256178,,,Fri Jul 27 08:46:01 UTC 2012,,,,,,,,,,"0|i0gw5b:",96638,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jul/12 07:49;slebresne;There is indeed an unfortunate typo in the code of upgradesstables that makes it purge every tombstone instead of purging none. Since we upgrade one sstable at a time, purging tombstone is a bug that will resurrect data.

Attached patch to fix (which also fix a 2nd occurrence of the same problem but that 2nd one was introduce by CASSANDRA-4456 so wasn't released yet).;;;","25/Jul/12 12:11;mheffner;Would that typo lead to the behavior we saw where non-tombstoned data would be removed from the sstable during an upgradesstables run?;;;","26/Jul/12 16:59;slebresne;Best way to make sure would be to test with the patch :)

But since you have ttl, I would say that yes, there is a good chance that's related.;;;","26/Jul/12 17:18;mheffner;Fair enough, though we are not quite equipped to test patch sets yet so it might take awhile.;;;","26/Jul/12 17:19;jbellis;+1 on the basic fix, but I'm not really convinced that GC_ALL and NO_GC are improvements since we don't really get any extra typesafety from it.  (Maybe switching to an enum for ALL, NONE, and CURRENT_TIME enum would be okay though?  But that's out of scope here.);;;","27/Jul/12 08:46;slebresne;bq. I'm not really convinced that GC_ALL and NO_GC are improvements since we don't really get any extra typesafety from it

The goal is not really to gain type safety, but to avoid having to take the few seconds to think about which of MIN_VALUE or MAX_VALUE means to not GC anything or conversely to GC everything. At least I have a tendency of screwing that up as showed by this issue, and I think having more explicit and readable constant names might avoid that kind of mistake in the future (again, at least for me). Anyway, I've committed as is, but if you really don't like, feel free to ninja edit it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SystemTable.setBootstrapState always sets bootstrap state to true,CASSANDRA-4460,12599929,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,dbrosius@apache.org,dbrosius@apache.org,24/Jul/12 00:45,16/Apr/19 09:32,14/Jul/23 05:52,26/Jul/12 01:38,,,,,,,0,,,,,,,"    public static void setBootstrapState(BootstrapState state)
    {
        String req = ""INSERT INTO system.%s (key, bootstrapped) VALUES ('%s', '%b')"";
        processInternal(String.format(req, LOCAL_CF, LOCAL_KEY, getBootstrapState()));
        forceBlockingFlush(LOCAL_CF);
    }

Third parameter %b is set from getBootstrapState() which returns an enum, thus %b collapses to null/non null checks. This would seem then to always set it to true.",,brandon.williams,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 21:52;brandon.williams;4460.txt;https://issues.apache.org/jira/secure/attachment/12537905/4460.txt","25/Jul/12 00:38;dbrosius@apache.org;use_bootstrap_enum_strings.txt;https://issues.apache.org/jira/secure/attachment/12537782/use_bootstrap_enum_strings.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256176,,,Thu Jul 26 01:38:15 UTC 2012,,,,,,,,,,"0|i0gw4f:",96634,,dbrosius@apache.org,,dbrosius@apache.org,Low,,,,,,,,,,,,,,,,,"24/Jul/12 20:43;brandon.williams;Actually, it breaks, and buildbot knows it:

{noformat}

ERROR [main] 2012-07-24 14:31:19,156 CassandraDaemon.java (line 335) Exception encountered during startup
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:520)
        at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:340)
        at org.apache.cassandra.utils.ByteBufferUtil.toInt(ByteBufferUtil.java:414)
        at org.apache.cassandra.cql.jdbc.JdbcInt32.compose(JdbcInt32.java:94)
        at org.apache.cassandra.db.marshal.Int32Type.compose(Int32Type.java:33)
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:104)
        at org.apache.cassandra.db.SystemTable.getBootstrapState(SystemTable.java:375)
        at org.apache.cassandra.db.SystemTable.setBootstrapState(SystemTable.java:391)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:691)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:476)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:367)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:228)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
{noformat}

Obviously we should be using %i instead.;;;","25/Jul/12 00:26;dbrosius@apache.org;switch the systemtable.bootstrap field to be text and hold BootstrapState.name() so that the schema is more readable and easier to mutate in the future, if needed.

I'm not sure how upgrades from old schema generally is handled, that part still needs to be added to the patch.;;;","25/Jul/12 17:51;brandon.williams;While doing this would certainly prevent anyone from wanting to bikeshed the names in the future, it doesn't seem any less fragile, nor worth the complexity of migrating the system table schema.;;;","25/Jul/12 20:44;dbrosius@apache.org;it has to be migrated anyway. The table is defined to be boolean currently. So either you migrate to integer or string. I chose string as 0, 1, 2 mean nothing to me.;;;","25/Jul/12 21:52;brandon.williams;bq. it has to be migrated anyway. The table is defined to be boolean currently

Actually, it's only boolean in trunk and we don't need to keep trunk compatible with itself.  It turns out upgradeSystemData() is handling the 1.1 to trunk transition for us already.

bq.  I chose string as 0, 1, 2 mean nothing to me.

Fair enough.  Attaching a new version which takes all of this into account, and fixes a bug in setBootstrapState using getBootstrapState instead of the state passed to it.;;;","26/Jul/12 01:25;dbrosius@apache.org;LGTM;;;","26/Jul/12 01:38;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig driver casts ints as bytearray,CASSANDRA-4459,12599883,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,cdaw,cdaw,23/Jul/12 18:51,16/Apr/19 09:32,14/Jul/23 05:52,25/Jul/12 17:09,1.1.3,,,,,,0,,,,,,,"we seem to be auto-mapping C* int columns to bytearray in Pig, and farther down I can't seem to find a way to cast that to int and do an average.  

{code}

grunt> cassandra_users = LOAD 'cassandra://cqldb/users' USING CassandraStorage();
grunt> dump cassandra_users;
(bobhatter,(act,22),(fname,bob),(gender,m),(highSchool,Cal High),(lname,hatter),(sat,500),(state,CA),{})
(alicesmith,(act,27),(fname,alice),(gender,f),(highSchool,Tuscon High),(lname,smith),(sat,650),(state,AZ),{})
 
// notice sat and act columns are bytearray values 
grunt> describe cassandra_users;
cassandra_users: {key: chararray,act: (name: chararray,value: bytearray),fname: (name: chararray,value: chararray),
gender: (name: chararray,value: chararray),highSchool: (name: chararray,value: chararray),lname: (name: chararray,value: chararray),
sat: (name: chararray,value: bytearray),state: (name: chararray,value: chararray),columns: {(name: chararray,value: chararray)}}

grunt> users_by_state = GROUP cassandra_users BY state;
grunt> dump users_by_state;
((state,AX),{(aoakley,(highSchool,Phoenix High),(lname,Oakley),state,(act,22),(sat,500),(gender,m),(fname,Anne),{})})
((state,AZ),{(gjames,(highSchool,Tuscon High),(lname,James),state,(act,24),(sat,650),(gender,f),(fname,Geronomo),{})})
((state,CA),{(philton,(highSchool,Beverly High),(lname,Hilton),state,(act,37),(sat,220),(gender,m),(fname,Paris),{}),(jbrown,(highSchool,Cal High),(lname,Brown),state,(act,20),(sat,700),(gender,m),(fname,Jerry),{})})

// Error - use explicit cast
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG(cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:15:04,361 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1045: Could not infer the matching function for org.apache.pig.builtin.AVG as multiple or none of them fit. Please use an explicit cast.

// Unable to cast as int
grunt> user_avg = FOREACH users_by_state GENERATE cassandra_users.state, AVG((int)cassandra_users.sat);
grunt> dump user_avg;
2012-07-22 17:07:39,217 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1052: Cannot cast bag with schema sat: bag({name: chararray,value: bytearray}) to int
{code}

*Seed data in CQL*
{code}
CREATE KEYSPACE cqldb with 
  strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=3;	


use cqldb;

CREATE COLUMNFAMILY users (
  KEY text PRIMARY KEY, 
  fname text, lname text, gender varchar, 
  act int, sat int, highSchool text, state varchar);

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (gjames, Geronomo, James, f, 24, 650, 'Tuscon High', 'AZ');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (aoakley, Anne, Oakley, m , 22, 500, 'Phoenix High', 'AX');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (jbrown, Jerry, Brown, m , 20, 700, 'Cal High', 'CA');

insert into users (KEY, fname, lname, gender, act, sat, highSchool, state)
values (philton, Paris, Hilton, m , 37, 220, 'Beverly High', 'CA');

select * from users;
{code}",C* 1.1.2 embedded in DSE,cdaw,jeromatron,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 16:16;brandon.williams;4459-v2.txt;https://issues.apache.org/jira/secure/attachment/12537848/4459-v2.txt","23/Jul/12 19:19;brandon.williams;4459.txt;https://issues.apache.org/jira/secure/attachment/12537596/4459.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256175,,,Wed Jul 25 17:09:56 UTC 2012,,,,,,,,,,"0|i0gw3z:",96632,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"23/Jul/12 19:19;brandon.williams;We actually have tests in examples/pig/test that explicitly cover this, but we populate that data with the cli which uses IntegerType, but cql uses Int32Type.  Trivial patch to cast both to pig's integer type.;;;","23/Jul/12 19:40;jbellis;casting IntegerType to pig's [32bit] int sounds broken to me.  shouldn't we fix the population script to use int32 as well?;;;","24/Jul/12 11:41;xedin;I agree with Jonathan on this, IntegerType could be larger than int32.;;;","24/Jul/12 16:04;jeromatron;fwiw - see https://issues.apache.org/jira/browse/PIG-2764 for the addition of BigInteger and BigDecimal as built-in pig data types.  Also, I'm not sure how much of an issue it is for users to use pig ints for now because I don't know how many users know that the cassandra IntegerType is actually a BigInteger and not just a regular Integer.  That's not to say that it's not dangerous to try to put a BigInteger value into an Integer type.  It's just that I don't know if it's common knowledge that Cassandra uses a BigInteger underneath.;;;","24/Jul/12 16:10;brandon.williams;bq. casting IntegerType to pig's [32bit] int sounds broken to me.

I agree, but the conundrum we're in now is, I'm almost certain someone is relying on the current behavior to work and always using ints under 2**31, so changing it now would break things for them, and really the only danger is exceeding that limit, which I suspect people who create int columns never intend to do (or they'd make them longs.)

So, I propose that when pig has a BigInteger, we switch to that, allowing a smooth transition (unless you're exceeding 2**31 already, which to my knowledge no one is.);;;","24/Jul/12 16:12;jbellis;That's reasonable, but we should still use int32type for cli population :);;;","25/Jul/12 16:16;brandon.williams;Update with a comment explaining that IntegerType is wrong, but we're doing it anyway.  Also switched all the IntegerTypes to Int32Types in the tests, which pass.  I don't see any point in explicitly testing IntegerType as well until pig has a BigInteger.;;;","25/Jul/12 16:40;jbellis;+1;;;","25/Jul/12 16:43;xedin;+1;;;","25/Jul/12 17:09;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in ColumnFamilyStore.getOverlappingSSTables() during repair,CASSANDRA-4456,12599726,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mheffner,mheffner,21/Jul/12 14:07,16/Apr/19 09:32,14/Jul/23 05:52,23/Jul/12 19:36,1.1.3,,,,,,0,,,,,,,"We have hit the following exception on several nodes while running repairs across our 1.1.2 ring. We've observed it happen on either the node executing the repair or a participating replica in the repair operation. The result in either case is that the repair hangs.


ERROR [ValidationExecutor:9] 2012-07-21 01:54:03,019 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:9,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:874)
        at org.apache.cassandra.db.compaction.CompactionController.<init>(CompactionController.java:69)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:834)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:698)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:68)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:438)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


In building this ring we migrated sstables from an identical 0.8.8 ring by:

 1. Creating the schema on our new 1.1.2 ring.
 2. Rsyncing over sstables from 0.8.8 ring.
 3. Renaming the sstables to match the directory and file naming structure of 1.1.x.
 4. Ran nodetool refresh <keyspace> <cf> for each CF across each node.
 5. Ran nodetool upgradesstables for each CF across each node.

When those steps had completed, we began rolling repairs. Not all of the repair operations have hit the exception -- some of the repairs have completed successfully.
",Ubuntu 11.04 64-bit,mheffner,mtheroux2,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/12 16:16;slebresne;4456.txt;https://issues.apache.org/jira/secure/attachment/12537576/4456.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256172,,,Mon Jul 23 19:36:36 UTC 2012,,,,,,,,,,"0|i0gw2v:",96627,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Jul/12 19:15;mtheroux2;I just hit this problem myself today, on a single node in a six node cluster.  I was running nodetool repair, and it halted with this exception in the log.  I was monitoring the repair pretty closely.  A couple of observations:

1) It happened while compaction of the same column family was happening simultaneously
2) When I re-ran it, it worked.

Note: I am not a cassandra developer, but I looked at the code.  A highly uneducated guess is that an sstable was compacted and deleted while validation was expecting it to be there?  ;;;","21/Jul/12 19:15;mtheroux2;I am also on 1.1.2.;;;","23/Jul/12 14:46;jbellis;I think this was introduced by CASSANDRA-3721: getOverlappingSSTables assumes that the sstables we check for overlaps are part of the live set, but now we can validate over a snapshot instead.;;;","23/Jul/12 16:16;slebresne;Actually I think this can happen even when snapshots are not used since a sstable can finish to be compacted just between when we chose sstable for repair and when we create the CompactionController for the validation compaction. In particular, I wonder if Michael and Mike have used -snapshot for their compaction. Though it's true that repair on snapshot will make that way more likely to happen.

But actually I don't think we need to call getOverlappingSStables at all in the first place for repair, since this is used only to decide if we can purge but repair does not do purging. Attaching a simple patch to skip the call entirely.
;;;","23/Jul/12 16:29;jbellis;You need to wire VCC in to ValidationCompactionIterable, but otherwise +1.;;;","23/Jul/12 19:36;slebresne;Oups indeed. Committed with the thing wired up. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool fail to setcompactionthreshold,CASSANDRA-4455,12599710,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jasont,jasont,21/Jul/12 10:17,16/Apr/19 09:32,14/Jul/23 05:52,25/Jul/12 19:03,1.1.4,,,Tool/nodetool,,,0,,,,,,,"first change compaction threshold from 4/32 to 2/2
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 2
It successful
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
Exception in thread ""main"" java.lang.RuntimeException: The min_compaction_threshold cannot be larger than the max.
        at org.apache.cassandra.db.ColumnFamilyStore.setMinimumCompactionThreshold(ColumnFamilyStore.java:1697)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeSetter(MBeanIntrospector.java:238)
        at com.sun.jmx.mbeanserver.PerInterface.setAttribute(PerInterface.java:84)
        at com.sun.jmx.mbeanserver.MBeanSupport.setAttribute(MBeanSupport.java:240)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.setAttribute(DefaultMBeanServerInterceptor.java:762)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.setAttribute(JmxMBeanServer.java:699)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.setAttribute(RMIConnectionImpl.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

The tool first try to set min then max, so it failed, since orign max is smaller the new min.
The work around is:
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 2 32
/opt/dve/cassandra/bin/nodetool -h 127.0.0.1 -p 7199 setcompactionthreshold ks cf 4 32
",Cassandra 1.0.3,aleksey,jasont,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 16:06;xedin;CASSANDRA-4455-v2.patch;https://issues.apache.org/jira/secure/attachment/12537847/CASSANDRA-4455-v2.patch","25/Jul/12 13:45;aleksey;CASSANDRA-4455.patch;https://issues.apache.org/jira/secure/attachment/12537832/CASSANDRA-4455.patch",,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256171,,,Wed Jul 25 19:03:34 UTC 2012,,,,,,,,,,"0|i0gw2f:",96625,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"25/Jul/12 16:06;xedin;Attaching alternative version which adds setCompactionThresholds(int, int) method instead of doing branch checking for old min/max values in nodetool. What do you think, Aleksey?;;;","25/Jul/12 18:06;aleksey;I think mine is acceptable, but your version is preferable.
The only thing I don't like is the inconsistency in the argument names between the old setters and the new one - min, max, minCompationThreshold and maxCompationThreshold. The latter ones are redundant in the context - I'd just use 'threshold'.
But this is bikeshedding on my part, doesn't really matter.
Go with v2.;;;","25/Jul/12 19:03;xedin;Committed with names changed to {min,max}Thresholds.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove RangeKeySample from attributes in jmx,CASSANDRA-4452,12599522,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jendap,jendap,jendap,19/Jul/12 20:15,16/Apr/19 09:32,14/Jul/23 05:52,25/Jul/12 18:13,1.1.3,,,,,,0,jmx,,,,,,"RangeKeySample in org.apache.cassandra.db:type=StorageService MBean can be really huge (over 200MB in our case). That's a problem for monitoring tools as they're not build for that. Recommended and often used mx4j may be killer in this situation.

It would be good enough to make RangeKeySample ""operation"" instead of ""attribute"" in jmx. Looking at how MBeanServer.registerMBean() works we can do one of the following:
a) add some dummy parameter to getRangeKeySample
b) name it differently - not like getter (next time somebody will rename it back)
c) implement MXBean instead of MBean (a lot of work)

Any of those work. All of them are ""hacks"". Any better idea?



BTW: It's blocker for some installations. Our update to 1.1.2 caused downtime, downgrade back to 1.0.x, repairs, etc.",,jendap,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/12 23:33;jendap;cassandra-1.1.2-4452.txt;https://issues.apache.org/jira/secure/attachment/12537768/cassandra-1.1.2-4452.txt",,,,,,,,,,,,,,,,,1.0,jendap,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256168,,,Wed Jul 25 18:13:26 UTC 2012,,,,,,,,,,"0|i0gw13:",96619,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Jul/12 22:42;jbellis;I would be fine with simply renaming it.;;;","20/Jul/12 09:21;slebresne;Same here. Renaming it to be an operation with a comment to explain why it shouldn't be renamed back to an attribute.;;;","24/Jul/12 23:33;jendap;""Sample"" is also verb. We can rename it from getRangeKeySample() to sampleKeyRange(). Ok?;;;","25/Jul/12 18:13;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool drain sometimes doesn't mark commitlog fully flushed,CASSANDRA-4446,12599347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,rcoli,rcoli,18/Jul/12 21:30,16/Apr/19 09:32,14/Jul/23 05:52,15/Jan/13 15:14,1.2.1,,,Tool/nodetool,,,2,,,,,,,"I recently wiped a customer's QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. 

It appears to show the following :
1) Drain begins
2) Drain triggers flush
3) Flush triggers compaction
4) StorageService logs DRAINED message
5) compaction thread excepts
6) on restart, same CF creates a memtable
7) and then flushes it [1]

The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.

In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.

[1] Isn't commitlog replay not supposed to automatically trigger a flush in modern cassandra?","ubuntu 10.04 64bit
Linux HOSTNAME 2.6.32-345-ec2 #48-Ubuntu SMP Wed May 2 19:29:55 UTC 2012 x86_64 GNU/Linux
sun JVM
cassandra 1.0.10 installed from apache deb",arodrime,christianmovi,hsn,khahn,kmueller,mauzhang,mkjellman,omid,rcoli,scode,slebresne,tamarfraenkel,tpatterson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5911,,,,,,,,"11/Jan/13 23:46;jbellis;4446.txt;https://issues.apache.org/jira/secure/attachment/12564518/4446.txt","18/Jan/13 23:46;rcoli;CASSANDRA-4446--1.0.12_to_1.1.8.txt;https://issues.apache.org/jira/secure/attachment/12565581/CASSANDRA-4446--1.0.12_to_1.1.8.txt","18/Jul/12 21:35;rcoli;cassandra.1.0.10.replaying.log.after.exception.during.drain.txt;https://issues.apache.org/jira/secure/attachment/12537076/cassandra.1.0.10.replaying.log.after.exception.during.drain.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,245708,,,Fri Jan 18 23:45:47 UTC 2013,,,,,,,,,,"0|i06ctz:",35013,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"08/Sep/12 04:02;scode;In general, nodetool drain never seems to completely eliminate on-startup log replay. I observe this all the time on all clusters. It certainly cuts down the amount of replay done, but either never or fairly seldom eliminates it completely - at least not based on log messages indicating replay.

Never had time to investigate.;;;","08/Oct/12 23:40;kmueller;Also seeing this in an upgrade from 1.0.xx to 1.1.15:

 INFO 16:29:17,486 completed pre-loading (3 keys) key cache.
 INFO 16:29:17,495 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:17,503 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:18,495 GC for ParNew: 3506 ms for 4 collections, 1963062320 used; max is 17095983104
 INFO 16:29:18,498 Finished reading /data2/commit-cassandra/CommitLog-1349727956484.log
 INFO 16:29:18,499 Log replay complete, 0 replayed mutations


This is a standard upgrade process which includes a drain;;;","15/Oct/12 15:41;omid;I also experience this every time I drain / restart (up until latest 1.1.6 but not on 1.1.6 itself any more) and getting this message in log:

{quote}
2012-10-12_15:50:36.92191  INFO 15:50:36,921 Log replay complete, N replayed mutations   
{quote}

with N being non-zero. I wonder if this is a cause of double-counts for Counter mutations.;;;","21/Nov/12 06:36;tamarfraenkel;I had the same experience, when I upgraded my cluster from 1.0.9 to 1.0.11. I ran drain before the upgrade, upgrade on the node finished and node restarted at 2012-11-20 10:20:58, but then I see in the logs reply of commit log:
{quote} 
 INFO [main] 2012-11-20 09:41:13,918 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353402218337.log
 INFO [main] 2012-11-20 09:41:20,360 CommitLog.java (line 179) Log replay complete, 0 replayed mutations
 INFO [main] 2012-11-20 10:11:35,635 CommitLog.java (line 167) No commitlog files found; skipping replay
 INFO [main] 2012-11-20 10:21:11,631 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353404473899.log
 INFO [main] 2012-11-20 10:21:18,119 CommitLog.java (line 179) Log replay complete, 6413 replayed mutations
 INFO [main] 2012-11-20 10:55:46,435 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353406871619.log
 INFO [main] 2012-11-20 10:55:54,139 CommitLog.java (line 179) Log replay complete, 3 replayed mutations
{quote} 
This caused over increment of counters
;;;","21/Nov/12 12:00;jbellis;This is going to stand as a known limitation with 1.0.x; so far it looks like it is fixed in latest 1.1.;;;","04/Jan/13 00:16;mkjellman;did a nodetool drain before 1.1.7 -> 1.2.0. upon starting 1.2.0 every node in my cluster still replayed the commit logs and created mutations.

{code}
 INFO 13:17:06,529 DRAINING: starting drain process
 INFO 13:17:06,529 Stop listening to thrift clients
 INFO 13:17:06,532 Announcing shutdown
 INFO 13:17:07,536 Waiting for messaging service to quiesce
 INFO 13:17:07,537 MessagingService shutting down server thread.

... normal startup stuff..
 INFO 13:20:20,182 Replaying /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Finished reading /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Replaying /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,700 Finished reading /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,701 Replaying /ssd/commitlog/CommitLog-1355265349915.log
 INFO 13:20:28,118 Finished reading /ssd/commitlog/CommitLog-1355265349915.log
... more replay lines ...
 INFO 13:22:00,061 Log replay complete, 8052 replayed mutations
 INFO 13:22:00,358 Possible old-format hints found. Truncating
 INFO 13:22:00,370 Enqueuing flush of Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,372 Writing Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,494 Cassandra version: 1.2.0
 INFO 13:22:00,495 Thrift API version: 19.35.0
 INFO 13:22:00,495 CQL supported versions: 2.0.0,3.0.0 (default: 3.0.0)
 INFO 13:22:00,534 Loading persisted ring state
 INFO 13:22:00,537 Starting up server gossip
 WARN 13:22:00,557 No host ID found, created dd3a40e2-fef1-4574-87b8-e2929fd80235 (Note: This should happen exactly once per node).
{code};;;","04/Jan/13 10:11;slebresne;Let's reopen then if it doesn't sound like it's fixed in recent releases.;;;","04/Jan/13 10:59;arodrime;+1. Good to see this ticket reopen. Drain didn't work for a while. I remove all the commit logs files before a restart to avoid counters operations to replay.;;;","04/Jan/13 20:11;tpatterson;It always replays 3 mutations when I follow these steps:
{code}
bin/cassandra
tools/bin/cassandra-stress --operation=INSERT --num-keys=100000
bin/nodetool drain
bin/cassandra
{code}

This is on trunk, commit acf30622;;;","11/Jan/13 23:46;jbellis;System tables were not getting flushed.  This is the source of the extra replaying.  Patch attached to fix this, and also parallelize flushing.;;;","14/Jan/13 20:28;yukim;+1;;;","14/Jan/13 22:07;jbellis;Committed.

Note that earlier releases can workaround by manually running flush against system KS before drain.;;;","18/Jan/13 00:23;rcoli;While I'm sure that this does fix one real cause of drain not working in trunk (yay!), one of the symptoms I've heard reported in the 1.0.x - 1.1.5 timeframe is that ""my counters over-counted on upgrade, despite drain"". Most recent report was 1.0.12->1.1.8 with drain being run as part of the upgrade process.

NEWS.txt says :

""If you using counters and upgrading from a version prior to 1.1.6, you should drain existing Cassandra nodes prior to the upgrade to prevent overcount during commitlog replay (see CASSANDRA-4782).  For non-counter uses, drain is not required but is a good practice to minimize restart time.""

If drain in these versions can't be counted on (heh) to actually work for this purpose (which reports suggest it cannot), then I propose changing this line to read ""drain existing nodes and remove their commitlog"".;;;","18/Jan/13 00:34;jbellis;No counter mutations are double-counted, only the unflushed system changes are replayed.;;;","18/Jan/13 01:04;rcoli;If only unflushed system changes are replayed, how do you account for :

""I upgraded from 1.0.12 to 1.1.8, using drain, and I noticed overcounting counters"" ?

It's quite possible that upgrading from 1.1.x to 1.1.y>x does not in fact replay anything other than system keyspace and does not incur double counting of counters. I am however pretty confident based on multiple reports of the above quoted issue that counter increments may be over-replayed if one uses drain (as NEWS.txt suggests) while upgrading from 1.0.x to >1.1.6.

If this is being dealt with as ""known limitation of 1.0.x"", then I continue to suggest the above change to NEWS.txt, as otherwise people using counters in 1.0.x WILL incur double-increment while upgrading per the instructions in NEWS.txt.;;;","18/Jan/13 01:11;jbellis;Show me how to reproduce it and I will re-evaluate my position, but as near as I can tell the advice in NEWS is still best practice.;;;","18/Jan/13 23:45;rcoli;How to reproduce it, from the multiple reports :

1) Drain and stop cluster with counters on 1.0.x
2) Start same cluster on 1.1.x
3) Notice commitlog replay of the counter columnfamily and that your counters have over-counted

Attached is a log from the latest reporter, CASSANDRA-4446--1.0.12_to_1.1.8.txt. It shows the following.

1) Drain starts and completes on 1.0.12
2) Cluster then starts on 1.1.8, and replays the commit log
3) As part of commitlog replay, it flushes various CFs including titan3/RMEntityCount/, which is a counter columnfamily; machine has 4gb of heap and the flush is while thrift is down and the node has not jumped state to normal, so it seems reasonable to conjecture this flush is part of commitlog replay
4) It then logs ""10698 replayed mutations"", which adds further support to the idea that these Counts are part of replay
5) Operator then noticed a significant percentage of records had overcounted in this columnfamily;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix validation of dates,CASSANDRA-4441,12598998,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,16/Jul/12 17:04,16/Apr/19 09:32,14/Jul/23 05:52,16/Jul/12 18:34,1.2.0 beta 1,,,,,,0,,,,,,,"Our date validation (timestamp type) doesn't validate that the date is correct, i.e. it allows dates like 2011-42-42, because DateUtils.parseDate() doesn't do the validation (don't ask me how it can generate a timestamp from bogus date, apparently it can).

The easy fix is to use DateUtils.parseDateStrictly(), which does the validation. This does require to update commons-lang to >= 2.5 (we have 2.4).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/12 17:10;slebresne;4441.txt;https://issues.apache.org/jira/secure/attachment/12536658/4441.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256161,,,Mon Jul 16 18:34:09 UTC 2012,,,,,,,,,,"0|i0gvx3:",96601,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Jul/12 17:10;slebresne;Attached trivial patch. As said this require commons-lang 2.6 (which I'll add when committing);;;","16/Jul/12 17:15;jbellis;+1, but I'd rather push this to 1.2 only, since DateType has behaved this way since 0.8.;;;","16/Jul/12 18:34;slebresne;You're right, committed to trunk only.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Updating column family using cassandra-cli results in ""Cannot modify index name""",CASSANDRA-4439,12598764,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,a.schultz,a.schultz,13/Jul/12 22:01,16/Apr/19 09:32,14/Jul/23 05:52,18/Jul/12 22:16,1.1.3,,,Legacy/Tools,,,0,cli,schema,,,,,"Using cassandra-cli the following update to a column family worked in 1.1.0:
{code}
create keyspace testing;
use testing;

create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

After upgrading to 1.1.2, the update statement fails with the following exception in system.log:
{quote}
ERROR [Thrift:16] 2012-07-13 14:51:54,558 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1063)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
        ... 11 more
Caused by: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
ERROR [MigrationStage:1] 2012-07-13 14:51:54,561 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
{quote}


After further testing the following works in 1.1.2:
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

So it appears that if you did not specify an index_name when creating the column originally, you cannot update the column family anymore.","cassandra 1.1.2, RHEL6.3, running under java-1.6.0-sun",a.schultz,hudson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/12 14:22;xedin;CASSANDRA-4439.patch;https://issues.apache.org/jira/secure/attachment/12536821/CASSANDRA-4439.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256159,,,Mon Jul 23 17:04:28 UTC 2012,,,,,,,,,,"0|i0gvw7:",96597,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"16/Jul/12 12:57;a.schultz;For some extra information I modified ./src/java/org/apache/cassandra/config/ColumnDefinition.java:214 to show me what the index name issue was and it appears if you don't specify the index on the update, the index is automatically created at ""Album_profileId_idx_2"".  

{quote}
ERROR [Thrift:1] 2012-07-16 05:50:11,566 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1063)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
        ... 11 more
Caused by: java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
ERROR [MigrationStage:1] 2012-07-16 05:50:11,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOException: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:676)
        at org.apache.cassandra.db.DefsTable.updateColumnFamily(DefsTable.java:463)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:407)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Cannot modify index name Album_profileId_idx != Album_profileId_idx_2
        at org.apache.cassandra.config.ColumnDefinition.apply(ColumnDefinition.java:214)
        at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:758)
        at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:672)
        ... 9 more
{quote}


I was able to work around the issue if I specify the auto-generated index name in the update statement.
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

Also for anyone wondering, you can find the auto-generated index name by doing a show schema on the keyspace.  I'm going to drop the severity since I can work around the issue by adjusting my update statements.;;;","16/Jul/12 13:41;a.schultz;Also despite the fact that it says you cannot rename the index, it will rename the index if you run the command and restart cassandra.  The exception does not stop the schema from being updated.  Also if you attempt to update the schema again with a corrected index name after it throws the exception, it will not accept it until you restart.  At which point your index name has already been changed to the first index name you attempted.

An example of this is:
{code}
create keyspace testing;
use testing;
create column family Album
with comparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata =
[
    {column_name: profileId, validation_class: UTF8Type, index_type: KEYS}
];

update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'badindex', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

The previous update will throw an exception and if you attempt to run the following it will still throw the exception:
{code}
update column family Album
and column_metadata =
[
    {column_name: 'profileId', validation_class: UTF8Type, index_name: 'Album_profileId_idx', index_type: KEYS},
    {column_name: postedDate, validation_class: LongType}
];
{code}

Also if you restart and do a show schema, your index is now named ""badindex"".
{code}

[default@testing] show schema;
create keyspace testing
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {datacenter1 : 1}
  and durable_writes = true;

use testing;

create column family Album
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'profileId',
    validation_class : UTF8Type,
    index_name : 'badindex',
    index_type : 0},
    {column_name : 'postedDate',
    validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};
{code};;;","17/Jul/12 15:02;jbellis;Isn't existingIndexNames supposed to take care of this case?;;;","17/Jul/12 15:08;xedin;No, it doesn't do that.;;;","17/Jul/12 15:14;a.schultz;I have verified this patch fixes my issue.  thanks.;;;","17/Jul/12 18:45;xedin;Thanks, Alex. addDefaultIndexNames() works just fine when cql where we have all of the metadata (index_names/index_type) but for cli, where we need to provide all of the column attributes by hand, it wasn't because index_name should be filled in by user every time even if it was auto generated, so just checking that index_name is not set there (when no explicit name were given) is not good enough for thrift access.;;;","18/Jul/12 01:58;yukim;+1;;;","18/Jul/12 22:16;xedin;Committed.;;;","23/Jul/12 17:04;hudson;Integrated in Cassandra #1768 (See [https://builds.apache.org/job/Cassandra/1768/])
    fixes small bug introduced by CASSANDRA-4439 (Revision e220efa2a87c8232d87bdca9f2a02cfcef9f1a4c)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters in columns don't preserve correct values after cluster restart,CASSANDRA-4436,12598552,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,pvelas,pvelas,12/Jul/12 13:32,16/Apr/19 09:32,14/Jul/23 05:52,26/Jul/12 16:07,1.1.3,,,,,,0,,,,,,,"Similar to #3821. but affecting normal columns. 


Set up a 2-node cluster with rf=2.
1. Create a counter column family and increment a 100 keys in loop 5000 times. 
2. Then make a rolling restart to cluster. 
3. Again increment another 5000 times.
4. Make a rolling restart to cluster.
5. Again increment another 5000 times.
6. Make a rolling restart to cluster.


After step 6 we were able to reproduce bug with bad counter values. 
Expected values were 15 000. Values returned from cluster are higher then 15000 + some random number.
Rolling restarts are done with nodetool drain. Always waiting until second node discover its down then kill java process. ",,hudson,pvelas,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/12 07:22;slebresne;4436-1.0-2.txt;https://issues.apache.org/jira/secure/attachment/12537966/4436-1.0-2.txt","24/Jul/12 08:59;slebresne;4436-1.0-2.txt;https://issues.apache.org/jira/secure/attachment/12537669/4436-1.0-2.txt","18/Jul/12 18:31;slebresne;4436-1.0.txt;https://issues.apache.org/jira/secure/attachment/12537037/4436-1.0.txt","26/Jul/12 07:22;slebresne;4436-1.1-2.txt;https://issues.apache.org/jira/secure/attachment/12537967/4436-1.1-2.txt","24/Jul/12 08:59;slebresne;4436-1.1-2.txt;https://issues.apache.org/jira/secure/attachment/12537670/4436-1.1-2.txt","18/Jul/12 18:31;slebresne;4436-1.1.txt;https://issues.apache.org/jira/secure/attachment/12537038/4436-1.1.txt","17/Jul/12 15:14;pvelas;increments.cql.gz;https://issues.apache.org/jira/secure/attachment/12536828/increments.cql.gz",,,,,,,,,,,7.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,245636,,,Fri Aug 03 23:07:20 UTC 2012,,,,,,,,,,"0|i06cbj:",34930,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Jul/12 17:15;slebresne;Can you reproduce every time with those steps? I tried reproducing with those exact steps (as far as I can tell) a few times on both 1.0 and 1.1 (the counter code didn't change much between 1.0 and 1.1) and wasn't able to reproduce.;;;","14/Jul/12 07:55;pvelas;

create keyspace test_old
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use test_old;

create column family cf1_increment
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'CounterColumnType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 1.0
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};



In version 1.0.10 am always able to reproduce with this steps.. but its not reproducible in 1.1.2 .

When I stop writing and shutdown node with ""nodetool drain"" there are some small commitlog files, but I don't bother to delete them just restart cassandra process. Maybe this is case ?;;;","17/Jul/12 10:58;slebresne;The only difference I could see with the test I ran previously was the use of compression. So while I strongly doubt compression can have anything to do with that in any way, I rerun the test against 1.0 a bunch of time but I was still not able to reproduce any error.

Since you seem to be able to reproduce easily, would you mind sharing the scripts you use to reproduce? I.e. mainly the code you use for insertion, preferably in plain thrift or CQL2 as this would eliminate the possibility of a client library bug.;;;","17/Jul/12 15:40;pvelas;You are right its not affected by compression.
I was just curious if its problem with our python code using pycassa ... 
So I created increments.cql containing 100k lines with 1000 increments for each of 100 key values.
{code}
cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B >/dev/null 
{code}

after 3 rolling restarts each value was correct with value 3000 
after 4 rolling restart values are incorrect see bellow

{code}
col1	5479
col10	5507
col100	5531
col11	5480
col12	5501
col13	5499
col14	5516
{code}

Its 2 node cluster with replication=2. 




{code}
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra-cli -h $HOSTNAME -p 9160 -f increments.cql -B >/dev/null 
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain

[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.20.30.160    datacenter1 rack1       Down   Normal  97.67 KB        50.00%  0                                           
10.20.30.161    datacenter1 rack1       Up     Normal  113.45 KB       50.00%  85070591730234615865843651857942052864  

[root@cass-bug1 ~]# killall java
[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra

[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME drain

[root@cass-bug1 ~]# /opt/apache-cassandra-1.0.10/bin/nodetool -h $HOSTNAME ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.20.30.160    datacenter1 rack1       Up     Normal  97.67 KB        50.00%  0                                           
10.20.30.161    datacenter1 rack1       Down   Normal  86.13 KB        50.00%  85070591730234615865843651857942052864 

[root@cass-bug2 ~]# killall java
[root@cass-bug2 ~]# /opt/apache-cassandra-1.0.10/bin/cassandra


{code}



Here is dump of keyspace and CF 


{code}
create keyspace inc_test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use inc_test;

create column family cf1_increment
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'CounterColumnType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'SerializingCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';
{code}


Hope that helps you reproduce ..;;;","17/Jul/12 15:41;pvelas;Increment for batch loading through cassandra-cli.;;;","18/Jul/12 18:31;slebresne;Thanks a lot Peter for helping out reproducing this issue.

The problem is that when a node stops (or is drained for that matter, we don't wait for all compaction to end during drain as this could mean waiting for a very long time, at least with SizeTieredCompaction) just when a compaction is finishing, it is possible for some of the compacted file to not have -Compacted components even if the compacted file is not temporary anymore. In other words, it is possible that when the node is restart, it will load both the compacted files and some of the file used to compact it. While this is harmless (though inefficient) for normal column family, this means overcounting for counters.

I'll note that even though I can't reproduce the counter bug on 1.1 with the test case above, it is just ""luck"" as 1.1 is affected as well.

What we need to guarantee is that we will never use both a compacted file and one of it's ancestor. One way to ensure that is to keep in the metadata of the compacted file, the list of it's ancestors (we only need to keep the generation). Then when a node start, it can gather all the ancestors of all the sstable in the data dir, and delete all those sstable that are in this ancestor set. Since we don't want to keep ever going list of ancestors however, a newly compacted sstable only need to keep the list of it's still live ancestor (which 99% of the time means keeping only the generation of the file that were compacted to obtain it). I note that if we do that, we don't need to generate -Compacted components.

Attaching patch to implement this. Attaching a patch for 1.0 and 1.1 (which aren't very different). I wrote the 1.0 version because it's on this version that I knew how to reproduce the counter bug reliably, and I've checked that this patch does fix the issue. However, this patch doesn't only affect counter code and is not trivial per se, so I don't know how I feel about risking to breaking things on 1.0 for non-counter user at this point. I think it might me wiser to put this in 1.1.3 only and say that counter users should either apply the attached patch at their own risk or upgrade to 1.1.3.
;;;","20/Jul/12 10:02;pvelas;Thanks for your interest and time to fix it. We currently move to 1.1.2 version to avoid some random aws failure and patiently waiting for 1.1.3 release. ;;;","23/Jul/12 20:12;jbellis;Looks like skipCompacted in Directories.SSTableLister can be removed (since we scrubDataDirectories on startup and no new compacted components will be created).

Using a List means we can add an ancestor multiple times.  Suggest using a Set instead.

Nits:
- would prefer Ancestor to LiveAncestor, since we only check liveness at creation time, so ""Live"" is misleading when iterating over them later.
- the deleting code feels more at home in CFS constructor than addInitialSSTables.
- tracker parameter is unused now in SSTR.open;;;","24/Jul/12 08:59;slebresne;bq. Looks like skipCompacted in Directories.SSTableLister can be removed (since we scrubDataDirectories on startup and no new compacted components will be created).

True, though there is the (arguably remote) possibility that people call loadNewSSTables() (or the offline scrub from CASSANDRA-4441) on sstables having some -Compacted components. So I would prefer leaving it in 1.1 and removing it during the merge to trunk, just to be sure minor upgrade are as little disrupting as can be.

bq. Using a List means we can add an ancestor multiple times. Suggest using a Set instead.

But we won't have the same ancestor multiple times. Otherwise that would be a bug (and at least for counters, a particularly bad one). But for sanity I've added an assertion to check this doesn't happen (I've a list however, I figured that since the list will be small, the difference between List.contains() and Set.contains() will be negligeable, and it's checked in an assertion and only once a the sstable creation. On the other Lists have a smaller memory footprint. Though I admit in either case we're talked minor differences).

bq. would prefer Ancestor to LiveAncestor, since we only check liveness at creation time, so ""Live"" is misleading when iterating over them later.

Renamed.

bq. the deleting code feels more at home in CFS constructor than addInitialSSTables.

Moved.

bq. tracker parameter is unused now in SSTR.open

Removed. I realized that setTrackedBy was already always call through the DataTracker.addNewSSTablesSize, so I also removed the call duplication.
;;;","25/Jul/12 19:17;jbellis;bq. But we won't have the same ancestor multiple times

I don't think that's true.  Suppose for instance we have leveled compaction with A and B in L0.  They are larger than 5MB so we split the result into X, Y, and Z.  Next we flush C to L0.  It overlaps with Y and Z, so we're compacting C, Y, and Z.  Now we have Y and Z both with A and B as ancestors.

(Switching from LCS back to STCS is another way you could get duplicate ancestors.);;;","26/Jul/12 07:22;slebresne;You're completely right, I'm still thinking too much in terms of SizeTieredCompaction.

Updated patches to use a Set.;;;","26/Jul/12 14:34;jbellis;+1;;;","26/Jul/12 16:07;slebresne;Committed (to >= 1.1 as per my earlier comment), thanks.

I've also removed Directories.skipCompacted() while merging to trunk.;;;","03/Aug/12 23:07;hudson;Integrated in Cassandra #1861 (See [https://builds.apache.org/job/Cassandra/1861/])
    Fix ScrubTest after file format change in CASSANDRA-4436 (Revision a075385d05c3e1d26475f448363958bad4645f17)

     Result = ABORTED
yukim : 
Files : 
* test/data/corrupt-sstables/Keyspace1-Standard3-ia-1-Statistics.db
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hints compaction loop over same sstable,CASSANDRA-4435,12598461,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,11/Jul/12 21:48,16/Apr/19 09:32,14/Jul/23 05:52,19/Jul/12 16:53,1.2.0 beta 1,,,,,,0,compaction,hintedhandoff,,,,,"Noticed the following while testing something else:

{noformat}
INFO 22:14:48,496 Completed flushing /var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db (109645 bytes) for commitlog position ReplayPosition(segmentId=9372773011543415, position=30358488)
 INFO 22:14:48,498 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-1-Data.db')]
 INFO 22:14:48,500 SSTables for user defined compaction are already being compacted.
 INFO 22:14:48,500 Finished hinted handoff of 16893 rows to endpoint /10.179.64.227
 INFO 22:14:48,658 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db,].  109,645 to 899 (~0% of original) bytes for 1 keys at 0.005392MB/s.  Time: 159ms.
 INFO 22:14:48,660 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-2-Data.db')]
 INFO 22:14:48,668 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,669 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-3-Data.db')]
 INFO 22:14:48,679 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.095261MB/s.  Time: 9ms.
 INFO 22:14:48,680 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-4-Data.db')]
 INFO 22:14:48,697 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.050433MB/s.  Time: 17ms.
 INFO 22:14:48,698 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-5-Data.db')]
 INFO 22:14:48,714 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,715 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-6-Data.db')]
 INFO 22:14:48,722 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,723 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-7-Data.db')]
 INFO 22:14:48,736 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,737 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-8-Data.db')]
 INFO 22:14:48,744 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,745 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-9-Data.db')]
 INFO 22:14:48,753 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,754 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-10-Data.db')]
 INFO 22:14:48,761 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,762 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-11-Data.db')]
 INFO 22:14:48,775 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.065950MB/s.  Time: 13ms.
 INFO 22:14:48,776 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-12-Data.db')]
 INFO 22:14:48,783 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,784 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-13-Data.db')]
 INFO 22:14:48,792 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,793 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-14-Data.db')]
 INFO 22:14:48,800 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,802 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-15-Data.db')]
 INFO 22:14:48,809 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,810 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-16-Data.db')]
 INFO 22:14:48,826 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,827 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-17-Data.db')]
 INFO 22:14:48,834 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,835 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-18-Data.db')]
 INFO 22:14:48,842 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,842 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-19-Data.db')]
 INFO 22:14:48,850 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,850 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-20-Data.db')]
 INFO 22:14:48,867 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,868 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-21-Data.db')]
 INFO 22:14:48,876 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,877 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-22-Data.db')]
 INFO 22:14:48,884 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.122479MB/s.  Time: 7ms.
 INFO 22:14:48,885 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-23-Data.db')]
 INFO 22:14:48,893 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,895 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-24-Data.db')]
 INFO 22:14:48,901 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,902 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-25-Data.db')]
 INFO 22:14:48,910 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,910 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-26-Data.db')]
 INFO 22:14:48,926 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.057157MB/s.  Time: 15ms.
 INFO 22:14:48,930 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-27-Data.db')]
 INFO 22:14:48,938 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.107169MB/s.  Time: 8ms.
 INFO 22:14:48,943 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-28-Data.db')]
 INFO 22:14:48,949 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,950 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-29-Data.db')]
 INFO 22:14:48,966 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.053585MB/s.  Time: 16ms.
 INFO 22:14:48,967 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-30-Data.db')]
 INFO 22:14:48,974 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,974 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-31-Data.db')]
 INFO 22:14:48,980 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,981 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-32-Data.db')]
 INFO 22:14:48,988 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.142892MB/s.  Time: 6ms.
 INFO 22:14:48,989 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-33-Data.db')]
 INFO 22:14:48,995 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:48,995 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-34-Data.db')]
 INFO 22:14:49,002 Compacted to [/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db,].  899 to 899 (~100% of original) bytes for 1 keys at 0.171471MB/s.  Time: 5ms.
 INFO 22:14:49,002 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/hints/system-hints-ia-35-Data.db')]
{noformat}

After this, the loop stopped.  It also did not occur on another member which delivered hints, so it may not be easy to replicate.  I suspect something related to CASSANDRA-3442 caused this, though the odd thing is there shouldn't even be a tombstone left.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/12 07:36;jbellis;4435-v2.txt;https://issues.apache.org/jira/secure/attachment/12536963/4435-v2.txt","18/Jul/12 15:22;yukim;4435-v3.txt;https://issues.apache.org/jira/secure/attachment/12537003/4435-v3.txt","17/Jul/12 15:28;yukim;4435.txt;https://issues.apache.org/jira/secure/attachment/12536829/4435.txt",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256156,,,Thu Jul 19 16:53:49 UTC 2012,,,,,,,,,,"0|i0gvuf:",96589,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Jul/12 15:23;yukim;So far I have not able to reproduce this(If you still have one of those SSTables, it may help me figure out easier).

But I noticed from the log and the HHOM code, HHOM tries user defined compaction to compact all SSTables at once after forcing memtable to flush.
Flushing triggers background compaction, so sometimes, it competes with user defined one. You may notice from the line in above log,

bq.  INFO 22:14:48,500 SSTables for user defined compaction are already being compacted.

shows this situation that SSTables are already marked for compaction when running user defined.

It may be not the right solution here, but since recursive compaction only occurs when doing background compaction, I think it is better to add an option not to submit background compaction after flushing and use it here to make sure you only run user defined compaction.;;;","16/Jul/12 15:27;jbellis;That would actually make a lot of sense, since hints are otherwise append-only, so there's no real benefit from normal background compaction.;;;","17/Jul/12 15:28;yukim;Patch to not submit background compaction after flush. HHOM uses this to only perform user defined compaction after delivering hints.;;;","18/Jul/12 07:36;jbellis;Can we just use the existing isCompactionDisabled logic?  v2 attached;;;","18/Jul/12 15:22;yukim;Yes it works, except your syntax doesn't work. Attaching v3.;;;","18/Jul/12 17:26;jbellis;+1;;;","19/Jul/12 16:53;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change nanoTime() to currentTimeInMillis() in schema related code.,CASSANDRA-4432,12598217,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,10/Jul/12 16:51,16/Apr/19 09:32,14/Jul/23 05:52,10/Jul/12 17:17,1.1.3,,,,,,0,,,,,,,"From nanoTime() description:

""The value returned represents nanoseconds since some fixed but arbitrary time (perhaps in the future, so values may be negative). This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change.""

Also see http://www.mail-archive.com/dev@cassandra.apache.org/msg04992.html
",,arya,hudson,mtheroux2,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/12 17:00;xedin;CASSANDRA-4432.patch;https://issues.apache.org/jira/secure/attachment/12535872/CASSANDRA-4432.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256153,,,Tue Jul 10 22:08:22 UTC 2012,,,,,,,,,,"0|i0gvt3:",96583,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Jul/12 17:08;jbellis;+1;;;","10/Jul/12 17:09;mtheroux2;In order to be consistent with timestamps elsewhere, shouldn't the changes read System.currentTimeMills * 1000?;;;","10/Jul/12 17:14;xedin;We have millis * 1000 for columns but I don't see a reason to do that for schema as all columns would be created with the same millis resolution by all migrations.;;;","10/Jul/12 17:17;mtheroux2;Its a consideration for existing users.  Any user who would have created a keyspace before this fix, will have timestamps in whatever System.nanoTime() returns.  The bigger you make the timestamp, the less users will be stuck attempting to fix their timestamps so they can update their schema (I guess using sstable2json?).  Plus, its consistent with columns :)  ;;;","10/Jul/12 17:17;xedin;Committed.;;;","10/Jul/12 17:18;slebresne;I suppose that since we'll access the system table (including the schema) directly in CQL3, it might be worth using micro-seconds.;;;","10/Jul/12 17:23;jbellis;Good point.;;;","10/Jul/12 17:26;xedin;Ok, I will add * 1000 there.;;;","10/Jul/12 17:39;slebresne;nit: we have a FBUtilities.timestampMicros();;;","10/Jul/12 17:42;xedin;Right, this is what I was thinking about :);;;","10/Jul/12 20:46;xedin;Committed change to timestampMicros().;;;","10/Jul/12 22:08;hudson;Integrated in Cassandra #1673 (See [https://builds.apache.org/job/Cassandra/1673/])
    change System.currentTimeMillis() to FBUtilities.timestampMicros(), related to CASSANDRA-4432 (Revision 346ac03c29cd1fe763bd01077a5e0c59f12181b3)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/service/MigrationManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool ring throws java.lang.AssertionError in TokenMetadata.getTopology,CASSANDRA-4429,12598029,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,soverton,soverton,soverton,09/Jul/12 13:04,16/Apr/19 09:32,14/Jul/23 05:52,09/Jul/12 15:36,,,,Tool/nodetool,,,0,,,,,,,"
{noformat}
$ bin/nodetool -h localhost ring
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.getTopology(TokenMetadata.java:851)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:2781)
        at org.apache.cassandra.service.StorageService.effectiveOwnership(StorageService.java:70)
{noformat}

TokenMetadata.getTopology() can only be called on a clone of TokenMetadata, not the StorageService instance.
",,soverton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/12 13:20;soverton;4429.patch;https://issues.apache.org/jira/secure/attachment/12535665/4429.patch",,,,,,,,,,,,,,,,,1.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256150,,,Mon Jul 09 15:36:55 UTC 2012,,,,,,,,,,"0|i0gvrz:",96578,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Jul/12 13:10;soverton;Another minor issue with CASSANDRA-3047 is that the IPs are printed using InetAddress.toString() which results in output like this:

{noformat}
$ bin/nodetool -h localhost ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: dc1
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               127605887595351923798765477786913079296     
miles/10.2.129.41rack1       Up     Normal  22.57 KB        25.00%              0                                           
/10.2.129.51    rack2       Up     Normal  4.63 KB         25.00%              127605887595351923798765477786913079296     

Datacenter: dc2
==========
Address         Rack        Status State   Load            Owns                Token                                       
                                                                               85070591730234615865843651857942052864      
/10.2.129.15    rack1       Up     Normal  13.54 KB        25.00%              42535295865117307932921825928971026432      
/10.2.129.16    rack2       Up     Normal  9.06 KB         25.00%              85070591730234615865843651857942052864      
{noformat}

It should be using InetAddress.getHostAddress() instead.;;;","09/Jul/12 13:20;soverton;Attached patch against trunk;;;","09/Jul/12 15:36;jbellis;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restarting a failed bootstrap instajoins the ring,CASSANDRA-4427,12597963,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Jul/12 23:06,16/Apr/19 09:32,14/Jul/23 05:52,31/Jul/12 20:57,1.1.3,,,,,,0,,,,,,,"I think when we made auto_bootstrap = true the default, we broke the check for the bootstrap flag, creating a dangerous situation.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/12 17:48;jbellis;4427-4.txt;https://issues.apache.org/jira/secure/attachment/12538579/4427-4.txt","31/Jul/12 18:51;brandon.williams;4427-5.txt;https://issues.apache.org/jira/secure/attachment/12538597/4427-5.txt","13/Jul/12 18:14;brandon.williams;4427-v2.txt;https://issues.apache.org/jira/secure/attachment/12536426/4427-v2.txt","13/Jul/12 18:27;brandon.williams;4427-v3.txt;https://issues.apache.org/jira/secure/attachment/12536429/4427-v3.txt","12/Jul/12 19:48;brandon.williams;4427.txt;https://issues.apache.org/jira/secure/attachment/12536264/4427.txt",,,,,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256148,,,Tue Jul 31 20:57:53 UTC 2012,,,,,,,,,,"0|i0gvr3:",96574,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"12/Jul/12 19:20;brandon.williams;Patch to remove the check for non-system tables.  As far as I can tell, it makes no sense to have this condition, since you can partially stream some data during bootstrap, then die.  If you try again without clearing the machine, instajoin.

It looks like this condition was added as part of a fix for CASSANDRA-3219 in commit de829f17, but the check was inverted and later fixed in CASSANDRA-3285.  However, the reasoning behind having such a check at all is never explained and I can't see why it should be there.;;;","12/Jul/12 19:48;brandon.williams;Updated to also remove strange seed check (if seed, then bootstrap?) which is impossible anyway due to previous check.;;;","12/Jul/12 20:10;jbellis;Here's what we were trying to address there:

bq. Now there is a actual new problem with 1.0.0. That problem is that when you start an initial cluster, i.e, when in 0.8 you would start node with auto-boostrap=false, you do often end up starting nodes simultaneously. That is why older version were using random token when auto-bootstrap was false. This problem does need to be fix for 1.0.0 because that is a serious regression. However, my argument is that even though we now default to auto-boostrap=true, that doesn't mean that there is no difference between setting up the initial nodes of a cluster and the latter bootstrapping of nodes to add capacity to an existing cluster. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not (we call the bootstrap() method based on that). Imho, this means that we have no boostrap option and the ""I have no schema"" is the old auto-boostrap=false. So we should use random token in that case and balanced one otherwise the same way we are doing it in 0.8.;;;","12/Jul/12 21:30;brandon.williams;bq. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not

This seems more dangerous than it was worth, since you can easily receive even partial schema within a couple of seconds, realize you made some sort of mistake (forgot to mount the data dir, etc) and restart it, possibly wrecking your production app.

(The seed check still seems strange regardless);;;","13/Jul/12 18:14;brandon.williams;v2 takes a different approach that Sylvain kindly suggested, and changes the bootstrap flag from a boolean to a 3-way value so that we can detect previous attempts that failed, and try again.;;;","13/Jul/12 18:19;jbellis;I think you're misreading the original seed logic...  !(isBootstrapped || isSeed) expands to !isBootstrapped && !isSeed.  Still need that so that single-node clusters don't try to bootstrap.;;;","13/Jul/12 18:27;brandon.williams;Oh, I see now.  v3 restores the seed check.;;;","16/Jul/12 21:06;jbellis;+1

nit: worth adding a comment to explain wtf all the clauses of that if statement are, so we don't have to dig through ticket history next time;;;","16/Jul/12 21:19;brandon.williams;Committed with comments added.;;;","16/Jul/12 22:18;jbellis;Started trying to improve the comments and got stuck on the schema check: it's basically a no-op (except for the purposes of screwing up a partial bootstrap like this), since we perform the check before waiting for gossip to fill in the schema.

Simpler fix at https://github.com/jbellis/cassandra/tree/4427-4 to move the schema check into getBootstrapToken.;;;","17/Jul/12 09:39;slebresne;I believe this simpler fix doesn't handle the case of boostrapping multiple nodes into an existing cluster. Namely, in that case, that will have a schema and so the node will have a system table by the time it checks for it and we'll end up picking the same token for multiple nodes.

Also, I think checking system tables existence is fairly fragile and I would prefer moving away from it. It is way too easy to screw that up by having something (anything) written to those system tables. Typically, I don't know if that fix works for multiple nodes started in a brand new cluster (with not all being seeds), because without careful checking I don't know if we can end up writing some info in the system tables before checking for getBootstrapToken.

Overall I do like the idea of registering that the bootstrap is in process, because on top of (I think) fixing the problem in a non-fragile way, it also allows us better reporting. Even outside of the problem of generating tokens, I think it is reassuring for a user that restart a node that failed to boostrap to have the software acknowledge that it understand and handle correctly the situation.;;;","18/Jul/12 07:35;jbellis;bq. I believe this simpler fix doesn't handle the case of boostrapping multiple nodes into an existing cluster.

We've never tried to prevent this in the existing cluster case, except by saying ""thou shalt space bootstraps apart two minutes,"" because the only way to stop it is to drop the ""balanced"" token picking altogether.  Adding ""bootstrap in progress"" concept does nothing for this one way or the other.

bq. Namely, in that case, that will have a schema and so the node will have a system table by the time it checks for it and we'll end up picking the same token for multiple nodes.

This is exactly how it's supposed to work: if there's a schema, we use ""existing cluster mode"" and pick a token to divide the range of the heaviest node (and cross our fingers that the user is spacing things out enough between node additions).  If there's no schema, we use ""new cluster mode"" and pick a random token.

Let the record show that back in CASSANDRA-3219 I said this was confusing behavior and we should add explicit initial_token modes instead of trying to make it magical. :)
;;;","18/Jul/12 08:44;slebresne;bq. Adding ""bootstrap in progress"" concept does nothing for this one way or the other.

You're right, brain fart, sorry.

Anyway, there is still one behavior that the patch changes, that is it will always boobstrap non seeds node, while previously the system table check was making sure we never bootstrapped a node in a new cluster, independently of whether it was a seed or not.

It is clearly not a bad idea when you start a new cluster to set all those nodes as seeds, but I just want to point out that the behavior is changed and I'm not sure everyone always set all of its initial node as seeds today. I'll also note that boostrapping some of the node in an initial cluster don't break anything, it just makes the node start much less quickly that they would otherwise.

I'm not sure how I feel about changing that behavior, especially in a minor release. The fact is that recording that bootstrap is in progress (along with the system table check) would allow to fix the instajoin while keeping the current behavior unchanged otherwise, and I do feel that recording the info is not a bad idea in itself, so that would have my preference. But that is not an extremely strong preference either.;;;","18/Jul/12 11:51;brandon.williams;bq. The fact is that recording that bootstrap is in progress (along with the system table check) would allow to fix the instajoin while keeping the current behavior unchanged otherwise, and I do feel that recording the info is not a bad idea in itself, so that would have my preference.

I tend to agree that having an explicit, persisted flag feels a lot less fragile than the current logic, and being able to indicate a failure to the user seems like a good improvement.;;;","23/Jul/12 16:06;jbellis;bq. there is still one behavior that the patch changes, that is it will always boobstrap non seeds node

You're right.  Okay, take five: https://github.com/jbellis/cassandra/tree/4427-5

4 patches here on top of Brandon's work.  The main ones are the 1st and 4th.  In the first, I remove the seed special case since it's a subset of the empty schema case.  (Unless you're Doing It Wrong and adding seed nodes directly to an active cluster, which always surprises people when it burns them.  So I say good riddance.)

The first also adds a 2-gossip-round sleep so that (always assuming seeds are set correctly) we eliminate the risk of thinking schema is empty incorrectly due to a race w/ gossip.  The fourth patch follows this up by making the schema check based on other peers' schema uuids instead of local data.  Which is unlikely to be a problem today, but is is still a race-y approach and the correct alternative was straightforward.;;;","27/Jul/12 10:06;slebresne;In the check for bootstrap:
{noformat}
if (DatabaseDescriptor.isAutoBootstrap()
    && (SystemTable.bootstrapInProgress() || (!SystemTable.bootstrapComplete() && !schemaPresent)))
{noformat}
I believe the schemaPresent condition shouldn't be negated. We want to skip boostrap is there is no schema, but bootstrap if there is one.

Even with that fixed, this breaks some of the unit tests (BoostrapperTest, EmbeddedCassandraServiceTest, StreamingTransferTest and AntiEntropyServiceStandardTest). Namely:
{noformat}
junit] java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
junit] 	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:127)
junit] 	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:109)
junit] 	at org.apache.cassandra.dht.BootStrapper.getBootstrapToken(BootStrapper.java:104)
junit] 	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:629)
junit] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:526)
junit] 	at org.apache.cassandra.dht.BootStrapperTest.testTokenRoundtrip(BootStrapperTest.java:50)
{noformat}

On committing to 1.0, I'm not sure what was the intention, but this feels a bit bigger than what I'm plainly confortable pushing in 1.0 at this point, and it feels we can tell people on 1.0 to wipe the data dir on a failed boostrap before retrying. That's not a strong opposition though, more an opinion.

Nits:
* Instead of calculateEmptySchema(), I would have put the initialization fo Schema.emptyVersion in a static block to make it explicit that it's a one time initialization. Though if you made that on purpose because you don't like static blocks, that's good enough for me.
* We log when we detect a boostrap failure, but it could be nice to also log whether we're going to boostrap or not and why in the other case.
;;;","27/Jul/12 14:48;slebresne;I suspect the test failures are due to the removal of the seeds special case and because our tests are not fully realistic. Namely, in the tests, while localhost is a seed, it gets a schema loaded before joinTokenRing is called, and so it ends up with schemaPresent = true and tries to bootstrap (even though it's the only node). That shouldn't happen in real life but at least on the short term fixing the tests themselves is more work than is worth it, so maybe we can:
* Either we back the isSeed test
* Or exclude ourselves when we check for schemaPresent

Some Preference?;;;","27/Jul/12 22:02;jbellis;bq. I believe the schemaPresent condition shouldn't be negated

Right, fix pushed to same github branch.

bq. I would have put the initialization fo Schema.emptyVersion in a static block to make it explicit that it's a one time initialization

I thought you couldn't declare emptyVersion final that way...  I was wrong, the compiler is smart enough to recognize the static block.  Also fixed.

bq. it could be nice to also log whether we're going to boostrap or not and why in the other case.

Added a debug line.

bq. exclude ourselves when we check for schemaPresent

Done.  (Since we can't have one ourselves unless another does too -- or unless we already joined the ring successfully -- there is no loss of correctness.)

bq. this feels a bit bigger than what I'm plainly confortable pushing in 1.0 at this point

+1, let's leave it as a known issue in 1.0.;;;","28/Jul/12 09:54;slebresne;lgtm, +1;;;","29/Jul/12 14:59;brandon.williams;This doesn't quite work, because we're looking for the SCHEMA app state, which at startup won't always exist:

{noformat}
ERROR [main] 2012-07-29 01:08:28,476 CassandraDaemon.java (line 335) Exception encountered during startup
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:527)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:475)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:366)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:228)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:318)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:361)
{noformat}
;;;","29/Jul/12 16:24;brandon.williams;Note: that was with autobootstrap disabled.  But, I'm also not convinced that waiting two gossiper rounds is sufficient either (alert the ring_delay police!)

It's possible that you could have 3 seeds and all but one could be down, thus 2 gossip rounds doesn't guarantee you'll have any appstates.;;;","29/Jul/12 17:27;jbellis;bq. This doesn't quite work, because we're looking for the SCHEMA app state, which at startup won't always exist

Added a quick fix for this case.  If the cluster is so new that there is no SCHEMA state, then there's no actual schema info either.

bq. It's possible that you could have 3 seeds and all but one could be down, thus 2 gossip rounds doesn't guarantee you'll have any appstates

Granted, but surely two rounds is a better measure than the zero we had before.  (Which apparently worked most of the time...)  Remember, our goal is to avoid the full RING_DELAY sleep when we don't need to bootstrap.;;;","29/Jul/12 17:39;brandon.williams;bq. Added a quick fix for this case. If the cluster is so new that there is no SCHEMA state, then there's no actual schema info either.

LGTM.

bq. Granted, but surely two rounds is a better measure than the zero we had before. (Which apparently worked most of the time...) Remember, our goal is to avoid the full RING_DELAY sleep when we don't need to bootstrap.

I know.  It's a situation with no perfect solution unfortunately (but I agree 2 > 0 ;);;;","31/Jul/12 16:13;brandon.williams;Well, bad news, something here is still broken:

{noformat}
DEBUG [main] 2012-07-31 10:52:40,564 MigrationManager.java (line 240) Gossiping my schema version 59adb24e-f3cd-3e02-97f0-5b395827453f
...
DEBUG [main] 2012-07-31 10:52:42,584 StorageService.java (line 591) Bootstrap variables: true false false false
{noformat}

and it joins the ring, where it should have bootstrapped.  I'm not sure why the schema check is failing, but it's causing problems for the dtests and happens in a very reproducible manner.;;;","31/Jul/12 16:35;jbellis;59adb24e-f3cd-3e02-97f0-5b395827453f is emptyVersion, so from that snippet it looks like it's working as designed.;;;","31/Jul/12 16:56;brandon.williams;Here's the real problem:

{noformat}

 INFO 16:49:57,531 Starting up server gossip
 INFO 16:49:57,547 Enqueuing flush of Memtable-LocationInfo@1547338589(126/157 serialized/live bytes, 3 ops)
 INFO 16:49:57,548 Writing Memtable-LocationInfo@1547338589(126/157 serialized/live bytes, 3 ops)
 INFO 16:49:57,586 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-1-Data.db (234 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=595)
 INFO 16:49:57,616 Starting Messaging Service on port 7000
 INFO 16:49:59,634 Saved token not found. Using 113427455640312821154458202477256070484 from configuration
 INFO 16:49:59,636 Enqueuing flush of Memtable-LocationInfo@1088940267(53/66 serialized/live bytes, 2 ops)
 INFO 16:49:59,636 Writing Memtable-LocationInfo@1088940267(53/66 serialized/live bytes, 2 ops)
 INFO 16:49:59,652 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-2-Data.db (163 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=776)
 INFO 16:49:59,655 Node cassandra-3/10.179.111.137 state jump to normal
 INFO 16:49:59,656 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 16:49:59,690 Binding thrift service to cassandra-3/10.179.111.137:9160
 INFO 16:49:59,694 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 16:49:59,698 Using synchronous/threadpool thrift server on cassandra-3/10.179.111.137 : 9160
 INFO 16:49:59,699 Listening for thrift clients...
 INFO 16:49:59,873 Node /10.179.64.227 is now part of the cluster
 INFO 16:49:59,874 InetAddress /10.179.64.227 is now UP
 INFO 16:49:59,876 Enqueuing flush of Memtable-LocationInfo@1301257077(35/43 serialized/live bytes, 1 ops)
 INFO 16:49:59,877 Writing Memtable-LocationInfo@1301257077(35/43 serialized/live bytes, 1 ops)
 INFO 16:49:59,892 Completed flushing /var/lib/cassandra/data/system/LocationInfo/system-LocationInfo-he-3-Data.db (89 bytes) for commitlog position ReplayPosition(segmentId=10938112371080118, position=874)
 INFO 16:49:59,894 Node /10.179.65.102 is now part of the cluster
 INFO 16:49:59,894 InetAddress /10.179.65.102 is now UP
{noformat}

Gossip hasn't quite discovered any other nodes yet when the schema check fires.;;;","31/Jul/12 17:48;jbellis;Patch attached to add back seed logic and move the schema check into getBootstrapToken.;;;","31/Jul/12 18:25;brandon.williams;+1;;;","31/Jul/12 18:42;brandon.williams;We can save an appreciable amount of time by checking for schema during the delay, and then short circuiting to the isReadyForBootstrap check.;;;","31/Jul/12 20:01;jbellis;Don't you still want the full ring delay to make sure you know about everyone in the cluster (so if you are picking a ""balanced"" token it does the Right Thing)?;;;","31/Jul/12 20:04;brandon.williams;bq. Don't you still want the full ring delay to make sure you know about everyone in the cluster (so if you are picking a ""balanced"" token it does the Right Thing)?

Well, if we got any non-empty schema, a full gossip round has occurred so we should be good to go at that point, since it will have also populated our knowledge of the ring.;;;","31/Jul/12 20:22;jbellis;+1 then.

nit: i'd also change sleep(delay) in the MigrationManager loop to sleep(1000), or even sleep(100);;;","31/Jul/12 20:57;brandon.williams;Committed w/nit fixed to sleep(1000), since if it's not complete logging every 100ms would be a bit annoying.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Possible schema corruption with cql 3.0,CASSANDRA-4420,12597764,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,bertpassek,bertpassek,06/Jul/12 12:53,16/Apr/19 09:32,14/Jul/23 05:52,08/Jul/12 11:11,1.1.3,,,,,,0,,,,,,,"Hi,

i've got some problems while creating schemas with cql 3.0. After that i can't even start cassandra anymore.

Following steps for reproduction were done on a new installation of cassandra:

1. simply create a keyspace test via ""cqlsh -3""

create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;

2. add cf with composite columns via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    d int,
    primary key (a, b, c)
);

3. drop column family 

drop columnfamily test1;

So until now everything went fine. Now i'm trying to insert a slightly modified column family with the same name above.

4. create new cf via ""cqlsh -3""

create table test1 (
    a int,
    b int,
    c int,
    primary key (a, b)
);

This creation fails with following exception:


java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
        at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
        at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
        at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Now at this point you can't do anything anymore via cql or cli. Shutting down and starting cassandra again throws same exceptions:


ERROR 14:48:41,705 Exception encountered during startup
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
java.lang.IndexOutOfBoundsException: Index: 2, Size: 2Exception encountered during startup: Index: 2, Size: 2

	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionComparator(CFMetaData.java:1280)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:275)
	at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:158)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:535)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)


Actually it's the result of a slightly different problem in combination with composite columns, but i will describe this later.

I've got no idea, what the problem is, there might be some corruption in table schemas, even after dropping tables.

I have to delete cassandra data in order to get cassandra running again.

Best Regards 

Bert Passek",Lenny Squeeze,bertpassek,cherro,ggargani,goir,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/12 15:58;xedin;CASSANDRA-4420.patch;https://issues.apache.org/jira/secure/attachment/12535534/CASSANDRA-4420.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256142,,,Sun Jul 08 11:11:43 UTC 2012,,,,,,,,,,"0|i0gvo7:",96561,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Jul/12 15:58;xedin;DROP TABLE wasn't deleting ""component_index"" column.;;;","08/Jul/12 02:46;jbellis;+1;;;","08/Jul/12 11:11;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
leveled compaction generates too small sstables,CASSANDRA-4419,12597744,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,noa,noa,06/Jul/12 10:55,16/Apr/19 09:32,14/Jul/23 05:52,12/Jul/12 17:52,1.0.11,,,,,,0,,,,,,,"When I set sstable_size_in_mb to 96 I end up with sstable data files no larger than 60M.

This in turn messes up the LeveledManifest calculation since it finds compaction candidates by summing up the size of sstables at a particular level and comparing it to the configured size multiplied by the desired number of sstables at a level, resulting in ~20 sstables in level 1 instead of the 10 that one would expect from looking at LeveledManifest.

Some additional logging here reveals that the position parameter passed to LeveledCompactionTask.newSSTableSegmentThresholdReached() is significantly higher than the size of the output file.",,noa,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6698,,,,,,,,"11/Jul/12 08:48;slebresne;4419.txt;https://issues.apache.org/jira/secure/attachment/12536000/4419.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256141,,,Thu Jul 12 17:52:51 UTC 2012,,,,,,,,,,"0|i0gvnr:",96559,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Jul/12 13:41;slebresne;The problem is that compaction is considering uncompressed size for position (and thus cut segments at 96MB of uncompressed data) while the manifest consider compressed sizes for level sizes. This was fixed by CASSANDRA-4341 for cassandra-1.1, but I guess we might want to backport that part of the patch to 1.0.;;;","11/Jul/12 08:48;slebresne;Patch that extract the relevant part from CASSANDRA-4341.;;;","11/Jul/12 20:24;jbellis;+1;;;","12/Jul/12 17:52;slebresne;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion with LCS compaction,CASSANDRA-4411,12597524,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,05/Jul/12 04:52,16/Apr/19 09:32,14/Jul/23 05:52,16/Jul/12 15:33,1.1.3,1.1.4,,,,,2,lcs,,,,,,"As instructed in CASSANDRA-4321 I have raised this issue as a continuation of that issue as it appears the problem still exists.

I have repeatedly run sstablescrub across all my nodes after the 1.1.2 upgrade until sstablescrub shows no errors.  The exceptions described in CASSANDRA-4321 do not occur as frequently now but the integrity check still throws exceptions on a number of nodes.  Once those exceptions occur compactionstats shows a large number of pending tasks with no progression afterwards.

{code}
ERROR [CompactionExecutor:150] 2012-07-05 04:26:15,570 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:150,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
",,awinter,bryceg,minaguib,omid,pvelas,rvanderleeden,schnidrig,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/12 13:17;slebresne;0001-Add-debugging-info-for-LCS.txt;https://issues.apache.org/jira/secure/attachment/12536382/0001-Add-debugging-info-for-LCS.txt","09/Aug/12 16:27;omid;0001-Fix-off-by-one-for-out-of-order-and-overlapping-ssta.patch;https://issues.apache.org/jira/secure/attachment/12540058/0001-Fix-off-by-one-for-out-of-order-and-overlapping-ssta.patch","16/Jul/12 15:15;slebresne;4411-followup.txt;https://issues.apache.org/jira/secure/attachment/12536645/4411-followup.txt","16/Jul/12 11:27;slebresne;4411.txt;https://issues.apache.org/jira/secure/attachment/12536617/4411.txt","15/Jul/12 13:51;omid;assertion-w-more-debugging-info-omid.log;https://issues.apache.org/jira/secure/attachment/12536552/assertion-w-more-debugging-info-omid.log","14/Jul/12 15:04;rvanderleeden;assertion.moreinfo.system.log;https://issues.apache.org/jira/secure/attachment/12536509/assertion.moreinfo.system.log","11/Jul/12 12:44;schnidrig;system.log;https://issues.apache.org/jira/secure/attachment/12536021/system.log",,,,,,,,,,,7.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249306,,,Tue Aug 28 17:09:49 UTC 2012,,,,,,,,,,"0|i0a6qn:",57365,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Jul/12 06:24;schnidrig;I got the same AssertionError on a 1.1.2 version cluster which I did not upgrade from an earlier version.;;;","10/Jul/12 14:10;slebresne;Would one of your guys have the log leading to that exception? If you have it at DEBUG, even better.;;;","11/Jul/12 11:42;rvanderleeden;I could reproduce the problem on a 3-node testcluster with 1.1.2 and LCS.
Replication factor is 3 and number of total keys is 24m.
I added SSTables from a previous backup to node1.
Then running on node1:  nodetool repair -pr 
Result: 

 INFO [CompactionExecutor:7] 2012-07-11 10:06:57,632 CompactionTask.java (line 109) Compacting [SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-4937-Data.db')]
 INFO [CompactionExecutor:7] 2012-07-11 10:06:58,601 CompactionTask.java (line 221) Compacted to [/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5591-Data.db,].  5,252,617 to 5,252,617 (~100% of original) bytes for 51,419 keys at 5.174882MB/s.  Time: 968ms.
 INFO [CompactionExecutor:6] 2012-07-11 10:06:58,602 CompactionTask.java (line 109) Compacting [SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5590-Data.db'), SSTableReader(path='/mnt/cassandra/data/highscores/highscore/highscores-highscore-hd-5571-Data.db')]
ERROR [CompactionExecutor:6] 2012-07-11 10:06:59,655 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)


The next repair command throws the following assertion:

ERROR [ValidationExecutor:2] 2012-07-11 10:31:28,020 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.AssertionError: row DecoratedKey(162957119114255422766928006879345246467, c9e91cfb77634f32b9399dd4ad6b784e93dec9d0b11f431dad58a35e9f623de9) received out of order wrt DecoratedKey(165755005851296361665897424577644629314, ac63200da3fb452ca0b57a648b90c8a427a3d45b2d2146e089c6d04b959bb207)
	at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:349)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:712)
         ...etc...

Let me know if you need more from the log.
Thanks, -Rudolf.


;;;","11/Jul/12 12:44;schnidrig;Didn't have the old log, but I created a new keyspace and started using it until I hit the error. See attached file.

;;;","11/Jul/12 15:03;omid;I could also reproduce it from the data I had:

{code}
DEBUG 12:47:37,353 adding /home/omid/data/KSP/CF/KSP-CF-hd-121136 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121137 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121138 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121139 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121140 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121141 to list of files tracked for KSP.CF
DEBUG 12:47:37,354 adding /home/omid/data/KSP/CF/KSP-CF-hd-121142 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121143 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121144 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121145 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121146 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121147 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121148 to list of files tracked for KSP.CF
DEBUG 12:47:37,355 adding /home/omid/data/KSP/CF/KSP-CF-hd-121149 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121150 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121151 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121152 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121153 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121154 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121155 to list of files tracked for KSP.CF
DEBUG 12:47:37,356 adding /home/omid/data/KSP/CF/KSP-CF-hd-121156 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121157 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121158 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121159 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121160 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121161 to list of files tracked for KSP.CF
DEBUG 12:47:37,357 GC for ParNew: 14 ms for 1 collections, 5438330152 used; max is 8506048512
DEBUG 12:47:37,357 adding /home/omid/data/KSP/CF/KSP-CF-hd-121162 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121163 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121164 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121165 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121166 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121167 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121168 to list of files tracked for KSP.CF
DEBUG 12:47:37,358 adding /home/omid/data/KSP/CF/KSP-CF-hd-121169 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121170 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121171 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121172 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121173 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121174 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 adding /home/omid/data/KSP/CF/KSP-CF-hd-121175 to list of files tracked for KSP.CF
DEBUG 12:47:37,359 removing /home/omid/data/KSP/CF/KSP-CF-hd-121068 from list of files tracked for KSP.CF
DEBUG 12:47:37,360 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121068-Data.db compacted
DEBUG 12:47:37,360 All segments have been unmapped successfully
DEBUG 12:47:37,360 removing /home/omid/data/KSP/CF/KSP-CF-hd-120844 from list of files tracked for KSP.CF
DEBUG 12:47:37,360 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120844-Data.db compacted
DEBUG 12:47:37,360 All segments have been unmapped successfully
DEBUG 12:47:37,360 removing /home/omid/data/KSP/CF/KSP-CF-hd-121051 from list of files tracked for KSP.CF
DEBUG 12:47:37,361 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121051-Data.db compacted
DEBUG 12:47:37,361 All segments have been unmapped successfully
DEBUG 12:47:37,361 removing /home/omid/data/KSP/CF/KSP-CF-hd-120830 from list of files tracked for KSP.CF
DEBUG 12:47:37,361 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120830-Data.db compacted
DEBUG 12:47:37,361 All segments have been unmapped successfully
DEBUG 12:47:37,361 removing /home/omid/data/KSP/CF/KSP-CF-hd-121050 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121050-Data.db compacted
DEBUG 12:47:37,362 All segments have been unmapped successfully
DEBUG 12:47:37,362 removing /home/omid/data/KSP/CF/KSP-CF-hd-120843 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120843-Data.db compacted
DEBUG 12:47:37,362 All segments have been unmapped successfully
DEBUG 12:47:37,362 removing /home/omid/data/KSP/CF/KSP-CF-hd-120847 from list of files tracked for KSP.CF
DEBUG 12:47:37,362 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120847-Data.db compacted
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-CompressionInfo.db
DEBUG 12:47:37,363 All segments have been unmapped successfully
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Statistics.db
DEBUG 12:47:37,363 removing /home/omid/data/KSP/CF/KSP-CF-hd-120834 from list of files tracked for KSP.CF
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Index.db
DEBUG 12:47:37,363 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120834-Data.db compacted
DEBUG 12:47:37,363 Deleting KSP-CF-hd-121068-Filter.db
DEBUG 12:47:37,363 All segments have been unmapped successfully
DEBUG 12:47:37,363 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121068
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-120840 from list of files tracked for KSP.CF
DEBUG 12:47:37,364 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120840-Data.db compacted
DEBUG 12:47:37,364 All segments have been unmapped successfully
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-121047 from list of files tracked for KSP.CF
DEBUG 12:47:37,364 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121047-Data.db compacted
DEBUG 12:47:37,364 All segments have been unmapped successfully
DEBUG 12:47:37,364 removing /home/omid/data/KSP/CF/KSP-CF-hd-120816 from list of files tracked for KSP.CF
DEBUG 12:47:37,365 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120816-Data.db compacted
DEBUG 12:47:37,365 All segments have been unmapped successfully
DEBUG 12:47:37,365 removing /home/omid/data/KSP/CF/KSP-CF-hd-121133 from list of files tracked for KSP.CF
DEBUG 12:47:37,365 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121133-Data.db compacted
DEBUG 12:47:37,365 All segments have been unmapped successfully
DEBUG 12:47:37,365 removing /home/omid/data/KSP/CF/KSP-CF-hd-120821 from list of files tracked for KSP.CF
DEBUG 12:47:37,366 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120821-Data.db compacted
DEBUG 12:47:37,366 All segments have been unmapped successfully
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-CompressionInfo.db
DEBUG 12:47:37,366 removing /home/omid/data/KSP/CF/KSP-CF-hd-120831 from list of files tracked for KSP.CF
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Statistics.db
DEBUG 12:47:37,366 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120831-Data.db compacted
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Index.db
DEBUG 12:47:37,366 All segments have been unmapped successfully
DEBUG 12:47:37,366 Deleting KSP-CF-hd-120844-Filter.db
DEBUG 12:47:37,367 removing /home/omid/data/KSP/CF/KSP-CF-hd-120856 from list of files tracked for KSP.CF
DEBUG 12:47:37,367 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120844
DEBUG 12:47:37,367 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120856-Data.db compacted
DEBUG 12:47:37,367 All segments have been unmapped successfully
DEBUG 12:47:37,367 removing /home/omid/data/KSP/CF/KSP-CF-hd-120842 from list of files tracked for KSP.CF
DEBUG 12:47:37,367 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120842-Data.db compacted
DEBUG 12:47:37,367 All segments have been unmapped successfully
DEBUG 12:47:37,368 removing /home/omid/data/KSP/CF/KSP-CF-hd-120849 from list of files tracked for KSP.CF
DEBUG 12:47:37,368 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120849-Data.db compacted
DEBUG 12:47:37,368 All segments have been unmapped successfully
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-CompressionInfo.db
DEBUG 12:47:37,368 removing /home/omid/data/KSP/CF/KSP-CF-hd-120848 from list of files tracked for KSP.CF
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-Statistics.db
DEBUG 12:47:37,368 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120848-Data.db compacted
DEBUG 12:47:37,368 Deleting KSP-CF-hd-121051-Index.db
DEBUG 12:47:37,369 All segments have been unmapped successfully
DEBUG 12:47:37,369 Deleting KSP-CF-hd-121051-Filter.db
DEBUG 12:47:37,369 removing /home/omid/data/KSP/CF/KSP-CF-hd-120851 from list of files tracked for KSP.CF
DEBUG 12:47:37,369 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121051
DEBUG 12:47:37,369 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120851-Data.db compacted
DEBUG 12:47:37,369 All segments have been unmapped successfully
DEBUG 12:47:37,369 removing /home/omid/data/KSP/CF/KSP-CF-hd-120855 from list of files tracked for KSP.CF
DEBUG 12:47:37,369 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120855-Data.db compacted
DEBUG 12:47:37,370 All segments have been unmapped successfully
DEBUG 12:47:37,370 removing /home/omid/data/KSP/CF/KSP-CF-hd-120815 from list of files tracked for KSP.CF
DEBUG 12:47:37,370 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120815-Data.db compacted
DEBUG 12:47:37,370 All segments have been unmapped successfully
DEBUG 12:47:37,370 removing /home/omid/data/KSP/CF/KSP-CF-hd-120836 from list of files tracked for KSP.CF
DEBUG 12:47:37,370 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120836-Data.db compacted
DEBUG 12:47:37,371 All segments have been unmapped successfully
DEBUG 12:47:37,371 removing /home/omid/data/KSP/CF/KSP-CF-hd-120841 from list of files tracked for KSP.CF
DEBUG 12:47:37,371 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120841-Data.db compacted
DEBUG 12:47:37,371 All segments have been unmapped successfully
DEBUG 12:47:37,371 removing /home/omid/data/KSP/CF/KSP-CF-hd-120814 from list of files tracked for KSP.CF
DEBUG 12:47:37,371 Deleting KSP-CF-hd-120830-CompressionInfo.db
DEBUG 12:47:37,371 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120814-Data.db compacted
DEBUG 12:47:37,371 Deleting KSP-CF-hd-120830-Statistics.db
DEBUG 12:47:37,372 All segments have been unmapped successfully
DEBUG 12:47:37,372 Deleting KSP-CF-hd-120830-Index.db
DEBUG 12:47:37,372 removing /home/omid/data/KSP/CF/KSP-CF-hd-120819 from list of files tracked for KSP.CF
DEBUG 12:47:37,372 Deleting KSP-CF-hd-120830-Filter.db
DEBUG 12:47:37,372 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120819-Data.db compacted
DEBUG 12:47:37,372 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120830
DEBUG 12:47:37,372 All segments have been unmapped successfully
DEBUG 12:47:37,372 removing /home/omid/data/KSP/CF/KSP-CF-hd-121048 from list of files tracked for KSP.CF
DEBUG 12:47:37,372 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121048-Data.db compacted
DEBUG 12:47:37,373 All segments have been unmapped successfully
DEBUG 12:47:37,373 removing /home/omid/data/KSP/CF/KSP-CF-hd-120860 from list of files tracked for KSP.CF
DEBUG 12:47:37,373 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120860-Data.db compacted
DEBUG 12:47:37,373 All segments have been unmapped successfully
DEBUG 12:47:37,373 removing /home/omid/data/KSP/CF/KSP-CF-hd-120818 from list of files tracked for KSP.CF
DEBUG 12:47:37,373 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120818-Data.db compacted
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,374 removing /home/omid/data/KSP/CF/KSP-CF-hd-120832 from list of files tracked for KSP.CF
DEBUG 12:47:37,374 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120832-Data.db compacted
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,374 removing /home/omid/data/KSP/CF/KSP-CF-hd-120846 from list of files tracked for KSP.CF
DEBUG 12:47:37,374 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120846-Data.db compacted
DEBUG 12:47:37,374 Deleting KSP-CF-hd-121050-CompressionInfo.db
DEBUG 12:47:37,374 All segments have been unmapped successfully
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Statistics.db
DEBUG 12:47:37,375 removing /home/omid/data/KSP/CF/KSP-CF-hd-120853 from list of files tracked for KSP.CF
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Index.db
DEBUG 12:47:37,375 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120853-Data.db compacted
DEBUG 12:47:37,375 Deleting KSP-CF-hd-121050-Filter.db
DEBUG 12:47:37,375 All segments have been unmapped successfully
DEBUG 12:47:37,375 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121050
DEBUG 12:47:37,375 removing /home/omid/data/KSP/CF/KSP-CF-hd-120857 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120857-Data.db compacted
DEBUG 12:47:37,376 All segments have been unmapped successfully
DEBUG 12:47:37,376 removing /home/omid/data/KSP/CF/KSP-CF-hd-120854 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120854-Data.db compacted
DEBUG 12:47:37,376 All segments have been unmapped successfully
DEBUG 12:47:37,376 removing /home/omid/data/KSP/CF/KSP-CF-hd-120850 from list of files tracked for KSP.CF
DEBUG 12:47:37,376 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120850-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,377 removing /home/omid/data/KSP/CF/KSP-CF-hd-120820 from list of files tracked for KSP.CF
DEBUG 12:47:37,377 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120820-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,377 removing /home/omid/data/KSP/CF/KSP-CF-hd-121049 from list of files tracked for KSP.CF
DEBUG 12:47:37,377 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121049-Data.db compacted
DEBUG 12:47:37,377 All segments have been unmapped successfully
DEBUG 12:47:37,378 removing /home/omid/data/KSP/CF/KSP-CF-hd-120817 from list of files tracked for KSP.CF
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-CompressionInfo.db
DEBUG 12:47:37,378 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120817-Data.db compacted
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Statistics.db
DEBUG 12:47:37,378 All segments have been unmapped successfully
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Index.db
DEBUG 12:47:37,378 removing /home/omid/data/KSP/CF/KSP-CF-hd-121123 from list of files tracked for KSP.CF
DEBUG 12:47:37,378 Deleting KSP-CF-hd-120843-Filter.db
DEBUG 12:47:37,378 Marking /home/omid/data/KSP/CF/KSP-CF-hd-121123-Data.db compacted
DEBUG 12:47:37,379 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120843
DEBUG 12:47:37,379 All segments have been unmapped successfully
DEBUG 12:47:37,379 removing /home/omid/data/KSP/CF/KSP-CF-hd-120845 from list of files tracked for KSP.CF
DEBUG 12:47:37,379 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120845-Data.db compacted
DEBUG 12:47:37,379 All segments have been unmapped successfully
DEBUG 12:47:37,379 removing /home/omid/data/KSP/CF/KSP-CF-hd-120823 from list of files tracked for KSP.CF
DEBUG 12:47:37,379 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120823-Data.db compacted
DEBUG 12:47:37,380 All segments have been unmapped successfully
DEBUG 12:47:37,380 removing /home/omid/data/KSP/CF/KSP-CF-hd-120838 from list of files tracked for KSP.CF
DEBUG 12:47:37,380 Marking /home/omid/data/KSP/CF/KSP-CF-hd-120838-Data.db compacted
DEBUG 12:47:37,380 All segments have been unmapped successfully
DEBUG 12:47:37,381 L0 contains 6413 SSTables (12319161290 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-CompressionInfo.db
DEBUG 12:47:37,381 L1 contains 15 SSTables (145544851 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-Statistics.db
DEBUG 12:47:37,381 L2 contains 108 SSTables (1045242822 bytes) in Manifest@1395029065
DEBUG 12:47:37,381 Deleting KSP-CF-hd-120847-Index.db
DEBUG 12:47:37,381 L3 contains 212 SSTables (2066259940 bytes) in Manifest@1395029065
DEBUG 12:47:37,382 Replacing [CF-121068(L2), CF-120844(L2), CF-121051(L2), CF-120830(L2), CF-121050(L2), CF-120843(L2), CF-120847(L2), CF-120834(L2), CF-120840(L2), CF-121047(L2), CF-120816(L2), CF-121133(L2), CF-120821(L2), CF-120831(L2), CF-120856(L2), CF-120842(L2), CF-120849(L2), CF-120848(L2), CF-120851(L2), CF-120855(L2), CF-120815(L2), CF-120836(L2), CF-120841(L2), CF-120814(L2), CF-120819(L2), CF-121048(L2), CF-120860(L2), CF-120818(L2), CF-120832(L2), CF-120846(L2), CF-120853(L2), CF-120857(L2), CF-120854(L2), CF-120850(L2), CF-120820(L2), CF-121049(L2), CF-120817(L2), CF-121123(L1), CF-120845(L2), CF-120823(L2), CF-120838(L2), ]
DEBUG 12:47:37,382 Deleting KSP-CF-hd-120847-Filter.db
DEBUG 12:47:37,382 Adding [CF-121136(L-1), CF-121137(L-1), CF-121138(L-1), CF-121139(L-1), CF-121140(L-1), CF-121141(L-1), CF-121142(L-1), CF-121143(L-1), CF-121144(L-1), CF-121145(L-1), CF-121146(L-1), CF-121147(L-1), CF-121148(L-1), CF-121149(L-1), CF-121150(L-1), CF-121151(L-1), CF-121152(L-1), CF-121153(L-1), CF-121154(L-1), CF-121155(L-1), CF-121156(L-1), CF-121157(L-1), CF-121158(L-1), CF-121159(L-1), CF-121160(L-1), CF-121161(L-1), CF-121162(L-1), CF-121163(L-1), CF-121164(L-1), CF-121165(L-1), CF-121166(L-1), CF-121167(L-1), CF-121168(L-1), CF-121169(L-1), CF-121170(L-1), CF-121171(L-1), CF-121172(L-1), CF-121173(L-1), CF-121174(L-1), CF-121175(L-1), ] at L2
DEBUG 12:47:37,382 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120847
ERROR 12:47:37,383 Exception in thread Thread[CompactionExecutor:577,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
DEBUG 12:47:37,384 Deleting KSP-CF-hd-120834-CompressionInfo.db
DEBUG 12:47:37,385 Deleting KSP-CF-hd-120834-Statistics.db
DEBUG 12:47:37,385 Deleting KSP-CF-hd-120834-Index.db
DEBUG 12:47:37,386 Deleting KSP-CF-hd-120834-Filter.db
DEBUG 12:47:37,386 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120834
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-CompressionInfo.db
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-Statistics.db
DEBUG 12:47:37,388 Deleting KSP-CF-hd-120840-Index.db
DEBUG 12:47:37,389 Deleting KSP-CF-hd-120840-Filter.db
DEBUG 12:47:37,389 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120840
DEBUG 12:47:37,391 Deleting KSP-CF-hd-121047-CompressionInfo.db
DEBUG 12:47:37,391 Deleting KSP-CF-hd-121047-Statistics.db
DEBUG 12:47:37,392 Deleting KSP-CF-hd-121047-Index.db
DEBUG 12:47:37,392 Deleting KSP-CF-hd-121047-Filter.db
DEBUG 12:47:37,392 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121047
DEBUG 12:47:37,394 Deleting KSP-CF-hd-120816-CompressionInfo.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Statistics.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Index.db
DEBUG 12:47:37,395 Deleting KSP-CF-hd-120816-Filter.db
DEBUG 12:47:37,395 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120816
DEBUG 12:47:37,395 Deleting KSP-CF-hd-121133-CompressionInfo.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Statistics.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Index.db
DEBUG 12:47:37,396 Deleting KSP-CF-hd-121133-Filter.db
DEBUG 12:47:37,396 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121133
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-CompressionInfo.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Statistics.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Index.db
DEBUG 12:47:37,399 Deleting KSP-CF-hd-120821-Filter.db
DEBUG 12:47:37,399 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120821
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-CompressionInfo.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Statistics.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Index.db
DEBUG 12:47:37,402 Deleting KSP-CF-hd-120831-Filter.db
DEBUG 12:47:37,402 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120831
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-CompressionInfo.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Statistics.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Index.db
DEBUG 12:47:37,405 Deleting KSP-CF-hd-120856-Filter.db
DEBUG 12:47:37,406 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120856
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-CompressionInfo.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Statistics.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Index.db
DEBUG 12:47:37,408 Deleting KSP-CF-hd-120842-Filter.db
DEBUG 12:47:37,409 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120842
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-CompressionInfo.db
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-Statistics.db
DEBUG 12:47:37,411 Deleting KSP-CF-hd-120849-Index.db
DEBUG 12:47:37,412 Deleting KSP-CF-hd-120849-Filter.db
DEBUG 12:47:37,412 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120849
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-CompressionInfo.db
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-Statistics.db
DEBUG 12:47:37,414 Deleting KSP-CF-hd-120848-Index.db
DEBUG 12:47:37,415 Deleting KSP-CF-hd-120848-Filter.db
DEBUG 12:47:37,415 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120848
DEBUG 12:47:37,417 Deleting KSP-CF-hd-120851-CompressionInfo.db
DEBUG 12:47:37,417 Deleting KSP-CF-hd-120851-Statistics.db
DEBUG 12:47:37,418 Deleting KSP-CF-hd-120851-Index.db
DEBUG 12:47:37,418 Deleting KSP-CF-hd-120851-Filter.db
DEBUG 12:47:37,418 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120851
DEBUG 12:47:37,420 Deleting KSP-CF-hd-120855-CompressionInfo.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Statistics.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Index.db
DEBUG 12:47:37,421 Deleting KSP-CF-hd-120855-Filter.db
DEBUG 12:47:37,421 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120855
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-CompressionInfo.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Statistics.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Index.db
DEBUG 12:47:37,424 Deleting KSP-CF-hd-120815-Filter.db
DEBUG 12:47:37,424 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120815
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-CompressionInfo.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Statistics.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Index.db
DEBUG 12:47:37,427 Deleting KSP-CF-hd-120836-Filter.db
DEBUG 12:47:37,427 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120836
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-CompressionInfo.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Statistics.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Index.db
DEBUG 12:47:37,430 Deleting KSP-CF-hd-120841-Filter.db
DEBUG 12:47:37,430 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120841
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-CompressionInfo.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Statistics.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Index.db
DEBUG 12:47:37,433 Deleting KSP-CF-hd-120814-Filter.db
DEBUG 12:47:37,433 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120814
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-CompressionInfo.db
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-Statistics.db
DEBUG 12:47:37,436 Deleting KSP-CF-hd-120819-Index.db
DEBUG 12:47:37,437 Deleting KSP-CF-hd-120819-Filter.db
DEBUG 12:47:37,437 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120819
DEBUG 12:47:37,439 Deleting KSP-CF-hd-121048-CompressionInfo.db
DEBUG 12:47:37,439 Deleting KSP-CF-hd-121048-Statistics.db
DEBUG 12:47:37,440 Deleting KSP-CF-hd-121048-Index.db
DEBUG 12:47:37,440 Deleting KSP-CF-hd-121048-Filter.db
DEBUG 12:47:37,440 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121048
DEBUG 12:47:37,442 Deleting KSP-CF-hd-120860-CompressionInfo.db
DEBUG 12:47:37,442 Deleting KSP-CF-hd-120860-Statistics.db
DEBUG 12:47:37,443 Deleting KSP-CF-hd-120860-Index.db
DEBUG 12:47:37,443 Deleting KSP-CF-hd-120860-Filter.db
DEBUG 12:47:37,443 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120860
DEBUG 12:47:37,445 Deleting KSP-CF-hd-120818-CompressionInfo.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Statistics.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Index.db
DEBUG 12:47:37,446 Deleting KSP-CF-hd-120818-Filter.db
DEBUG 12:47:37,446 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120818
DEBUG 12:47:37,448 Deleting KSP-CF-hd-120832-CompressionInfo.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Statistics.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Index.db
DEBUG 12:47:37,449 Deleting KSP-CF-hd-120832-Filter.db
DEBUG 12:47:37,449 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120832
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-CompressionInfo.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Statistics.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Index.db
DEBUG 12:47:37,452 Deleting KSP-CF-hd-120846-Filter.db
DEBUG 12:47:37,452 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120846
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-CompressionInfo.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Statistics.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Index.db
DEBUG 12:47:37,455 Deleting KSP-CF-hd-120853-Filter.db
DEBUG 12:47:37,456 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120853
DEBUG 12:47:37,458 Deleting KSP-CF-hd-120857-CompressionInfo.db
DEBUG 12:47:37,458 Deleting KSP-CF-hd-120857-Statistics.db
DEBUG 12:47:37,459 Deleting KSP-CF-hd-120857-Index.db
DEBUG 12:47:37,459 Deleting KSP-CF-hd-120857-Filter.db
DEBUG 12:47:37,459 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120857
DEBUG 12:47:37,461 Deleting KSP-CF-hd-120854-CompressionInfo.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Statistics.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Index.db
DEBUG 12:47:37,462 Deleting KSP-CF-hd-120854-Filter.db
DEBUG 12:47:37,462 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120854
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-CompressionInfo.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Statistics.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Index.db
DEBUG 12:47:37,465 Deleting KSP-CF-hd-120850-Filter.db
DEBUG 12:47:37,465 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120850
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-CompressionInfo.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Statistics.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Index.db
DEBUG 12:47:37,468 Deleting KSP-CF-hd-120820-Filter.db
DEBUG 12:47:37,469 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120820
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-CompressionInfo.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Statistics.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Index.db
DEBUG 12:47:37,471 Deleting KSP-CF-hd-121049-Filter.db
DEBUG 12:47:37,472 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121049
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-CompressionInfo.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Statistics.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Index.db
DEBUG 12:47:37,474 Deleting KSP-CF-hd-120817-Filter.db
DEBUG 12:47:37,475 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120817
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-CompressionInfo.db
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-Statistics.db
DEBUG 12:47:37,477 Deleting KSP-CF-hd-121123-Index.db
DEBUG 12:47:37,478 Deleting KSP-CF-hd-121123-Filter.db
DEBUG 12:47:37,478 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-121123
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-CompressionInfo.db
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-Statistics.db
DEBUG 12:47:37,480 Deleting KSP-CF-hd-120845-Index.db
DEBUG 12:47:37,481 Deleting KSP-CF-hd-120845-Filter.db
DEBUG 12:47:37,481 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120845
DEBUG 12:47:37,483 Deleting KSP-CF-hd-120823-CompressionInfo.db
DEBUG 12:47:37,483 Deleting KSP-CF-hd-120823-Statistics.db
DEBUG 12:47:37,484 Deleting KSP-CF-hd-120823-Index.db
DEBUG 12:47:37,484 Deleting KSP-CF-hd-120823-Filter.db
DEBUG 12:47:37,484 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120823
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-CompressionInfo.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Statistics.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Index.db
DEBUG 12:47:37,487 Deleting KSP-CF-hd-120838-Filter.db
DEBUG 12:47:37,487 Deleted /home/omid/data/KSP/CF/KSP-CF-hd-120838
{code};;;","12/Jul/12 10:50;rvanderleeden;I attached the file assertion.system.log with DEBUG lines around the assertion ERROR.
This is just an excerpt from system.log which is much larger.;;;","13/Jul/12 13:17;slebresne;Unfortunately the log doesn't give us much to chew on.

I've actually be able to reproduce this once (using stress). Unfortunately, I hadn't added more debugging yet and since I added more debug info I haven't been able to reproduce (despite having retried from scratch like 3 times letting it run for multiple hours each time).

So I'm attaching a simple patch that adds more debugging. If you guys can try applying the patch and see if you can reproduce. If so, the log file produced should be helpful. I'll note that it is preferable to *not* turn DEBUG logging with this patch as this is not useful and would only generate awfully large logs.
;;;","14/Jul/12 15:04;rvanderleeden;includes extended INFO lines from using 0001 patch;;;","14/Jul/12 15:18;rvanderleeden;The file assertion.moreinfo.system.log has been attached. 

The assertion ERROR could be reproduced by doing the following:
(1) Bulk load 200 SSTables from a snapshot into a new 3-node cluster
(2) Run nodetool compact and repair
(3) Add ~500 SSTables from the same snapshot
(4) Run nodetool repair

RESULT: 
- we see immediately a StackOverflowError (from the repair command)
- after 2 minutes compaction starts
- after 11 minutes there is the AssertionError
;;;","15/Jul/12 13:51;omid;Attached the log (assertion-w-more-debugging-info-omid.log) with more debugging info that leads to AssertionError on LeveledManifest.promote.;;;","16/Jul/12 11:27;slebresne;Thanks a lot for the logs everyone.

The problem was with the handling of sstables having the same first and last token. More precisely, Bounds.contains uses Range.contains(), but while the Range (a, a] selects the whole ring, the Bounds [a,a] only selects a. This means sstable with the same first and last on level N+1 were included in compaction of any sstable of level N even when there wasn't any intersection.

Attaching simple patch to fix.;;;","16/Jul/12 12:33;jbellis;+1;;;","16/Jul/12 13:35;slebresne;Committed, thanks.;;;","16/Jul/12 15:15;slebresne;Alright, the patch did fix the issue but also introduce a small bug in that the Bounds.contains does not handle correctly the special Bounds(min, min) that should include everything (rather than nothing). Patch attached to fix that.;;;","16/Jul/12 15:23;jbellis;+1;;;","16/Jul/12 15:33;slebresne;Alright, committed, thanks.;;;","18/Jul/12 12:14;omid;Awesome. Thanks for the patch. Tested it and it works.

Sylvain, regarding your earlier comment on CASSANDRA-4321:

{quote} This is not really a new bug, but I believe that prior to CASSANDRA-4142, this had less consequences. {quote}

Does it mean LC-compacted SSTables created by 1.1.0 or earlier are as well affected and need to be scrubbed?;;;","18/Jul/12 16:42;slebresne;bq. Does it mean LC-compacted SSTables created by 1.1.0 or earlier are as well affected and need to be scrubbed?

Potentially, yes, unfortunately.;;;","23/Jul/12 18:14;minaguib;Hi

I'm running cassandra 1.1.2 + the 2 patches in this ticket applied.

On one node in my cluster I just got the same assertion error - shortly after the node started up and a compaction was attempted:

    ERROR [CompactionExecutor:19] 2012-07-23 14:05:43,312 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:19,1,main]
    java.lang.AssertionError
            at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
            at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
            at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
            at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
            at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
            at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
            at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
            at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
            at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
            at java.util.concurrent.FutureTask.run(FutureTask.java:138)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:662)


Does this indicate a problem with a bad sstable that's fixable with scrubbing ?  Or does this ticket + patches deserve a second look ?;;;","23/Jul/12 18:18;jbellis;The former.;;;","24/Jul/12 00:14;minaguib;
Unfortunately the problem did not go away after scrubbing.

I scrubbed 2 of the problematic nodes.  Immediately after the scrub (5 hours) finished, a compaction was attempted and again failed:

{code}
ERROR [CompactionExecutor:47] 2012-07-23 19:48:52,500 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:47,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

I've verified (using ls and the timestamps of *-Data.db sstables) that there are no old sstables and all of the sstables in the CF are the ones generated during the 5 hours of scrubbing.

I've also stopped and restarted one of these nodes, and again shortly after restart the compaction failed with a different stack trace:

{code}
java.lang.RuntimeException: Last written key DecoratedKey(225595347341523546110318866012608496, 64313635626665302d333764372d313165302d393933622d303032366239333763386531) >= current key DecoratedKey(221078382620949716286900834756484795, 37303538643361662d616362662d343030312d313565382d633662303030303030336131) writing into /var/lib/cassandra/data/MYKS/MYCF/MYKS-MYCF-tmp-hd-520277-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Is the fix for this ticket contained in some other code beyond the 2 posted patches (4411.txt, 4411-followup.txt) ?  That's what I'm running with on top of 1.1.2.
;;;","24/Jul/12 00:37;awinter;I can also confirm that after multiple offline sstablescrubs across all nodes that I still had several nodes (but not all) spread across multiple DC's still exhibiting this problem as described above by Mina.  

In an attempt to work around the problem I shut down the affected instances, deleted all data and re-bootstrapped them as if they were dead nodes.  Since doing so I haven't had the problem return however it is still early days.;;;","24/Jul/12 07:14;slebresne;@Mina Did you run the offline scrub introduced with CASSANDRA-4321. Otherwise, it won't fix the problem. So you need to 1) shut down the node (this is important before running the offline scrub) and 2) run ./bin/sstablescrub. That last step should print some lines indicating having corrected some problems (otherwise, something is wrong with the scrubbing).

If after that you still get an exception, it might be helpful if you could run with 0001-Add-debugging-info-for-LCS.txt applied.;;;","24/Jul/12 12:20;minaguib;I ran the scrub in online mode.

I just took down a node and am now running it in offline mode.  Will report back.

BTW, the default ""sstablescrub"" does not respect the memory limits set in cassandra.in.sh, so it failed for me with:
{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at sun.security.provider.DigestBase.engineDigest(DigestBase.java:146)
        at java.security.MessageDigest$Delegate.engineDigest(MessageDigest.java:546)
        at java.security.MessageDigest.digest(MessageDigest.java:323)
        at org.apache.cassandra.utils.FBUtilities.hash(FBUtilities.java:229)
        at org.apache.cassandra.utils.FBUtilities.hashToBigInteger(FBUtilities.java:213)
        at org.apache.cassandra.dht.RandomPartitioner.getToken(RandomPartitioner.java:154)
        at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:47)
        at org.apache.cassandra.cache.AutoSavingCache.readSaved(AutoSavingCache.java:118)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:230)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:341)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:313)
        at org.apache.cassandra.db.Table.initCf(Table.java:371)
        at org.apache.cassandra.db.Table.<init>(Table.java:304)
        at org.apache.cassandra.db.Table.open(Table.java:119)
        at org.apache.cassandra.db.Table.openWithoutSSTables(Table.java:102)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:65)
{code}

I edited it to update the hardocded limit of 256MB to a more reasonable value (the same as my cassandra.in.sh) to allow it to run without crashing.

;;;","24/Jul/12 15:43;minaguib;Things appear better after an offline scrub.  While the scrubbing itself was uneventful, at the very end it did ""Checking leveled manifest"", found 14 sstables in level 3 and level 4 that were problematic and moved them back to level 0.

I started the node back up and all (+/- 10) compactions ran successfully.

I'll keep an eye on it and if it stays well I'll do the same to the other nodes.  Perhaps I'll try my luck with sstablescrub --manifest-check to see if I can keep the downtime to a minimum.;;;","26/Jul/12 14:33;minaguib;Quick follow-up

All the problematic nodes have been offline scrubbed (successfully using --manifest-check to speed things up).  There are no more compaction errors / pending compactions.

Like Anton, I'm a bit weary and keeping an eye on things - but so far so good.

On a tangent, it occurred to me that the amount of time it takes to run ( sstablescrub --manifest-check ) is mostly reading the sstables - the check itself and demoting the bad sstables to L0 appears very cheap - would it be a good idea to perform that check automatically on cassandra startup (after the sstables have been read) ?  It *may* be a quick fix for 1.1.3 to help people out who have been bitten by this but don't know it yet.

;;;","06/Aug/12 16:55;omid;@Mina were the sstables created after CASSANDRA-4321 patch? Otherwise offline-scrub with --manifest-check is unlikely to solve the problem (or at least I don't understand how) since there would still be out-of-order sstables existing.;;;","08/Aug/12 18:31;omid;I can confirm that the problem is still there. I offline-scrubbed using 1.1.3 (sstables were generated by 1.1.0) , but the scrubber did not report any out-of-order sstables, but sent some sstables back to L0. On compaction though, I get the exception:

{quote}
2012-08-08_18:15:41.85260 java.lang.RuntimeException: Last written key DecoratedKey(135076574692378869287086649376333921820, SOME_KEY_1) >= current key DecoratedKey(135076574692378869287086649376333921820, SOME_KEY_1) writing into /var/lib/cassandra/abcd/data/KSP/CF1/KSP-CF1-tmp-he-178793-Data.db
2012-08-08_18:15:41.85303 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-08-08_18:15:41.85314 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-08-08_18:15:41.85326 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-08-08_18:15:41.85338 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-08-08_18:15:41.85351 	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:288)
2012-08-08_18:15:41.85364 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-08-08_18:15:41.85375 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-08-08_18:15:41.85385 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-08-08_18:15:41.85395 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-08-08_18:15:41.85403 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-08-08_18:15:41.85414 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-08-08_18:15:41.85424 	at java.lang.Thread.run(Unknown Source)
{quote}

;;;","08/Aug/12 18:56;omid;Not sure, but from the fact that the two keys are identical and the ""Last written key""-check checks for greater-or-equal, shouldn't the "">"" be "">="" in:

https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/Scrubber.java#L178

?;;;","08/Aug/12 21:53;jbellis;I think you're right.  I'll push a fix for that shortly.;;;","09/Aug/12 14:43;jbellis;done in 115f380a86912e5918f534db2ec2935253909fad;;;","09/Aug/12 16:27;omid;Thanks!

There are two more off-by-ones. 

One is the Scrubber to detect out-of-order keys (similar to the one already patched). 

The other one is in manifestCheck to send overlapping sstables back to L0 (which has causes assertion errors in LeveledManifest::promote)

Patch is attached.;;;","09/Aug/12 16:40;jbellis;committed, thanks!;;;","28/Aug/12 17:09;jbellis;Updating to note that this was ""mostly"" fixed in 1.1.3, with Omid's extra >= fixes in 1.1.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Only consider whole row tombstone in collation controller,CASSANDRA-4409,12597426,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/Jul/12 09:55,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/12 17:36,1.2.0 beta 1,,,,,,0,,,,,,,"CollationController has that optimization that if it has already seen a row tombstone more recent that a sstable max timestamp, it skips the sstable.  However, this was not updated correctly while introducing range tombstone and currently the code might skip a sstable based on the timestamp of a tombstone that does not cover the full row.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/12 09:58;slebresne;0001-Use-only-top-level-row-deletion-to-avoid-sstable-durin.txt;https://issues.apache.org/jira/secure/attachment/12535065/0001-Use-only-top-level-row-deletion-to-avoid-sstable-durin.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256136,,,Wed Jul 04 17:36:04 UTC 2012,,,,,,,,,,"0|i0gvkf:",96544,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/Jul/12 09:58;slebresne;Patch attached to fix. I will note that in practice this was not really a bug because the deletionInfo used were those of the columnIterator, and at that point those deletionInfo should only contain whole row tombstone, not range tombstone. Yet, the code was misleading and could have trigger a bug if the code change underneath.;;;","04/Jul/12 15:28;jbellis;+1;;;","04/Jul/12 17:36;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tombstone estimation needs to avoid using global partitioner against index sstables,CASSANDRA-4404,12596837,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,jbellis,jbellis,02/Jul/12 22:26,16/Apr/19 09:32,14/Jul/23 05:52,03/Jul/12 19:37,1.2.0 beta 1,,,,,,0,compaction,,,,,,,,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256131,,,Wed Jul 04 23:16:47 UTC 2012,,,,,,,,,,"0|i0gvif:",96535,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Jul/12 19:37;yukim;Patch is available at https://github.com/yukim/cassandra/commit/1644a5b701b054b646e049ef5cf725b2b2670709.diff
Reviewed and committed while JIRA is down.;;;","04/Jul/12 23:16;hudson;Integrated in Cassandra #1646 (See [https://builds.apache.org/job/Cassandra/1646/])
    use proper partitioner for Range; patch by yukim, reviewed by jbellis for CASSANDRA-4404 (Revision d2b60f28935466f6e37fc9d64a44c5c81bc14fb4)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup uses global partitioner to estimate ranges in index sstables,CASSANDRA-4403,12596836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Jul/12 22:19,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/12 02:29,1.1.3,1.2.0 beta 1,,,,,0,compaction,,,,,,"Introduced in CASSANDRA-1404, CleanupTest is showing this on trunk (on stderr, so test doesn't actually fail):

{noformat}
    [junit] java.lang.ClassCastException: org.apache.cassandra.dht.Token$KeyBound cannot be cast to org.apache.cassandra.dht.Token
    [junit]     at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:24)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:386)
    [junit]     at org.apache.cassandra.dht.Range$1.compare(Range.java:383)
    [junit]     at java.util.Arrays.mergeSort(Arrays.java:1270)
    [junit]     at java.util.Arrays.sort(Arrays.java:1210)
    [junit]     at java.util.Collections.sort(Collections.java:159)
    [junit]     at org.apache.cassandra.dht.Range.normalize(Range.java:382)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:570)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:549)
    [junit]     at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:111)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:136)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't happen on the 1.1 branch (less robust test?) but the problem is still there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/12 22:25;jbellis;4403.txt;https://issues.apache.org/jira/secure/attachment/12534468/4403.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256130,,,Wed Jul 04 02:29:05 UTC 2012,,,,,,,,,,"0|i0gvhz:",96533,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"02/Jul/12 22:25;jbellis;Actually, looks like there are two places to fix: one in getExpectedCompactedFileSize that I've attached a fix for, and another in STCS from CASSANDRA-4022.  Not sure off the top of my head how to fix that one so I'll leave to Yuki. :);;;","02/Jul/12 22:27;jbellis;Decided to split the second part out to CASSANDRA-4404 since it affects only 1.2.;;;","04/Jul/12 02:29;yukim;Reviewed and committed during today's JIRA crisis. Closing as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Atomicity violation bugs because of misusing concurrent collections,CASSANDRA-4402,12596739,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jacklondongood,jacklondongood,jacklondongood,02/Jul/12 20:40,16/Apr/19 09:32,14/Jul/23 05:52,09/Nov/12 22:09,1.2.0,,,,,,0,gossip,,,,,,"My name is Yu Lin. I'm a Ph.D. student in the CS department at
UIUC. I'm currently doing research on mining Java concurrent library
misusages. I found some misusages of ConcurrentHashMap in Cassandra
1.1.1, which may result in potential atomicity violation bugs or harm
the performance.

The code below is a snapshot of the code in file
src/java/org/apache/cassandra/db/Table.java from line 348 to 369

L348        if (columnFamilyStores.containsKey(cfId))
L349        {
L350            // this is the case when you reset local schema
L351            // just reload metadata
L352            ColumnFamilyStore cfs = columnFamilyStores.get(cfId);
L353            assert cfs.getColumnFamilyName().equals(cfName);
            ...
L364        }
L365        else
L366        {
L367            columnFamilyStores.put(cfId, ColumnFamilyStore.createColumnFamilyStore(this, cfName));
L368        }

In the code above, an atomicity violation may occur between line 348
and 352. Suppose thread T1 executes line 348 and finds that the
concurrent hashmap ""columnFamilyStores"" contains the key
""cfId"". Before thread T1 executes line 352, another thread T2 removes
the ""cfId"" key from ""columnFamilyStores"". Now thread T1 resumes
execution at line 352 and will get a null value for ""cfs"". Then the
next line will throw a NullPointerException when invoking the method
on ""cfs"".

Second, the snapshot above has another atomicity violation. Let's look
at lines 348 and 367. Suppose a thread T1 executes line 348 and finds
out the concurrent hashmap does not contain the key ""cfId"". Before it
gets to execute line 367, another thread T2 puts a pair <cfid, v> in
the concurrent hashmap ""columnFamilyStores"". Now thread T1 resumes
execution and it will overwrite the value written by thread T2. Thus,
the code no longer preserves the ""put-if-absent"" semantics.

I found some similar misusages in other files:

In src/java/org/apache/cassandra/gms/Gossiper.java, similar atomicity
violation may occur if thread T2 puts a value to map
""endpointStateMap"" between lines <1094 and 1099>, <1173 and 1178>. Another
atomicity violation may occur if thread T2 removes the value on key
""endpoint"" between lines <681 and 683>.
",,jacklondongood,jeromatron,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,"02/Jul/12 23:03;jbellis;4402-v2.txt;https://issues.apache.org/jira/secure/attachment/12534477/4402-v2.txt","02/Jul/12 22:21;jacklondongood;cassandra-1.1.1-4402.txt;https://issues.apache.org/jira/secure/attachment/12534467/cassandra-1.1.1-4402.txt",,,,,,,,,,,,,,,,2.0,jacklondongood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255954,,,Fri Nov 09 22:11:25 UTC 2012,,,,,,,,,,"0|i0fw9z:",90827,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Jul/12 22:15;jacklondongood;This is the patch that may fix the atomicity violation problem.;;;","02/Jul/12 23:03;jbellis;Thanks for the patch, Yu.  The getExpireTimeForEndpoint is the most serious since we can fairly easily have concurrent contains with remove calls.  maybeInitializeLocalState shouldn't be called concurrently but it doesn't hurt to clean that up too.

I'm not sure what to do about Table.initCf, though.  It seems like CASSANDRA-2963 has made that inherently unsafe.  A ""mechanical"" fix like the one here won't help since if two CFS objects are created for the same CF, they will conflict on the mbean definition as well.  I'll follow up on that over on the 2963 ticket.

In the meantime, v2 is attached with some cleanup.;;;","08/Nov/12 14:30;slebresne;That v2 patch lgtm on principle from reading the patch, but it'll need to be rebased.;;;","09/Nov/12 22:09;jbellis;committed;;;","09/Nov/12 22:11;jbellis;(with comments explaining the CASSANDRA-2963 situation);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If processor is missing from /proc/cpuinfo, cassandra will not start",CASSANDRA-4401,12596665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,acobley,acobley,02/Jul/12 15:17,16/Apr/19 09:32,14/Jul/23 05:52,11/Jul/12 14:30,1.1.3,,,Packaging,,,0,,,,,,,"cassandra.env.sh does an egrep on /proc/cpuinfo in order to find the number of processors on the system.  If /proc/cpuinfo does not contain a processor :# line then the script will fail because of a divide  by 0 error.  Changing the Linux section of cassandra.env.sh to:


Linux)
            system_memory_in_mb=`free -m | awk '/Mem:/ {print $2}'`
            system_cpu_cores=`egrep -c 'processor([[:space:]]+):.*' /proc/cpuinfo`
            if [ ""$system_cpu_cores"" -lt ""1"" ]
            then
               system_cpu_cores=""1""
            fi
is a possible fix",,acobley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/12 12:53;acobley;4401.txt;https://issues.apache.org/jira/secure/attachment/12535192/4401.txt",,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256129,,,Wed Jul 11 14:45:27 UTC 2012,,,,,,,,,,"0|i0gvhb:",96530,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"05/Jul/12 12:54;acobley;Patch attached, hope I've done it right !;;;","11/Jul/12 14:30;brandon.williams;Committed, but move outside of the linux-specific block in case this happens elsewhere, since zero cores is always invalid.;;;","11/Jul/12 14:45;acobley;Good idea !
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correctly catch exception when Snappy cannot be loaded,CASSANDRA-4400,12596570,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,02/Jul/12 07:08,16/Apr/19 09:32,14/Jul/23 05:52,05/Jul/12 16:09,1.1.3,,,,,,0,,,,,,,"From the mailing list, on C* 1.1.1:
{noformat}
INFO 14:22:07,600 Global memtable threshold is enabled at 35MB
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1681)
        at java.lang.Runtime.loadLibrary0(Runtime.java:840)
        at java.lang.System.loadLibrary(System.java:1047)
        at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
        ... 17 more
ERROR 14:22:09,934 Exception encountered during startup
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
        at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:229)
        at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
        at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:45)
        at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:55)
        at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:37)
        at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
        at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
        at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
{noformat}",,acobley,drew_kutchar,miggi,slebresne,trajano,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/12 07:10;slebresne;4400.txt;https://issues.apache.org/jira/secure/attachment/12534183/4400.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256128,,,Mon Dec 10 15:29:38 UTC 2012,,,,,,,,,,"0|i0gvgv:",96528,,acobley,,acobley,Low,,,,,,,,,,,,,,,,,"02/Jul/12 07:10;slebresne;Patch attached to be a little bit more thorough in which errors we catch.;;;","02/Jul/12 15:01;acobley;I added your fix to the source of apache-cassandra-1.1.1-src and it does allow a startup if SnappyJava is missing.  The only thing I not is the log from the startup is a bit messy with exceptions thrown, but I'm guessing this acceptable ?

INFO 15:42:57,873 Global memtable threshold is enabled at 35MB
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.xerial.snappy.SnappyLoader.loadNativeLibrary(SnappyLoader.java:317)
	at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:219)
	at org.xerial.snappy.Snappy.<clinit>(Snappy.java:44)
	at org.apache.cassandra.io.compress.SnappyCompressor.create(SnappyCompressor.java:46)
	at org.apache.cassandra.io.compress.SnappyCompressor.isAvailable(SnappyCompressor.java:56)
	at org.apache.cassandra.io.compress.SnappyCompressor.<clinit>(SnappyCompressor.java:38)
	at org.apache.cassandra.config.CFMetaData.<clinit>(CFMetaData.java:76)
	at org.apache.cassandra.config.KSMetaData.systemKeyspace(KSMetaData.java:79)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:439)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:118)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:126)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1738)
	at java.lang.Runtime.loadLibrary0(Runtime.java:823)
	at java.lang.System.loadLibrary(System.java:1028)
	at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
	... 17 more
 WARN 15:42:59,586 Cannot initialize native Snappy library. Compression on new tables will be disabled.
 INFO 15:43:00,141 Initializing key cache with capacity of 5 MBs.;;;","04/Jul/12 10:03;slebresne;Actually, I'd like to get rid of that exception at startup but I'm not sure why that InvocationTargetException is not caught by the 'catch (Exception e)' in SnappyCompressor.isAvailable(). Unless it's printed in the log by the snappy library itself.;;;","04/Jul/12 10:27;acobley;I think it's being printed by :

static {
        try {
            impl = SnappyLoader.load();
        }
        catch (Exception e) {
            e.printStackTrace();
        }

in Snappy.java line 47.  I can experiment and confirm 
;;;","04/Jul/12 12:26;slebresne;:(

So I guess the current output is the best we can do and we can open a ticket on Snappy-Java to have them remove that.;;;","04/Jul/12 12:33;acobley;let me confirm thats the problem if thats the case, yes a ticket should be opened.
;;;","04/Jul/12 21:25;acobley;Sylvian,
I've done a little digging into the snappy source code, looks like they are not catching a java.lang.reflect.InvocationTargetException at line 319 of SnappyLoader:

 loadMethod.invoke(null, nativeLib.getAbsolutePath());

I'll try and raise a ticket with them.

Are you happy to commit your changes for 1.1.3?;;;","05/Jul/12 14:18;slebresne;Yeah so there is not much more we can do in the meantime. @Andy you tested the attached patch succesfully, right? If so I'll commit that.;;;","05/Jul/12 14:51;acobley;Yep Sylvian, I've tested it and am happy.;;;","05/Jul/12 16:09;slebresne;Alright, committed, thanks;;;","06/Aug/12 06:53;trajano;Although it catches it properly, it still throws a large stack trace.  Is it possible to get rid of it?;;;","06/Aug/12 10:36;slebresne;As explain above, the large stack trace is thrown by the Snappy library itself, so until they fix that there is nothing we can do about it, sorry.;;;","13/Nov/12 16:29;miggi;Hey Guys,
Was it fixed in 1.1.6 ? 
See the same trace: 

Caused by: java.lang.UnsatisfiedLinkError: no snappyjava in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1860)
	at java.lang.Runtime.loadLibrary0(Runtime.java:845)
	at java.lang.System.loadLibrary(System.java:1084)
	at org.xerial.snappy.SnappyNativeLoader.loadLibrary(SnappyNativeLoader.java:52)
	... 17 more

Cassandra-1.1.6
OSX 10.8 JDK ""1.7.0_07""
Any Ideas ?
;;;","13/Nov/12 16:39;slebresne;As said above, this is logged by SnappyJava itself so there is nothing we can do about that. But this is only cosmetic, you should disregard the trace itself (it does indicate that you won't have compression enable but that's a totally different problem).;;;","13/Nov/12 17:04;yukim;[~miggi] I  think you are seeing this https://github.com/xerial/snappy-java/issues/6.;;;","08/Dec/12 18:49;drew_kutchar;Is there a reason Cassandra's not using the pure Java version of Snappy? https://github.com/dain/snappy
The performance numbers are very similar. https://github.com/ning/jvm-compressor-benchmark/wiki;;;","10/Dec/12 15:29;slebresne;bq. Is there a reason Cassandra's not using the pure Java version of Snappy?

An very quick look at those benchmarks seems to suggest that the JNI impl is still faster on read, which is what we mainly care about. That being said, it wouldn't be crazy to fallback to the pure Java version if the JNI one can't be loaded, provided this is not too complicated to do. At least patches are welcome :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use data size ratio in liveRatio instead of live size : serialized throughput,CASSANDRA-4399,12596527,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Jul/12 06:59,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/12 09:49,1.1.3,,,,,,0,,,,,,,,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/12 18:25;jbellis;4399-v2.txt;https://issues.apache.org/jira/secure/attachment/12534307/4399-v2.txt","01/Jul/12 07:24;jbellis;4399.txt;https://issues.apache.org/jira/secure/attachment/12534144/4399.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256127,,,Wed Jul 04 09:49:39 UTC 2012,,,,,,,,,,"0|i0gvgf:",96526,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"01/Jul/12 07:24;jbellis;patch attached.  primarily this means that overwritten columns no longer artificially inflate liveRatio.;;;","02/Jul/12 08:30;slebresne;We'll need to reinitialize sizeDelta to 0 in the do-loop in AtomicSortedColumns.addAllColumns otherwise we'll report buggy numbers when there is contention.

Otherwise lgtm, except maybe that we might want to add in the comment on ISortedColumns.addAll() that the returned long is a delta and is not always defined correctly (I am not a fan  how having the return value not always be a valid value but I guess it might a necessary evil for performance).;;;","02/Jul/12 18:25;jbellis;Patch to split out addAllWithSizeDelta to make it clear when we try to do an unsupported operation.;;;","04/Jul/12 09:49;slebresne;This was committed by Jonathan as 8674784932ee80 while JIRA was down.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subcolumns not removed when compacting tombstoned super column,CASSANDRA-4396,12596424,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,29/Jun/12 17:34,16/Apr/19 09:32,14/Jul/23 05:52,10/Jul/12 13:36,1.0.11,1.1.3,,,,,1,compaction,,,,,,"When we compact a tombstone for a super column with the old data for that super column, we end up writing the deleted super column and all the subcolumn data that is now worthless to the new sstable. This is especially inefficient when reads need to scan tombstones during a slice.

Here is the output of a simple test I ran to confirm:

insert supercolumn, then flush
{noformat}
Nicks-MacBook-Pro:12:20:52 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-1-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": -9223372036854775808, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
{noformat}

delete supercolumn, flush again

{noformat}
[Nicks-MacBook-Pro:12:20:59 cassandra-1.0] cassandra$ bin/nodetool -h localhost flush
[Nicks-MacBook-Pro:12:22:41 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-2-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": []}}
}
{noformat}

compact and check resulting sstable

{noformat}
[Nicks-MacBook-Pro:12:22:55 cassandra-1.0] cassandra$ bin/nodetool -h localhost compact 
[Nicks-MacBook-Pro:12:23:09 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-3-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
[Nicks-MacBook-Pro:12:23:20 cassandra-1.0] cassandra$ 
{noformat}",,ctrahman,nickmbailey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/12 23:02;jbellis;4396-2.txt;https://issues.apache.org/jira/secure/attachment/12535455/4396-2.txt","29/Jun/12 23:01;jbellis;4396.txt;https://issues.apache.org/jira/secure/attachment/12534068/4396.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256124,,,Tue Jul 10 13:36:00 UTC 2012,,,,,,,,,,"0|i0gvf3:",96520,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"29/Jun/12 18:04;jbellis;This code in removeDeletedSuper is intended to address this scenario:

{code}
.               // remove subcolumns if
                // (a) the subcolumn itself is tombstoned or
                // (b) the supercolumn is tombstoned and the subcolumn is not newer than it
                if (subColumn.timestamp() <= minTimestamp
                    || subColumn.getLocalDeletionTime() < gcBefore)
                {
                    subIter.remove();
                }
{code}

Unclear why it's not actually working here.;;;","29/Jun/12 21:43;nickmbailey;This is also a problem with simply flushing super column deletes:

From a fresh cluster I can create a supercolumn with subcolumns, delete that supercolumn, trigger a flush with nodetool, and observe the subcolumn data in the flushed sstable with sstable2json.;;;","29/Jun/12 22:04;jbellis;I'm okay with not calling removeDeleted on flush, in fact I think it's probably the right tradeoff given that the extra overhead will be a no-op most of the time, but compaction should definitely evict it.;;;","29/Jun/12 23:01;jbellis;This was introduced by CASSANDRA-3234.  Fix attached to allow dropping columns shadowed by supercolumn (and row) tombstones that are not yet expired.

Reducing gcgs is one way to work around the problem without this patch.  This only affects in-memory compactions, so reducing in_memory_compaction_limit_in_mb could also mitigate it.;;;","02/Jul/12 07:00;slebresne;+1;;;","02/Jul/12 08:05;jbellis;committed;;;","06/Jul/12 21:52;nickmbailey;[~jbellis] I still see the same behavior with this patch applied. Major compaction of an sstable with subcolumns and an sstable with a tombstone for the relevant super column, produces a new sstable with the super column tombstoned but the subcolumns still present.;;;","06/Jul/12 23:02;jbellis;patch to fix mixing user-provided timestamps and local deletion time.  includes unit test.;;;","06/Jul/12 23:03;jbellis;(also fixes < to <=);;;","06/Jul/12 23:05;jbellis;patch is against 1.0, merging to trunk fixed the bug there...;;;","07/Jul/12 00:12;ctrahman;v2 is working here - thanks!;;;","10/Jul/12 10:19;slebresne;v2 lgtm, +1.;;;","10/Jul/12 13:36;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableNamesIterator misses some tombstones,CASSANDRA-4395,12596377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,29/Jun/12 11:07,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/12 16:34,1.2.0 beta 1,,,,,,0,,,,,,,The title says it all.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/12 11:09;slebresne;4395.txt;https://issues.apache.org/jira/secure/attachment/12533972/4395.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256123,,,Fri Jun 29 16:34:04 UTC 2012,,,,,,,,,,"0|i0gven:",96518,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"29/Jun/12 11:09;slebresne;Patch attached to fix (the tombstone were only skipped in the case where the row was not indexed). The patch also adds some unit tests for range tombstones.;;;","29/Jun/12 16:14;yukim;+1;;;","29/Jun/12 16:34;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Query with WHERE statement delivers wrong results,CASSANDRA-4390,12596249,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,bite,bite,28/Jun/12 12:00,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/12 14:03,,,,,,,0,,,,,,,"Data is written to cassandra via phpcassa ( https://github.com/thobbs/phpcassa v1.0).

Inspection the records via cqlsh,the query with a WHERE statement doesn't deliver all records.

https://issues.apache.org/jira/secure/attachment/12533818/Cassandra_Query_Issue.png

I'am working with a 2 node cluster and the effect is the same on both nodes. Furthermore a nodetool repair, nodetool rebuild has no positive effect. The data type for this column is varint. 


******

DESCRIBE COLUMNFAMILY rm_advertisements

CREATE TABLE rm_advertisements (
  KEY text PRIMARY KEY,
  css text,
  form text,
  custom_field2 text,
  vacancy_type text,
  custom_field3 text,
  active boolean,
  vacancy_responsibles text,
  job_site text,
  vacancy_email_notification text,
  custom_field4 text,
  vacancy_name text,
  startdate text,
  vacancy_id varint,
  custom_field5 text,
  html text,
  company_id varint,
  ctime text,
  title text,
  expires text,
  atime text,
  custom_field1 text,
  vacancy_language text,
  tags text,
  mtime text,
  mailapply boolean,
  vacancy_description text,
  vacancy_function text,
  shorthash text
) WITH
  comment='Advertisements' AND
  comparator=text AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

CREATE INDEX rm_advertisements_active ON rm_advertisements (active);

CREATE INDEX rm_advertisements_job_site ON rm_advertisements (job_site);

CREATE INDEX rm_advertisements_vacancy_id ON rm_advertisements (vacancy_id);

CREATE INDEX rm_advertisements_company_id ON rm_advertisements (company_id);

CREATE INDEX rm_advertisements_title ON rm_advertisements (title);

CREATE INDEX rm_advertisements_expires ON rm_advertisements (expires);

CREATE INDEX rm_advertisements_vacancy_language ON rm_advertisements (vacancy_language);

CREATE INDEX rm_advertisements_mailapply ON rm_advertisements (mailapply);

CREATE INDEX rm_advertisements_shorthash ON rm_advertisements (shorthash);
","Linux version 2.6.32-5-amd64 (Debian 2.6.32-45) |cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 2.0.0 | Thrift protocol 19.32.0]",bite,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 12:06;bite;Cassandra_Query_Issue.png;https://issues.apache.org/jira/secure/attachment/12533818/Cassandra_Query_Issue.png",,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256119,,,Thu Jun 28 14:07:10 UTC 2012,,,,,,,,,,"0|i0gvcn:",96509,,,,,Normal,,,,,,,,,,,,,,,,,"28/Jun/12 13:12;slebresne;Do you also get wrong result when using cassandra-cli or when using phpcassa to fetch the results, or is that a CQL thing only?;;;","28/Jun/12 13:14;bite;I get the wrong results even with cassandra-cli also as with phpcassa.;;;","28/Jun/12 13:20;slebresne;Have you tried a 'nodetool rebuild_index'?;;;","28/Jun/12 14:01;bite;That's it! It works. Thanks a lot for your support.

(Now I also found this thread: http://comments.gmane.org/gmane.comp.db.cassandra.user/26569 )

;;;","28/Jun/12 14:07;slebresne;I guess that doesn't explain why the 2ndary index was not up to date in the first place but glad that fixed it for you.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHOM.scheduleAllDeliveries still expects tokens as row keys,CASSANDRA-4389,12596244,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,soverton,soverton,soverton,28/Jun/12 11:44,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/12 21:20,,,,,,,0,,,,,,,scheduleAllDeliveries needs updating to expect hostIds instead of tokens in the HINTS_CF,,soverton,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 11:46;soverton;4389.patch;https://issues.apache.org/jira/secure/attachment/12533812/4389.patch",,,,,,,,,,,,,,,,,1.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256118,,,Thu Jun 28 21:20:00 UTC 2012,,,,,,,,,,"0|i0gvc7:",96507,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"28/Jun/12 11:46;soverton;Attached patch;;;","28/Jun/12 21:20;urandom;committed; thanks Sam!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug when trying to describe a cf in a pre cql3 case sensitive keyspace,CASSANDRA-4385,12596170,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,al@ooyala.com,al@ooyala.com,27/Jun/12 21:36,16/Apr/19 09:32,14/Jul/23 05:52,26/Jul/12 17:40,1.1.3,,,,,,0,cqlsh,,,,,,"I can't describe column families in my schema defined via cassandra-cli. Update also seems to fail for the same CF's.

CREATE KEYSPACE Hastur
  with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
  and strategy_options = {replication_factor:2};

CREATE COLUMN FAMILY LookupByKey
  with compaction_strategy = 'LeveledCompactionStrategy'
  and compression_options = null;

Then later, https://gist.github.com/3006886","Linux, Hotspot JDK6, Cassandra 1.1.0 from tarball unmodified, stock config.",al@ooyala.com,jeromatron,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/12 22:48;thepaul;0001-cqlsh-Fix-error-reporting-for-unknown-CFs.patch;https://issues.apache.org/jira/secure/attachment/12535756/0001-cqlsh-Fix-error-reporting-for-unknown-CFs.patch",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256114,,,Thu Jul 26 17:40:12 UTC 2012,,,,,,,,,,"0|i0gvaf:",96499,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"29/Jun/12 20:08;al@ooyala.com;Recreating the keyspace and CF via cqlsh instead of cassandra-cli resolves the issue. This isn't really practical on my production systems.;;;","30/Jun/12 00:41;brandon.williams;You need to quote the cf/ks in cql3 since it is case insensitive.;;;","30/Jun/12 01:17;al@ooyala.com;Quoted is also broken.;;;","30/Jun/12 20:39;thepaul;(Pasting the contents of the gist for posterity.)

{noformat}
cqlsh:system> use Hastur;
Bad Request: Keyspace 'hastur' does not exist
cqlsh:system> use ""Hastur"";
cqlsh:""Hastur""> describe columnfamily lookupbykey;

Keyspace NotFoundException() not found.
cqlsh:""Hastur""> describe columnfamily LookupByKey;

Keyspace NotFoundException() not found.
cqlsh:""Hastur""> describe columnfamily ""LookupByKey"";

Keyspace NotFoundException() not found.

cqlsh:system> use system;
cqlsh:system> desc columnfamily NodeIdInfo;

CREATE COLUMNFAMILY NodeIdInfo (
  KEY blob PRIMARY KEY
) WITH
  comment='nodeId and their metadata' AND
  comparator='TimeUUIDType' AND
  read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write=True AND
  compaction_strategy_class='SizeTieredCompactionStrategy';
{noformat};;;","09/Jul/12 22:45;thepaul;The root cause of this should be fixed in the 1.1 branch already by CASSANDRA-4173 and CASSANDRA-4198, but the error reporting is clearly also broken here. ""{{Keyspace NotFoundException() not found.}}"" is messed up.

Al, you can safely use a more recent cqlsh from the 1.1 branch if you'd like.

I'll put up a fix for the error reporting.;;;","09/Jul/12 22:48;thepaul;Attached patch fixes error reporting when a columnfamily is not found.

Also available in my github, in the 4385 branch; current version tagged pending/4385.

https://github.com/thepaul/cassandra/tree/pending/4385;;;","10/Jul/12 01:38;al@ooyala.com;If I 'desc schema' in cqlsh --cql3 using a build of either HEAD or 4385 from your tree, I still can't see my application keyspace, likely because it's called ""Hastur"". If I run cqlsh --cql2 everything shows up.

It could be that the answer is, ""rebuild your schema using CQL 3"" ...;;;","10/Jul/12 01:42;al@ooyala.com;The ""Hastur"" schema was created with this file: https://github.com/ooyala/hastur-server/blob/master/tools/cassandra/create_keyspace.cass

Weirdly, the ""HasturTrigger"" keyspace shows up in CQL3 but ""Hastur"" does not.
;;;","10/Jul/12 04:30;thepaul;Interesting. Do you see any errors, or does it just not show up?

There are some related issues with DESC that are fixed in CASSANDRA-4380, which will probably help too. I've merged the 4380 branch into the 4385 one in my github (new tag is pending/4385-2). Give that a try.;;;","26/Jul/12 17:40;brandon.williams;Worked for me.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff can begin before SS knows the hostID,CASSANDRA-4384,12596137,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,27/Jun/12 19:00,16/Apr/19 09:32,14/Jul/23 05:52,15/Nov/12 17:58,1.2.0 beta 3,,,,,,0,,,,,,,"Since HH fires from the FD, SS won't quite have the hostId yet:

{noformat}
 INFO 18:58:04,196 Started hinted handoff for host: null with IP: /10.179.65.102
 INFO 18:58:04,197 Node /10.179.65.102 state jump to normal
ERROR 18:58:04,197 Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:120)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:304)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:250)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:87)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:433)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

Simple solution seems to be getting the hostId from gossip instead.",,slebresne,soverton,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/12 11:41;brandon.williams;4384.txt;https://issues.apache.org/jira/secure/attachment/12545711/4384.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,239484,,,Thu Nov 15 17:58:48 UTC 2012,,,,,,,,,,"0|i00hsf:",814,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Jun/12 22:32;urandom;This is a corner-case that only happens to a node that is restarted (and lost its hostId map as a result).  It is the result of a hint delivery being triggered (by the _reception_ of a gossip message) before the hostId could be processed from the message.

I see two ways to fix:

# Skip hint delivery when we don't (yet) have a hostId.
# Persist hostIds the way we do tokens.

(1) has the benefit of being a one-liner.  Presumably this code exists to schedule hint delivery for a remote host that was dead and is now alive.  Since in this case it is _us_ that was down, I don't think skipping would be Evil.

(2) adds complexity, but guards against any future cases of an {{IEndpointStateChangeSubscriber.onAlive()}} relying on {{TokenMetadata.isMember()}} before looking up a hostId.

;;;","28/Jun/12 14:28;soverton;When a node first starts up without a hostId map, it may be asked to store a hint for a node which is down and it won't know the hostId of that node since it never saw it alive.

We should persist the hostIds to prevent both of these cases where we are missing required information when we first start up.

;;;","28/Jun/12 18:03;brandon.williams;I'm inclined to agree with Sam, since this is how the problem is solved with tokens.;;;","28/Jun/12 18:09;slebresne;CASSANDRA-4351 is relevant (or in other words, I agree too since we wanted to persist them anyway for that issue).;;;","29/Jun/12 15:23;brandon.williams;I'll note that while CASSANDRA-4351 is similar in spirit, that is information we do NOT want to use, so I would prefer this be in LocationInfo with the tokens, and have everything else in a different CF, so the old method of blowing away LI when something goes bad is still valid.;;;","29/Jun/12 16:13;urandom;bq. ...so I would prefer this be in LocationInfo with the tokens...

Tokens are now in the {{peers}} columnfamily, keyed by the token with one column for the peer (inet address).  With vnodes, that will become many-to-one (the endpoints will not be unique).  Host IDs are one-to-one.  The most straightforward approach seemed to be to create another columnfamily (peer_ids?), similar to {{peers}}, but keyed by host ID.;;;","29/Jun/12 16:25;jbellis;or we could replace the existing {{peers}} entirely (I'd prefer to retain the table name) and add an index on the token column for that lookup.;;;","29/Jun/12 16:30;slebresne;bq. or we could replace the existing peers entirely

I would agree with that, but going a bit further I would imagine that a natural schema for that peers table could be something like:
{noformat}
CREATE TABLE peers (
  host_id UUID PRIMARY KEY,
  peer inet,
  tokens set<blob>,
)
{noformat}
with maybe even more info for each host but that's for CASSANDRA-4351. Of course that specific schema means waiting for CASSANDRA-3647, but I don't see that as a big problem. This wouldn't let use query by token but I don't think that really matter because we'll read the whole table content at startup anyway.;;;","11/Jul/12 15:19;soverton;I have a branch at https://github.com/acunu/cassandra/tree/CASSANDRA-4384 which
* gossips host ID on startup
* persists host IDs to the peers CF and loads them on startup

I ended up changing the peers schema to be keyed off IP address instead of UUID:
{noformat}
CREATE TABLE peers (
  peer inet PRIMARY KEY,
  token_bytes blob,
  ring_id uuid,
)
{noformat}

I agree with Sylvain that it seems more natural to use UUID as the key rather than by IP, but that didn't seem as straightforward to update when tokens or hostIds change.

There is some overlap with CASSANDRA-4122 so it would be best to wait for that to go in before changing the SystemTable again for this.

;;;","11/Jul/12 15:57;brandon.williams;I like making HOST_ID a full app state, that seems much cleaner.  Unsure on the ip vs uuid change at the moment.;;;","04/Sep/12 18:48;brandon.williams;HOST_ID is a full app state after CASSANDRA-4383 now so we can revisit this.;;;","19/Sep/12 11:41;brandon.williams;Not sure where we stand on the schema changes, but we can trivially prevent this exception by checking the gossiper instead of SS now.;;;","16/Oct/12 12:15;slebresne;We do persist the ring_id for all nodes now, so maybe we should use that.;;;","15/Nov/12 17:53;jbellis;+1 on Brandon's fix for 1.2.0.;;;","15/Nov/12 17:58;brandon.williams;Committed.  Let's leave future changes to a new ticket if needed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH: describe command doesn't output valid CQL command.,CASSANDRA-4380,12595967,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,nickmbailey,nickmbailey,26/Jun/12 19:20,16/Apr/19 09:32,14/Jul/23 05:52,11/Jul/12 14:44,1.1.3,,,,,,1,cqlsh,,,,,,"{noformat}
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

I can create a cf in cql3 and then use describe cf to get the above output. However trying to run that create statement says that all of the following are invalid options:

* default_validation
* min_compaction_threshold
* max_compaction_threshold
* comparator",,dyrby76,mdione,nickmbailey,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256110,,,Wed Sep 19 16:28:35 UTC 2012,,,,,,,,,,"0|i0gv8f:",96490,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"27/Jun/12 08:00;slebresne;I'll not that at lest for CQL3, as explained in CASSANDRA-4377, it will be much easier to fix on trunk than on 1.1 because of CASSANDRA-4018. As far as my personal opinion is involved, I would be fine in saying that cqlsh describe command is broken with CQL3 until C* 1.2 (since CQL3 is beta until then anyway).;;;","27/Jun/12 14:42;nickmbailey;My issue was more with the fact that it seems impossible to set any of those four properties on a cf via cql. ;;;","29/Jun/12 17:35;thepaul;This was actually intended behavior- since it's possible to create tables in cql2 which can't be duplicated in cql3, I wanted to still note the possibly-relevant options on certain tables even if the syntax wasn't valid cql3.

I've since changed my mind, however, since it is confusing and makes it harder to do automated tests on DESCRIBE output, so I've already made this change in a personal branch. I'll put that up here.;;;","09/Jul/12 23:14;thepaul;Nick, can you try with the bin/cqlsh from my 4380 branch, here:

https://github.com/thepaul/cassandra/tree/4380

This should fix the cases where cql2-style DESCRIBE was used sometimes, even when the connection was cql3, but looking closer at your case, it might not actually fix that.;;;","10/Jul/12 16:16;nickmbailey;Seems to work fine for me.;;;","11/Jul/12 14:44;brandon.williams;Committed.;;;","10/Sep/12 14:30;mdione;what about the rest of the output? if I run «cqlsh --cqlversion=3», I expect that the output is CQLv3, but for a particular CF that was created with a CQLv2 command, I get:

CREATE TABLE logs (
  day timestamp PRIMARY KEY
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

which is only valid in CQLv2; with CQLv3 I get this message:

<stdin>:17:Bad Request: No definition found that is not part of the PRIMARY KEY;;;","10/Sep/12 16:01;thepaul;Marcos- there isn't any way to recreate some CQL 2 tables in CQL 3. CQL 3 can model the same data, but the structure of the table needs to be different. Cqlsh does the best that it can here, but if you want a better guarantee of valid reconstruction of CQL 2 tables, use cqlsh in CQL 2 mode.;;;","10/Sep/12 16:31;jbellis;Doesn't COMPACT STORAGE provide the ability to model thrift/cql2-style tables?;;;","10/Sep/12 16:46;thepaul;bq. Doesn't COMPACT STORAGE provide the ability to model thrift/cql2-style tables?

It does for some. It probably would in this case. It doesn't for the cases where there are multiple columns with metadata in addition to undefined/wide-row columns.

Maybe that's not a case worth worrying about, though. We could open a new ticket for recreating thrift/cql2-style tables in CQL 3 mode with COMPACT STORAGE and some made-up default column names like ""colname"" and ""value"" or something.;;;","10/Sep/12 17:07;slebresne;bq. It doesn't for the cases where there are multiple columns with metadata in addition to undefined/wide-row columns

That's true on 1.1 but I'll note that for 1.2, provided you use the system.schema_columnfamilies table, it should (almost) always be possible to create a valid CQL3 definition from an existing CF (potentially, and almost always in the case of CF created from thrift/CQL2, with COMPACT STORAGE). The only thing that comes in mind that makes me say ""almost"" is the case where people have created on the thrift side a column_metadata on a CompositeType. In that last case we can create a CQL3 query to recreate the CF, but not the column_metadata. I think it's safe to say that we can ignore that.

bq. some made-up default column names

The Cassandra already does that (so that you can always access thrift CF from CQL3), see CFDefinition.java. It could be worth using the same made-up names here.;;;","10/Sep/12 18:34;thepaul;bq. for 1.2, provided you use the system.schema_columnfamilies table, it should (almost) always be possible to create a valid CQL3 definition from an existing CF

Could you elaborate on how that should be done? Say I made the following in CQL2:

{noformat}
CREATE TABLE foo (t text PRIMARY KEY, f float, x uuid) WITH default_validation=timestamp;
{noformat}

If that can be recreated in CQL 3, then I've missed a development somewhere.

bq. The Cassandra already does that (so that you can always access thrift CF from CQL3), see CFDefinition.java. It could be worth using the same made-up names here.

That's good to know. We should probably make that new ticket then.;;;","10/Sep/12 18:45;slebresne;bq. If that can be recreated in CQL 3, then I've missed a development somewhere

Good point, I think too much in term of CQL3 now I guess, forgot about those. I suppose we could generate a valid CQL3 definition that is ""compatible"" with that (the same thing without the default_validation_validation part), but it's unclear it's a better idea.;;;","19/Sep/12 15:58;dyrby76;Using CQL3, how are you now expected to create a column family like:

CREATE TABLE UserProducts
(
  userId uuid PRIMARY KEY
);

Where I would then dynamically add the product ids in the column name for each product, and leaving the column value null for each. The type of the product id is uuid.

When I try to create this column family I am told: No definition found that is not part of the PRIMARY KEY

Any help is greatly appreciated.

It works with CQL2, so is it a bug that CQL3 cannot? 






;;;","19/Sep/12 16:28;jbellis;CREATE TABLE UserProducts (
  user_id uuid,
  product_id uuid,
  PRIMARY KEY (user_id, product_id)
);

More details at http://www.datastax.com/dev/blog/schema-in-cassandra-1-1.

Let's take any further discussion on usage to the mailing list.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup optimization can delete data but not corresponding index entries,CASSANDRA-4379,12595941,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Jun/12 17:18,16/Apr/19 09:32,14/Jul/23 05:52,27/Jun/12 16:40,1.1.2,,,,,,0,compaction,,,,,,introduced by CASSANDRA-4079,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/12 17:21;jbellis;4379.txt;https://issues.apache.org/jira/secure/attachment/12533503/4379.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256109,,,Wed Jun 27 16:40:34 UTC 2012,,,,,,,,,,"0|i0gv7z:",96488,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"26/Jun/12 17:21;jbellis;fix attached;;;","27/Jun/12 08:25;slebresne;+1;;;","27/Jun/12 16:40;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 column value validation bug,CASSANDRA-4377,12595934,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,nickmbailey,nickmbailey,26/Jun/12 16:44,16/Apr/19 09:32,14/Jul/23 05:52,11/Sep/12 09:03,1.2.0 beta 1,,,Legacy/CQL,,,2,cql3,,,,,,"{noformat}
cqlsh> create keyspace test with strategy_class = 'SimpleStrategy' and strategy_options:replication_factor = 1;
cqlsh> use test;
cqlsh:test> CREATE TABLE stats (
        ...   gid          blob,
        ...   period     int,
        ...   tid          blob, 
        ...   sum        int,
        ...   uniques           blob,
        ...   PRIMARY KEY(gid, period, tid)
        ... );
cqlsh:test> describe columnfamily stats;

CREATE TABLE stats (
  gid blob PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.Int32Type,org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat}

You can see in the above output that the stats cf is created with the column validator set to text, but neither of the non primary key columns defined are text. It should either be setting metadata for those columns or not setting a default validator or some combination of the two.",,bertpassek,hubaghdadi,liqusha,mauzhang,mbulman,nickmbailey,omid,shamim_ru,slebresne,thepaul,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4907,,,,,,"06/Sep/12 11:16;slebresne;4377-2.txt;https://issues.apache.org/jira/secure/attachment/12544028/4377-2.txt","10/Jul/12 15:40;slebresne;4377.txt;https://issues.apache.org/jira/secure/attachment/12535860/4377.txt","22/Aug/12 09:50;xedin;CASSANDRA-4377.patch;https://issues.apache.org/jira/secure/attachment/12541883/CASSANDRA-4377.patch",,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255103,,,Tue Sep 11 09:03:56 UTC 2012,,,,,,,,,,"0|i0ep2n:",83824,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"27/Jun/12 07:00;slebresne;What happens is that columns metadata for composite CQL3 columns refers to only one of the component of the column name. So internally they have a 'componentIndex' parameter which allow to handle them correctly (otherwise you don't know which comparator to use to display those column metadata). However, we decided that thrift users shouldn't have to care about this, so we don't expose those metadata to thrift at all. In other words, the CF does have metadata for 'sum' and 'uniques' internally, but they are not exposed to thrift.

So I guess it is more of a cqlsh problem that shouldn't use the thrift describe call for CQL3. Instead, it should directly query the system.keyspace, system.columnfamilies and system.columns table. However, it'd be much more easier to do that with CASSANDRA-4018, but that is not in 1.1.;;;","27/Jun/12 14:45;nickmbailey;bq. So I guess it is more of a cqlsh problem that shouldn't use the thrift describe call for CQL3.

The main problem here from my perspective is that it was impossible to insert data into the column family except with cqlsh. Using a basic thrift batch mutate failed on the validation step because it tried to validate the value in the sum column as text (default_validation_class).;;;","27/Jun/12 22:01;hubaghdadi;True, I'm unable to save any data to Cassandra.
Trying to save with Hector (uses Mutators):
#<HInvalidRequestException me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:(String didn't validate.) [mks][stats][1234:7461726765742d3131:sum] failed validation)>;;;","28/Jun/12 11:51;slebresne;bq. it was impossible to insert data into the column family except with cqlsh. Using a basic thrift batch mutate failed on the validation step because it tried to validate the value

Alright, that part is not expected and is likely a bug.

Though I note that even if we fix that, since we don't expose on the thrift side everything needed to interpret the value correctly, thrift client won't be able to work with those kind of table very well. In particular they won't know how to interpret the value of a get. So I guess we need to either say clearly that CQL3 table are not meant to be accessed from thrift, or we should probably start exposing all the column metatada on the thrift side (but we need to expose the componentIndex part of the metadata in particular and the discussion on CASSANDRA-4093 is relevant for that).;;;","10/Jul/12 15:40;slebresne;Attaching simple patch to allow the insertion in the case above. Truth is, I'm not really satisfied (though I don't have a clearly better option) by such a patch for 2 reasons:
# it will break the case for thrift where people were using compositeType and column_metadata on them. That might be 0 people we're talking about but it's still a bit annoying. An alternative here would be that for each composite column, we iterater over all column_metadata and check where one apply. This would work but this feels butt ugly.
# it feels to me we supporting either not enough or too much in the thrift side. Even if we support this, we still don't expose the CQL3 metadata to thrift, so one has to know the CQL schema definition to be able to create the column in the first place (or deserialize it on read). In particular, most advanced thrift client will likely still break at one point or another.

Overall I see two reasonable approaches:
* Either we start exposing enough on the thirft side so that thrift client can work correctly with CQL3. While this may be doable reasonably easily now, this will be increasingly difficult with things like CASSANDRA-4179, CASSANDRA-3647, ... Besides, even if we make it possible to work with CQL3 table, it doesn't mean it will be convenient since thrift won't do the grouping of columns in sparse table.
* Be clear that you cannot work with CQL3 created table from thrift.

But imo the in-the-middle approach that this patch would start takes the risk of polluting the thrift side without adding much.
;;;","10/Jul/12 15:48;slebresne;To be clear, if this issue is for instance a blocker for map-reduce for CQL3, I'm not against committing it, but on a more long term/general level, I do want to note that accessing CQL3 tables to thrift is imho a larger problem and we should be clear on what we want to guarantee.;;;","10/Jul/12 16:32;nickmbailey;It doesn't sound like its going to be possible to make things work well in the case of accessing cql3 data from thrift.

If thats the case I'm in favor of making that incompatibility *very* explicit. I would say explicitly throwing an exception when trying to access a cql3 column family from thrift would be an acceptable solution.

The fact that it took me quite a bit of time as well as digging around in both client code and cassandra source code to figure this bug out makes me worried for other users hitting similar problems.;;;","10/Jul/12 17:11;thepaul;bq. I would say explicitly throwing an exception when trying to access a cql3 column family from thrift would be an acceptable solution.

-1 on that; it's very useful to have a lower-level access method to look at the ""storage engine"" layer instead of the logical CQL3 rows at times.;;;","10/Jul/12 17:14;nickmbailey;so disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

The only other option I see is to say that we are going to identify and fix all bugs like this one.;;;","10/Jul/12 17:22;jbellis;bq. disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

Sounds reasonable to me.;;;","10/Jul/12 17:44;slebresne;bq. disable write access to cql3 cfs from thrift? And leave read access with the qualification that everything will be returned as BytesType?

A variation in the same spirit could be to disable access to CQL3 table by default but add a thrift debug mode (either per-connection through or globally through JMX if we don't want to add a new thrift method) that would make thrift disable every validation. That way, by default the message that CQL3 tables should be access through CQL3 would be clear but we would have low-level read/write for debugging.  ;;;","10/Jul/12 17:46;thepaul;bq. disable access to CQL3 table by default but add a thrift debug mode

+lots.;;;","10/Jul/12 17:53;nickmbailey;sgtm;;;","10/Jul/12 18:15;jbellis;+1

(and I'd prefer JMX otherwise I foresee clients using this as an escape hatch and fubarring things up.);;;","10/Jul/12 22:38;thepaul;So, from an outside conversation, it sounds like there may be some confusion on exactly what is being discussed here. Are we talking about disabling Thrift access entirely to columnfamilies which use named metadata with composites, or are we just talking about not supporting Thrift addressing columns inside composites by their CQL3 names?

The second seems eminently reasonable. The former sounds crazy.;;;","11/Jul/12 07:36;slebresne;bq. are we just talking about not supporting Thrift addressing columns inside composites by their CQL3 names

That is not what I was talking about, but I don't even understand how we could ever support that.

bq. disabling Thrift access entirely to columnfamilies which use named metadata with composites

That is what I'm talking about (though to be precise, it would be for composites using named metadata *created through CQL3*). Anyway, I don't know if that's so crazy but in any case calling it crazy doesn't help solving the problem.

;;;","30/Jul/12 23:26;nickmbailey;So here is where I ended up on this.

Assuming we aren't extremely interested in fixing bugs like these when the come up then I'm all for disabling thrift access by default to cql3 cfs. From what I can tell there isn't an immediately easy way to know a cf was created through cql3 at the moment but we could add that I guess.

On the other hand if we want to just go ahead and fix these bugs when they happen, I don't see much reason to go out of our way to disable thrift access to cql3 cfs.;;;","09/Aug/12 15:17;jbellis;bq. one has to know the CQL schema definition to be able to create the column in the first place (or deserialize it on read)

Can you remind me why we don't translate {{PRIMARY KEY(gid, period, tid)}} into a comparator of {{CompositeType(Int32Type, BytesType, UTF8Type)}}?  (period -> int, tid -> bytes, sparse columns -> utf8);;;","09/Aug/12 15:41;slebresne;bq. Can you remind me why we don't translate PRIMARY KEY(gid, period, tid) into a comparator of CompositeType(Int32Type, BytesType, UTF8Type)

We do. The problem is actually with columns that are not part of the key. Let me try to sum that up:
* Pre-CQL3, the ColumnDefinition name was always a full column name.
* In CQL3, the ColumnDefinition name only correspond to one of the component of the column name, i.e. to the UTF8Type component above.
* To be able to distinguish both case internally, we've introduce the componentIndex field in ColumnDefinition. However, we decided that we didn't wanted to expose this field to thrift, and so we don't expose to thrift the ColumnDefinition from CQL3 table.

The net result is that as far as thrift is concerned, the CQL3 tables have no columns_metadata whatsoever. It follows that thrift clients don't know what are the correct value for the last UTF8Type component, and don't know what is the type of the corresponding value (and thus cannot serialize/deserialize said value correctly). ;;;","09/Aug/12 15:53;jbellis;So basically, the problem is we're trying to maintain compatibility with (non-cql3) wide-row named columns?   If we're willing to break that scenario, does the problem go away?

I still think that naming wide row columns is nonsensical, but we could add an extra layer of protection by warning on startup in 1.2 that you need to update your schema:

- if you have named columns and non-utf8-or-bytes comparator
- if you have named columns but metadata shows more columns than names;;;","09/Aug/12 16:41;slebresne;I'm not sure I understand what's a named columns above to be honest.

There is basically two informations from CFMetadata you need to know to insert a column correctly in a table (CQL3 or no CQL3): the comparator and *all* of column_metadata. The comparator is necessary to know what is a valid column name and the column_metadata is necessary to know what is a valid column value (I'm simplifying a bit, I'm assuming that the key_validation and default_validator are BytesType but that doesn't matter for the problem at hand).

Now the problem is that for any table created through CQL3 that doesn't use COMPACT STORAGE (let's call those CQL3 tables), all the ColumnDefinition of column_metada will have a componentIndex. So none of those ColumnDefinition are exposed in thrift. In practice it means that if I do:
{noformat}
CREATE TABLE user {
    user_id blob PRIMARY KEY,
    name text,
    age int
}
{noformat}
then if a thrift client do a describe, it will basically get:
{noformat}
comparator = CompositeType(UTF8Type) // it's a composite so that we can add collection later on
column_metadata = []
{noformat}

At that point we have two slightly separate problems:
# Even if a user produces a valid column, with say a composite name being ""age"" and a value being an int, then currently the code throw an exception. Fixing that exception is the goal of the attached patch (though it would have to be updated to work with collections in 1.2). I'm fine fixing that, though I'm pointing that there is a second, more general problem.
# Since the thrift client doesn't know about the actual column_metadata, how can we expect it to correctly insert data. In particular I'm pretty sure higher level clients like pycassa or astyanax will serialize data incorrectly if they don't know the right value validator. Besides, there is many way to be confused if you use a CQL3 table from thrift. For instance if you create the wrong column (i'ts enough to mess up the case), you'll be surprised to not be able to access it when you go back to CQL3. So be clear, I do am suggesting that we don't allow accessing table created from CQL3 *without* COMPACT STORAGE from thrift, because I think it will be more sane, even if it does mean that you're not coming back from CQL3 once you've start really using it.
;;;","09/Aug/12 17:07;jbellis;Right, so what I was saying was, if we're willing to say that columndefinition name is always the cql3 column name, then we don't need componentIndex, at the price of potentially breaking a corner case in non-cql3 schemas.;;;","09/Aug/12 17:33;slebresne;But what the componentIndex give us is which of the composite component is the cql3 column name. Typically, with collections, it's not even necessarily the last of the component. Now, if you know the table is a CQL3 one, you could try to infer which component it is by saying that it's the last component, except if the last is a collection type, in which case that's the previous one, but that feels a bit messy. And besides, thrift client libraries don't have a simple automatic way to know if the table is a CQL3 one in the first place. I guess you could say that if you have a composite comparator *and* some column_metadata then you are likely a CQL3 table, but again, not very clean imo.


At least internally I would be in favor of keeping the componentIndex as it is cleaner. I guess we could start returning the ColumnDefinition from CQL3 table without the componentIndex and let thrift client infer what they can. As said, I still think using CQL3 table from thrift has other way to be confusing, but why not.;;;","17/Aug/12 16:00;jbellis;Here's what I think our goals are, in order of importance:

# Existing ""high level"" clients should have a reasonable upgrade path to read and update collections and cql3 CFs.
# CLI and other tools that don't speak cql should have a way to tell that they can't cope with CQL3 CF definitions.  This is the problem Nick described originally in this ticket description.  Actually making such tools able to manipulate CQL3 definitions is NOT a goal, but we should, as Nick says, make that more obvious.
# We should allow updates to CQL3 CFs from Thrift, if someone manually composes the correct CompositeType bytes.  This is what most of the rest of the discussion here involves.

Analysis:

# This we have done--they will have to use cql-over-thrift, but IMO this is reasonable.  Thrift RPC methods to deal with collections have never been on the roadmap.
# This is tough since if this involves new information (like exposing component_index, or even adding a cql3 boolean to CfDef) then old tools by definition won't know about it.  *Proposal:* what if we omit CQL3 CfDefs from those we return to describe_keyspace[s] calls?  Not quite as good as returning a CfDef that explicitly says ""I am here but you can't touch this,"" but compare to displaying incomplete information (that the cli doesn't know is incomplete) it's much more obvious that the cli and other old thrift-base schema manipulators can't cope with such.
# Sylvain mentioned adding a thrift or JMX method to enable validation-free updates but I'd rather make ThriftValidation cql3-aware, which would let this work without any special flags.  I don't see any downside to this except added complexity for ThriftValidation.;;;","17/Aug/12 16:11;jbellis;I do think we should push this to 1.2 however, since I'm leery of changing the behavior or describe_keyspace[s] when we are fairly deep into 1.1's stable lifetime.;;;","22/Aug/12 09:50;xedin;CQL3 defined CFs won't be exposed to Thrift API anymore (with warning), I also checked CassandraServer and ThriftValidation and figured that column name validation already in place via ThriftValidation.validateColumnNames(...).;;;","31/Aug/12 15:25;nickmbailey;Just glanced at the patch, but it looks like this just makes cql3 cfs not show up in describe_keyspaces calls right? Reading/writing to the cf would still go through and error?

If I'm looking at that right then it seems like we would want the opposite behavior. At least from my perspective, I would like to have an indication that the cf exists in my client even if i can't write to it.;;;","31/Aug/12 23:43;jbellis;The idea was to do two things:

- update ThriftValidation to be cql3-aware, so if a Thrift user manually composes a valid column, we accept it
- but, we don't expose cql3 CFs via describe_keyspaces since clients do not have enough information to generate valid requests automatically;;;","03/Sep/12 09:21;slebresne;I agree that the two points above (make ThriftValidation cql3-aware but don't expose cql3 CFs defs) make sense as a strategy.;;;","06/Sep/12 11:16;slebresne;On Pavel's patch:
* I'm not a fan of logging a warning (or to log anything really) if someone has CQL3 CFs. We're trying to push CQL3 as a good thing, let's not log anything that could be interpreted as if something was wrong/abnormal.
* The check is excluding composite CF without any ColumnDefinition while it shouldn't.

Attaching a v2 that:
* Fix the two remarks above.
* Rename the check as isThriftIncompatible. I think it's more about detecting CF definitions that cannot be exploided fully by thrift rather than discriminate between what is a thrift CF and CQL3 CF. Especially since the intersection between those two notions is not empty.
* Ship the changes to ThriftValidation from my first patch, though modified a bit to be more generic and handle correctly collections. I'll note that this part makes validation potentially iterate over all ColumnDefinition for composite CF, but that's not really an issue since composite CF created on the thrift side are almost guaranteed to have no ColumnDefinition.
;;;","06/Sep/12 11:42;xedin;bq. I'm not a fan of logging a warning (or to log anything really) if someone has CQL3 CFs. We're trying to push CQL3 as a good thing, let's not log anything that could be interpreted as if something was wrong/abnormal.

Wouldn't that create confusion when some CFs are visible through Thrift and some are not or do we rely on that users should know what is so special about CQL3 CFs?;;;","06/Sep/12 11:52;slebresne;bq. Wouldn't that create confusion when some CFs are visible through Thrift and some are not

I'm not saying this shouldn't be documented at all, but merely that it's a documentation issue and as such logging it at each startup is not the right place. ;;;","06/Sep/12 11:57;xedin;If everyone else is ok with that, lgtm.;;;","11/Sep/12 02:36;jbellis;So, this is good to commit?;;;","11/Sep/12 09:03;slebresne;Alright, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,
FD incorrectly using RPC timeout to ignore gossip heartbeats,CASSANDRA-4375,12595863,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,26/Jun/12 04:41,16/Apr/19 09:32,14/Jul/23 05:52,22/Jan/14 19:45,1.2.14,2.0.5,,,,,0,gossip,,,,,,"Short version: You can't run a cluster with short RPC timeouts because nodes just constantly flap up/down.

Long version:

CASSANDRA-3273 tried to fix a problem resulting from the way the failure detector works, but did so by introducing a much more sever bug: With low RPC timeouts, that are lower than the typical gossip propagation time, a cluster will just constantly have all nodes flapping other nodes up and down.

The cause is this:

{code}
+    // in the event of a long partition, never record an interval longer than the rpc timeout,
+    // since if a host is regularly experiencing connectivity problems lasting this long we'd
+    // rather mark it down quickly instead of adapting
+    private final double MAX_INTERVAL_IN_MS = DatabaseDescriptor.getRpcTimeout();
{code}

And then:

{code}
-        tLast_ = value;            
-        arrivalIntervals_.add(interArrivalTime);        
+        if (interArrivalTime <= MAX_INTERVAL_IN_MS)
+            arrivalIntervals_.add(interArrivalTime);
+        else
+            logger_.debug(""Ignoring interval time of {}"", interArrivalTime);
{code}

Using the RPC timeout to ignore unreasonably long intervals is not correct, as the RPC timeout is completely orthogonal to gossip propagation delay (see CASSANDRA-3927 for a quick description of how the FD works).

In practice, the propagation delay ends up being in the 0-3 second range on a cluster with good local latency. With a low RPC timeout of say 200 ms, very few heartbeat updates come in fast enough that it doesn't get ignored by the failure detector. This in turn means that the FD records a completely skewed average heartbeat interval, which in turn means that nodes almost always get flapped on interpret() unless they happen to *just* have had their heartbeat updated. Then they flap back up whenever the next heartbeat comes in (since it gets brought up immediately).

In our build, we are replacing the FD with an implementation that simply uses a fixed {{N}} second time to convict, because this is just one of many ways in which the current FD hurts, while we still haven't found a way it actually helps relative to the trivial fixed-second conviction policy.

For upstream, assuming people won't agree on changing it to a fixed timeout, I suggest, at minimum, never using a value lower than something like 10 seconds or something, when determining whether to ignore. Slightly better is to make it a config option.

(I should note that if propagation delays are significantly off from the expected level, other things than the FD already breaks - such as the whole concept of {{RING_DELAY}}, which assumes the propagation time is roughly constant with e.g. cluster size.)",,brandon.williams,cburroughs,j.casares,jasobrown,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6558,,,,,,,,,,,"08/Jan/14 15:40;brandon.williams;4375.txt;https://issues.apache.org/jira/secure/attachment/12621991/4375.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256107,,,Wed Jan 22 19:45:06 UTC 2014,,,,,,,,,,"0|i0gv6f:",96481,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Jun/12 14:21;brandon.williams;What if we set MAX_INTERVAL_IN_MS to the greater of DD.getRpcTimeout() or the gossip interval * 2?;;;","26/Jun/12 17:00;scode;Why do we believe gossip interval * 2 is a good value? Empirically, that seems low given the 0-3x prop delay empirically observed. Remember that the gossip interval does not imply that the expected propagation delay is equal to the interval, since you're only gossiping to 1-2 random hosts.
;;;","26/Jun/12 17:07;brandon.williams;bq. since you're only gossiping to 1-2 random hosts

You're always gossiping to a seed, though I suppose with enough seeds that too isn't reliable.  I'd be fine with setting it to something static like 10s to match the default rpc timeout, we just need some kind of reasonable bound to prevent CASSANDRA-3273.;;;","26/Jun/12 22:54;scode;Don't rely on seeds, unless we want to say that seeds are required to be up for gossip propagation to happen in a supported fashion (beyond just acting as a common set fo avoid partitions).

Also, in my previous testing the seed:s ended up not impacting gossip propagation delay anyway; at least with a handful of seeds. With a single seed it might be different.;;;","20/Nov/13 00:06;jbellis;bq.  the RPC timeout to ignore unreasonably long intervals is not correct

I agree; the two are unrelated.

Suggest either hardcoding it to 10s or making it configurable.;;;","21/Nov/13 21:22;brandon.williams;Well, let's just set it to 10s.  Probably what most people have been using anyway, and I don't think it's quite yet worth making configurable.;;;","21/Nov/13 22:38;jbellis;Is that long enough for a Really Big Cluster?  It's only 10 gossip rounds.  Seems like RING_DELAY would be a better choice for ""this is the longest I should go w/o hearing from any given node."";;;","22/Nov/13 22:38;brandon.williams;10s seemed 'most compatible' but I think you're right, that RING_DELAY is more semantically correct.  That said, I tend to think we should just set it to 30s instead of ring delay itself, since people often inflate ring delay for various things that might not translate well to this, and since we already seed the initial value at 30s.;;;","22/Nov/13 22:56;jbellis;Allowing people to tune it could be a feature. :);;;","22/Nov/13 22:58;brandon.williams;Shouldn't they be able to tune what we seed the FD with then too? ;);;;","22/Nov/13 23:08;jbellis;I look forward to your patch. :);;;","08/Jan/14 15:40;brandon.williams;I've thought about this a bit, and still think it should default to ring delay, but not be coupled to it.  However, in normal operation, I do think it makes send to couple the initial value we seed the FD with and the max interval we accept.  I don't think most people should be tweaking these though, so I've made them system properties (as ring delay is.)

Patch adds cassandra.fd_initial_value_ms to control the value the FD is seeded with, which the max interval will also default to, but also adds cassandra.fd_max_interval_ms if you really need them to be disjoint (most likely for testing like CASSANDRA-6558 where you want the seed ridiculously low, but the max interval reasonable.)

I will note that I changed the max interval from a double to an int, because a double just didn't make any sense.;;;","21/Jan/14 06:37;jbellis;Shouldn't it actually default to StorageService.getRingDelay then?

+1 otherwise, modulo whitespace issues;;;","22/Jan/14 18:47;brandon.williams;bq. Shouldn't it actually default to StorageService.getRingDelay then?

This goes back to what I was saying about coupling RING_DELAY (which is set by getRingDelay) too closely with FD_INITIAL_VALUE.  Someone unaware of this ticket, who is changing ring delay for some reason, could ,get a nasty surprise, which would be pretty bad, especially in a minor since we're targeting 1.2.14 for this.  On the other hand, with the patch as it is if they *do* want them to move in lockstep for some odd reason, they can just define both properties to be the same.

bq. +1 otherwise, modulo whitespace issues

I couldn't see any whitespace issues.
;;;","22/Jan/14 19:33;jbellis;bq. if they do want them to move in lockstep for some odd reason, they can just define both properties to be the same.

Good reasoning, +1

bq. I couldn't see any whitespace issues.

{{if (newvalue!= null)}};;;","22/Jan/14 19:45;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Range Query contains unwanted results with composite columns,CASSANDRA-4372,12595805,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,grinser,grinser,25/Jun/12 16:33,16/Apr/19 09:32,14/Jul/23 05:52,27/Jun/12 07:53,1.1.2,,,Legacy/CQL,,,1,bug,composite,compositeColumns,cql3,query,range,"Here is a CQL3 range query sample where I get wrong results (tested using cqlsh --cql3) from my perspective:

CREATE KEYSPACE testing WITH strategy_class = 'SimpleStrategy' AND strategy_options:replication_factor = 1;

USE testing;

CREATE TABLE bug_test (a int, b int, c int, d int, e int, f text, PRIMARY KEY (a, b, c, d, e) );

INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 2, '2');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 2, 1, '1');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 3, '3');
INSERT INTO bug_test (a, b, c, d, e, f) VALUES (1, 1, 1, 1, 5, '5');

----------

Normal select everything query:

SELECT * FROM bug_test;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 1 | 1
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Everything fine so far.

----------

Select with greater equal comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e >= 2;

Results:

 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 2 | 2
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

----------

Select with greater comparison for last column of composite key:

SELECT a, b, c, d, e, f FROM bug_test WHERE a = 1 AND b = 1 AND c = 1 AND d = 1 AND e > 2;

Results:
 a | b | c | d | e | f
---+---+---+---+---+---
 1 | 1 | 1 | 1 | 3 | 3
 1 | 1 | 1 | 1 | 5 | 5
 1 | 1 | 1 | 2 | 1 | 1

Bug:
Why was the last row returned? It shouldn't be there, right?

The same issue is also present with between ranges (e >= 1 AND e <= 2)...","Mac OS 10.7.4, Cassandra 1.1.1, single node for testing",grinser,liqusha,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/12 16:34;slebresne;4372.txt;https://issues.apache.org/jira/secure/attachment/12533498/4372.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256104,,,Wed Jun 27 07:53:55 UTC 2012,,,,,,,,,,"0|i0gv5b:",96476,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Jun/12 16:34;slebresne;Fix attached (and I've pushed the test to the distributed tests).;;;","26/Jun/12 17:10;jbellis;LGTM, although if you want to leave the logging in, it should be converted to .debug w/o string concatenation;;;","27/Jun/12 07:53;slebresne;Oups, didn't meant to leave the logging in. Committed with it removed. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hsha server may stop responding and will not close selectors,CASSANDRA-4370,12595782,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kvaster,kvaster,kvaster,25/Jun/12 12:20,16/Apr/19 09:32,14/Jul/23 05:52,26/Jun/12 01:42,1.1.2,,,,,,0,,,,,,,"Cassandra launches several threads to listen on selectors. There can be CancelledKeyException and cassandra will log ""Unexpected exception"". In that case there two problems:
1) listener thread will be closed and cassandra will stop after all listener threads will stop.
2) selector will be not closed",,kvaster,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/12 12:21;kvaster;hsha.patch;https://issues.apache.org/jira/secure/attachment/12533285/hsha.patch",,,,,,,,,,,,,,,,,1.0,kvaster,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256102,,,Tue Jun 26 01:42:23 UTC 2012,,,,,,,,,,"0|i0gv4f:",96472,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"25/Jun/12 12:21;kvaster;Patch;;;","25/Jun/12 16:57;vijay2win@yahoo.com;1) I am wondering when will we get the CancelledKeyException? 
We might get it when the client decides to exit out, isn't it safe to ignore it (may ignore the log as well) and let the following code deal with it?

{code}
                    if (!key.isValid())
                    {
                        // if invalid cleanup.
                        cleanupSelectionkey(key);
                        continue;
                    }
{code}

2) Why should we close the selector when one of the key is invalid?;;;","25/Jun/12 22:14;kvaster;1) This code is not enough. Even if key is valid in that expression, it may be invalid in next statement - I was able to reproduce it on my boxes with cassandra 1.1.1.
2) We should not close selector at all until server shutdown.;;;","26/Jun/12 01:42;vijay2win@yahoo.com;+1, Committed to 1.1 and trunk, (Misread the patch earlier);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bulkLoad() method in StorageService throws AssertionError,CASSANDRA-4368,12595636,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,dinoscottie,dinoscottie,22/Jun/12 21:19,16/Apr/19 09:32,14/Jul/23 05:52,25/Jun/12 17:39,1.1.2,,,Legacy/Tools,,,0,,,,,,,"Firstly, I apologize if this is a duplicate, as I cannot find a bug related to that.

We tried to stream some data to our Cassandra cluster by using JMX bulkLoad method. However, jmx reports a AssertionError since 1.1.0. I haven't really debugged into Cassandra, but by eyeballing the code it seems the AssertionError is thrown from SSTableReader.open() method with the line:
{code}
assert practitioner != null;
{code}
and tracing the code backwards, it seems the code in SSTableLoader.openSSTables() method has been changed to get the partitioner from the impl of inner class SSTableLoader.Client:
{code}
sstables.add(SSTableReader.open(desc, components, null, client.getPartitioner()));
{code}
This is different than 1.0.x codebase, when the partitioner is retrieved from StorageService:
{code}
sstables.add(SSTableReader.open(desc, components, null, StorageService.getPartitioner()));
{code}
The problem seems to me is when StorageService.bulkLoad instantiaties an impl of SSTableLoader.Client() it never does anything with the partitioner, resulting in the call 'client.getPartitioner()' returning null, thus the AssertionError.

(Note: this is me eyeballing the code only without debugging into it).
",,dinoscottie,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/12 16:49;brandon.williams;4368.txt;https://issues.apache.org/jira/secure/attachment/12533332/4368.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256100,,,Mon Jun 25 22:04:58 UTC 2012,,,,,,,,,,"0|i0gv3j:",96468,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jun/12 16:49;brandon.williams;Your analysis was completely correct (and thanks for that, since errors from jmx are stacktraceless!)

Patch to specify the partitioner in bulkLoad();;;","25/Jun/12 17:34;jbellis;LGTM, +1.

Nit: could make the ""setPartitioner(String partclass)"" overload chain to this new one.;;;","25/Jun/12 17:39;brandon.williams;Committed with setPartitioner chained.;;;","25/Jun/12 22:04;hudson;Integrated in Cassandra #1589 (See [https://builds.apache.org/job/Cassandra/1589/])
    removed duplicate SSTableLoader.setPartitioner(IPartitioner) method introduced by patch for CASSANDRA-4368 (Revision 112ce0c69b940d6b663c1e44d2c765bbfc526e8f)

     Result = ABORTED
xedin : 
Files : 
* src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use CF comparator to sort indexed columns in SecondaryIndexManager,CASSANDRA-4365,12595431,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Jun/12 13:57,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/12 09:27,1.1.2,,,,,,0,,,,,,,"SecondaryIndexManager is supposed to have it's internal map sorted according to the base CF comparator, but instead it sorts using the byte buffer natural ordering.

This order is carried along by the sorted set returned by getIndexedColumns(), which in turns end up in a NamesQueryFilter when reading indexed columns, so the order should really be the CF one.

I'll note that I don't think this is a bug because SSTableNamesIterator don't in fact rely on the actual ordering of the names. But it's worth fixing to avoid future problems.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/12 13:57;slebresne;4365.txt;https://issues.apache.org/jira/secure/attachment/12532871/4365.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256097,,,Fri Jun 29 09:27:35 UTC 2012,,,,,,,,,,"0|i0gv27:",96462,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Jun/12 16:57;jbellis;+1;;;","29/Jun/12 09:27;slebresne;Forgot to close that one but the patch has been committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction invalidates row cache,CASSANDRA-4364,12595422,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,marcuse,marcuse,21/Jun/12 12:29,16/Apr/19 09:32,14/Jul/23 05:52,02/Jul/12 07:52,1.2.0 beta 1,,,,,,0,compaction,,,,,,"Compactions invalidate row cache after CASSANDRA-3862

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java#L87",,marcuse,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/12 06:23;jbellis;4364.txt;https://issues.apache.org/jira/secure/attachment/12534143/4364.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256096,,,Thu Nov 15 01:39:41 UTC 2012,,,,,,,,,,"0|i0gv1j:",96459,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"21/Jun/12 18:28;jbellis;What do you sugggest instead?;;;","21/Jun/12 18:37;marcuse;we discussed this a bit on irc, and the reason that the cached rows were invalidated was to get rid of expired tombstones in the cached rows

maybe it is not a big deal to just leave them there?

;;;","22/Jun/12 07:00;slebresne;I think that at the very least we could simply remove expired tombstones for the in-heap cache as we were doing before. At least I don't understand why that was removing since I don't see a problem with that. For the off-heap cache, I guess the choice is between keeping the invalidation or leaving the expired tombstone in cache. Not sure what is best, it will probably depend a bit on the use cases, but my hunch is that just leaving the tombstone in cache is probably a win most of the time.;;;","01/Jul/12 06:23;jbellis;Agreed, the earlier compromise (removeDeletedInCache for on-heap cache; ignore for serializing) is probably best.  Patch attached to restore that.

See CASSANDRA-2304 and CASSANDRA-3921 for more background.;;;","02/Jul/12 06:18;slebresne;+1;;;","02/Jul/12 07:52;jbellis;committed;;;","15/Nov/12 01:39;rcoli;Affects Version/s: 1.2.0 beta 1 
Fix Version/s: 1.1.3

It affects versions after the version it was fixed in? Seems unlikely? Are these transposed? :)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken streaming after CASSANDRA-4311,CASSANDRA-4360,12595278,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,20/Jun/12 15:38,16/Apr/19 09:32,14/Jul/23 05:52,20/Jun/12 19:17,1.2.0 beta 1,,,,,,0,streaming,,,,,,"CASSANDRA-4311 made change in message exchange, that the message will contain header only at the beginning of exchange. This causes FileStreamTask to fail since it expects message header in StreamReply message.",,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/12 15:41;yukim;4360.txt;https://issues.apache.org/jira/secure/attachment/12532713/4360.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256093,,,Wed Jun 20 21:06:50 UTC 2012,,,,,,,,,,"0|i0gv07:",96453,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"20/Jun/12 15:41;yukim;Patch attached to remove assumption that replay message contains message header.
This removal is fine since nodes already reject streaming from different versions.;;;","20/Jun/12 19:10;brandon.williams;+1;;;","20/Jun/12 19:16;yukim;Committed, thanks!;;;","20/Jun/12 21:06;hudson;Integrated in Cassandra #1540 (See [https://builds.apache.org/job/Cassandra/1540/])
    fix streaming for messaging change patch by yukim, reviewed by brandonwilliams for CASSANDRA-4360 (Revision d59be21e8a936d5c893a67e8a3c98505739a7279)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/streaming/FileStreamTask.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no error propagated to client when updating a column family with an invalid column def,CASSANDRA-4353,12594970,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,soverton,soverton,soverton,18/Jun/12 17:31,16/Apr/19 09:32,14/Jul/23 05:52,19/Jun/12 00:00,1.1.2,,,Legacy/CQL,,,0,,,,,,,"CASSANDRA-3761 appears to have introduced a regression which is exposed by test_system_column_family_operations in test/system/test_thrift_server.py

The test fails with this stack trace:
{noformat}
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_system_column_family_operations
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nose/case.py"", line 183, in runTest
    self.test(*self.arg)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1469, in test_system_column_family_operations
    _expect_exception(fail_invalid_field, InvalidRequestException)
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 209, in _expect_exception
    r = fn()
  File ""/opt/acunu/tests/cassandra-tests.hg/thrift/system/test_thrift_server.py"", line 1468, in fail_invalid_field
    client.system_update_column_family(modified_cf)
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1892, in system_update_column_family
    return self.recv_system_update_column_family()
  File ""/usr/lib/python2.6/site-packages/cassandra/Cassandra.py"", line 1903, in recv_system_update_column_family
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""/usr/lib64/python2.6/site-packages/thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 272, in read
    self.readFrame()
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 276, in readFrame
    buff = self.__trans.readAll(4)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib64/python2.6/site-packages/thrift/transport/TSocket.py"", line 108, in read
    raise TTransportException(type=TTransportException.END_OF_FILE, message='TSocket read 0 bytes')
TTransportException: TSocket read 0 bytes

----------------------------------------------------------------------

{noformat}

The logs have the following stack trace:

{noformat}
ERROR [Thrift:1] 2012-06-18 18:17:27,865 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:47)
        at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
        at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1303)
        at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:228)
        at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:648)
        at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3520)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3508)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

",,soverton,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/12 17:41;soverton;4353.patch;https://issues.apache.org/jira/secure/attachment/12532431/4353.patch",,,,,,,,,,,,,,,,,1.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256087,,,Tue Jun 19 00:00:05 UTC 2012,,,,,,,,,,"0|i0gux3:",96439,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"18/Jun/12 17:41;soverton;Attached patch which re-throws MarshalException as InvalidRequestException in CFMetaData.fromThrift;;;","19/Jun/12 00:00;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: ASSUME functionality broken by CASSANDRA-4198 fix,CASSANDRA-4352,12594967,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,18/Jun/12 17:10,16/Apr/19 09:32,14/Jul/23 05:52,18/Jun/12 18:11,1.1.2,,,Legacy/Tools,,,0,cqlsh,,,,,,"All uses of the {{ASSUME}} command in cqlsh now appear to be wholly ineffective at affecting subsequent value output.

This is due to a change in the grammar definition introduced by the fix for CASSANDRA-4198, upon which definition the ASSUME functionality relied.

All that's needed to fix is to update the token-binding names used.",,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/12 17:12;thepaul;4352.patch.txt;https://issues.apache.org/jira/secure/attachment/12532427/4352.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256086,,,Mon Jun 18 18:11:58 UTC 2012,,,,,,,,,,"0|i0guwn:",96437,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Jun/12 17:12;thepaul;Fix attached, or also available in the 4352 patch in my github:

https://github.com/thepaul/cassandra/tree/4352

Current revision of patch is tagged as pending/4352.;;;","18/Jun/12 18:11;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PFS should give a friendlier error message when a node has not been configured,CASSANDRA-4349,12560856,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,16/Jun/12 03:06,16/Apr/19 09:32,14/Jul/23 05:52,18/Jun/12 19:15,1.1.2,,,,,,0,,,,,,,see CASSANDRA-4345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/12 03:16;jbellis;4349.txt;https://issues.apache.org/jira/secure/attachment/12532288/4349.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256084,,,Mon Jun 18 19:15:45 UTC 2012,,,,,,,,,,"0|i0guvj:",96432,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"16/Jun/12 03:16;jbellis;Patch attached.;;;","18/Jun/12 16:07;brandon.williams;+1;;;","18/Jun/12 19:15;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows tools don't work and litter the environment,CASSANDRA-4344,12560755,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,h2o,h2o,h2o,15/Jun/12 11:49,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/12 15:05,1.1.2,,,Legacy/Tools,,,0,,,,,,,"On Windows the tools either don't work at all (cassandra-stress) and/or litter the shell environment (cassandra-stress & sstablemetadata) by repeatedly appending the same information to variables, eventually running out of space.
","any Windows, any JDK",h2o,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/12 11:52;h2o;sstablemeta.patch;https://issues.apache.org/jira/secure/attachment/12532187/sstablemeta.patch","15/Jun/12 11:52;h2o;stress.patch;https://issues.apache.org/jira/secure/attachment/12532188/stress.patch",,,,,,,,,,,,,,,,2.0,h2o,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256079,,,Sat Jun 16 15:05:54 UTC 2012,,,,,,,,,,"0|i0gutb:",96422,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Jun/12 11:52;h2o;Trivial fixes for tools to consistently find the right classes and not litter the environment.;;;","16/Jun/12 13:29;jbellis;Does stress work for you with these changes?  I still get ""Could not find the main class: org.apache.cassandra.stress.Stress."";;;","16/Jun/12 14:21;h2o;Yes, everything runs for me (I just re-tested): start cassandra node on localhost, open second shell in ../tools/bin, run cassandra-stress.bat, it goes to work. I made these changes directly in the unpacked 1.1.1 distro tree. The main problem was that resolving ""*.jar"" as single string did not work for the stress.jar, so it was never found. The other .bat scripts do the same find-append loop, as well as the setlocal.
;;;","16/Jun/12 15:05;jbellis;Ah, I see, this works if you have the tarball extracted with stress.jar in tools/lib ...  Added the build/ directories so it will work from a dev environment as well, and committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix sstable blacklisting for LCS,CASSANDRA-4343,12560701,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/Jun/12 23:56,16/Apr/19 09:32,14/Jul/23 05:52,15/Jun/12 17:28,1.1.2,,,,,,0,compaction,lcs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/12 01:08;jbellis;4343.txt;https://issues.apache.org/jira/secure/attachment/12532151/4343.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256078,,,Fri Jun 15 17:28:06 UTC 2012,,,,,,,,,,"0|i0gusv:",96420,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"15/Jun/12 01:08;jbellis;CompactionsTest failed after CASSANDRA-4341 but the cause was an existing bug in the blacklisting code.

Consider L0 sstables A and B and L1 sstable C.  A and B overlap with C, and C is suspect/blacklisted.  getCandidatesFor will return {A, B, C}, and getCompactionCandidates will then remove C from the candidate list.  A and B will be compacted to D in L1, so L1 will now contain two overlapping sstables C and D.

Patch to fix by check blacklist in getCandidatesFor.  If an overlapping sstable is suspect, we'll proceed to the next possible range instead of doing an incorrect compaction.  Overlapping suspect sstables in L0 are also handled.;;;","15/Jun/12 16:48;yukim;+1 (I didn't see any test failure without patch, though.);;;","15/Jun/12 17:28;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data insertion fails because of commitlog rename failure,CASSANDRA-4337,12560470,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,patrycjusz,patrycjusz,13/Jun/12 09:34,16/Apr/19 09:32,14/Jul/23 05:52,31/Jul/12 05:14,1.1.4,,,,,,0,commitlog,,,,,,"h3. Configuration
Cassandra server configuration:
{noformat}heap size: 4 GB
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""xxx.xxx.xxx.10,xxx.xxx.xxx.11""
listen_address: xxx.xxx.xxx.10
rpc_address: 0.0.0.0
rpc_port: 9160
rpc_timeout_in_ms: 20000
endpoint_snitch: PropertyFileSnitch{noformat}

cassandra-topology.properties
{noformat}xxx.xxx.xxx.10=datacenter1:rack1
xxx.xxx.xxx.11=datacenter1:rack1
default=datacenter1:rack1{noformat}

Ring configuration:
{noformat}Address         DC          Rack        Status State   Load            Effective-Ownership Token
                                                                                           85070591730234615865843651857942052864
xxx.xxx.xxx.10  datacenter1 rack1       Up     Normal  23,11 kB        100,00%             0
xxx.xxx.xxx.11  datacenter1 rack1       Up     Normal  23,25 kB        100,00%             85070591730234615865843651857942052864{noformat}

h3.Problem
I have ctreated keyspace and column family using CLI commands:
{noformat}create keyspace testks with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options = {datacenter1:2};
use testks;
create column family testcf;{noformat}

Then I started my Java application, which inserts 50 000 000 rows to created column family using Hector client. Client is connected to node 1.
After about 30 seconds (160 000 rows were inserted) Cassandra server on node 1 throws an exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-13 10:26:38,393 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7345742389552.log to 7475933520374 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}
	
Then, few seconds later Cassandra server on node 2 throws the same exception:
{noformat}ERROR [COMMIT-LOG-ALLOCATOR] 2012-06-14 10:26:44,005 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:204)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:166)
	at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from c:\apache-cassandra\storage\commitlog\CommitLog-7320337904033.log to 7437675489307 failed
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
	... 5 more{noformat}

After that, my application cannot insert any more data. Hector gets TimedOutException from Thrift:
{noformat}Thread-4 HConnectionManager.java 306 2012-06-14 10:26:56,034 HConnectionManager  operateWithFailover 	 WARN  	 %Could not fullfill request on this host CassandraClient<xxx.xxx.xxx.10:9160-10> 
Thread-4 HConnectionManager.java 307 2012-06-14 10:26:56,034 HConnectionManager operateWithFailover 	 WARN  	 %Exception:  
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:35)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at patrycjusz.nosqltest.db.cassandra.CassandraHectorDbAdapter.commitTransaction(CassandraDbAdapter.java:63)
	at patrycjusz.nosqltest.DbTest.insertData(DbTest.java:459)
	at patrycjusz.nosqltest.gui.InsertPanel.executeTask(NePanel.java:154)
	at patrycjusz.nosqltest.gui.InsertPanel$1.run(NePanel.java:141)
	at java.lang.Thread.run(Unknown Source)
Caused by: TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20269)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)
	... 8 more{noformat}","- Node 1:
   Hardware: Intel Xeon 2.83 GHz (4 cores), 24GB RAM, Dell VIRTUAL DISK SCSI 500GB
   System: Windows Server 2008 R2 x64
   Java version: 7 update 4 x64
- Node 2:
    Hardware: Intel Xeon 2.83 GHz (4 cores), 8GB RAM, Dell VIRTUAL DISK SCSI 500GB
    System: Windows Server 2008 R2 x64
	Java version: 7 update 4 x64",h2o,hbarot,patrycjusz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/12 01:12;jbellis;4337-poc.txt;https://issues.apache.org/jira/secure/attachment/12534138/4337-poc.txt","25/Sep/12 18:35;hbarot;Cassandra-Error.txt;https://issues.apache.org/jira/secure/attachment/12546563/Cassandra-Error.txt","15/Jun/12 05:58;patrycjusz;system-node1-stress-test.log;https://issues.apache.org/jira/secure/attachment/12532164/system-node1-stress-test.log","13/Jun/12 09:36;patrycjusz;system-node1.log;https://issues.apache.org/jira/secure/attachment/12531940/system-node1.log","15/Jun/12 05:58;patrycjusz;system-node2-stress-test.log;https://issues.apache.org/jira/secure/attachment/12532165/system-node2-stress-test.log","13/Jun/12 09:37;patrycjusz;system-node2.log;https://issues.apache.org/jira/secure/attachment/12531941/system-node2.log",,,,,,,,,,,,6.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256073,,,Tue Sep 25 18:40:54 UTC 2012,,,,,,,,,,"0|i0guqf:",96409,,patrycjusz,,patrycjusz,Normal,,,,,,,,,,,,,,,,,"13/Jun/12 09:36;patrycjusz;Attached Cassandra server logs;;;","13/Jun/12 15:14;jbellis;Why would commitlog rename fail?  Do you have some kind of permissions problem?;;;","14/Jun/12 06:26;patrycjusz;I don't think that it has anything to do with permissions. I'm running Cassandra as administrator.
Last night I found a probable cause of the problem. I switched to Java 6 update 33 x64 on both machines and was able to perform all planed tests without any errors.
It seems that this bug is related to Java version I used. Problem doesn't occur when using Java 6.;;;","14/Jun/12 21:02;jbellis;Can you reproduce w/ java7 and the Stress tool that ships with Cassandra?;;;","15/Jun/12 05:58;patrycjusz;I've attached logs from test with Stress tool on Java 7 update 5.
I had to run Stress tool twice to reproduce this bug. At first run no exception was thrown.;;;","01/Jul/12 00:53;jbellis;It may be the CommitLogSegment mmap'd buffer preventing rename.  Can you test the attached patch?;;;","03/Jul/12 13:48;patrycjusz;I've tested with attached patch and couldn't reproduce this bug.;;;","31/Jul/12 05:14;jbellis;I was going to try to make this super robust for JVMs that don't provide an munmap cleaner, but I decided that since (a) it's extremely unlikely that anyone runs such JVMs on windows, and (b) such JVMs work fine anyway on *nix since you can rename w/o unmapping first, and (c) this doesn't make things worse for such JVMs; it just fixes it for OpenJDK / Oracle JDK, I'll just commit this as is.

(In any case, the ""right"" solution for such a posited JVM is to wrap its munmap funcationality the way we did for Oracle's...  such functionality has to exist for the GC to clean up direct buffers.);;;","25/Sep/12 18:40;hbarot;I ran into this same problem and I have tried all the solutions mentioned above and I still cant fix the problem.
I am running Cassandra on my windows 7.
    Cassandra : 1.1.5 with Java 6, 
    64 bit version on Windows 7. 

ERROR 11:03:01,454 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
 java.io.IOError: java.io.IOException: Rename from C:\DataStax Community\data\commitlog\CommitLog-83930807354964.log to 84059497979959 failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$3.run(CommitLogAllocator.java:203)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Rename from C:\DataStax Community\data\commitlog\CommitLog-83930807354964.log to 84059497979959 failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:105)
        ... 4 more

Any help is greatly appreciated.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh tab completion error in ""CREATE KEYSPACE""",CASSANDRA-4334,12560374,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,tpatterson,tpatterson,12/Jun/12 16:25,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/12 15:09,1.1.6,,,,,,0,,,,,,,"The following:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'S<TAB>
{code}
will tab complete like this:
{code}
cqlsh> CREATE KEYSPACE test WITH strategy_class = 'SimpleStrategy '
{code}
Note the extra space after SimpleStrategy. Not a big deal to remove, but it could be misleading to people.","ubuntu, git:cassandra-1.1. I see the error in cqlsh with cql2 and cql3.",aleksey,thepaul,tpatterson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 15:53;aleksey;4334-v2-patch.txt;https://issues.apache.org/jira/secure/attachment/12546534/4334-v2-patch.txt",,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256070,,,Wed Sep 26 15:09:12 UTC 2012,,,,,,,,,,"0|i0gup3:",96403,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"25/Sep/12 06:19;thepaul;Aleksey: nice, this was the same solution I came up with tonight before realizing you'd already solved it. You'll also want to check, though, that {{lasttype != 'unclosedName'}} - the same bug will manifest for double-quoted names in cql3 mode otherwise.;;;","25/Sep/12 15:40;aleksey;Thanks, Paul. I'll update the patch.;;;","26/Sep/12 15:09;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json error,CASSANDRA-4331,12560299,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,gang0713,gang0713,12/Jun/12 06:30,16/Apr/19 09:32,14/Jul/23 05:52,27/Jun/12 07:20,1.1.2,,,,,,1,,,,,,,"/apache-cassandra-1.1.1/bin> ./sstable2json  /home/cassandra/data/pimda/CF_bookmark/pimda-CF_bookmark-hd-48-Data.db > test.json


ERROR 22:27:14,215 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
	at java.util.TreeMap.getEntry(TreeMap.java:328)
	at java.util.TreeMap.containsKey(TreeMap.java:209)
	at java.util.TreeSet.contains(TreeSet.java:217)
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:396)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
	at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:27:14,219 Error in ThreadPoolExecutor
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)",,gang0713,marco.matarazzo,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4289,,,,,,,,"12/Jun/12 18:34;jbellis;4331.txt;https://issues.apache.org/jira/secure/attachment/12531862/4331.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256067,,,Wed Jun 27 07:20:30 UTC 2012,,,,,,,,,,"0|i0gunz:",96398,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"12/Jun/12 18:34;jbellis;Same patch I attached to CASSANDRA-4289.;;;","12/Jun/12 18:35;jbellis;Note that as a workaround, deleting the key cache files for your index CFs should fix this.;;;","18/Jun/12 11:35;xedin;Duplicates CASSANDRA-4289;;;","18/Jun/12 20:29;jbellis;Not a duplicate, 4289 was for a 1.2-only issue; this is against 1.1;;;","18/Jun/12 20:31;xedin;Oh, I'm sorry, +1 then.;;;","27/Jun/12 07:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL client timeout when inserting data after creating index,CASSANDRA-4328,12560155,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,evilezh,evilezh,11/Jun/12 11:44,16/Apr/19 09:32,14/Jul/23 05:52,12/Jun/12 13:07,1.1.2,,,,,,0,,,,,,,"After creating index on table inserts fails.
steps (from cqlsh -3)
create table myapp (pidh text, cn text, tn text, s text, m text, ts bigint, PRIMARY KEY (pidh, ts));
INSERT INTO myapp(pidh, cn, tn, s, m, ts) VALUES ('4274@localhost','Test.tests','main','text','bzzzzz',2231897614162493);
create index idx_cn on myapp(cn);

Next insert from cql client time outs without showing error.
Each insert in systemlog gives ERROR [MutationStage:xx] ....
from log file:

 INFO [MigrationStage:1] 2012-06-11 12:28:35,715 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,716 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,868 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db (1312 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [MigrationStage:1] 2012-06-11 12:28:35,869 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:35,869 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-141-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-142-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-140-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db')]
 INFO [FlushWriter:4] 2012-06-11 12:28:35,869 Memtable.java (line 266) Writing Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,104 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-65-Data.db (325 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:36,130 CompactionTask.java (line 221) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-144-Data.db,].  42,461 to 38,525 (~90% of original) bytes for 3 keys at 0.140767MB/s.  Time: 261ms.
 INFO [MigrationStage:1] 2012-06-11 12:28:36,140 SecondaryIndexManager.java (line 208) Creating new index : ColumnDefinition{name=636e, validator=org.apache.cassandra.db.marshal.UTF8Type, index_type=KEYS, index_name='idx_cn', component_index=1}
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,141 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,141 Memtable.java (line 266) Writing Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,255 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db (170 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,256 SecondaryIndex.java (line 159) Submitting index build of myapp.idx_cn for data in SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db')
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,258 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,258 Memtable.java (line 266) Writing Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,390 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/IndexInfo/system-IndexInfo-hd-14-Data.db (84 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8744)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,390 SecondaryIndex.java (line 200) Index build of myapp.idx_cn complete
ERROR [MutationStage:37] 2012-06-11 12:28:39,657 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:37,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more


ERROR [MutationStage:39] 2012-06-11 12:29:39,876 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:39,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more
",Linux emka 3.3.4-gentoo #4 SMP Mon May 14 17:03:02 BST 2012 x86_64 Intel(R) Core(TM) i7-3820 CPU @ 3.60GHz GenuineIntel GNU/Linux,evilezh,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/12 12:05;slebresne;4328.txt;https://issues.apache.org/jira/secure/attachment/12531814/4328.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256064,,,Tue Jun 12 13:07:00 UTC 2012,,,,,,,,,,"0|i0gumn:",96392,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"11/Jun/12 12:41;slebresne;For information secondary indexes are not supported on composite primary keys. We have a ticket for that (CASSANDRA-3680) but this won't be before Cassandra 1.2.

That being said, there is a clear lack of validation here and we will fix that (in other words, the 'create index' should return an meaningful error message).;;;","12/Jun/12 12:05;slebresne;Attaching patch to reject index creation on table with composite primary keys.;;;","12/Jun/12 12:24;jbellis;+1;;;","12/Jun/12 13:07;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in CLI when updating keyspace,CASSANDRA-4322,12559863,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,richardlow,richardlow,08/Jun/12 16:03,16/Apr/19 09:32,14/Jul/23 05:52,11/Sep/12 11:18,1.1.6,,,,,,0,,,,,,,"To repro:

1. Open the cli
2. Create a keyspace:
  create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};
3. Update the keyspace:
  update keyspace ks1 with strategy_options = {replication_factor:3};

The output is:

[default@unknown] create keyspace ks1 with placement_strategy = SimpleStrategy and strategy_options = {replication_factor:1};                               
8ecd5e16-e0f7-37e7-850e-38ee1a3a510e
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};                                        
857af387-6677-3e39-bdf6-e1132673c25b
Waiting for schema agreement...
... schemas agree across the cluster
org.apache.thrift.protocol.TProtocolException: Required field 'keyspace' was not present! Struct: describe_keyspace_args(keyspace:null)
[default@unknown]

The problem is that the patch in CASSANDRA-4052 assumes the CLI is authenticated to a working keyspace.  getKSMetaData in executeUpdateKeySpace is called with keySpace, which is null.

Changing this to keyspaceName partially solves it, we now get:

[default@unknown] update keyspace ks1 with strategy_options = {replication_factor:3};
Not authenticated to a working keyspace.
18d750fc-19d9-30f0-b8b9-18b2e4a0a0d4
Waiting for schema agreement...
... schemas agree across the cluster
Not authenticated to a working keyspace.

This comes from replayAssumptions in getKSMetaData.

It seems that the refresh code needs to be reworked slightly to not assume the CLI is authenticated to a keyspace.",,dbrosius@apache.org,richardlow,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/12 18:59;xedin;CASSANDRA-4322.patch;https://issues.apache.org/jira/secure/attachment/12544362/CASSANDRA-4322.patch","09/Jun/12 00:14;dbrosius@apache.org;auth_for_mod_ks.txt;https://issues.apache.org/jira/secure/attachment/12531471/auth_for_mod_ks.txt",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256059,,,Tue Sep 11 11:18:47 UTC 2012,,,,,,,,,,"0|i0guk7:",96381,,dbrosius@apache.org,,dbrosius@apache.org,Low,,,,,,,,,,,,,,,,,"09/Jun/12 00:14;dbrosius@apache.org;don't allow updating or dropping of a keyspace unless you authenticate to it (use ks).;;;","11/Jun/12 09:25;richardlow;This patch breaks backwards compatibility, so now operations that used to not require setting a keyspace do.  Do we aim to keep the cassandra-cli interface compatible within the same x.x release?;;;","08/Sep/12 18:59;xedin;patch allows to create keyspace without being authorized to one + fixes describe.;;;","11/Sep/12 02:45;dbrosius@apache.org;+1 patch LGTM;;;","11/Sep/12 11:18;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stackoverflow building interval tree & possible sstable corruptions,CASSANDRA-4321,12559783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,08/Jun/12 02:00,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/12 09:20,1.1.2,,,,,,5,lcs,,,,,,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring.",,al@ooyala.com,awinter,doubleday,jeromatron,jsotelo,omid,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 17:54;slebresne;0001-Fix-overlapping-computation-v7.txt;https://issues.apache.org/jira/secure/attachment/12533857/0001-Fix-overlapping-computation-v7.txt","28/Jun/12 17:54;slebresne;0002-Scrub-detects-and-repair-outOfOrder-rows-v7.txt;https://issues.apache.org/jira/secure/attachment/12533858/0002-Scrub-detects-and-repair-outOfOrder-rows-v7.txt","28/Jun/12 17:54;slebresne;0003-Create-standalone-scrub-v7.txt;https://issues.apache.org/jira/secure/attachment/12533859/0003-Create-standalone-scrub-v7.txt","28/Jun/12 17:54;slebresne;0004-Add-manifest-integrity-check-v7.txt;https://issues.apache.org/jira/secure/attachment/12533860/0004-Add-manifest-integrity-check-v7.txt","28/Jun/12 18:44;jbellis;cleanup.txt;https://issues.apache.org/jira/secure/attachment/12533865/cleanup.txt","14/Jun/12 20:18;al@ooyala.com;ooyala-hastur-stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12532132/ooyala-hastur-stacktrace.txt",,,,,,,,,,,,6.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,249133,,,Thu Jul 05 04:53:11 UTC 2012,,,,,,,,,,"0|i0a4zr:",57082,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Jun/12 02:31;jbellis;bq. SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)

Cassandra checks key ordering for correctness with every row that is added at write time, so unless you changed your partitioner (which is emphatically Not Supported), I'm not sure how this can happen.  Whatever it is, it's unlikely to be related to the 1.1.1 upgrade.

Scrub checks that the sstable content is well-formed and readable.  It doesn't check for out-of-order rows.  You can use a tool like sstablekeys to do that.;;;","08/Jun/12 02:48;awinter;The partitioner (RP) was not changed.;;;","08/Jun/12 16:15;jsotelo;We are also seeing this. We also upgraded from 1.1.0 to 1.1.1. This problem only started after the upgrade. Our cluster is smaller, two DCs of 3 nodes each.;;;","08/Jun/12 16:27;jsotelo;Our stack overflow is a little different though:


ERROR [CompactionExecutor:35] 2012-06-08 15:52:42,086 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:35,1,main]
java.lang.StackOverflowError
        at java.util.AbstractList$Itr.hasNext(Unknown Source)
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
        at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)

[repeats]

        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:517)
        at com.google.common.collect.Iterators$3.hasNext(Iterators.java:114)
        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators.size(Iterators.java:129)
        at com.google.common.collect.Sets$3.size(Sets.java:670)
        at com.google.common.collect.Iterables.size(Iterables.java:80)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:557)
        at org.apache.cassandra.db.compaction.CompactionController.<init>(CompactionController.java:79)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:105)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

;;;","08/Jun/12 20:17;omid;We're seeing the same issue after upgrading from 1.0.9 to 1.1.1 on only a single node in a 16 node cluster. Wiping the data off and bootstrapping again didn't help. Compaction looks to be not progressing (according to compactionstats) and I can reproduce this on every ""nodetool flush"".;;;","11/Jun/12 12:26;jbellis;Omid, just to verify -- are you also using LeveledCompactionStrategy?;;;","11/Jun/12 12:34;omid;Jonathan, yes I use LeveledCompactionStrategy with non-default sstable_size_in_mb = 10;;;","11/Jun/12 15:44;slebresne;I believe I may have an idea on what's going on here. When computing overlapping sstables, LeveledManifest was using Range, which excludes its left bound, but sstable intervals are fully inclusive. This means the computation is incorrect when multiple sstables have the same start token. But that in turn is very much possible across levels. This also seem to fit the stack traces above as those mention interval trees where all the sstable actually have the same first and last token.

This is not really a new bug, but I believe that prior to CASSANDRA-4142, this had less consequences.

Attaching a patch that fixes this by using Bounds instead of Range. But since this but ended up creating sstables with out of order keys, I'm also attaching a second patch that add the ability to scrub to detect this and ""repair"" the situation. This is far from perfect in that the way to ""repair"" consists in buffering the out of order rows and write them all in order in a new sstable. So this can easily OOM if the sstable has lots of out of order rows. But I suppose this is better than nothing. There is a unit test included to check that new feature of scrub.

Note that I believe this patch (at least the first one) must be committed to 1.0 too.;;;","11/Jun/12 15:53;jbellis;bq. this can easily OOM if the sstable has lots of out of order rows

Is that likely, given that Range vs Bounds is basically an off-by-one?;;;","11/Jun/12 16:03;jbellis;Backing up: how does incorrect overlapping sets result in out-of-order key writes (that pass that last-written-key check)?;;;","11/Jun/12 16:04;slebresne;bq. Is that likely, given that Range vs Bounds is basically an off-by-one

The thing is, even an off-by-one is enough to have essentially 2 identical sstable on the same level. If so, our new by-level-iterator will happily write a new sstable that is a concatenation of both of those and we'll end up with half of the resulting sstable being out of order.

That being said, now that you mention it, leveled limits the size of sstables so we should be good on that front.;;;","11/Jun/12 16:04;jbellis;Comment on the patch itself: intersects is missing the case of {{that}} entirely containing {{this}}.;;;","11/Jun/12 18:09;slebresne;bq. how does incorrect overlapping sets result in out-of-order key writes (that pass that last-written-key check)?

The last-written-key check is an assertion, so if assertion are disabled this may happen. And the following excerpt of the description above make me believe assertions weren't enabled when the problem occured first:
{noformat}
Running with assertions enabled allows me to start the instance but when doing so I get errors such as:
ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
{noformat}

bq. intersects is missing the case of that entirely containing this

Oups, you're right. Attaching a v2 of the first patch that fixes that and also turn the last-written-key check into a RuntimeException, since  this is an important check.
;;;","11/Jun/12 19:23;jbellis;v2 LGTM (nit: would like to rename variables to xBounds instead of xRange)

re 0002, does this actually work w/ LCR?  ;;;","12/Jun/12 07:35;awinter;If I use the v2 patch startup stops with the following:
{code}
 INFO [main] 2012-06-12 14:23:33,899 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-LocationInfo@1141455324(41/51 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2012-06-12 14:23:33,903 Memtable.java (line 266) Writing Memtable-LocationInfo@1141455324(41/51 serialized/live bytes, 1 ops)
ERROR [FlushWriter:2] 2012-06-12 14:23:33,953 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[FlushWriter:2,5,main]java.lang.RuntimeException: Last written key null >= current key DecoratedKey(61078635599166706937511052402724559481, 4c) writing into /var/lib/XXXX/cassandra/system/LocationInfo/system-LocationInfo-tmp-hd-65597-Data.db
{code}

Given the above I (probably incorrectly) scrubbed the system keyspace which removed all sstables, leaving only the snapshots eg:

{code}
 WARN [CompactionExecutor:5] 2012-06-12 14:29:41,672 CompactionManager.java (line 651) Row at 100 is unreadable; skipping to next
 WARN [CompactionExecutor:5] 2012-06-12 14:29:41,672 CompactionManager.java (line 602) Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(135285944860343992175601105924967452217, 63716c) writing into /var/lib/XXXX/data/cassandra/system/Versions/system-Versions-tmp-hd-37-Data.db
{code}
..eventually resulting in
{code}
WARN [CompactionExecutor:5] 2012-06-12 14:29:41,674 CompactionManager.java (line 692) No valid rows found while scrubbing SSTableReader(path='/var/lib/XXXX/data/cassandra/system/Versions/system-Versions-hd-35-Data.db'); it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot
{code}

A clean bootstrap also stops with similar errors:
{code}
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(61078635599166706937511052402724559481, 4c) writing into /var/lib/XXXX/data/cassandra/system/LocationInfo/system-LocationInfo-tmp-hd-1-Data.db
{code}
and 
{code}
java.lang.RuntimeException: Last written key null >= current key DecoratedKey(93220794208128599841715671226150005828, 746872696674) writing into /var/lib/XXXX/data/cassandra/system/Versions/system-Versions-tmp-hd-1-Data.db
{code}
;;;","12/Jun/12 08:45;slebresne;My bad, I screwed up the assertion -> RuntimeException transition. Attaching v3 that fixes this and renames variables from xRange to xBounds.

bq. re 0002, does this actually work w/ LCR?

Good question :). I honestly haven't tested it, but it should since LCR resets the underlying SSTableIdentityIterator before each iteration (it shouldn't make any difference that you reset while at the end of the row or at the end of the file).
;;;","12/Jun/12 16:52;omid;Tried the patch but the server still doesn't start. The StackOverFlow that gets thrown causes an already loaded column family to be loaded again:

Load CF1:

{code}
reading saved cache /var/lib/cassandra/abcd/saved_caches/SOMEKSP-CF1-KeyCache
2012-06-12_16:18:04.12387  INFO 16:18:04,123 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-2248
...
{code}

Load CF3 which has the corrupted sstables
{code}
2012-06-12_15:31:20.56185  INFO 15:31:20,561 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-7924 (2372910 bytes)
2012-06-12_15:31:20.81897 ERROR 15:31:20,811 Exception in thread Thread[OptionalTasks:1,5,main]
2012-06-12_15:31:20.81901 java.lang.StackOverflowError
2012-06-12_15:31:20.81901 	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:90)
2012-06-12_15:31:20.81906 	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
2012-06-12_15:31:20.81918 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81927 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81934 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81940 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81946 	at java.util.Arrays.mergeSort(Unknown Source)
2012-06-12_15:31:20.81954 	at java.util.Arrays.sort(Unknown Source)
2012-06-12_15:31:20.81960 	at java.util.Collections.sort(Unknown Source)
2012-06-12_15:31:20.81980 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
2012-06-12_15:31:20.81981 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
2012-06-12_15:31:20.81990 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

// stacktrace goes on

2012-06-12_15:31:20.88633 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
2012-06-12_15:31:20.88643 	at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
2012-06-12_15:31:20.88654 	at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
2012-06-12_15:31:20.88664 	at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
2012-06-12_15:31:20.88673 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
2012-06-12_15:31:20.88683 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
2012-06-12_15:31:20.88692 	at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
2012-06-12_15:31:20.88702 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
2012-06-12_15:31:20.88712 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:31:20.88723 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:31:20.88734 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:31:20.88742 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:31:20.88750 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:31:20.88758 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:31:20.88766 	at org.apache.cassandra.db.Table$2.apply(Table.java:574)
2012-06-12_15:31:20.88773 	at org.apache.cassandra.db.Table$2.apply(Table.java:571)
2012-06-12_15:31:20.88782 	at com.google.common.collect.Iterators$8.next(Iterators.java:751)
2012-06-12_15:31:20.88790 	at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
2012-06-12_15:31:20.88800 	at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
2012-06-12_15:31:20.88810 	at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
2012-06-12_15:31:20.88818 	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
2012-06-12_15:31:20.88833 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-12_15:31:20.88842 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(Unknown Source)
2012-06-12_15:31:20.88851 	at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
2012-06-12_15:31:20.88860 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(Unknown Source)
2012-06-12_15:31:20.88870 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(Unknown Source)
2012-06-12_15:31:20.88882 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
2012-06-12_15:31:20.88892 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-12_15:31:20.88901 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-12_15:31:20.88910 	at java.lang.Thread.run(Unknown Source)
{code}

Then it tries to load CF1 again:

{code}

2012-06-12_15:33:52.92593  INFO 15:33:52,925 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-3578 (2528503 bytes)
2012-06-12_15:33:52.92792  INFO 15:33:52,927 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF1/SOMEKSP-CF1-hd-2613 (3374796 bytes)
{code}

Therefore the server fails with the following exception:

{code}
2012-06-12_15:33:53.17913 ERROR 15:33:53,178 Exception encountered during startup
2012-06-12_15:33:53.17919 java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.17934 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:257)
2012-06-12_15:33:53.17940 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:33:53.17948 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:33:53.17956 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:33:53.17962 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:33:53.17967 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:33:53.17972 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:33:53.17979 	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
2012-06-12_15:33:53.17987 	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
2012-06-12_15:33:53.17996 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
2012-06-12_15:33:53.18002 Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18013 	at com.sun.jmx.mbeanserver.Repository.addMBean(Unknown Source)
2012-06-12_15:33:53.18019 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(Unknown Source)
2012-06-12_15:33:53.18028 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(Unknown Source)
2012-06-12_15:33:53.18038 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(Unknown Source)
2012-06-12_15:33:53.18045 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(Unknown Source)
2012-06-12_15:33:53.18053 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(Unknown Source)
2012-06-12_15:33:53.18060 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:253)
2012-06-12_15:33:53.18067 	... 9 more
2012-06-12_15:33:53.18069 java.lang.RuntimeException: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18083 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:257)
2012-06-12_15:33:53.18092 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
2012-06-12_15:33:53.18100 	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
2012-06-12_15:33:53.18109 	at org.apache.cassandra.db.Table.initCf(Table.java:367)
2012-06-12_15:33:53.18114 	at org.apache.cassandra.db.Table.<init>(Table.java:299)
2012-06-12_15:33:53.18119 	at org.apache.cassandra.db.Table.open(Table.java:114)
2012-06-12_15:33:53.18124 	at org.apache.cassandra.db.Table.open(Table.java:97)
2012-06-12_15:33:53.18129 	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
2012-06-12_15:33:53.18139 	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
2012-06-12_15:33:53.18148 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
2012-06-12_15:33:53.18155 Caused by: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
2012-06-12_15:33:53.18167 	at com.sun.jmx.mbeanserver.Repository.addMBean(Unknown Source)
2012-06-12_15:33:53.18173 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(Unknown Source)
2012-06-12_15:33:53.18181 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(Unknown Source)
2012-06-12_15:33:53.18191 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(Unknown Source)
2012-06-12_15:33:53.18198 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(Unknown Source)
2012-06-12_15:33:53.18206 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(Unknown Source)
2012-06-12_15:33:53.18212 	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:253)
2012-06-12_15:33:53.18219 	... 9 more
2012-06-12_15:33:53.18221 Exception encountered during startup: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=ColumnFamilies,keyspace=SOMEKSP,columnfamily=CF1
{code}

Enabling assertions though causes the corrupted sstables to be ignored:

{code}
2012-06-12_16:25:43.32075  INFO 16:25:43,320 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hc-6965 (2105724 bytes)
2012-06-12_16:25:43.32562 ERROR 16:25:43,325 Exception in thread Thread[SSTableBatchOpen:1,5,main]
2012-06-12_16:25:43.32577 java.lang.AssertionError: SSTable first key DecoratedKey(41255474878128469814942789647212295629, 31303132393937357c3337313730333536) > last key DecoratedKey(41219536226656199861610796307350537953, 31303234323538397c3331383436373338)
2012-06-12_16:25:43.32614   at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
2012-06-12_16:25:43.32626   at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
2012-06-12_16:25:43.32638   at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
2012-06-12_16:25:43.32651   at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-12_16:25:43.32662   at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-12_16:25:43.32672   at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-12_16:25:43.32681   at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-12_16:25:43.32692   at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-12_16:25:43.32703   at java.lang.Thread.run(Unknown Source)
{code}

which leads to cassandra booting up. I wonder if scrub will pick up the ignored sstables.

Shouldn't the assertion above (SSTable first key > last key) turn into an exception and get handled properly?

;;;","14/Jun/12 17:14;omid;Scrubbed the column family on a node which had booted up with assertions `on` and there were still corrupt sstables.;;;","14/Jun/12 20:20;al@ooyala.com;I think I just hit the same thing. We're using reverse comparator with bytescomparator on the CF's that seem to be having trouble if that's relevant at all.

Cluster is 1.1.1 on Ubuntu 12.04 and only has about 7GB per node at the moment.

Stacktrace attached.;;;","14/Jun/12 20:22;al@ooyala.com;BTW if somebody points me to a build, tag, or commit ID to test, I'll push it out right away. It's only a 3-node cluster and I can easily take a filesystem snapshot before running.;;;","15/Jun/12 14:49;slebresne;bq. Tried the patch but the server still doesn't start.

Right. So the problem is, as you noticed, that there is really no way to start the server and having it load a broken sstable, which means there is no way to run scrub on it. Even without assertions, we rely on interval trees which breaks if the sstable first key is not before the last one.

After having look a bit more closely on that problem, I think the cleaner way to solve this is to provide a way to run scrub ""offline"", which allows to skip the interval trees. So attaching a 3rd patch that provide that. It adds a new binary 'sstablescrub' that takes as argument a keyspace name and column family name and scrub the relevant sstables, and does this without breaking if the sstable have some out of order keys. I kind of think that having an offline scrub is not a bad idea anyway.

With that, you should be able to stop the node, run 'sstablescrub ksname cfname' and then restart the node and you should be good to go.
;;;","16/Jun/12 00:26;al@ooyala.com;What SHA / tag should these patches apply against? I've tried trunk, 1.1.1 and 1.1.0 and can't get a clean apply. I'll try a manual merge tomorrow.;;;","16/Jun/12 00:31;jbellis;cassandra-1.1 branch;;;","18/Jun/12 21:04;omid;Thanks for the patch. Offline scrub is indeed very useful.

Tried the v3 patches and the scrub didn't complete, possibly because of a different issue, with the following failed assertion:

{code}
Exception in thread ""main"" java.lang.AssertionError: Unexpected empty index file: RandomAccessReader(filePath='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-33827-Index.db', skipIOCache=true)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:221)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:376)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:203)
        at org.apache.cassandra.io.sstable.SSTableReader.openNoValidation(SSTableReader.java:143)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:79)
{code}

which consequently, encountered corrupt SSTables during start-up:

{code}
2012-06-18_20:36:19.89543  INFO 20:36:19,895 Opening /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-24984 (1941993 bytes)
2012-06-18_20:36:19.90217 ERROR 20:36:19,900 Exception in thread Thread[SSTableBatchOpen:9,5,main]
2012-06-18_20:36:19.90222 java.lang.IllegalStateException: SSTable first key DecoratedKey(41255474878128469814942789647212295629, 31303132393937357c3337313730333536) > last key DecoratedKey(41219536226656199861610796307350537953, 31303234323538397c3331383436373338)
2012-06-18_20:36:19.90261 	at org.apache.cassandra.io.sstable.SSTableReader.validate(SSTableReader.java:441)
2012-06-18_20:36:19.90275 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:208)
2012-06-18_20:36:19.90291 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
2012-06-18_20:36:19.90309 	at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:245)
2012-06-18_20:36:19.90324 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-18_20:36:19.90389 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-18_20:36:19.90391 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-18_20:36:19.90391 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-18_20:36:19.90392 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-18_20:36:19.90392 	at java.lang.Thread.run(Unknown Source)
{code}

although didn't prevent Cassandra from starting up, but compaction failed subsequently:

{code}
2012-06-18_20:51:41.79122 ERROR 20:51:41,790 Exception in thread Thread[CompactionExecutor:81,1,main]
2012-06-18_20:51:41.79131 java.lang.RuntimeException: Last written key DecoratedKey(12341204629749023303706929560940823070, 33363037353338) >= current key DecoratedKey(12167298275958419273792070792442127650, 31363431343537) writing into /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-40992-Data.db
2012-06-18_20:51:41.79161 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-06-18_20:51:41.79169 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-06-18_20:51:41.79180 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-06-18_20:51:41.79189 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-06-18_20:51:41.79199 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
2012-06-18_20:51:41.79210 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-06-18_20:51:41.79218 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-18_20:51:41.79227 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-18_20:51:41.79235 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-18_20:51:41.79242 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-18_20:51:41.79250 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-18_20:51:41.79259 	at java.lang.Thread.run(Unknown Source)
{code};;;","18/Jun/12 22:34;al@ooyala.com;Offline scrub ran fine for me.  I downgraded to 1.1.0 and ran a compaction and it looks fine.  (edit) finished offline scrub on both affected nodes and they're back to normal.;;;","19/Jun/12 01:06;awinter;I can confirm I also experienced the ""Unexpected empty index file"" errors on some of the nodes that I have run sstablescrub on.

Other nodes had this error when running sstablescrub:
{code}
Scrub of SSTableReader(path='/var/lib/XXXX/data/cassandra/KS/CF/KS-CF-hd-259648-Data.db') complete: 1592 rows in new sstable and 0 empty (tombstoned) rows dropped
EOF after 6 bytes out of 8
{code}

Compactions stop with the ""java.lang.RuntimeException: Last written key DecoratedKey"" error on the nodes affected by either of the above 2 errors .

Nodes that seem to have been repaired by the sstablescrub still continue to have ""java.lang.RuntimeException: Last written key DecoratedKey"" errors scattered through the logs but are still compacting.

Is there any further information we can supply to help debug?;;;","19/Jun/12 06:37;slebresne;My bad. Forgot to exclude temporary and compacted files from the scrubbed files.

Attaching a v4 of last patch to fix. Hopefully this should fix the offline scrub.;;;","19/Jun/12 14:28;omid;Tried v4 patch and offline scrub went through completely. Cassandra started without any error but compaction halted again:

{code}
2012-06-19_14:01:03.47432  INFO 14:01:03,474 Compacting [SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-67792-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65607-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-63279-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65491-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-68332-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-64720-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65322-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-66557-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-64504-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-68179-Data.db'), SSTableReader(path='/var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-hd-65005-Data.db')]
2012-06-19_14:01:08.73528 ERROR 14:01:08,733 Exception in thread Thread[CompactionExecutor:11,1,main]
2012-06-19_14:01:08.73538 java.lang.RuntimeException: Last written key DecoratedKey(42351003983459534782466386414991462257, 313632303432347c3130303632313432) >= current key DecoratedKey(38276735926421753773204634663641518108, 31343638373735327c3439343837333932) writing into /var/lib/cassandra/abcd/data/SOMEKSP/CF3/SOMEKSP-CF3-tmp-hd-68399-Data.db
2012-06-19_14:01:08.73572 	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
2012-06-19_14:01:08.73581 	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
2012-06-19_14:01:08.73590 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
2012-06-19_14:01:08.73600 	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
2012-06-19_14:01:08.73611 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
2012-06-19_14:01:08.73622 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
2012-06-19_14:01:08.73633 	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2012-06-19_14:01:08.73642 	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
2012-06-19_14:01:08.73650 	at java.util.concurrent.FutureTask.run(Unknown Source)
2012-06-19_14:01:08.73657 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
2012-06-19_14:01:08.73665 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2012-06-19_14:01:08.73672 	at java.lang.Thread.run(Unknown Source)
{code}

All SSTables that participated in compaction were new ones written by the offline scrub (according their timestamp and also id range.) although the first one didn't exist any more (already promoted before the exception?)

{quote}This is not really a new bug, but I believe that prior to CASSANDRA-4142, *this had less consequences*.{quote}

Sylvain, could you please elaborate on this? I'd like to know how pre-1.1.1 data is affected by the Range-vs-Bounds bug. Only overlapping/duplicate sstables on the same level leading to slower reads caused by unneeded sstable lookups?
;;;","19/Jun/12 15:40;slebresne;Just to make sure: you did apply 0001-Change-Range-Bounds-in-LeveledManifest.overlapping-v3.txt before restarting the server after having run the offline scrub, right?

If so, that would mean we have yet another bug that generates out of order keys during compaction.;;;","19/Jun/12 16:15;omid;Exactly.

- Applied v3
- Ran offline scrub and it failed because of tmp files.
- Started Cassandra and saw failures.
- Applied v4 to cassandra-1.1 branch.
- Ran offline scrub successfully.
- Started Cassandra successfully.
- Compaction failed because of above error.

All done on the same instance.;;;","19/Jun/12 16:25;omid;Let me know if I can provide more data.;;;","19/Jun/12 16:33;slebresne;Did the new exception happened quickly after having started the node with the scrubbed files? Are you able to reproduce easily (i.e. if you restart the node and compact, do you still get the same error). If you are able to reproduce, would you be at liberty to provide a set of sstables that produce the error during compaction (in private for instance). It would be much more easy to tack that down with an easy way to reproduce.;;;","20/Jun/12 11:44;omid;The exceptions happens not quickly afterwards but after some rounds of compaction on the CF. I had re-bootstrapped so there are tons of ~(1500) pending compaction tasks. If I restart the node and compact the problem happens again and I can reproduce it. I'll send you an email about the data.;;;","20/Jun/12 15:40;slebresne;Alright, I think the problem may just be that my offline scrub was kind of broken in that it wasn't dealing with the leveled manifest correctly. Attaching a v5 that update the manifest correctly but also check that there is no overlapping sstables in the manifest (and send back sstables to L0 if that happens).

So hopefully running this new version of the offline scrub should fix it (the 2 first patch are untouched, I only rebased them).;;;","20/Jun/12 17:05;omid;Will try it again. LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel fails because of junit timeout when I run it together with all other suits, but passes when I only run LeveledCompactionStrategyTest suit. Is it related?
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED (timeout)
{code};;;","20/Jun/12 19:37;omid;Got ""java.lang.OutOfMemoryError: Java heap space"" with -Xmx256M.

Tried with -Xmx512M and the scrub failed with:

{code}
Checking leveled manifest
d != org.apache.cassandra.io.sstable.SSTableReader
java.util.IllegalFormatConversionException: d != org.apache.cassandra.io.sstable.SSTableReader
        at java.util.Formatter$FormatSpecifier.failConversion(Formatter.java:3999)
        at java.util.Formatter$FormatSpecifier.printInteger(Formatter.java:2709)
        at java.util.Formatter$FormatSpecifier.print(Formatter.java:2661)
        at java.util.Formatter.format(Formatter.java:2433)
        at java.util.Formatter.format(Formatter.java:2367)
        at java.lang.String.format(String.java:2769)
        at org.apache.cassandra.tools.StandaloneScrubber.checkManifest(StandaloneScrubber.java:179)
        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:148)
{code};;;","20/Jun/12 19:43;jbellis;Are you using Java6 or Java7?;;;","20/Jun/12 19:51;omid;The above error is due to the %d in StandaloneScrubber.java:179's interpolation. Will fix and try again.

Jonathan: Sun Java 6, 1.6.0_26;;;","21/Jun/12 05:34;awinter;After working around the issue with the 0003 v5 patch that Omid refers I've had an sstablescrub complete on one of my servers.  sstablescrub did detected several overlapping sstables, resetting them to L0, but no out of order keys.

The Last written key DecoratedKey >= current key exception however resurfaces again after the first set of compactions, 5 minutes after startup, in the exact same manner as before.  The same exception occurs for various CF's until compactions stop completely.  compactionstats still shows a large number of pending compaction tasks after this event.;;;","21/Jun/12 10:19;slebresne;bq. The above error is due to the %d in StandaloneScrubber.java:179's interpolation.

Yes, sorry for that typo. I've updated the v5 patch to fix it.

bq. Got ""java.lang.OutOfMemoryError: Java heap space"" with -Xmx256M

The last version of the offline scrub ""loads"" all sstable readers, which means in particular that it loads the summary of the key index and the sstable bloom filter. In other words, it does use a bit more memory, so it's not necessarily surprising that -Xmx256M is not enough.

bq. LeveledCompactionStrategyTest:testValidationMultipleSSTablePerLevel fails because of junit timeout when I run it together with all other suits, but passes when I only run LeveledCompactionStrategyTest suit. Is it related?

I doubt it. I've already seen test timeout when run with the full suit but not alone. I wouldn't worry too much about that. At least that test is working fine on my machine.;;;","21/Jun/12 16:40;omid;I experienced the same as Anton's. One observation is that the out-of-order key being wrongly iterated by CompactionIterable's MergeIterator which causes the exception, happen to be the start of an interval:

DEBUG 18:10:41,693 Creating IntervalNode from [... Interval(DecoratedKey(33736808147257072541807562080745136438, ... ), 

which leads me to suspect if it's due to the ""Range"" (vs. Bounds) used in LeveledCompactionStrategy::getScanners. Any ideas?;;;","22/Jun/12 11:38;slebresne;bq. which leads me to suspect if it's due to the ""Range"" (vs. Bounds) used in LeveledCompactionStrategy::getScanners. Any ideas?

No, that Range is correct because this correspond to repair ranges and is correctly interpreted. And in fact, when doing compaction that range is actually null so that cannot be the problem.


So Anton sent me some sstables that triggered an out-of-order exception when compacting them. What I did with that is:
1) apply the last version of the v5 patch on this issue on top of the current git cassandra-1.1 branch (using the release of CASSANDRA 1.1.1 would probably work too because I don't think there is any other related fix since 1.1.1 that went into the git branch but anyway, that's what I used).
2) I ran the offline scrub *while the node was stopped*. I insist on that last part because that having the node run during the offline scrub would mess things up. I'd actually like to make the offline scrub check if the node is running and refuse to work in that case but I'm not sure of what is the best way to do that.
3) I restarted the node once the scrub was done

I was then able to compact the node fully (i.e, I ran compaction until there was nothing more to compact) and this without hitting any more error.

Was I lucky? Are you guys able to reproduce those steps and still get more errors?;;;","22/Jun/12 13:03;omid;So I had offline-scrubbed the live Cassandra node and I had copied the sstables that participated in one of the failed compaction. Assuming the sstables had been offline-scrubbed, I had skipped step 2 above locally, so unfortunately I can't yet reproduce it locally with a limited set of data.;;;","23/Jun/12 00:44;awinter;bq. Was I lucky? Are you guys able to reproduce those steps and still get more errors?

As discussed, but repeated here just for the ticket's reference; I was patching and scrubbing in the same way as described above.  Once the scrubbed nodes were restarted in the cluster they were then under normal read/write load and experienced the exceptions again.  Given that the sstablescrub and subsequent compactions run fine in Sylvain's test, using my out of order sstables, means that the sstablescrub command appears to do its job fine.  The root cause, originally expected to be resolved with the 0001 patch, still appears to be occurring so Sylvain was going to investigate the code further.;;;","26/Jun/12 16:09;slebresne;So, still not sure what would cause out of order keys outside of the bug fixed by the first patch on this issue. But I've rebased the patch as v6 and added a small last patch to check for overlapping sstables in levels each time we modify the manifest.

Could you try doing an offline scrub (while the node is shutdown) and then restart the node with all those patches. If the problem comes back, hopefully the last patch should give us a bit more info. So if the error reproduce and as soon as it does, it would be useful to get the error log as well as the manifest files (the 2 json files along the sstables).;;;","26/Jun/12 16:28;jbellis;Could the CASSANDRA-4341 regression have caused what Anton saw, if he was running from the 1.1 branch?;;;","26/Jun/12 16:31;slebresne;Hum, yes that's possible. I figured this wouldn't be the problem since they were using 1.1.1 but you are right that if they tried against the 1.1 branch that could have been it. Worth checking on current 1.1 branch I guess.;;;","27/Jun/12 20:02;jbellis;CompactionsTest.testBlacklistingWithLeveledCompactionStrategy is currently failing in trunk because of a similar integrity check that I added to promote() for CASSANDRA-1991:

{code}
.       DecoratedKey last = null;
        Collections.sort(generations[newLevel], SSTable.sstableComparator);
        for (SSTableReader sstable : generations[newLevel])
        {
            assert last == null || sstable.first.compareTo(last) > 0;
            last = sstable.last;
        }
{code}

Patch 0001 does not fix that test failure.;;;","28/Jun/12 10:39;slebresne;bq. CompactionsTest.testBlacklistingWithLeveledCompactionStrategy is currently failing in trunk because of a similar integrity check that I added to promote()

That's a bug in the integrity check in fact, that should skip the check if newLevel=0 (since we can have newLevel=0 following CASSANDRA-4341). With that fixed there is no more failure of that unit test (I've committed the fix as 4725bf71e18550ac60f9).;;;","28/Jun/12 17:54;slebresne;Alright, so I'm pretty sure I've found the root cause of all this. On top of the Range->Bound problem we were not correctly computing the set of overlapping sstable in L1 when compacting multiple L0 files. Citing the comment of the attached fix (where sstables = 'sstables in L0 to compact' and candidates = 'sstable in L1'):
{noformat}
/*
 * Note that picking each sstable from candidates that overlap one of the sstable of sstables is not enough
 * because you could have the following situation:
 *   sstables = [ s1(a, c), s2(m, z) ]
 *   candidates = [ s3(e, g) ]
 * In that case, s2 overlaps none of s1 or s2, but if we compact s1 with s2, the resulting sstable will be
 * overlapping s3, so we must return s3.
 */
{noformat}

So I'm attaching a v7 of the patches with the fix for that in the first patch. The last patch (the integrity tests) was also breaking the offline scrub so that's fix in v7 too. Though I note that the integrity tests of the last patch is probably overkill and I didn't really intended to commit it (i.e. the integrity check that is in trunk is probably enough).;;;","28/Jun/12 18:44;jbellis;Nice work, I think you nailed it!

+1 from me, and agreed that I'd rather backport the limited sanity check from trunk than use 0004 here.

Also attached a cleanup patch (applies after 0001) to add javadoc, improve parameter names, and simplify ""overlapping"" by making union explicit when desired.;;;","29/Jun/12 07:59;awinter;I've applied the v7 patches and have successfully offline scrubbed & reinserted a number of nodes in my ring without further occurrence of the previous issues.  Thanks :);;;","29/Jun/12 09:20;slebresne;Great. So I committed the first 3 patches along with the cleanup patch and I backported the simpler integrity check from trunk.;;;","30/Jun/12 00:09;awinter;Maybe I spoke too soon.  Overnight I've seen the exceptions happen again on nodes that were v7 patched & scrubbed.  

{code}
ERROR [CompactionExecutor:1301] 2012-06-29 21:54:12,078 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:1301,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(116816802911061669023614481109871014436, 4faa631ca88ef85b8e26ddeb) >= current key DecoratedKey(115179899219377463875853982254751557438, 4fa892bf42d3f24479f627b6) writing into /var/lib/XXXX/data/cassandra/KS/CF/KS-CF-tmp-hd-837655-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code};;;","30/Jun/12 00:19;jbellis;To clarify, is this 1.1.1 release + patches, or 1.1 dev branch + patches?;;;","30/Jun/12 00:20;awinter;1.1 dev branch + patches;;;","04/Jul/12 03:28;awinter;I have repeatedly run sstablescrub across all my nodes and the exceptions do not occur as frequently now, however, the integrity check still throws exceptions and compactionstats shows a large number of pending tasks but no progression afterwards.

Should this ticket be reopened or a new one raised?

{code}
ERROR [CompactionExecutor:912] 2012-07-04 01:07:16,470 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:912,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:214)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:158)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:531)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:254)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:978)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}
;;;","04/Jul/12 09:37;slebresne;Damn. Ok, since this has been released with 1.1.2 already, would you mind opening a new one?;;;","05/Jul/12 04:53;awinter;New issue raised as requested: CASSANDRA-4411;;;"
Assertion error while delivering the hints.,CASSANDRA-4320,12559780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,08/Jun/12 00:25,16/Apr/19 09:32,14/Jul/23 05:52,15/Jun/12 03:04,1.1.2,1.2.0 beta 1,,,,,0,,,,,,,"java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:269)
        at org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:442)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Did some digging and looks like we just need to skip the deleted columns.",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/12 03:47;vijay2win@yahoo.com;0001-CASSANDRA-4320-v2.patch;https://issues.apache.org/jira/secure/attachment/12531366/0001-CASSANDRA-4320-v2.patch","08/Jun/12 00:31;vijay2win@yahoo.com;0001-CASSANDRA-4320.patch;https://issues.apache.org/jira/secure/attachment/12531352/0001-CASSANDRA-4320.patch",,,,,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256058,,,Wed Jun 13 18:58:20 UTC 2012,,,,,,,,,,"0|i0gujj:",96378,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Jun/12 02:27;jbellis;So...  I guess the deletion during one page, doesn't get purged by removeDeleted in the next since it's happening in the same ms?

What if we just increased the gcBefore in the removeDeleted call to MAX_VALUE?;;;","08/Jun/12 03:47;vijay2win@yahoo.com;ahaaa that works too :) Plz see the attached. Thanks!;;;","08/Jun/12 17:46;vijay2win@yahoo.com;Actually v2 wont work because RowMutation.hintFor we do 
{code}
ttl = Math.min(ttl, cf.metadata().getGcGraceSeconds()); 
{code}

so v1 is still better.;;;","09/Jun/12 15:25;jbellis;I don't follow, using max_value means that if an ExpiringColumn has expired we will ignore it, if it has not then we don't want to.;;;","09/Jun/12 17:49;vijay2win@yahoo.com;Sorry i should have been clear, because of 

CFS.removeDeletedSuper
{code}
if (subColumn.getLocalDeletionTime() < gcBefore
                    || cf.deletionInfo().isDeleted(c.name(), subColumn.timestamp())
                    || c.deletionInfo().isDeleted(subColumn))
                {
                    subIter.remove();
                }
{code}

Hence we will remove everything and never replay any column if we set the max_value. Makes sense?;;;","13/Jun/12 18:58;jbellis;makes sense.  committed v1 with a comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool compactionstats fails with NullPointerException,CASSANDRA-4318,12559766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,sj.climber,sj.climber,07/Jun/12 22:25,16/Apr/19 09:32,14/Jul/23 05:52,09/Jun/12 15:37,1.1.2,,,,,,0,caching,,,,,,"Test uses Column family C defined as follows:

create column family C with caching = 'keys_only' and key_validation_class = 'LongType' and compression_options = { sstable_compression: SnappyCompressor, chunk_length_kb: 64 } and max_compaction_threshold=0; 

max_compaction_threshold is set to 0 to disable auto compaction.

SSTables are streamed via sstableloader, after which a major compaction is triggered using ""nodetool compact MyKeyspace C"".

Thereafter, attempts to request compaction stats via ""nodetool compactionstats"" fail with the following exception:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.compaction.CompactionInfo.asMap(CompactionInfo.java:103)
        at org.apache.cassandra.db.compaction.CompactionManager.getCompactions(CompactionManager.java:1115)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1404)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:600)
        at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662) ","2 node cluster running on Ubuntu 10.10 64-bit
16GB Max Heap allocated to each node.
Test keyspace using replication factor 2",sj.climber,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/12 03:14;jbellis;4318.txt;https://issues.apache.org/jira/secure/attachment/12531362/4318.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256056,,,Sat Jun 09 15:37:45 UTC 2012,,,,,,,,,,"0|i0guin:",96374,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"08/Jun/12 03:14;jbellis;patch attached to deal with cfm directly instead of looking up by id, and create a dummy cfm for cache saving instead of leaving null;;;","08/Jun/12 11:34;xedin;+1;;;","09/Jun/12 15:37;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in handleStateNormal in a mixed cluster,CASSANDRA-4317,12559719,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Jun/12 16:51,16/Apr/19 09:32,14/Jul/23 05:52,20/Jun/12 02:12,1.2.0 beta 1,,,,,,0,,,,,,,"In a 3 node cluster with one seed on trunk, a member on trunk, and another member on a previous version, the following occurs only on the non-seed trunk member:

{noformat}

ERROR 16:44:18,708 Exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1072)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:995)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1568)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:819)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:897)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:57)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

This doesn't repro if a non-trunk member is the seed, however upgrading the seed first should still be valid.",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4311,CASSANDRA-4120,,,CASSANDRA-4101,,"19/Jun/12 16:22;brandon.williams;0001-Gossip-current-network-version.txt;https://issues.apache.org/jira/secure/attachment/12532575/0001-Gossip-current-network-version.txt","19/Jun/12 16:22;brandon.williams;0002-Check-both-ms-and-gossip-for-version-when-handling-sta.txt;https://issues.apache.org/jira/secure/attachment/12532576/0002-Check-both-ms-and-gossip-for-version-when-handling-sta.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256055,,,Wed Jun 20 02:12:47 UTC 2012,,,,,,,,,,"0|i0gui7:",96372,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"14/Jun/12 15:26;brandon.williams;The problem here is similar to what CASSANDRA-4311 is trying to solve, but with a small twist.

Consider node X, Y, and Z.  Both X and Y are on the newer version v2, but Z is on the older version v1.  X is the seed, and Z is the first to talk to X, and they complete a gossip round and both speak v1 to each other.  Now Y contacts X and they complete a gossip round, where Y learns about Z.  The problem, however, is that Y has never talked directly to Z, so it doesn't actually know Z's version yet but needs to handle events for it.  StorageService then does version checks to determine how to handle the events for Z, but getVersion assumes the current version (v2 in this example) when it doesn't actually know what the version is, and thus ends up processing old-style information as new-style and fails.  This isn't limited to handleStateNormal, but likely any event called from onChange.

Solving this is somewhat tricky.  Reviving CASSANDRA-4101 would do it, but then we have two sources of version information and I'm not sure how clean that will be.  getVersion would have to check for null, and if found check for a gossip version, and if that too is not found assume the *lowest* version.

;;;","19/Jun/12 16:22;brandon.williams;The first patch is CASSANDRA-4101 rebased, which gossips the net version.

The second adds a way to see is MS _really_ knows the version (so that we don't have to mess with its current behavior) and encapsulates the logic to check both it and gossip to determine if hostids should be used inside SS, replacing all the current checks there as well.;;;","20/Jun/12 01:59;vijay2win@yahoo.com;+1;;;","20/Jun/12 02:12;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index CF tombstones can cause OOM,CASSANDRA-4314,12559612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,wpoziombka,wpoziombka,06/Jun/12 22:16,16/Apr/19 09:32,14/Jul/23 05:52,08/Jun/12 07:26,1.0.11,1.1.2,,,,,0,,,,,,,"My database (now at 1.0.10) is in a state in which it goes out of memory with hardly any activity at all.  A key slice nothing more.

The logs attached are this including verbose gc in stdout.  I started up cassandra and waited a bit to ensure that it was unperturbed.

Then (about 15:46) I ran this slice (using Pelops), which in this case should return NO data.  My client times out and the database goes OOM.

                  ConsistencyLevel cl = ConsistencyLevel.TWO;//TWO nodes in my cluster
                  Selector s = new Selector(this.pool);
                  List<IndexExpression> indexExpressions = new ArrayList<IndexExpression>();
                  IndexExpression e = new IndexExpression(
                              ByteBuffer.wrap(""encryptionSettingsID"".getBytes(ASCII)), IndexOperator.EQ,
                              ByteBuffer.wrap(encryptionSettingsID.getBytes(Utils.ASCII)));
                  indexExpressions.add(e);
                  IndexClause indexClause = new IndexClause(indexExpressions,
                              ByteBuffer.wrap(EMPTY_BYTE_ARRAY), 1);
                  SlicePredicate predicate = new SlicePredicate();
                  predicate.setColumn_names(Arrays.asList(new ByteBuffer[]
                        { ByteBuffer.wrap(COL_PAN_ENC_BYTES) }));
                  List<KeySlice> slices = s.getKeySlices(CF_TOKEN, indexClause, predicate, cl);

Note that “encryptionSettingsID” is an indexed column.  When this is executed there should be no columns with the supplied value.

I suppose I may have some kind of blatant error in this query but it is not obvious to me.  I’m relatively new to cassandra.

My key space is defined as follows:

KsDef(name:TB_UNIT, strategy_class:org.apache.cassandra.locator.SimpleStrategy, strategy_options:{replication_factor=3}, 
cf_defs:[

CfDef(keyspace:TB_UNIT, name:token, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:70 61 6E 45 6E 63, validation_class:BytesType), ColumnDef(name:63 72 65 61 74 65 54 73, validation_class:DateType), ColumnDef(name:63 72 65 61 74 65 44 61 74 65, validation_class:DateType, index_type:KEYS, index_name:TokenCreateDate), ColumnDef(name:65 6E 63 72 79 70 74 69 6F 6E 53 65 74 74 69 6E 67 73 49 44, validation_class:UTF8Type, index_type:KEYS, index_name:EncryptionSettingsID)], caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:pan_d721fd40fd9443aa81cc6f59c8e047c6, column_type:Standard, comparator_type:BytesType, caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:counters, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:75 73 65 43 6F 75 6E 74, validation_class:CounterColumnType)], default_validation_class:CounterColumnType, caching:keys_only)

])


tpstats show pending tasks many minutes after time out:


[root@r610-lb6 bin]# ../cassandra/bin/nodetool -h 127.0.0.1 tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         3         3            107         0                 0
RequestResponseStage              0         0             56         0                 0
MutationStage                     0         0              6         0                 0
ReadRepairStage                   0         0              0         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0           2231         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemtablePostFlusher               0         0              3         0                 0
StreamStage                       0         0              0         0                 0
FlushWriter                       0         0              3         0                 0
MiscStage                         0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              9         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
BINARY                       0
READ                         0
MUTATION                     0
REQUEST_RESPONSE             0

cfstats:

Keyspace: keyspace
        Read Count: 118
        Read Latency: 0.14722033898305084 ms.
        Write Count: 0
        Write Latency: NaN ms.
        Pending Tasks: 0
                Column Family: token
                SSTable count: 7
                Space used (live): 4745885584
                Space used (total): 4745885584
                Number of Keys (estimate): 18626048
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 118
                Read Latency: 0.147 ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 55058352
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 150
                Compacted row maximum size: 258
                Compacted row mean size: 201

                Column Family: pan_2fef6478b62242dd94aecaa049b9d7bb
                SSTable count: 7
                Space used (live): 1987147156
                Space used (total): 1987147156
                Number of Keys (estimate): 14955264
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 28056224
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 104
                Compacted row maximum size: 124
                Compacted row mean size: 124

                Column Family: counters
                SSTable count: 11
                Space used (live): 3433469364
                Space used (total): 3433469364
                Number of Keys (estimate): 21475328
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 40271696
                Key cache capacity: 4652
                Key cache size: 4652
                Key cache hit rate: NaN
                Row cache: disabled
                Compacted row minimum size: 125
                Compacted row maximum size: 179
                Compacted row mean size: 150

","AS5 64, 64 GB ram, 12 core, Intel SSD ",christianmovi,cywjackson,garyogasawara,kirktrue,slebresne,wpoziombka,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/12 17:22;wpoziombka;2012-06-07-compact.zip;https://issues.apache.org/jira/secure/attachment/12531274/2012-06-07-compact.zip","07/Jun/12 15:40;wpoziombka;2012-06-07.zip;https://issues.apache.org/jira/secure/attachment/12531270/2012-06-07.zip","08/Jun/12 02:48;jbellis;4314-1.0.txt;https://issues.apache.org/jira/secure/attachment/12531360/4314-1.0.txt","08/Jun/12 02:48;jbellis;4314-1.1.txt;https://issues.apache.org/jira/secure/attachment/12531361/4314-1.1.txt","06/Jun/12 22:16;wpoziombka;oom.zip;https://issues.apache.org/jira/secure/attachment/12531173/oom.zip","07/Jun/12 05:42;wpoziombka;yourkitsnapshot.png;https://issues.apache.org/jira/secure/attachment/12531220/yourkitsnapshot.png",,,,,,,,,,,,6.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256053,,,Sat Jun 09 15:29:23 UTC 2012,,,,,,,,,,"0|i0guh3:",96367,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"06/Jun/12 22:16;wpoziombka;log files.;;;","06/Jun/12 22:26;jbellis;Sounds like ""I'm using a row as a queue and building up a ton of tombstones"" to me, which is an antipattern: Cassandra has to send the tombstones back to the coordinator for read repair.;;;","07/Jun/12 00:31;wpoziombka;I'm sorry but I don't understand the statement.  I have done no deletes and the rows are very small (max is like 285 bytes according to cfstats, which is inline with what I know about these data).  I did drop a column family before and I have updated many column values.  I don't know if that creates tombstones too.  

the model is this:

token - is the primary column family.  Has a column in it called ""pan"" which contains nearly unique binary values.  We need to be able to uniquely search pan so I have a pan_XXX family with pan as the key and token is a column name with a timestamp as a value.  pan_XXX is basically an index to the token column family.  

In the current scenario, there are very few token columns in the pan column family (indeed the largest row is 124 bytes by cfstat's measure).  At some point I need to essentially re-index (pan values change).  So I create a new dynamic column family (pan_YYY), modify the token's pan column and add new column to pan_YYY then when fully done I drop pan_XXX column family.

So at the end of it a new column family (an index) is populated and the old one is dropped.  All values in one column of the token column family are modified.

What is shown here in these logs is none of the above though.  I have restarted cassandra and done nothing but run the one query.

AND ONE MORE THING

I neglected to mention that during the update of the ""token"" column family it updates the indexed column too.  The indexed column essentially holds either XXX or YYY so we can resolve pan_XXX etc.  This may be important.  As it goes through each is eventually changed from XXX to YYY.  This index is the same that is used in the query above.;;;","07/Jun/12 05:42;wpoziombka;A screenshot from Yourkit snapshot I took while it was growing.

Looks like 4 GB of DeletedColumn stuff.;;;","07/Jun/12 14:02;jbellis;bq. I have done no deletes

Then either you're using TTLs -- expired columns are basically the same as deletes, in this respect -- or you're doing a lot of indexed column overwrites, which also generate deletes in the index CF.;;;","07/Jun/12 15:33;wpoziombka;I am doing the latter.  However, I have run repair which I would expect to clear up the tombstones no?  I still observe the same problem after repair.;;;","07/Jun/12 15:40;wpoziombka;Here are log files taken showing running repair then running the query in question demonstrating the out of memory condition.;;;","07/Jun/12 15:56;wpoziombka;I guess repair may not be the ticket... it should be done during compaction I guess.  But compaction should run automatically.  I have heeded the note on Tuning Compaction and not run a major compaction:

""Also, once you run a major compaction, automatic minor compactions are no longer triggered frequently forcing you to manually run major compactions on a routine basis. So while read performance will be good immediately following a major compaction, it will continually degrade until the next major compaction is manually invoked. For this reason, major compaction is NOT recommended by DataStax."";;;","07/Jun/12 16:55;brandon.williams;Repair doesn't remove tombstones, and compactions only remove them if they are older than gc_grace_seconds, so you're always holding however many occurred in that time period.;;;","07/Jun/12 17:00;jbellis;Hmm.  Looks like we don't override index gcgs to 0.  I can't think of any reason to keep tombstones around on a purely local table.;;;","07/Jun/12 17:22;wpoziombka;I have run nodetool compact on each node then reran the query.  Still run out of memory.  Attached is logs.;;;","08/Jun/12 02:48;jbellis;bq. Looks like we don't override index gcgs to 0

Patches to do this attached for 1.0 and 1.1.  If you compact the *index* CF with this patch applied, that should get rid of the tombstones. (compacting the data CF won't do anything.);;;","08/Jun/12 07:17;slebresne;+1;;;","08/Jun/12 07:26;slebresne;(Committed);;;","08/Jun/12 16:17;wpoziombka;Jonathan, thanks a lot for this.  I was hedging my bets and duplicating column families instead of the column overwrites but this is WAY more preferrable.

A couple of quick questions: so I should do explicit compactions on the index CF only?  Once I do this explicit compaction I must do for all column families as part of routing maintenance?  Again I am speaking in reference to the comment in the doc: 

""Also, once you run a major compaction, automatic minor compactions are no longer triggered frequently forcing you to manually run major compactions on a routine basis. So while read performance will be good immediately following a major compaction, it will continually degrade until the next major compaction is manually invoked. For this reason, major compaction is NOT recommended by DataStax.""

;;;","08/Jun/12 19:32;wpoziombka;How to compact the index CF?  What is its name?  I've tried a variety of things and can't seem to find the magic expression.
;;;","09/Jun/12 15:29;jbellis;It's not exposed through nodetool, but you can invoke forceMajorCompaction on the index cfs mbean directly (o.a.c.db.IndexColumnFamilies), or you can drop and recreate the index.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS always try to load key cache,CASSANDRA-4313,12559572,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,06/Jun/12 17:42,16/Apr/19 09:32,14/Jul/23 05:52,06/Jun/12 19:00,1.2.0 beta 1,,,,,,0,,,,,,,"Inside constructor, below condition is always evaluated to true:

{code}
if (caching != Caching.NONE || caching != Caching.ROWS_ONLY)
    CacheService.instance.keyCache.loadSaved(this);
{code}

should be

{code}
 if (caching == Caching.ALL || caching == Caching.KEYS_ONLY)
{code}",,hudson,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/12 17:42;yukim;4313.txt;https://issues.apache.org/jira/secure/attachment/12531127/4313.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256052,,,Sat Jun 16 14:42:50 UTC 2012,,,,,,,,,,"0|i0gugn:",96365,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"06/Jun/12 17:47;slebresne;+1

Does that affect 1.1? If so, we should commit there too.;;;","06/Jun/12 19:00;yukim;Committed to trunk only, 1.1 is not affected.
Thanks for the review!;;;","16/Jun/12 14:42;hudson;Integrated in Cassandra #1502 (See [https://builds.apache.org/job/Cassandra/1502/])
    only load key cache when caching is ALL/KEYS_ONLY, fix by yukim, reviewed by slebresne for CASSANDRA-4313 (Revision 383a608e7ff73befaaf34ad7cc0aab4cc26d1316)

     Result = FAILURE
yukim : 
Files : 
* src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix OOM with ReadMessageTest.testNoCommitLog,CASSANDRA-4312,12559485,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius@apache.org,dbrosius@apache.org,dbrosius@apache.org,06/Jun/12 04:06,16/Apr/19 09:32,14/Jul/23 05:52,07/Jun/12 02:41,1.2.0 beta 1,,,Legacy/Testing,,,0,,,,,,,"this test can throw OOMs, because it uses a FileReader and readLine to read the commit log. However, some commit logs are fully allocated, but not initialized, (all 0s) so finding an EOL means reading 134M of data. Even for commit logs that have data they really aren't filereader-type streams.

changed to do simple byte finding in the streams instead.",,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/12 04:07;dbrosius@apache.org;fix_oom_in_readmessagetest.txt;https://issues.apache.org/jira/secure/attachment/12531061/fix_oom_in_readmessagetest.txt",,,,,,,,,,,,,,,,,1.0,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256051,,,Thu Jun 07 02:41:32 UTC 2012,,,,,,,,,,"0|i0gugf:",96364,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"06/Jun/12 16:08;yukim;+1 with nit: you need to remove whitespaces on empty row.;;;","07/Jun/12 02:41;dbrosius@apache.org;committed to trunk as commit de6dba73352b8406452bd2b0ab792e9af817c901;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up messagingservice protocol limitations,CASSANDRA-4311,12559479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,06/Jun/12 02:18,16/Apr/19 09:32,14/Jul/23 05:52,14/Jun/12 23:03,1.2.0 beta 1,,,,,,0,jmx,,,,,,"Weaknesses of the existing protocol:

- information asymmetry: node A can know what version node B expects, but not vice versa (see CASSANDRA-4101)
- delayed information: node A will often not know what version node B expects, until after first contacting node B -- forcing it to throw that first message away and retry for the next one
- protocol cannot handle both cross-dc forwarding and broadcast_address != socket address (see bottom of CASSANDRA-4099)
- version is partly global, partly per-connection, and partly per-message, resulting in some interesting hacks (CASSANDRA-3166) and difficulty layering more sophisticated OutputStreams on the socket (CASSANDRA-3127, CASSANDRA-4139)",,brandon.williams,cburroughs,marcuse,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4317,,,,,,,,"06/Jun/12 03:11;jbellis;4311-skeleton.txt;https://issues.apache.org/jira/secure/attachment/12531058/4311-skeleton.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256050,,,Thu Jun 14 23:03:46 UTC 2012,,,,,,,,,,"0|i0gug7:",96363,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"06/Jun/12 02:22;jbellis;I think we can address all of these by making these changes to VERSION_12 protocol:

- send version and connection metadata just once per connection.  This will give the other side enough information to know whether to expect a compressed or varint stream
- *both* sides exchange CURRENT_VERSION when connection is established; each side will then have version information immediately upon contacting another node

As a consequence,
- When version changes, we need to drop existing connections and reconnect (already implied but not yet implemented by CASSANDRA-3127);;;","06/Jun/12 03:11;jbellis;I think the changes to ITC demonstrate what I have in mind pretty clearly (attached).  I'll flesh the rest of this out shortly.;;;","06/Jun/12 16:52;brandon.williams;This looks like a good solution so far.

bq. When version changes, we need to drop existing connections and reconnect

Let me take this opportunity to suggest that set/getVersion be moved out of Gossiper (which does nothing but expose a Map) and into MS.;;;","06/Jun/12 18:17;jbellis;Pushed to https://github.com/jbellis/cassandra/branches/4311-2;;;","07/Jun/12 12:36;marcuse;looks good to me, just a bit confusing calling it a 'header' when it is essentially a handshake packet

could we negotiate SSL the way we do compression? nothing 'secret' is shared during the handshake phase anyway, after that we could upgrade sockets;;;","07/Jun/12 17:49;brandon.williams;MS.setVersion needs a minor fix to prevent NPE:

{noformat}
     public Integer setVersion(InetAddress address, int version)
     {
         logger.debug(""Setting version {} for {}"", version, address);
         Integer v = versions.put(address, version);
         return v == null ? version : v;
     }
{noformat}

+1 otherwise.;;;","14/Jun/12 23:03;jbellis;bq. MS.setVersion needs a minor fix to prevent NPE

Done and committed.

bq. could we negotiate SSL the way we do compression

Sure, can you open a separate ticket for that?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3: cqlsh exception running ""describe schema""",CASSANDRA-4309,12559451,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,cdaw,cdaw,05/Jun/12 21:45,16/Apr/19 09:32,14/Jul/23 05:52,13/Jul/12 15:46,1.1.3,,,Legacy/Tools,,,0,cqlsh,,,,,,"{code}
cqlsh> describe schema;

CREATE KEYSPACE system WITH strategy_class = 'LocalStrategy';

USE system;

Traceback (most recent call last):
  File ""./cqlsh"", line 811, in onecmd
    self.handle_statement(st, statementtext)
  File ""./cqlsh"", line 839, in handle_statement
    return custom_handler(parsed)
  File ""./cqlsh"", line 1329, in do_describe
    self.describe_schema()
  File ""./cqlsh"", line 1264, in describe_schema
    self.print_recreate_keyspace(k, sys.stdout)
  File ""./cqlsh"", line 1091, in print_recreate_keyspace
    self.print_recreate_columnfamily(ksname, cf.name, out)
  File ""./cqlsh"", line 1114, in print_recreate_columnfamily
    layout = self.get_columnfamily_layout(ksname, cfname)
  File ""./cqlsh"", line 706, in get_columnfamily_layout
    layout = self.fetchdict()
  File ""./cqlsh"", line 605, in fetchdict
    return dict(zip([d[0] for d in desc], row))
TypeError: 'NoneType' object is not iterable
{code}",,cdaw,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256049,,,Fri Jul 13 15:46:14 UTC 2012,,,,,,,,,,"0|i0gufz:",96362,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"05/Jun/12 21:47;thepaul;Occurs when using CQL3 and when the keyspace has no columnfamilies. (Workaround: make a columnfamily :);;;","09/Jul/12 23:44;thepaul;This should be fixed for the system keyspace case by CASSANDRA-4380. Can't reproduce my earlier theory that this was caused by empty keyspaces in general; they seem to be working fine in 1.1.1 and on the 1.1 tip.;;;","13/Jul/12 15:46;brandon.williams;Resolving since CASSANDRA-4380 is in.  Cathy, if you still experience this on 1.1 tip please feel free to reopen.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isMarkedForDelete can return false if it is a few seconds in the future,CASSANDRA-4307,12559327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,wadey,wadey,05/Jun/12 06:09,16/Apr/19 09:32,14/Jul/23 05:52,06/Jun/12 08:06,1.1.2,,,,,,0,,,,,,,"The patch in CASSANDRA-3716 causes some weird issues to arrise when server times don't exactly match up (and since the resolution is seconds, it is easy to be off by just enough to see it).

I am seeing a case where during schema propagation .isMarkedForDelete() is checked, but the timestamp is a few seconds in the future because the schema was sent from a different node. The code then happily tries to interpret the value of the column as a String, but it is actually the Int encoded deletion time.

Here is an example in the code that does this check and will do the wrong thing if the deletion timestamp is even just a few seconds in the future: https://github.com/apache/cassandra/blob/47f0cc5d38d272ec9f7d6179eb3ffa28c6f74107/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L607-609

To prove that this is a problem, here is a stack trace of a machine trying to interpret the ""localDeletionTime"" value of a DeletedColumn as UTF-8 because the .isMarkedForDeletion() check failed:

https://gist.github.com/deb064d4377d206368d3",,slebresne,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/12 17:40;wadey;4307-test.txt;https://issues.apache.org/jira/secure/attachment/12530986/4307-test.txt","05/Jun/12 14:20;slebresne;4307.txt;https://issues.apache.org/jira/secure/attachment/12530959/4307.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256047,,,Wed Jun 06 08:06:42 UTC 2012,,,,,,,,,,"0|i0guev:",96357,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Jun/12 14:20;slebresne;I believer you're right, good catch!
Attaching patch to re-add the overwrite of isMarkedForDelete in DeletedColumn. We shouldn't have removed it since DeletedColumn should never ever be interpreted as live columns, even with lack of node synchronization (it's ok for expiring columns though).;;;","05/Jun/12 15:49;jbellis;+1;;;","05/Jun/12 17:40;wadey;Test case to ensure no future regressions. Fails if 4307 patch is not applied.;;;","06/Jun/12 08:06;slebresne;Committed. I included the unit test, thanks Wade.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-stress scripts should be executable,CASSANDRA-4302,12558854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,thobbs,thobbs,31/May/12 20:15,16/Apr/19 09:32,14/Jul/23 05:52,31/May/12 20:38,1.1.1,1.2.0 beta 1,,Legacy/Tools,,,0,,,,,,,Just need to {{chmod u+x tools/bin/cassandra-stress tools/bin/cassandra-stressd}}.,,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256042,,,Thu May 31 20:38:29 UTC 2012,,,,,,,,,,"0|i0gucn:",96347,,,,,Low,,,,,,,,,,,,,,,,,"31/May/12 20:19;thobbs;I suppose tools/bin/sstablemetadata needs to be executable, as well.;;;","31/May/12 20:38;yukim;Committed in a5e0994a130dac325dfde8b8d258b14677788c60, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing host ID results in NPE when delivering hints,CASSANDRA-4300,12558773,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,31/May/12 12:48,16/Apr/19 09:32,14/Jul/23 05:52,31/May/12 16:11,,,,,,,0,vnodes,,,,,,"
In {{StorageService.handledStateNormal()}} the token-to-endpoint map is updated before the id-to-endpoint map, creating a small window where {{TokenMetadata.isMember()}} can return true before a host ID is available.

Trivial patch forthcoming.

{noformat}
[...]
 INFO [GossipStage:1] 2012-05-30 21:59:10,683 Gossiper.java (line 833) Node /10.2.131.32 has restarted, now UP
 INFO [GossipStage:1] 2012-05-30 21:59:10,684 Gossiper.java (line 799) InetAddress /10.2.131.32 is now UP
 INFO [HintedHandoff:1] 2012-05-30 21:59:10,697 HintedHandOffManager.java (line 304) Started hinted handoff for host: null with IP: /10.2.131.32
ERROR [HintedHandoff:1] 2012-05-30 21:59:10,698 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:112)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:305)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:265)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:86)
	at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:439)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 Events.java (line 130) Node 10.2.131.32 now available: true
 INFO [GossipStage:1] 2012-05-30 21:59:10,700 StorageService.java (line 1218) Node /10.2.131.32 state jump to normal
 INFO [GossipStage:1] 2012-05-30 21:59:10,701 Events.java (line 120) Node 10.2.131.32 is now in state NORMAL
[...]
{noformat}",,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/12 12:54;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4300-update-host-ID-before-token.txt;https://issues.apache.org/jira/secure/attachment/12530384/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4300-update-host-ID-before-token.txt",,,,,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256040,,,Thu May 31 16:11:06 UTC 2012,,,,,,,,,,"0|i0gubz:",96344,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"31/May/12 14:34;brandon.williams;+1;;;","31/May/12 16:11;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: create table don't always validate access to the right keyspace,CASSANDRA-4296,12558575,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,30/May/12 10:41,16/Apr/19 09:32,14/Jul/23 05:52,30/May/12 14:06,1.1.1,,,Legacy/CQL,,,0,,,,,,,"Create table allows (like other queries) to override the currently set keyspace ({{CREATE TABLE foo.bar ...}}). However, when we do that, the access check is done on the wrong keyspace. In particular if no keyspace was set, this end up in a NPE.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/12 10:44;slebresne;4296.patch;https://issues.apache.org/jira/secure/attachment/12530186/4296.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256036,,,Wed May 30 14:06:53 UTC 2012,,,,,,,,,,"0|i0gua7:",96336,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/May/12 10:44;slebresne;Patch attached;;;","30/May/12 10:59;jbellis;+1;;;","30/May/12 14:06;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading encounters: 'SimpleStrategy requires a replication_factor strategy option.' and refuses to start,CASSANDRA-4294,12558481,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,29/May/12 19:48,16/Apr/19 09:32,14/Jul/23 05:52,01/Jun/12 15:41,1.1.1,,,,,,0,,,,,,,"I've seen this reported quite a few times now:

{noformat}
ERROR [main] 2012-05-29 19:33:40,589 AbstractCassandraDaemon.java (line 370) Exception encountered during startup
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.db.Table.<init>(Table.java:275)
  at org.apache.cassandra.db.Table.open(Table.java:114)
  at org.apache.cassandra.db.Table.open(Table.java:97)
  at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
  at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
  at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
  at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:71)
  at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:218)
  at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:295)
  at org.apache.cassandra.db.Table.<init>(Table.java:271)
  ... 5 more
{noformat}

The common thread seems to be old lineage, from at least 0.7.  1.0.x works fine, but upgrading to 1.1 causes the problem.",,pasthelod,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/12 08:51;slebresne;4294.txt;https://issues.apache.org/jira/secure/attachment/12530524/4294.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256035,,,Fri Jun 01 15:41:25 UTC 2012,,,,,,,,,,"0|i0gu9j:",96333,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"29/May/12 19:57;brandon.williams;A common workaround for this is to delete the schema (Schema* and Migrations* in the system keyspace) and then recreate it.;;;","31/May/12 05:03;jbellis;To clarify:

0.7 -> 1.0: fine
1.0 -> 1.1: fine
0.7 -> 1.1: not fine

?;;;","31/May/12 05:10;brandon.williams;I strongly believe the path originates from 0.7, but 1.1 is the culprit.;;;","01/Jun/12 08:45;slebresne;Well, apparently we did broke compatibility of 0.7 -> 1.1. In 1.0, in {{KSMetadata.fromAvro()}}, we have the following line:
{noformat}
maybeAddReplicationFactor(strategyOptions, ks.strategy_class.toString(), ks.replication_factor);
{noformat}
which allows compatibility with the old ways where the replication factor was part of the KsDef directly. In 1.1, we've removed that line (the fromAvro method has been moved to config/Avro.java).

I think the idea of removing this was that we were fine not supporting direct upgrade from 0.7 -> 1.1 and force upgrade to 1.0 first. Unfortunately, since maybeAddReplicationFactor() was called by fromAvro, it means that one would have to upgrade from 0.7 to 1.0 *and* do some schema upgrade to every keyspace before upgrading to 1.1. Otherwise, I don't think that the fix of maybeAddReplicationFactor is persisted (I haven't tested so I'm not 100% sure but I think that's the case).

So I think we may want to add back the maybeAddReplicationFactor to fromAvro for now. We can drop that in 1.2 if we want however, since 1.1 will guarantee that this is persisted since the first thing 1.1 does is to save schema in the new format.;;;","01/Jun/12 08:51;slebresne;Attaching patch that adds back {{maybeAddReplicationFactor}} for compatibility sake as suggested in previous comment.;;;","01/Jun/12 12:39;pasthelod;Upgrading from 1.0.2 to 1.1.0, I've encountered this problem.

Also, I wouldn't mind the KeySpace upgrade provided there's a guide for it, which I haven't been able to locate so far.;;;","01/Jun/12 14:02;jbellis;There was a bunch more code using mARF in 1.0, mainly from forwardsCompatibleOptions.  Do we need that too?;;;","01/Jun/12 14:10;slebresne;I don't think we do, {{forwardsCompatibleOptions}} was used on the thrift side (so not on the upgrade path), to ""support"" KsDef.replication_factor. But it's marked deprecated so people shouldn't use it (if they do, we will ignore it and likely trigger an exception since the replication factor will be missing).;;;","01/Jun/12 14:42;jbellis;+1 from me, did you want to test Brandon?;;;","01/Jun/12 15:41;slebresne;Committed, thanks.

Brandon: feel free to complain if you do test and this doesn't fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oversize integer in CQL throws NumberFormatException,CASSANDRA-4291,12558264,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,ssadler,ssadler,27/May/12 06:08,16/Apr/19 09:32,14/Jul/23 05:52,31/May/12 00:40,1.0.11,1.1.1,,Legacy/CQL,,,0,cql,,,,,,"In CQL, the parser does not handle an oversize Integer, the client socket get closed and an exception is output in the log.

{noformat}cqlsh:TEST1> select count(*) from Items limit 10000000000000;
TSocket read 0 bytes
cqlsh:TEST1> select count(*) from Items limit 1;
TSocket read 0 bytes{noformat}

{noformat}
ERROR 02:51:28,600 Error occurred during processing of message.
java.lang.NumberFormatException: For input string: ""10000000000""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:461)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:631)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:221)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:951)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:873)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1234)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

The INTEGER type in Cql.g matches digits but not to any particular limit.",,dbrosius@apache.org,ssadler,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/12 16:39;dbrosius@apache.org;4291_catch_misc_excs_processing_statements.txt;https://issues.apache.org/jira/secure/attachment/12529971/4291_catch_misc_excs_processing_statements.txt",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256032,,,Thu May 31 00:20:40 UTC 2012,,,,,,,,,,"0|i0gu87:",96327,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"28/May/12 16:39;dbrosius@apache.org;catch misc runtime exceptions from cql.getStatement and return InvalidRequestExceptions with 'useful messages' for these so that the connection from cql to the server doesn't get into a bad state.

against trunk;;;","30/May/12 15:46;xedin;This one is related to 1.0 CQL(2) and also CQL3 in 1.1. +1, so just add a patch for cql3 and you are good to go commit it to 1.0 and 1.1;;;","31/May/12 00:20;dbrosius@apache.org;committed to cassandra-1.0 as commit edfc06fdd0794831cf7d1401fa98ff38bb4e4210

added fix for cql3 in cassandra-1.1 as commit 59f349de705f38daa8869c64817e27e9657ccc45;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary Indexes fail following a system restart,CASSANDRA-4289,12558250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,26/May/12 16:56,16/Apr/19 09:32,14/Jul/23 05:52,01/Jun/12 15:57,1.2.0 beta 1,,,Feature/2i Index,,,0,,,,,,,"Create a new cf with a secondary index, and queries with indexes predicates work fine until the server is restarted, after which they error and the following stacktrace is output to the log:

{code}
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
	at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:44)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:88)
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:1)
	at org.apache.cassandra.utils.IntervalTree.comparePoints(IntervalTree.java:191)
	at org.apache.cassandra.utils.IntervalTree.contains(IntervalTree.java:203)
	at org.apache.cassandra.utils.IntervalTree.access$3(IntervalTree.java:201)
	at org.apache.cassandra.utils.IntervalTree$IntervalNode.searchInternal(IntervalTree.java:293)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:140)
	at org.apache.cassandra.utils.IntervalTree.search(IntervalTree.java:146)
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1259)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:229)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1300)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1174)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1104)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:144)
	at org.apache.cassandra.db.index.keys.KeysSearcher$1.computeNext(KeysSearcher.java:1)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1409)
	at org.apache.cassandra.db.index.keys.KeysSearcher.search(KeysSearcher.java:88)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:595)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1398)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:47)
	at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:870)
	at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:259)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:134)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:1)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:184)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

Tested with a single node setup & verified that this behaviour is only present in trunk, cassandra-1.0.10 works as expected.
","Single node, trunk",dbrosius@apache.org,samt,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4331,,,,,,"01/Jun/12 03:27;jbellis;4289-keycache.txt;https://issues.apache.org/jira/secure/attachment/12530503/4289-keycache.txt","01/Jun/12 14:18;samt;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4289-Fix-errors-with-secondary-index-caused-.txt;https://issues.apache.org/jira/secure/attachment/12530550/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4289-Fix-errors-with-secondary-index-caused-.txt",,,,,,,,,,,,,,,,2.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256030,,,Fri Jun 01 15:57:26 UTC 2012,,,,,,,,,,"0|i0gu7b:",96323,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"27/May/12 05:02;dbrosius@apache.org;easily reproducable. The index's column family's IntervalTree's head BigIntegerToken compared to the searched against column value.;;;","31/May/12 05:26;dbrosius@apache.org;It appears this commit introduced this regression

Save IndexSummary into new SSTable 'Summary' component for CASSANDRA-2392

http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=04874186892c86a20181a2f64c5dc24285021b2c;;;","31/May/12 10:12;xedin;The commit you mentioned doesn't touch CFS it merely saves/loads primary index into separate component, but anyway - what do you suggest as work around?;;;","01/Jun/12 02:48;jbellis;What am I doing wrong?

{noformat}
cqlsh> create schema ks with strategy_class ='SimpleStrategy' and strategy_options:replication_factor=1;
cqlsh> use ks;
cqlsh:ks> create table foo(key text primary key, i int);
cqlsh:ks> create index i_idx on foo(i);
cqlsh:ks> insert into foo (key, i) values('asdf', 1);
cqlsh:ks> select * from foo where i = 1;
  KEY | i
------+---
 asdf | 1

[restart Cassandra server and cqlsh]

cqlsh> use ks;
cqlsh:ks> select * from foo where i = 1;
 KEY  | i 
------+---
 asdf | 1
{noformat};;;","01/Jun/12 03:06;dbrosius@apache.org;before stopping the server do

nodetool flush;;;","01/Jun/12 03:27;jbellis;Still works for me.

I did fix a bug in the key cache for index CFs (attached) but I don't think that's what you're seeing.;;;","01/Jun/12 14:21;samt;SSTableReader.loadSummary wasn't taking the specific IPartitioner for the sstable into consideration, so when the IndexSummary for a secondary index cf was loaded from disk, the keys in the summary and the first & last keys of the sstable were decorated incorrectly. 

jbellis: I guess you weren't seeing this as the IndexSummary wasn't being written down to disk, but I don't why that would be;;;","01/Jun/12 15:57;xedin;Committed combination of Sam's patch and changes to CacheService so row/key cache serializer methods would use per-CF partitioner instead of global. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SizeTieredCompactionStrategy.getBuckets is quadradic in the number of sstables,CASSANDRA-4287,12558177,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/May/12 19:47,16/Apr/19 09:32,14/Jul/23 05:52,26/May/12 05:37,1.0.11,1.1.1,,,,,0,compaction,,,,,,"getBuckets first sorts the sstables by size (N log N) then adds each sstable to a bucket (N**2 in the worst case of all sstables the same size, because we use the bucket's contents as a hash key).",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256028,,,Sat May 26 05:37:20 UTC 2012,,,,,,,,,,"0|i0gu6f:",96319,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"25/May/12 20:23;jbellis;Changesets up on https://github.com/jbellis/cassandra/branches/4287;;;","25/May/12 20:58;thobbs;+1;;;","26/May/12 05:37;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kick off background compaction when min/max changed,CASSANDRA-4279,12557067,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,23/May/12 20:51,16/Apr/19 09:32,14/Jul/23 05:52,25/May/12 20:53,1.0.11,1.1.1,,,,,0,compaction,,,,,,"When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/12 20:54;jbellis;4279.txt;https://issues.apache.org/jira/secure/attachment/12528778/4279.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256021,,,Fri May 25 20:53:52 UTC 2012,,,,,,,,,,"0|i0gu33:",96304,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"23/May/12 20:54;jbellis;patch attached.;;;","24/May/12 14:50;slebresne;+1;;;","25/May/12 20:53;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't specify certain keyspace properties in CQL,CASSANDRA-4278,12557052,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,23/May/12 19:24,16/Apr/19 09:32,14/Jul/23 05:52,29/May/12 08:48,1.1.1,,,,,,0,cql,cql3,,,,,"A user using EC2MultiRegionSnitch, where the datacenter name has to match the AWS region names, will not be able to specify a keyspace's replica counts for those datacenters using CQL. AWS region names contain hyphens, which are not valid identifiers in CQL, and CQL keyspace/columnfamily properties must be identifiers or identifiers separated by colons.

Example:

{noformat}
CREATE KEYSPACE Foo
  WITH strategy_class = 'NetworkTopologyStrategy'
      AND strategy_options:""us-east""=1
      AND strategy_options:""us-west""=1;
{noformat}

(see http://mail-archives.apache.org/mod_mbox/cassandra-user/201205.mbox/browser for context)

..will not currently work, with or without the double quotes.

CQL should either allow hyphens in COMPIDENT, or allow quoted parts of a COMPIDENT token.",,psanford,slebresne,thepaul,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/12 14:34;slebresne;4278.txt;https://issues.apache.org/jira/secure/attachment/12529072/4278.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256020,,,Tue May 29 08:48:18 UTC 2012,,,,,,,,,,"0|i0gu2n:",96302,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"24/May/12 14:34;slebresne;Attaching patch that cleans up (I think) the definition of property names. Not only does it allow quoted identifiers, it also make the non quoted ones case insensitive. I'll note that it remove the support integer without quotes, i.e, one can't write ""strategy_options:4 = ..."", but I'm pretty sure this was neither used, nor is it useful.;;;","26/May/12 12:49;xedin;+1;;;","29/May/12 08:48;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hsha default thread limits make no sense, and yaml comments look confused",CASSANDRA-4277,12557047,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,23/May/12 18:38,16/Apr/19 09:32,14/Jul/23 05:52,29/May/12 16:27,1.2.0 beta 1,,,,,,0,,,,,,,"The cassandra.yaml states with respect to {{rpc_max_threads}}:

{code}
# For the Hsha server, the min and max both default to quadruple the number of
# CPU cores.
{code}

The code seems to indeed do this. But this makes, as far as I can tell, no sense what-so-ever since the number of concurrent RPC threads you need is a function of the throughput and the average latency of requests (that includes synchronously waiting on network traffic).

Defaulting to anything having to do with CPU cores seems inherently wrong. If a default is non-static, a closer guess might be to look at thread stack size and heap size and infer what ""might"" be reasonable.

*NOTE*: The effect of having this too low, is ""strange"" (if you don't know what's going on) latencies observed form the client on all thrift requests (*any* thrift request, including e.g. {{describe_ring()}}), that isn't visible in any latency metric exposed by Cassandra. This is why I consider this ""major"", since unwitting users may be seeing detrimental performance for no good reason.

In addition, I read this about async:

{code}
# async -> Nonblocking server implementation with one thread to serve 
#          rpc connections.  This is not recommended for high throughput use
#          cases. Async has been tested to be about 50% slower than sync
#          or hsha and is deprecated: it will be removed in the next major release.
{code}

This makes even less sense. Running with *one* rpc thread limits you to a single concurrent request. How was that 50% number even attained? By single-node testing being completely CPU bound locally on a node? The actual effect should be ""stupidly slow"" in any real situation with lots of requests on a cluster of many nodes and network traffic (though I didn't test that) - especially in the event of any kind of hiccup like a node doing GC. I agree that if the above is true, async should *definitely* be deprecated, but the reasons seem *much* stronger than implied.

I may be missing something here, in which case I apologize,, but I specifically double-checked after I fixed this setting on on our our clusters after seeing exactly the expected side-effect of having it be too low. I always was under the impression that rpc_max_threads affects the number of RPC requests running concurrently, and code inspection (it being used for the worker thread limit) + the effects of client-observed latency is consistent with my understanding.

I suspect the setting was set strangely by someone because the phrasing of the comments in {{cassandra.yaml}} strongly suggest that this should be tied to CPU cores, hiding the fact that this really has to do with the number of requests that can be serviced concurrently regardless of implementation details of thrift/networking being sync/async/etc.

",,jeromatron,mauzhang,scode,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/12 19:39;scode;CASSANDRA-4277-trunk-v2.txt;https://issues.apache.org/jira/secure/attachment/12529776/CASSANDRA-4277-trunk-v2.txt","25/May/12 07:07;scode;CASSANDRA-4277-trunk.txt;https://issues.apache.org/jira/secure/attachment/12529684/CASSANDRA-4277-trunk.txt",,,,,,,,,,,,,,,,2.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256019,,,Mon Jul 02 10:45:24 UTC 2012,,,,,,,,,,"0|i0gu27:",96300,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"23/May/12 18:40;scode;To be clear: I suggest changing the default to some pretty high number like 512, or possibly be a function of stack size/heap size.

Additionally, *if* my analysis is correct, the 'async' stuff should be completely deprecated as no one should ever be using it for any real cluster unless possibly if they use Cassandra as single-node local storage.

I also suggest changing comments to reflect the real meaning of this and what it affects.

I will provide a patch for the comments and changing the HSHA defaults to a fixed number, if anyone +1:s the idea.;;;","23/May/12 19:55;vijay2win@yahoo.com;Peter,

when implemented the solution was intrim (till thrift fix was in a released. THRIFT-1167), I dont remember or have the test classes which i used to come up with the data mentioned above... 
It is really hard to get all the real world traffic pattern to simulate... 

One thing which we can do have it configurable and by default (assuming that the clients behave well and keep the CPU busy) choose based on the CPU, i dont think there is a right number of threads which will make sense for the selector to be default.;;;","23/May/12 19:59;vijay2win@yahoo.com;Also, IMO
{quote}
To be clear: I suggest changing the default to some pretty high number like 512, or possibly be a function of stack size/heap size.
{quote}
512 is really really high and the context switch will be really high and you will not be seeing a lot of advantage with this.;;;","23/May/12 20:38;brandon.williams;bq. 512 is really really high and the context switch will be really high and you will not be seeing a lot of advantage with this.

I agree.  Logically the optimal amount is the number of cores, but benchmarking revealed better results with oversubscription, hence it defaulting to cores*4 now.;;;","23/May/12 21:49;scode;Please re-read. What you're saying is re-iterating the same confusion that seems to be the reason for this setting to begin with.

It has *nothing* to do with CPU cores. This is about concurrent requests, that are not CPU bound.

Logically the optimal amount *IS NOT* the number of cores. See my original submission.;;;","23/May/12 21:53;scode;The point is, zoom out of all implementation details. Forget about context switching. Just look at requests. If you have 10 000 requests with an average latency of 15 milliseconds for example, it would take 10000*15/1000=150 seconds to execute them with a single rpc thread. With 10 threads, 15 seconds.

So if you're trying to push 10 000 requests per second through a node, with an average latency of 15 milliseconds, you cannot possible do so if you limit the number of concurrent requests to less than 15*10=150.;;;","23/May/12 21:54;scode;This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request. Fixing that would mean making the entire thrift front-end asynchronous. But without that happening, it is not correct to consider the number of CPU cores when selecting what is a reasonable limit to the number of concurrent thrift requests.;;;","23/May/12 21:59;jbellis;bq. This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request

Isn't the whole point of HSHA to fix this?;;;","23/May/12 22:05;scode;Not unless there's an entire re-implementation of the StorageProxy interface to be asynchronous that exists in parallel with the synchronous one that I have somehow missed, or we've done some bytecode weaving tricks to create continuations on the JVM... Just look at {{fetchRows}} for example. Unless I've gone completely senile, it's still standard synchronous code that blocks on futures and finally returns.;;;","23/May/12 22:06;scode;The point of the HSHA as I have been treating it, is to be asynch on the thrift protocol side. In other words, instead of thread-per-*connection* we have thread-per-*active-request*, which is much much better.

I have taken it to mean that half (the front-end/connection i/o stuff) being asynch, and the other half (the implementation of the RPC calls) being synchronous.;;;","24/May/12 04:11;vijay2win@yahoo.com;Peter, i dont understand how will latency come into picture? Selectors are woken when the data is available, right? if for some reason your connection is taking 15 ms or what even in the middle of a read you are better off disconnecting and reconnecting... I still dont understand how 500 threads will help it will all hang right?

Basically these threads (4*CPU core's) are used for selection read/write (only during that time) and the TP executes it and the selector is woken up again when the data has to be written. Are we having the same conversation as in CASSANDRA-3590 (but this is within the DC's where latencies are really low)?;;;","24/May/12 05:44;scode;The RPC calls, I'm talking about the actual Java methods that correspond to names of methods in the thrift spec, run in threads. Both reads and writes run as such calls. The thread will be ""hogged"" for that request until the call returns. This is just standard Java, nothing having to do with thrift or selector threads. Whatever thrift is doing, at some point it's handing off the servicing of an RPC call to a Java method call.

In order to avoid having a thread per request, you would have to actually re-write the relevant code to be asynchronous (keep necessary state explicitly instead of implicitly on the stack, make it up-side-down at each block point, no longer have working exception handling across block points, etc) - that is, barring generalized things like JVM coroutines/bytecode weaving.

The ""necessary code"" here being things like fetchRows() for example, and everything else involving servicing thrift requests.
;;;","24/May/12 05:46;scode;And latency directly translates into the amount of time a thread is ""hogged"" for the purpose of a request. Thus, it directly affects concurrency.;;;","24/May/12 05:49;scode;If you want an analogy, just compare with any standard LAMP server where you have some PHP script blocking on remote network calls. In the same way as the apache admin would have to tweak max clients to get throughput and saturate resources, we have to have enough RPC worker threads in Cassandra.;;;","24/May/12 06:06;vijay2win@yahoo.com;haaa you are talking about 

{code}
            if (conf.rpc_min_threads == null)
                conf.rpc_min_threads = conf.rpc_server_type.toLowerCase().equals(""hsha"")
                                     ? Runtime.getRuntime().availableProcessors() * 4
                                     : 16;
            if (conf.rpc_max_threads == null)
                conf.rpc_max_threads = conf.rpc_server_type.toLowerCase().equals(""hsha"")
                                     ? Runtime.getRuntime().availableProcessors() * 4
                                     : Integer.MAX_VALUE;
{code}

I was talking about 

CassandraDaemon

{code}
serverEngine = new CustomTHsHaServer(serverArgs, executorService, Runtime.getRuntime().availableProcessors());
{code}

In our environment we run @ rpc_min_threads: 16 rpc_max_threads:1024 and has worked well for us :)

My bad for misreading the ticket.

{quote}
This is a result of the architecture of Cassandra, which fundamentally requires a thread for an active thrift request.
{quote}
This actually made me think you are really talking about CustomTHSHA server... because the whole point of THSHA server is to eliminate that as jonathan mentioned. anyways...;;;","24/May/12 06:23;scode;Yes, I'm talking about {{rpc_max_threads}}.

{quote}
This actually made me think you are really talking about CustomTHSHA server... because the whole point of THSHA server is to eliminate that as jonathan mentioned. anyways...
{quote}

I don't understand why this is still a point of contention though. There's still no code in Cassandra that in any way avoids having a thread per active (client) request. By ""active"" here I mean ""getting processed by Cassandra"" (so I'm not talking about requests that are being read from the client TCP connection, or are queueing to get executed on a worker thread).

We no longer requires a thread *per client connection*, which is good. But we still have a thread *per client request*.
;;;","24/May/12 16:19;slebresne;I agree with Peter, StorageProxy being synchronous, we do need on thread per active request (this is ""annoying"" for CASSANDRA-2478 too). It would be neat to make StorageProxy asynchronous but that's likely very much non-trivial. So on the thread numbers, I also agree that some big number would be much better. Those threads will mostly spend time waiting, so I don't think the context switching will kill us anyway.
;;;","25/May/12 07:07;scode;Attaching suggested patch against trunk.

Since the pre-existing comments claim async will be removed in the next major release, I removed it entirely from comments (but not the code).

I re-phrased some of the stuff and added an attempted explanation for the user as to how to figure out what limit to set. As usual I think I may be too verbose; maybe it's better to just refer to a separate wiki page than to try to explain inline?

As an aside, I'd favor making hsha the default despite it being slower on Windows, though that's a concern not within the scope of this ticket. I didn't make that change in the patch.;;;","25/May/12 09:54;slebresne;I do think it's maybe a bit too much info for a config file :). I'd remove the two last paragraphs and maybe rewrite the second one with something along the lines of: ""The default is unlimited and thus provide no protection against clients overwhelming the server. You are encouraged to set a maximum that makes sense for you in production, but do keep in mind that rpc_max_threads represents the maximum number of client requests this server may execute concurrently."".

Also, there's a typo: ""Regardless of your *cohice*"". And for ""# rpc_max_threads: (unlimited)"", it suggests that '(unlimited)' is a valid value which is not the case. I'd prefer leaving the 2048 value, it does not have to represent the actual default (it doesn't always for other configs).
;;;","25/May/12 19:39;scode;Attached v2 that incorporates those changes.;;;","29/May/12 16:27;slebresne;+1, committed, thanks.;;;","01/Jul/12 05:57;jeromatron;Would a StorageProxy rewrite to be completely asynchronous be a separate ticket to be tackled later?;;;","01/Jul/12 05:59;jeromatron;Sounds from Sylvain that a SP asynch rewrite would be good for implementing CASSANDRA-2478 as well.;;;","02/Jul/12 07:39;slebresne;bq. Would a StorageProxy rewrite to be completely asynchronous be a separate ticket to be tackled later?

Probably a separate ticket to be tackled never :) I mean, it would be neat intellectually speaking to get rid of the one-thread-per-active-request thing, but it's unclear it is a real problem in practice today and that would require a very substantial effort. Not sure at all it is worth the trouble. ;;;","02/Jul/12 10:45;jeromatron;Thanks for the clarification Sylvain.  I was just going through some past tickets and didn't want something that sounded like a good opportunity for the future fall through the cracks.  That's all.  Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple SLF4J bindings warning when running stress,CASSANDRA-4276,12557021,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,nickmbailey,mbulman,mbulman,23/May/12 15:38,16/Apr/19 09:32,14/Jul/23 05:52,29/May/12 22:37,1.1.1,,,Legacy/Tools,,,0,,,,,,,"{noformat}
> ./tools/bin/stress
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/tools/lib/stress.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/path/apache-cassandra-1.1.0/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
{noformat}",,brandon.williams,kirktrue,mbulman,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/12 22:29;nickmbailey;0001-Allow-stress-to-run-from-binary-tar.patch;https://issues.apache.org/jira/secure/attachment/12528803/0001-Allow-stress-to-run-from-binary-tar.patch",,,,,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256018,,,Tue May 29 22:37:37 UTC 2012,,,,,,,,,,"0|i0gu1r:",96298,,kirktrue,,kirktrue,Normal,,,,,,,,,,,,,,,,,"23/May/12 18:41;brandon.williams;Also looks like stress no longer works in the binary dist due to a classpath problem.;;;","23/May/12 22:29;nickmbailey;Fixes the slf4j warning as well as allowing running from the tarball dist.;;;","29/May/12 22:35;kirktrue;+1, patch applies and fixes issue against current trunk.;;;","29/May/12 22:37;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Oracle Java 1.7 u4 does not allow Xss128k,CASSANDRA-4275,12557011,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,appodictic,appodictic,23/May/12 14:38,16/Apr/19 09:32,14/Jul/23 05:52,19/Nov/13 22:44,1.1.2,,,,,,0,,,,,,,"Problem: This happens when you try to start it with default Xss setting of 128k
=======
The stack size specified is too small, Specify at least 160k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Solution
=======
Set -Xss to 256k

Problem: This happens when you try to start it with Xss = 160k
========
ERROR [Thrift:14] 2012-05-22 14:42:40,479 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thrift:14,5,main]
java.lang.StackOverflowError

Solution
=======
Set -Xss to 256k",,appodictic,cburroughs,chengas123,enigmacurry,eugenparaschiv,jeromatron,psanford,scode,scurrilous,shawn_,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4442,,,,,,,,"12/Jun/12 09:34;slebresne;4275.txt;https://issues.apache.org/jira/secure/attachment/12531799/4275.txt","25/Jun/12 22:48;scurrilous;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4275-Use-JVM-s-reported-minimum-stack-size-o.txt;https://issues.apache.org/jira/secure/attachment/12533394/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4275-Use-JVM-s-reported-minimum-stack-size-o.txt","23/May/12 14:46;appodictic;trunk-cassandra-4275.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12528739/trunk-cassandra-4275.1.patch.txt",,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256017,,,Tue Nov 19 22:44:01 UTC 2013,,,,,,,,,,"0|i0gu1j:",96297,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"23/May/12 14:46;appodictic;Ups Xss to a value that Java 7 will accept.;;;","23/May/12 14:48;jbellis;I could go for increasing to 160 but doubling is a big hit.  What is the full SOE stack trace?  Let's figure out what's doing a lot of stack allocation and make it play nice.;;;","23/May/12 14:53;appodictic;{noformat}
ERROR [Thrift:41] 2012-05-22 14:02:08,685 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thrift:41,5,main]
java.lang.StackOverflowError
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:150)
	at java.net.SocketInputStream.read(SocketInputStream.java:121)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2877)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat};;;","23/May/12 15:23;brandon.williams;How many clients are connecting to each node, socket-wise?;;;","23/May/12 15:38;appodictic;Port 9160 is given to apani in /etc/services :)

{noformat}
[root@cdbsd28 ~]# netstat -a | grep ESTABLISHED | grep apani | wc -l
672
{noformat}
;;;","23/May/12 16:24;scode;Just wanted to quickly chime in and say that I saw this too at 160k (used 512k and was done with it). I don't remember whether the stack trace matches exactly or not, but it was definitely one of the I/O paths - either thrift or internode. This was with a build of openjdk7, so not specific to the Oracle release.;;;","23/May/12 20:30;appodictic;I search for answers on stackoverflow.com but found none. This is interesting reading that suggests 512 is the default.
http://www.onkarjoshi.com/blog/209/using-xss-to-adjust-java-default-thread-stack-size-to-save-memory-and-prevent-stackoverflowerror/ ;;;","23/May/12 20:59;jbellis;Wouldn't surprise me; defaults are *way* too high for us...  we go out of our way to avoid recursion and other stack-eating constructs so we don't chew up a lot of memory per thread.  Our goal is to run w/ the minimum allowed by the jvm.;;;","25/May/12 14:37;appodictic;I understand wanting to keep this setting low. But there must be some fairly major underlying factor that made the JVM developers decided to raise the minimum allowable from whatever it was to 160. While the increase is large 125K->250K the net effect on 1000 connections is not a big deal. 125MB vs 250MB  This is stack space not heap correct? Will using 100MB more stack be a problem in the grand schema of Cassandra memory management? Also we do not want some minimum setting that is likely to blow up on someone at the first sign of load. 

I do not think it is a bad idea to dig in and determine if we can safely code/tune cassandra to lower the setting. But I will un-assign myself if that is the road we are going to take because that type of analysis is not my bread and butter skill set.;;;","25/May/12 15:21;jbellis;bq. there must be some fairly major underlying factor that made the JVM developers decided to raise the minimum allowable from whatever it was to 160

Doubt it.  Old minimum was 128, so it's not *that* big a change.

I've asked Jake Farrell to have a look at Thrift/Java7 to see if anything odd is going on here.;;;","12/Jun/12 09:34;slebresne;Since currently C* doesn't start with Oracle java7 u4, I suggest we fix this. On the other side, we know -Xss128 works fine for java 6 so why take any risk? Attaching patch that keeps 128 for java 6 but use 160 for java 7.

I suggest we start with that, and if it proves that indeed java 7 puts so much more on the stack that 160 is too low, it will still be time to up it later.;;;","12/Jun/12 12:23;jbellis;SGTM, +1;;;","12/Jun/12 13:06;slebresne;Committed, thanks;;;","25/Jun/12 16:45;scurrilous;If the goal is to run with the JVM minimum stack size, how about attempting to query it directly? Going to 160k on 32-bit/ARM systems with Java 7 is way too much.

On amd64 with 7u4:

\# java -Xss32k

The stack size specified is too small, Specify at least 160k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

On amd64 with 6u32 and 7u2:

\# java -Xss32k

The stack size specified is too small, Specify at least 104k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

On ARM with 7u4:

\# java -Xss32k

The stack size specified is too small, Specify at least 64k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

I can provide a patch and/or open a new issue if this sounds reasonable.;;;","25/Jun/12 17:13;jbellis;That sounds like a good improvement to have, we'd appreciate the patch.;;;","25/Jun/12 22:53;scurrilous;The patch is attached. Tested with Oracle 7u4 and OpenJDK 6u24 on amd64, and Oracle 7u4 on ARMv7. Reported stack sizes of 160k, 104k, and 64k, respectively. Just tested startup on amd64, but ran 'stress -o insert' on ARM.

Note that the patch also fixes an issue with CASSANDRA-4366 (UseCondCardMark) where JDK 1.7 was only detected on Linux.
;;;","26/Jun/12 18:15;brandon.williams;It seems like we should generalize this a bit more to cover FreeBSD et al as well since I imagine the same problem exists there.;;;","04/Jul/12 18:11;shawn_;Will this fix be back ported to 1.0.x?;;;","04/Jul/12 19:43;jbellis;No.  You can tweak your 1.0.x startup script manually if you really want to run it on java 7 (not really recommended, they're still shaking the bugs out).;;;","18/Jul/12 00:27;scurrilous;So regarding ARM, etc., should this be reopened or a new issue created?

As for FreeBSD, I have no experience at all with it. Unless someone else volunteers to help with that, I'd like to fix ARM/Linux now and keep the FreeBSD issue separate (as I could envision that languishing indefinitely).;;;","18/Jul/12 07:38;jbellis;Should probably open a new ticket either way since we committed the original for 1.1.2.;;;","20/Jul/12 21:53;jeromatron;Should this be reopened to set it to -Xss256k?  Both Ed Capriolo and another person posting in IRC have found 160 insufficient.  Ed is running in production with 256 and the other person is changing to 256.
<Rav|2> how should I set -Xss for oracle java 7? 160k causes java.lang.StackOverflowError :(
<ecapriolo> 256 or higher
<Rav|2> ecapriolo: everything is back to normal with 256. big thanks :);;;","20/Jul/12 21:57;scurrilous;My proposed fix for CASSANDRA-4442 would set it to 200k (160k * 1.25). Would that be large enough?;;;","21/Jul/12 03:39;jbellis;I'm -1 on just shrugging and throwing numbers at it until it appears to work.  I want to find out *why* 160 isn't enough, because there isn't supposed to be anything allocated on the stack anywhere near that size.  Let's either find the bug that is the root cause or advance our understanding of where that memory is going.;;;","21/Jul/12 14:59;jeromatron;I'll create another ticket then.  The problem is that out of the box it sounds like it just doesn't work for people.  I agree that it's better to find the root cause, but leaving it at a lower level and having everyone who uses it with jdk 7 have the same error until they go into IRC and ask and get the workaround is also undesirable in the meantime.

Could 256k be committed for now until the new ticket is resolved so people can have it start up properly in the meantime?

Created CASSANDRA-4457.;;;","21/Jul/12 17:28;jbellis;You do realize that JDK7 is still pretty buggy, right?  If anything I'd support a modification to echo ""JDK7 is not supported, use at your own risk.  You can do this by making the following edit to the stack size."";;;","21/Jul/12 17:53;jeromatron;Fair enough.  A clear notification would be nice so that they know to update the value without checking the user list or IRC.  It sounds like Ed and others are starting to use JDK7 in production based on the needs of their organization, so it would be nice to have something documented about support for JDK7 and tweaking as you've mentioned.;;;","21/Jul/12 17:57;scurrilous;""JDK7, get off my lawn!"" :p

Buggy or not though, Java 6 was originally scheduled for EOL this month, and is now EOL in November: https://blogs.oracle.com/henrik/entry/updated_java_6_eol_date

I'd argue that it's time to start supporting Java 7 at least experimentally so that any remaining bugs can be fixed before we're forced to rely on it.

(I'll also admit my bias toward Java 7 since the server VM for ARM is not available for Java 6.);;;","24/Sep/12 16:41;chengas123;I'd like to see Java 7 supported as well. It's going to be the default in the Ubuntu release next month and all my other software uses it just fine now.;;;","24/Sep/12 17:22;jbellis;This was addressed in CASSANDRA-4602;;;","31/Oct/13 19:24;enigmacurry;cassandra-1.1 and cassandra-1.2 both currently default to -Xss180k. Using Java 1.7.0_40 this setting is too small:

{code}
xss =  -ea -javaagent:/home/ryan/.ccm/repository/git_cassandra-1.2/bin/../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1982M -Xmx1982M -Xmn400M -XX:+HeapDumpOnOutOfMemoryError -Xss180k

The stack size specified is too small, Specify at least 228k
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
{code};;;","19/Nov/13 22:44;jbellis;Bumped Xss to 256k in 1.2 as well.  Not touching fixver here since it's really talking about a different issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Exit status of bin/cassandra without -f is wrong,CASSANDRA-4271,12556789,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thobbs,thobbs,22/May/12 01:27,16/Apr/19 09:32,14/Jul/23 05:52,23/Jul/12 16:46,1.1.3,,,,,,0,,,,,,,"The launch_service() function returns {{$?}} after exec'ing java, and the script then exits with that same status.

The problem is that we do a {{[ ! -z ""$pidpath""] && ...}} conditional statment after exec'ing when the foreground flag isn't set.  The value of {{$?}} then depends on that conditional and the statement, typically returning 1, because {{$pidpath}} isn't set.  So, even if everything appears to execute normally, you will get an exit status of 1 for the whole script.

I suspect the right thing to do is just return 0 when backgrounding.",,thepaul,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/12 20:56;thepaul;0001-startup-script-returns-0-after-backgrounding.patch;https://issues.apache.org/jira/secure/attachment/12536455/0001-startup-script-returns-0-after-backgrounding.patch",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256013,,,Mon Jul 23 16:46:51 UTC 2012,,,,,,,,,,"0|i0gtzj:",96288,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Jul/12 20:56;thepaul;Patch attached; also present in the 4271 branch in my github. Current version tagged pending/4271.

https://github.com/thepaul/cassandra/tree/4271;;;","23/Jul/12 16:46;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test broken due to incorrect config option,CASSANDRA-4270,12556774,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tpatterson,tpatterson,tpatterson,21/May/12 22:46,16/Apr/19 09:32,14/Jul/23 05:52,23/May/12 21:53,1.0.11,1.1.1,,Legacy/Testing,,,0,test,,,,,,"the long-test fails:
{code}
BUILD FAILED
/home/tahooie/datastax/cassandra/build.xml:1125: Problem: failed to create task or type jvmarg
Cause: The name is undefined.
{code}
The problem is that the build.xml file has the jvmarg outside the <testmacro> tag instead of inside it. A patch is forthcoming.",,tpatterson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/12 22:48;tpatterson;CASSANDRA-4270.patch;https://issues.apache.org/jira/secure/attachment/12528522/CASSANDRA-4270.patch",,,,,,,,,,,,,,,,,1.0,tpatterson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256012,,,Tue May 22 15:50:29 UTC 2012,,,,,,,,,,"0|i0gtz3:",96286,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/May/12 15:50;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting column metadata for non-string comparator CFs breaks,CASSANDRA-4269,12556760,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,21/May/12 21:39,16/Apr/19 09:32,14/Jul/23 05:52,22/May/12 15:13,1.1.1,,,,,,0,,,,,,,"For example, use a comparator of LongType and try to create an index on a column named 2 (0x0000000000000002).  You'll get a stracktrace in the logs similar to this:

{noformat}
java.lang.RuntimeException: java.nio.charset.MalformedInputException: Input length = 2
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:50)
	at org.apache.cassandra.cql3.CFDefinition.<init>(CFDefinition.java:115)
	at org.apache.cassandra.config.CFMetaData.updateCfDef(CFMetaData.java:1278)
	at org.apache.cassandra.config.CFMetaData.columnMetadata(CFMetaData.java:225)
	at org.apache.cassandra.config.CFMetaData.fromThrift(CFMetaData.java:636)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:1061)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3436)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.getResult(Cassandra.java:3424)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.nio.charset.MalformedInputException: Input length = 2
	at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
	at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:163)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:120)
	at org.apache.cassandra.cql3.ColumnIdentifier.<init>(ColumnIdentifier.java:46)
	... 13 more
{noformat}

This works in Cassandra 0.8 and 1.0.",,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/12 10:35;slebresne;4269.txt;https://issues.apache.org/jira/secure/attachment/12528576/4269.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256011,,,Tue May 22 15:13:57 UTC 2012,,,,,,,,,,"0|i0gtyn:",96284,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/May/12 22:10;jbellis;related to CASSANDRA-4093?;;;","22/May/12 10:35;slebresne;Patch attached. The column names == utf8 assumption was a bit too hardwired.;;;","22/May/12 14:31;jbellis;I assumed Tyler was referring to Thrift/CLI usage here -- I do think that requiring CQL3 column names to be strings is a Good Thing.;;;","22/May/12 14:45;slebresne;Agreed. The problem here is really much of an implementation detail. We happen to generate the CFDefinition object used by CQL3 in CFMetatData, and this even if CQL3 is not used per se. So internally it's easier if the code support non-utf8 columns, even though CQL3 won't allow to create them. Basically the patch make sure we correctly convert column names to string when going in the CQL3 side.;;;","22/May/12 14:59;jbellis;+1;;;","22/May/12 15:13;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate compression parameters,CASSANDRA-4266,12556747,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,21/May/12 20:32,16/Apr/19 09:32,14/Jul/23 05:52,23/May/12 13:46,1.0.11,1.1.1,,,,,0,,,,,,,"compression_parameters doesn't warn when unknown options are specified; see  http://ac31004.blogspot.co.uk/2012/05/snappy-compression-fails-for-apache.html",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/12 16:27;slebresne;4266.txt;https://issues.apache.org/jira/secure/attachment/12528617/4266.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256008,,,Wed May 23 13:46:50 UTC 2012,,,,,,,,,,"0|i0gtxj:",96279,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/May/12 16:27;slebresne;Attaching patch that does 2 things:
# Refuse unknown compression options
# Check at startup whether Snappy is supported (since after all it use native code underneath) and if it doesn't, it logs a warning and default table creation to umcompressed ones.;;;","22/May/12 16:29;slebresne;Note that the patch is against 1.1 but we can commit to 1.0 too.;;;","22/May/12 20:46;jbellis;+1;;;","23/May/12 13:46;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledManifest.maxBytesForLevel calculates wrong for sstable_size_in_mb larger than 512m,CASSANDRA-4263,12556574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,20/May/12 05:01,16/Apr/19 09:32,14/Jul/23 05:52,21/May/12 23:21,1.1.1,,,,,,0,,,,,,,"need to use long math

        if (level == 0)
            return 4 * maxSSTableSizeInMB * 1024 * 1024;",,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/12 05:01;dbrosius@apache.org;use_long_math.diff;https://issues.apache.org/jira/secure/attachment/12528299/use_long_math.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256005,,,Mon May 21 23:20:52 UTC 2012,,,,,,,,,,"0|i0gtw7:",96273,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/May/12 21:04;jbellis;+1;;;","21/May/12 21:05;jbellis;Let's go ahead to commit to 1.1 as well.;;;","21/May/12 23:20;dbrosius@apache.org;committed to 1.1 as e515e4b25b4839d8c15fbf0b747185d2fa93ca66;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.1 does not preserve compatibility w/ index queries against 1.0 nodes,CASSANDRA-4262,12556550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,jbellis,jbellis,19/May/12 22:09,16/Apr/19 09:32,14/Jul/23 05:52,01/Jun/12 16:08,1.1.1,,,,,,0,,,,,,,1.1 merged index + seq scan paths into RangeSliceCommand.  1.1 StorageProxy always sends a RSC for either scan type.  But 1.0 RSVH only does seq scans.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/12 15:58;slebresne;4262-v2.txt;https://issues.apache.org/jira/secure/attachment/12530562/4262-v2.txt","25/May/12 17:56;slebresne;4262.txt;https://issues.apache.org/jira/secure/attachment/12529756/4262.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256004,,,Fri Jun 01 16:08:04 UTC 2012,,,,,,,,,,"0|i0gtvr:",96271,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"25/May/12 17:56;slebresne;Attached patch to generate IndexScanCommand message when appropriate.

I've pushed a test in dtests too (in rolling_upgrade_test.py). Fails before the patch, works after.;;;","01/Jun/12 15:15;jbellis;Can we do something to make the problem more obvious when someone does a seq-scan-with-filters in a mixed-version cluster, than just send the RSC and have it error out?  Even just logging WARN would be better than nothing.;;;","01/Jun/12 15:31;slebresne;Actually, I think we don't allow at all seq-scan-with-filters (even the storage engine can do it). The reason it's not allowed we haven't changed the validation of index expression, so that an invalid request will be return upfront to the user saying no columns are indexed. This is also true of CQL3.

On the front of ""should we allow those seq-scan-with-filters"", that would be a case of seemingly simple queries that could take *a lot* of time (I suppose you could do that with 2ndary indexes too, but you have to try a little bit more harder), so maybe this is worth giving it a though?;;;","01/Jun/12 15:58;slebresne;Actually, I'm wrong. We do allow seq-scan-with-filters on the thrift side. So anyway, attaching a v2 that does throw an exception in that case on the sending node.

Still leave the question if we should allow those sequential queries with filters in CQL3 I guess.;;;","01/Jun/12 16:02;jbellis;+1

Ultimately I'd like to have a CqlInputFormat for Hadoop, which is why we opened that up on the Thrift side.;;;","01/Jun/12 16:08;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in SSTableReader.getSampleIndexesForRanges(...) causes uneven InputSplits generation for Hadoop mappers,CASSANDRA-4259,12556422,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,br1985,br1985,br1985,18/May/12 17:07,16/Apr/19 09:32,14/Jul/23 05:52,19/May/12 22:35,1.1.1,,,,,,0,,,,,,,"Running a simple mapreduce job on cassandra column family results in creating multiple small mappers for one half of the ring and one big mapper for the other half. Upper part (85... - 0) is cut into smaller slices. Lower part (0 - 85...) generates one big input slice. One mapper processing half of the ring causes huge inefficiency. Also the progress meter for this mapper is incorrect - it goes to 100% in a couple of seconds, than stays at 100% for an hour or two.

I've investigated the problem a bit. I think it is related to incorrect output of 'nodetool rangekeysample'. On the node resposible for part (0 - 85...) the output is empty! On the other node it works fine.

I think the bug is in SSTableReader.getSampleIndexesForRanges(...). These two lines:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.left.maxKeyBound();

should be changed to:

   RowPosition leftPosition = range.left.maxKeyBound();
   RowPosition rightPosition = range.right.maxKeyBound();

After that fix the output of nodetool is correct and the whole ring is split into small mappers.

The other half of the ring works fine because of extra 'if' in the code:

   int right = Range.isWrapAround(range.left, range.right)...

This causes that the bug does not show up in one-node cluster or in the ""last"" ring partition in muli-node clusters.

Can anyone look at it and verify my thoughts? I'm rather new to Cassandra.
","Small cassandra cluster with 2 nodes. Version 1.1.0. 

Tokens: 0, 85070591730234615865843651857942052864

Hadoop 1.0.1 and Pig 0.10.0.",br1985,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,br1985,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,256001,,,Mon Jun 18 15:48:26 UTC 2012,,,,,,,,,,"0|i0gtuf:",96265,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"19/May/12 22:35;jbellis;Good detective work! Committed, thanks.;;;","18/Jun/12 15:48;jbellis;To clarify: this was a regression introduced in 1.1.0, it should not affect 1.0.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 range query with secondary index fails,CASSANDRA-4257,12556274,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,sbillig,sbillig,17/May/12 19:06,16/Apr/19 09:32,14/Jul/23 05:52,24/May/12 14:45,1.1.1,,,Feature/2i Index,Legacy/CQL,,0,cql3,index,,,,,"This query fails:
select * from indextest where setid = 0 and row < 1;
when there's a secondary index on 'setid'; row isn't the primary key.

{code:title=CQL3}
bin$ ./cqlsh --cql3
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> create table indextest (id int primary key, row int, setid int);
cqlsh:warehouse1> create index indextest_setid_idx on indextest (setid);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (0, 0, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (1, 1, 0);
cqlsh:warehouse1> insert into indextest (id, row, setid) values (2, 2, 0);
cqlsh:warehouse1> select * from indextest where setid = 0;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
  2 |   2 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row = 1;
 id | row | setid
----+-----+-------
  1 |   1 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
TSocket read 0 bytes
{code}

{code:title=Error message}
ERROR 13:36:23,544 Error occurred during processing of message.
java.lang.NullPointerException
  at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:546)
  at org.apache.cassandra.cql3.statements.SelectStatement.multiRangeSlice(SelectStatement.java:253)
  at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
  at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
  at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
  at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
  at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
  at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
{code}

Works fine in CQL2:
{code:title=CQL2}
bin$ ./cqlsh_uuid --cql2
Connected to Git at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.31.0]
Use HELP for help.
cqlsh> use warehouse1;
cqlsh:warehouse1> select * from indextest where setid = 0 and row < 1;
 id | row | setid
----+-----+-------
  0 |   0 |     0

cqlsh:warehouse1> select * from indextest where setid = 0 and row < 2;
 id | row | setid
----+-----+-------
  0 |   0 |     0
  1 |   1 |     0
{code}","Cassandra 1.1.0 and git cassandra-1.1 branch, as of commit fd92c09d95a53d582cb8c4b0e77ac47fdd884935",sbillig,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/12 16:36;slebresne;4257.txt;https://issues.apache.org/jira/secure/attachment/12528260/4257.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255999,,,Thu May 24 14:45:36 UTC 2012,,,,,,,,,,"0|i0gttj:",96261,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"19/May/12 16:36;slebresne;That was kind of a typo. When 2ndary indexes was used, the code was expecting that a column that had a lesser-than clause on it, also had a greater-than one. Simple patch attached to fix. I've pushed a regression test to the dtests too.;;;","23/May/12 15:34;xedin;+1;;;","24/May/12 14:45;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include stress tool in debian packaging,CASSANDRA-4256,12556269,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,17/May/12 18:40,16/Apr/19 09:32,14/Jul/23 05:52,21/May/12 17:28,1.1.1,,,Legacy/Tools,,,0,,,,,,,"The stress tool isn't included in the debian packaging. We need to update that to grab the stress shell script as well as put the stress.jar file in lib.

Also the stress shell script needs to be updated to include looking in /usr/share/cassandra... when searching for the stress jar so it will run in packaged installations.",,mbulman,nickmbailey,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/12 16:25;nickmbailey;0001-Include-stress-in-debian-package-V2.patch;https://issues.apache.org/jira/secure/attachment/12528097/0001-Include-stress-in-debian-package-V2.patch","18/May/12 17:55;nickmbailey;0001-Include-stress-in-debian-package-V3.patch;https://issues.apache.org/jira/secure/attachment/12528111/0001-Include-stress-in-debian-package-V3.patch","21/May/12 16:06;nickmbailey;0001-Include-stress-in-debian-package-V4.patch;https://issues.apache.org/jira/secure/attachment/12528447/0001-Include-stress-in-debian-package-V4.patch","17/May/12 19:42;nickmbailey;0001-Include-stress-in-debian-package.patch;https://issues.apache.org/jira/secure/attachment/12527896/0001-Include-stress-in-debian-package.patch",,,,,,,,,,,,,,4.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255998,,,Mon May 21 17:28:50 UTC 2012,,,,,,,,,,"0|i0gtt3:",96259,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"17/May/12 19:42;nickmbailey;Includes stress stuff in debian package. Also slightly reorganizes the stress script so it will work better in packaged installs.;;;","17/May/12 19:45;nickmbailey;Also I'll note it would be cool to get a 1.1.0-2 debian package out if we can. I guess this can't completely be considered solely a packaging change since the stress script itself changed, but I guess I'll leave that up to the committers.;;;","18/May/12 02:49;thepaul;+1.;;;","18/May/12 10:22;slebresne;bq. Also I'll note it would be cool to get a 1.1.0-2 debian package out if we can

Honestly I'd better wait on 1.1.1 to be released as we can't really do a new debian package without doing a full release (as otherwise the md5 in the packages wouldn't match).

Other than that, I wonder if it wouldn't make sense to rename stress to something like cassandra-stress? I wonder if stress is not a tad generic to put in /usr/bin like that.;;;","18/May/12 15:46;nickmbailey;Would you propose changing it in general or just in the packaging?;;;","18/May/12 16:05;slebresne;I don't know :)

Probably it's enough to change it in the packaging (if we consider it a valid concern), but changing the script name in general could make it more likely to get uniformity across other (externally handled) packaging.;;;","18/May/12 16:14;nickmbailey;I'm against only changing it in the packaging :). I guess I'd be fine changing it everywhere. I'll update the patch.;;;","18/May/12 16:25;nickmbailey;V2 attached.;;;","18/May/12 16:52;thepaul;I see just a few places that ought to be updated to reflect the change:

1. A few bits of text in tools/stress/README
2. The stress tool's idea of its own name (in tools/stress/src/org/apache/cassandra/stress/Stress.java, after ""Usage:"")
3. The rename is probably big enough to warrant a mention in CHANGES.

+1 otherwise.;;;","18/May/12 17:55;nickmbailey;V3 attached;;;","21/May/12 15:40;thepaul;+1;;;","21/May/12 16:05;nickmbailey;V4 which renames stress.bat as well as stressd is attached.;;;","21/May/12 17:28;slebresne;Alright, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
concurrent modif ex when repair is run on LCS,CASSANDRA-4255,12556268,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,17/May/12 18:29,16/Apr/19 09:32,14/Jul/23 05:52,21/May/12 16:52,1.0.11,1.1.1,,,,,0,compaction,lcs,,,,,"came across this, will try to figure a way to systematically reprod this. But the problem is the sstable list in the manifest is changing as the repair is triggered:

{panel}
Exception in thread ""main"" java.util.ConcurrentModificationException 
 at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
 at java.util.AbstractList$Itr.next(Unknown Source)
 at org.apache.cassandra.io.sstable.SSTable.getTotalBytes(SSTable.java:250)
 at org.apache.cassandra.db.compaction.LeveledManifest.getEstimatedTasks(LeveledManifest.java:435)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getEstimatedRemainingTasks(LeveledCompactionStrategy.java:128)
 at org.apache.cassandra.db.compaction.CompactionManager.getPendingTasks(CompactionManager.java:1063)
 at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
 at com.sun.jmx.mbeanserver.PerInterface.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(Unknown Source)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(Unknown Source)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
 at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(Unknown Source)
 at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
 at java.lang.reflect.Method.invoke(Unknown Source)
 at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
 at sun.rmi.transport.Transport$1.run(Unknown Source)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
 at java.lang.Thread.run(Unknown Source)
{panel}

maybe we could change the list to a copyOnArrayList? just a suggestion, haven't investigated much yet:

{code:title=LeveledManifest.java}
generations[i] = new ArrayList<SSTableReader>();
{code}",,cywjackson,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/12 19:13;jbellis;4255.txt;https://issues.apache.org/jira/secure/attachment/12527888/4255.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255997,,,Mon May 21 16:52:46 UTC 2012,,,,,,,,,,"0|i0gtsn:",96257,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"17/May/12 19:13;jbellis;patch attached to synchronize getEstimatedTasks.  copyOnWriteArrayList would probably work too, but we already synchronize other accesses (to prevent cross-level races) so that's simplest here.;;;","17/May/12 20:23;jbellis;Looks like this only affects getEstimatedTasks, not actually repair.;;;","21/May/12 10:21;slebresne;+1;;;","21/May/12 16:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set operation mode to MOVING earlier,CASSANDRA-4252,12556136,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,16/May/12 22:15,16/Apr/19 09:32,14/Jul/23 05:52,18/May/12 16:33,1.1.1,,,,,,0,,,,,,,Right now when moving a node we set the OperationMode only once we've calculated the necessary ranges to transfer and if there actually are ranges to transfer. Due to the sleep for ring settling this means there are 30 seconds where the node is moving but the operation mode isn't set accordingly. Additionally if it turns out no data needs to be transferred then the move will complete without ever switching the OperationMode to moving.,,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/12 22:50;jbellis;4252.txt;https://issues.apache.org/jira/secure/attachment/12527946/4252.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255994,,,Fri May 18 16:33:42 UTC 2012,,,,,,,,,,"0|i0gtrb:",96251,,nickmbailey,,nickmbailey,Low,,,,,,,,,,,,,,,,,"17/May/12 22:01;jbellis;What difference does it make as long as we set MOVING before streamRanges / requestRanges?;;;","17/May/12 22:21;nickmbailey;From a monitoring application perspective its just nicer to set the mode immediately so that when a move is initiated the operation mode of the node changes immediately rather than waiting 30ish seconds. Also so in the admittedly uncommon case of no data being transferred the operation mode gets set to moving at all, which it currently won't.;;;","17/May/12 22:33;jbellis;bq. just nicer to set the mode immediately so that when a move is initiated the operation mode of the node changes immediately rather than waiting 30ish seconds

Fair enough.

bq. in the admittedly uncommon case of no data being transferred the operation mode gets set to moving at all

I think it's inside that check to avoid logging ""fetching new ranges and streaming old ranges"" when there's nothing to stream, which is also misleading.

Since we don't ""push"" modes I don't think there's any difference between not setting it at all, and setting it only to set back to NORMAL a nanosecond later.;;;","17/May/12 22:42;nickmbailey;Ah you are right. I was under the impression we did the RING_DELAY sleep no matter what, but apparently not.

Regarding the 'fetching new ranges...' log message, we can call setMode multiple times (a pattern we use for other modes). So we can just call setMode earlier on with a more accurate log message in addition to the later call.;;;","17/May/12 22:50;jbellis;Patch attached.;;;","18/May/12 16:15;nickmbailey;+1;;;","18/May/12 16:33;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing arrayOffset in FBUtilities.hash,CASSANDRA-4250,12555995,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,richardlow,richardlow,richardlow,16/May/12 09:26,16/Apr/19 09:32,14/Jul/23 05:52,16/May/12 09:58,1.1.1,,,,,,0,,,,,,,"In CASSANDRA-3869, FBUtilities.hash was optimised to use the backing byte array if there is one.  However, there is a missing +arrayOffset in the offset parameter.

This can cause incorrect hashes resulting in data going to the wrong place, etc..  I haven't observed any errors directly attributable to this so maybe we are lucky and all backing arrays start at 0 but this could cause data loss in the worst case.",,richardlow,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/12 09:28;richardlow;CASSANDRA-4250-v1.patch;https://issues.apache.org/jira/secure/attachment/12527595/CASSANDRA-4250-v1.patch",,,,,,,,,,,,,,,,,1.0,richardlow,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255992,,,Wed May 16 09:58:07 UTC 2012,,,,,,,,,,"0|i0gtqf:",96247,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"16/May/12 09:28;richardlow;Patch against 1.1.0;;;","16/May/12 09:58;slebresne;Good catch, +1. Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOGGING: Info log is not displaying number of rows read from saved cache at startup,CASSANDRA-4249,12555985,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,16/May/12 08:33,16/Apr/19 09:32,14/Jul/23 05:52,16/May/12 16:08,1.1.1,,,,,,0,caching,,,,,,"As part of commit with revision c9270f4e info logging for number of rows read from saved cache is not working. 
This is happening because we are not incrementing the counter cachedRowsRead in ColumnFamilyStore.initRowCache().",,jbellis,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,,,,,,,360,360,,0%,360,360,,,,,,,,,,,,,,,,,,,,"16/May/12 08:45;kohlisankalp;trunk-4249.txt;https://issues.apache.org/jira/secure/attachment/12527593/trunk-4249.txt",,,,,,,,,,,,,,,,,1.0,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255991,,,Wed May 16 16:08:17 UTC 2012,,,,,,,,,,"0|i0gtpz:",96245,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/May/12 08:45;kohlisankalp;This is a very small change.;;;","16/May/12 16:08;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ORDER BY not ordering,CASSANDRA-4246,12555858,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,15/May/12 19:05,16/Apr/19 09:32,14/Jul/23 05:52,17/May/12 17:38,1.1.1,,,,,,0,cql3,,,,,,"Creating the simplest composite-key cql3 table I can think of, populating it with a few rows of data, then trying to do a query with an ORDER BY does not yield ordered results.

Here's a cql script:

{noformat}
create keyspace test with strategy_class = 'SimpleStrategy'
   and strategy_options:replication_factor = 1;
use test;
create table moo (a int, b int, c int, primary key (a, b));

insert into moo (a, b, c) values (123, 12, 3400);
insert into moo (a, b, c) values (122, 13, 3500);
insert into moo (a, b, c) values (124, 10, 3600);
insert into moo (a, b, c) values (121, 11, 3700);

select * from moo;
select * from moo order by b;
{noformat}

Here is the output of those two queries:

{noformat}
 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400

 a   | b  | c
-----+----+------
 121 | 11 | 3700
 122 | 13 | 3500
 124 | 10 | 3600
 123 | 12 | 3400
{noformat}

I also tried these using the bare thrift interface, to make sure it wasn't python-cql or cqlsh doing something stupid. Same results. Am I totally missing something important here about how this is supposed to work?",,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/12 17:18;slebresne;4246-2.txt;https://issues.apache.org/jira/secure/attachment/12527859/4246-2.txt","17/May/12 12:45;slebresne;4246.txt;https://issues.apache.org/jira/secure/attachment/12527832/4246.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,255988,,,Thu May 17 17:38:21 UTC 2012,,,,,,,,,,"0|i0gton:",96239,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/May/12 19:31;slebresne;Hum, I though we were correctly validating this but apparently not. In any case, the fix will be to disallow the second query. We don't know how to do ordering across multiple C* rows (and probably won't before quite some time). So, we should only allow order by when we know the query only select one physical C* row, which I though we were already doing but I'll fix.;;;","17/May/12 12:45;slebresne;We were doing some validation that order by was not used when multiple internal rows are queried, but only for the reversed case for some reason. Patch attached to fix (it also refuse order by when a IN is used on the row key as this would require a bit more work to have it work correctly).;;;","17/May/12 16:30;jbellis;Is this right?

{code}
                if (stmt.keyRestriction == null || !(stmt.keyRestriction.isEquality() && stmt.keyRestriction.eqValues.size() == 1))
{code}

I would have expected
{code}
                if (stmt.keyRestriction == null || !stmt.keyRestriction.isEquality() || stmt.keyRestriction.eqValues.size() != 1)
{code};;;","17/May/12 16:39;slebresne;Given that !(A&&B) == (!A)||(!B), I'd suggest that both are fine :)
But I'm fine going with the second one if you find it more readable.;;;","17/May/12 17:05;jbellis;doh, demorgan fail :)

i do think it's a little clearer to have a chain of || than a mix of || and &&, though;;;","17/May/12 17:18;slebresne;I'm good with that. Attaching v2 with that change.;;;","17/May/12 17:34;jbellis;+1;;;","17/May/12 17:38;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
overlapping sstables in leveled compaction strategy,CASSANDRA-4233,12554549,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,09/May/12 19:53,16/Apr/19 09:32,14/Jul/23 05:52,18/May/12 16:35,1.1.1,,,,,,0,lcs,,,,,,"CASSANDRA-4142 introduces test failures, that are caused by overlapping tables within a level, which Shouldn't Happen.",,iscariot,omid,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/12 16:01;slebresne;0001-Make-sure-Leveled-compaction-always-use-the-right-min-.txt;https://issues.apache.org/jira/secure/attachment/12527850/0001-Make-sure-Leveled-compaction-always-use-the-right-min-.txt","09/May/12 20:00;jbellis;4233-assert.txt;https://issues.apache.org/jira/secure/attachment/12526201/4233-assert.txt","17/May/12 16:38;jbellis;4233-v2.txt;https://issues.apache.org/jira/secure/attachment/12527852/4233-v2.txt","17/May/12 16:48;jbellis;4233-v3.txt;https://issues.apache.org/jira/secure/attachment/12527855/4233-v3.txt","17/May/12 20:11;jbellis;4233-v4.txt;https://issues.apache.org/jira/secure/attachment/12527904/4233-v4.txt","11/May/12 15:06;brandon.williams;system.log.bz2;https://issues.apache.org/jira/secure/attachment/12526534/system.log.bz2",,,,,,,,,,,,6.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238799,,,Tue Jun 12 09:48:55 UTC 2012,,,,,,,,,,"0|i0gtjj:",96216,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/May/12 20:00;jbellis;I see CompactionsTest.testStandardColumnCompactions fail 100% of the time -- but only when run as part of the entire CompactionsTest suite; testStandardColumnCompactions run alone passes.

About 80% of the time the assertion in LCS fails, but sometimes the one in LM fails.  (How the former can fail, without the latter, is a mystery to me.)

Is there an off-by-one bug in IntervalTree?;;;","09/May/12 20:00;jbellis;(Assertions in question are attached.);;;","09/May/12 20:02;brandon.williams;An easy way to recreate this in a live situation that I just accidentally discovered is to set memtable_total_space_in_mb to something small like 3, and then run stress with LCS.;;;","09/May/12 21:26;jbellis;Reproduced assertion errors applying patch (just the LM part) to version 1686a36 (the version before CASSANDRA-4142 was committed).;;;","11/May/12 15:06;brandon.williams;Here's a log with compaction at debug illustrating the repro with small memtable space.;;;","11/May/12 15:31;slebresne;So it's not (only) a problem of overlapping sstables in a given level, since extracted from the log above we get:
{noformat}
DEBUG [CompactionExecutor:4] 2012-05-11 15:04:39,125 LeveledManifest.java (line 273) Compaction candidates for L0 are Standard1-4(L0), Standard1-3(L0), Standard1-2(L1), 
 INFO [CompactionExecutor:4] 2012-05-11 15:04:39,126 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-2-Data.db')]
ERROR [CompactionExecutor:4] 2012-05-11 15:04:41,829 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:4,1,main]
java.lang.AssertionError: Last written key DecoratedKey(170030165514395172434635544532010976042, 30303033313634) >= current key DecoratedKey(983360854569225448206418564333137310, 30303036353031) writing into /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-tmp-hd-6-Data.db
{noformat}
That is, we get an error even though we compact 2 L0 with 1 L1.;;;","11/May/12 15:49;jbellis;Brandon's error was from a node that didn't have 67ed39fa9bf71be4cfc13fccbdd7b76dcb46c062 applied.  So I guess we're dealing with a potential buggy test.;;;","11/May/12 15:49;brandon.williams;My mistake, that log is pre-67ed39fa9b which fixes it, so I think we just have a bad test now.;;;","17/May/12 16:01;slebresne;This was kind of a test problem, though this is kind of a bug.

The main problem is that LeveledCompaction should never get a maxCompactionThreshold < Integer.MAX_VALUE, otherwise it ends up not compaction what it should and this screw up the levels. However, the way we try to ensure that is by overriding the maxCompactionThreshold in LeveledCompactionStrategy constructor. This is however dangerous because it means that if anyone change the threshold afterward, it will break. And sure enough, that is what happens with CompactionsTest.

So attaching a patch that change the code to ensure this doesn't happen. To a large extend this patch is a hack and in the long run, we should refactor all this to move the min/max compaction threshold inside SizeTieredCompaction, where they belong. This is however a bigger refactor than I want to do on the 1.1 branch as currently the thresholds are used to deactivate automatic compaction and whatnot.;;;","17/May/12 16:38;jbellis;simpler v2 attached, to just ignore CFS min/max in the actual LeveledCompactionTask.

also respects isCompactionDisabled in LCS.;;;","17/May/12 16:48;jbellis;tweaked v3 to not mess w/ min/max settings at all (which would result in re-activating compaction if it's ""disabled"");;;","17/May/12 16:56;slebresne;I'd submit that my version was giving a slightly better feedback through JMX but I guess that's not a big deal.

But other than that, the definition of isCompactionDisabled so far has been to disallow automatic compaction, but forced compaction (""maximal"" ones) was still allowed. v2 change that for LCS. I'll note that disabling compaction was respected for background tasks by LCS by the fact that DataTracker.maxCompacting was skipping compaction when maxCompactionThreshold is 0, but allowed for maximal tasks since the maxCompactionThreshold is overriden in compactionManager in that case. I don't know if it's a big deal though (I suspect some tests would fail if run with LCS because they rely on that) but wanted to mention it.;;;","17/May/12 20:11;jbellis;I think the status quo (and v1, which perpetuates it) are too fragile...  Exhibit A being this bug, and exhibit B being my reading of the code leading me to believe that there was no way to disable LCS. Rather than try to prevent setting max inappropriately, much more clear to have LCS ignore min/max entirely, except via isCompactionDisabled.  This is closer to how it ""should"" be w/ min + max being STCS-specific options.

v4 rearranges background/maximal methods to continue allowing maximal to kick off a compaction when autocompaction is disabled.;;;","18/May/12 14:08;slebresne;+1;;;","18/May/12 16:35;jbellis;committed;;;","07/Jun/12 09:02;iscariot;I already have this bug in 1.1.1 version

java.lang.AssertionError: Last written key DecoratedKey(144093708553026671072703572185472027831, 00000000003eef36) >= current key DecoratedKey(50602742485951681279829043048319220646, 00000000004506bf) writing into /cassandra/data/XXXXXX/YYYYYY/AAAAAAA-BBBBBBB-tmp-hd-5217-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
...

any help?;;;","07/Jun/12 09:09;slebresne;I may or may not be related to this issue. It would help to get the full stack trace and a bit more information on when that happened, if that was a fresh 1.1.1 cluster or an upgraded one from 1.1.0 and the compaction strategy used. And it's probably worth creating a new ticket.;;;","07/Jun/12 09:36;iscariot;Stack trace of this error

ERROR 04:09:21,216 Exception in thread Thread[CompactionExecutor:410,1,main]
java.lang.AssertionError: Last written key DecoratedKey(144093708553026671072703572185472027831, 00000000003eef36) >= current key DecoratedKey(50602742485951681279829043048319220646, 00000000004506bf) writing into /cassandra/data/Server/Messages/Server-Messages-tmp-hd-5380-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

Linux version 2.6.32-5-amd64 (Debian 2.6.32-45)
32GB ram, 24 threads (12 cores)

We are upgraded Cassandra server from repository(config file was overwritten, but we change it later). 
And we REcreated keyspace with all CF (we in not a production stage yet).

all us CFs
UPDATE COLUMN FAMILY Messages WITH compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:1};
UPDATE COLUMN FAMILY Messages WITH compaction_strategy=LeveledCompactionStrategy AND compaction_strategy_options={sstable_size_in_mb: 10};
UPDATE COLUMN FAMILY Messages WITH gc_grace = 0;

This CF has many inserts in one time. 
Over 1000-3000 keys with 9 columns each in one sec.
And this CF has 4 secondary indexes (numbers ~ from 1 to 20)

More info about Cassandra server
partitioner: org.apache.cassandra.dht.RandomPartitioner
key_cache_size_in_mb - default
row_cache_size_in_mb - default
row_cache_provider: SerializingCacheProvider

MAX_HEAP_SIZE=""8G""
HEAP_NEWSIZE=""2G""


Thanks for any help.
;;;","12/Jun/12 09:48;slebresne;I believe this is the same than CASSANDRA-4321 and being tracked there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleting a CF always produces an error and that CF remains in an unknown state,CASSANDRA-4230,12554460,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,edevil,edevil,09/May/12 10:28,16/Apr/19 09:32,14/Jul/23 05:52,15/May/12 17:10,1.1.1,,,,,,0,,,,,,,"From the CLI perspective:

[default@Disco] drop column family client; 
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1222)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1209)
	at org.apache.cassandra.cli.CliClient.executeDelColumnFamily(CliClient.java:1301)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

Log:

 INFO [MigrationStage:1] 2012-05-09 11:25:35,686 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,687 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,748 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-Data.db (1041 bytes)
 INFO [MigrationStage:1] 2012-05-09 11:25:35,749 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,750 Memtable.java (line 266) Writing Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,812 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db (649 bytes)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,814 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-Data.db'), SSTableReader
(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-Data.db'), SSTableReader(path
='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db')]
 INFO [MigrationStage:1] 2012-05-09 11:25:35,918 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,919 Memtable.java (line 266) Writing Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,945 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-Data.db,].  22,486 to 20,621 (~91% of orig
inal) bytes for 2 keys at 0.150120MB/s.  Time: 131ms.
 INFO [FlushWriter:3] 2012-05-09 11:25:36,013 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db (407 bytes)
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
        at org.apache.cassandra.utils.CLibrary.link(Native Method)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

ERROR [Thrift:3] 2012-05-09 11:25:36,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
        ... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
ERROR [MigrationStage:1] 2012-05-09 11:25:36,051 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)

        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,052 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-4-Data.db')]
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,187 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-6-Data.db,].  728 to 458 (~62% of original) bytes for 8 keys at 0.003235MB/s.  Time: 135ms.

Schema:

CREATE COLUMN FAMILY Client WITH
       key_validation_class = UUIDType AND
       comparator = UTF8Type AND
       column_metadata = [ { column_name: key, validation_class: BytesType }
                           { column_name: name, validation_class: UTF8Type }
                           { column_name: userid, validation_class: UUIDType, index_type: KEYS }
                         ] AND
       compression_options = { sstable_compression:SnappyCompressor, chunk_length_kb:64 } AND
       compaction_strategy = LeveledCompactionStrategy AND
       compaction_strategy_options = { sstable_size_in_mb: 10 } AND
       gc_grace = 432000;

State of data dir after deletion attempt:

# ls -lah /var/lib/cassandra/data/Disco/Client/ 
total 76K
drwxr-xr-x  3 cassandra cassandra 4.0K May  9 11:25 .
drwxr-xr-x 17 cassandra cassandra 4.0K May  3 12:34 ..
-rw-r--r--  2 cassandra cassandra  420 May  9 11:25 Client-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx.json
-rw-r--r--  1 cassandra cassandra  418 May  9 11:25 Client.json
-rw-r--r--  1 cassandra cassandra   46 May  9 11:25 Disco-Client-hc-6-CompressionInfo.db
-rw-r--r--  1 cassandra cassandra  458 May  9 11:25 Disco-Client-hc-6-Data.db
-rw-r--r--  1 cassandra cassandra  976 May  9 11:25 Disco-Client-hc-6-Filter.db
-rw-r--r--  1 cassandra cassandra  208 May  9 11:25 Disco-Client-hc-6-Index.db
-rw-r--r--  1 cassandra cassandra 4.3K May  9 11:25 Disco-Client-hc-6-Statistics.db
-rw-r--r--  4 cassandra cassandra   46 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-CompressionInfo.db
-rw-r--r--  4 cassandra cassandra   92 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Data.db
-rw-r--r--  4 cassandra cassandra  496 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Filter.db
-rw-r--r--  4 cassandra cassandra   26 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Index.db
-rw-r--r--  4 cassandra cassandra 4.3K May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Statistics.db
drwxr-xr-x  6 cassandra cassandra 4.0K May  9 11:25 snapshots
",Debian Linux Squeeze with the cassandra debian package from Apache.,cvertiz,edevil,jbellis,JoshuaMcKenzie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/12 16:45;jbellis;4230-v2.txt;https://issues.apache.org/jira/secure/attachment/12526778/4230-v2.txt","13/May/12 14:47;xedin;CASSANDRA-4230.patch;https://issues.apache.org/jira/secure/attachment/12526679/CASSANDRA-4230.patch","12/Mar/15 15:18;cvertiz;cassandraDEBUG.log;https://issues.apache.org/jira/secure/attachment/12704165/cassandraDEBUG.log",,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238702,,,Thu Mar 12 17:21:05 UTC 2015,,,,,,,,,,"0|i0gti7:",96210,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/May/12 16:10;jbellis;probably related to CASSANDRA-4219;;;","09/May/12 16:17;xedin;From the exception message I see that it's related to the JNA hardlinks while it does a snapshot:

{noformat}
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
...
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
{noformat}

and errno of 17 means ""File exists"" so this is something with snapshot or the system rather then migrations.;;;","09/May/12 16:29;brandon.williams;Since snapshot dirs include a timestamp, perhaps we should just move along if it already exists.;;;","09/May/12 16:32;xedin;Yeah, we probably should do just that.;;;","09/May/12 16:43;jbellis;But why would it attempt the same snapshot twice?  Sounds like there's a real bug here.;;;","09/May/12 16:57;brandon.williams;snapshot on compaction is one way it can happen.;;;","09/May/12 17:02;xedin;Yeah, it's only called in snapshotWithoutFlush so if you try to drop CF in the middle of compaction snapshoting files it would be the only way to lead to such behavior. I think what we need to do here is to stop compaction and run drop after that...;;;","09/May/12 18:12;jbellis;bq. snapshot on compaction is one way it can happen.

I suppose there's a really small chance of that happening if you happen to time it just right, but that wouldn't explain it being easily reproducible (since the compaction and drop snapshots generate their snapshot name independently).

Also, snapshot-on-compaction is off by default, I doubt André has it enabled.  André, can you confirm?;;;","09/May/12 18:41;edevil;I suppose you mean this:
{noformat}
$ cat /etc/cassandra/cassandra.yaml |grep snapshot |grep compac
# Whether or not to take a snapshot before each compaction.  Be
snapshot_before_compaction: false
{noformat}

I did not enable it.;;;","13/May/12 14:14;xedin;I have figured out that this problem is caused only when LeveledCompaction is used for Secondary Index, it seems like when index's leveled manifest is snapshoted it uses the wrong name.;;;","13/May/12 22:53;jbellis;Do we need this in 1.0 too?;;;","13/May/12 23:00;xedin;No, this bug came together with directory-per-CF which was added into 1.1.;;;","14/May/12 16:45;jbellis;simpler v2 attached;;;","14/May/12 17:05;xedin;Heh, I overlooked that possibility, +1.;;;","14/May/12 17:07;xedin;Andre, can you please test and confirm that everything works as expected before we commit it?;;;","15/May/12 17:08;edevil;It works, thanks!;;;","15/May/12 17:10;jbellis;committed;;;","12/Mar/15 15:17;cvertiz;Note: Issue reproduced on cassandra 2.0.12 (datastax for windows)
Log: refer to cassandraDEBUG.log attached file.;;;","12/Mar/15 17:21;JoshuaMcKenzie;[~cvertiz]: doesn't necessarily look like the same issue to me. Your attached logs don't show any errors symlinking for snapshots during compaction though they do contain the same thrift error message:
{noformat}
DEBUG [Thrift:13] 2015-03-12 09:52:49,291 CustomTThreadPoolServer.java (line 214) Thrift transport error occurred during processing of message.
 org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
{noformat}

What behavior produced the errors you're seeing?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy throws NPEs for when there's no hostids for a target,CASSANDRA-4227,12554426,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,dbrosius@apache.org,dbrosius@apache.org,09/May/12 04:04,16/Apr/19 09:32,14/Jul/23 05:52,23/May/12 20:58,1.2.0 beta 1,,,,,,0,,,,,,,"On trunk...

if there is no host id due to an old node, an info log is generated, but the code continues to use the null host id causing NPEs in decompose... Should this bypass this code, or perhaps can the plain ip address be used in this case? don't know.

as follows...



                    UUID hostId = StorageService.instance.getTokenMetadata().getHostId(target);
                    if ((hostId == null) && (Gossiper.instance.getVersion(target) < MessagingService.VERSION_12))
                        logger.info(""Unable to store hint for host with missing ID, {} (old node?)"", target.toString());
                    RowMutation hintedMutation = RowMutation.hintFor(mutation, ByteBuffer.wrap(UUIDGen.decompose(hostId)));
                    hintedMutation.apply();
",,dbrosius@apache.org,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/12 21:47;urandom;4227_drop_hints.txt;https://issues.apache.org/jira/secure/attachment/12528656/4227_drop_hints.txt","22/May/12 00:30;dbrosius@apache.org;4227_guard_against_npes_for_old_gossip_versions.diff;https://issues.apache.org/jira/secure/attachment/12528538/4227_guard_against_npes_for_old_gossip_versions.diff",,,,,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238668,,,Wed May 23 20:58:48 UTC 2012,,,,,,,,,,"0|i0gth3:",96205,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/May/12 14:28;brandon.williams;We should just drop the hint in this case, see CASSANDRA-4120;;;","22/May/12 00:30;dbrosius@apache.org;against trunk;;;","22/May/12 15:24;jbellis;So the situation is, I've upgraded to 1.2 so I'm supposed to have node IDs, but one of the nodes starts slower than the others (for instance) so it hasn't broadcast its nodeid to the rest of the cluster yet?;;;","22/May/12 15:47;brandon.williams;I believe the more common situation is you're in the process of upgrading to 1.2 and have a mixed cluster, but need to generate a hint for an older version that hasn't generated a nodeid yet.;;;","22/May/12 21:46;urandom;bq. We should just drop the hint in this case, see CASSANDRA-4120

Yeah, I think this is a straightforward brainfart and that it was supposed to have returned after logging ""cannot store hint"", instead of you know, trying to store it. :)

See attached.;;;","22/May/12 22:16;jbellis;Hints are supposed to be reliable now, which I expect will lead more people to turn off read repair.  Which is a long way of saying I think we should
- log the message at WARN
- add a note to NEWS that you should upgrade when all nodes are up to minimize this

+1 otherwise.;;;","22/May/12 23:52;dbrosius@apache.org;committed as bd32d4f0b9f0f88fed97e8ddf2ee41b5b048d31d;;;","23/May/12 00:45;urandom;I reverted this.

I'm not sure which patch Jonathan was +1'ing, but the what was committed here wasn't from either.

I don't think the test for version < VERSION_12 should be removed.  A null hostid is ""normal"" when the node is pre-1.2 (as would be the case during a rolling upgrade).  I can't foresee a reason that it would be null otherwise, it would represent a much more serious error and shouldn't be swept away with a WARN message.

Also, Jonathan wanted to see something added to NEWS.  The second bullet point under ""Upgrading"" for 1.2 already mentions this, but it would probably be better to strengthen that wording with a recommendation to only upgrade when all nodes are up.;;;","23/May/12 01:49;dbrosius@apache.org;ok, but what ever the reason for a null host id, it's going to NPE.;;;","23/May/12 03:23;jbellis;Sorry for the confusion, I was +1ing the 2nd patch.

I'm okay with NPEing if we have a null host on version 12, since that's Not Supposed To Happen.  (Alternatively, we could add an assert to make that explicit.);;;","23/May/12 20:58;urandom;committed with assertion and additional NEWS.txt recommendation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non Unique Streaming session ID's,CASSANDRA-4223,12554104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,06/May/12 23:10,16/Apr/19 09:32,14/Jul/23 05:52,18/May/12 18:40,1.0.11,1.1.1,,,,,0,datastax_qa,,,,,,"I have observed repair processes failing due to duplicate Streaming session ID's. In this installation it is preventing rebalance from completing. I believe it has also prevented repair from completing in the past. 

The attached streaming-logs.txt file contains log messages and an explanation of what was happening during a repair operation. it has the evidence for duplicate session ID's.

The duplicate session id's were generated on the repairing node and sent to the streaming node. The streaming source replaced the first session with the second which resulted in both sessions failing when the first FILE_COMPLETE message was received. 

The errors were:

{code:java}
DEBUG [MiscStage:1] 2012-05-03 21:40:33,997 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:1] 2012-05-03 21:40:34,027 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/FMM_Studio/PartsData-hc-1-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}

and

{code:java}
DEBUG [MiscStage:2] 2012-05-03 21:40:36,497 StreamReplyVerbHandler.java (line 47) Received StreamReply StreamReply(sessionId=26132848816442266, file='/var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db', action=FILE_FINISHED)
ERROR [MiscStage:2] 2012-05-03 21:40:36,497 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[MiscStage:2,5,main]
java.lang.IllegalStateException: target reports current file is /var/lib/cassandra/data/OpsCenter/rollups7200-hc-3-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:195)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{code}


I think this is because System.nanoTime() is used for the session ID when creating the StreamInSession objects (driven from StorageService.requestRanges()) . 

From the documentation (http://docs.oracle.com/javase/6/docs/api/java/lang/System.html#nanoTime()) 

{quote}
This method provides nanosecond precision, but not necessarily nanosecond accuracy. No guarantees are made about how frequently values change. 
{quote}

Also some info here on clocks and timers https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks

The hypervisor may be at fault here. But it seems like we cannot rely on successive calls to nanoTime() to return different values. 

To avoid message/interface changes on the StreamHeader it would be good to keep the session ID a long. The simplest approach may be to make successive calls to nanoTime until the result changes. We could fail if a certain number of milliseconds have passed. 

Hashing the file names and ranges is also a possibility, but more involved. 

(We may also want to drop latency times that are 0 nano seconds.)
","Ubuntu 10.04.2 LTS

java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)

""Bare metal"" servers from https://www.stormondemand.com/servers/baremetal.html 
The servers run on a custom hypervisor.
 ",yukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/12 22:25;amorton;4223_counter_session_id-V2.diff;https://issues.apache.org/jira/secure/attachment/12526438/4223_counter_session_id-V2.diff","09/May/12 11:38;amorton;4223_counter_session_id.diff;https://issues.apache.org/jira/secure/attachment/12526146/4223_counter_session_id.diff","07/May/12 21:50;amorton;NanoTest.java;https://issues.apache.org/jira/secure/attachment/12525924/NanoTest.java","06/May/12 23:11;amorton;fmm streaming bug.txt;https://issues.apache.org/jira/secure/attachment/12525790/fmm+streaming+bug.txt",,,,,,,,,,,,,,4.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238328,,,Fri May 18 18:39:56 UTC 2012,,,,,,,,,,"0|i0gtfb:",96197,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"07/May/12 21:45;amorton;Did some more testing. On the stormondemand.com machines it looks like nanoTime() is updated every 10ms. On EC2 nodes nanoTime() was always unique. The test script is attached, examples are below...

{code}
root@db6:~/aaron# java -classpath ./ NanoTest 100
nanoTime 460519702418991 occurred 5176 times
nanoTime 460519712419053 occurred 12090 times
nanoTime 460519722419115 occurred 22602 times
nanoTime 460519732419177 occurred 36154 times
nanoTime 460519742419239 occurred 36089 times
nanoTime 460519752419301 occurred 36866 times
nanoTime 460519762419363 occurred 36997 times
nanoTime 460519772419425 occurred 36763 times
nanoTime 460519782419487 occurred 36910 times
nanoTime 460519792419549 occurred 35481 times
Ran for 100 milliseconds, got 295128 duplicates and 11 uniques.
{code}

If it takes 10ms to get a unique value calling multiple times is out of the question. Will put thinking hat back on.;;;","07/May/12 21:50;amorton;Test for unique nanoTime() results.;;;","07/May/12 22:51;jbellis;bq. The simplest approach may be to make successive calls to nanoTime until the result changes. We could fail if a certain number of milliseconds have passed. 

Hector has (had?) some code to make per-client unique timestamps by taking counter = max(currentTimeMillis, counter + 1).  I suppose you could do something similar here so you don't need to busywait.;;;","07/May/12 23:11;amorton;yeah was thinking an incrementing counter.
Any concern about how unique the id's have to be ? 

I was thinking of creating a long out the (int) gossip generation for the local machine and an int counter that wraps around.   ;;;","07/May/12 23:23;jbellis;That sounds reasonable.  They just need to be unique-per-host, not per-cluster.;;;","08/May/12 15:33;jbellis;... actually just an AtomicLong counter should be fine.  If we have more than 2^63 streaming sessions I'll live w/ the overflow. :);;;","08/May/12 21:02;amorton;What about if a node received streaming requests from two other nodes? There would be an (small) chance of the nodes generating the same session ID.

Adding the gossip generation adds a little entropy to the id's. Happy to go with the simpler counter idea if you think this is a non-problem. ;;;","08/May/12 21:19;jbellis;It doesn't need to be unique-per-cluster because StreamInSession tracks it as a Pair<InetAddress, Long>.;;;","08/May/12 21:23;yukim;I see possible collision here, since StreamOutSession is identified by destination host + timestamp(for now) and StreamInSession in destination node uses it when received it from remote.
Adding one more key (source IP?) would make ID unique?;;;","08/May/12 22:12;jbellis;Ugh, so the problem is that sometimes session IDs are generated by the target, and sometimes by the source?  That's broken...

I think there's several possible solutions:
# always generate session IDs on the source, so our Pair really is unique [the way I thought it worked :)]
# try to make the 64bit session IDs ""unique enough"" across the cluster [aaron's timestamp + counter]
# guarantee the session IDs are unique-per-host, and make the session context (source, id-generated-by-ip, id) instead of just (source, id) [yuki's suggestion]
# just switch to a UUID

#4 is probably simplest, but only #2 and #3 will be backwards-compatible.  Of those two I feel more confident about #3...  The only unique-id-in-64-bit schemas I know of require some coordination up front among participating nodes (e.g., http://engineering.twitter.com/2010/06/announcing-snowflake.html);;;","09/May/12 02:05;amorton;I'll take another look at how unique the session id need to be.;;;","09/May/12 11:38;amorton;Use an AtomicLong in StreamInSession and one in StreamOutSession for the session id. 

Sessions are always accessed using <inet_address, session_id>, and in and out session are in their own collections. ;;;","09/May/12 16:37;yukim;After looking closer to the code, I can say that AtomicLong is enough here, as Aaron's patch suggests.
There is no need to generate cluster wide unique id, it just need to be unique between two nodes(source, dest).
I thought a pair of host and long value is shared among nodes but that's not true. My apologies.

So +1 to the patch attached.;;;","09/May/12 16:47;jbellis;StreamIn.requestRanges will create a session ID using the *target* node's id generator (whatever it is) but in a Pair with the *source* node's IP.

Conversely, when a Stream is originated on the source node, it creates a session id using the *source* node's id generator, and sends that over in the stream header, where IncomingStreamReader picks it up and sticks it in the Session map.

So it looks to me like Yuki was right the first time ... ?;;;","09/May/12 17:07;yukim;Name *source* is probably misleading here. I found that it's actually *source* of data you're requesting.
When node A initiates streaming to node B with StreamIn.requestRanges, it creates StreamInSession with pair of id, say, <B, 1> and sends request with session id 1.
Node B receives request and creates StreamOutSession from sender's ip and received session id 1, ends up having StreamOutSession of id <A, 1>.

||A                    ||                              ||B                    ||
|StreamInSession<B, 1> | \-\-(session ID of 1 from A)\-\-> |StreamOutSession<A, 1> |
|                      | <\-(session ID of 1 from B)\-\-- |                       |;;;","09/May/12 17:33;jbellis;Right, so the problem is when A creates a {{<B, 1>}} Session in StreamIn, while B simultaneously creates a {{<B, 1>>}} for an unrelated StreamOut.transferRanges (move or unbootstrap).  ;;;","09/May/12 22:27;amorton;Cassandra 4226

Agree. We could end up with the StreamOutSessions with the same <ip, id> context, one generated externally one internally. 

bq. 3. guarantee the session IDs are unique-per-host, and make the session context (source, id-generated-by-ip, id) instead of just (source, id) [yuki's suggestion]

how does ""id-generated-by-ip"" work ? We can only generate a streaming context from the StreamHeader or Stream Reply and sender ip address. Unfortunately these messages are not extensible. 

Another idea: make the session id [source_flag + local_int_counter]. Source flag is:

* 0 for a StreamInSession generated to request data.
*  1 for a StreamOutSession generated to transfer/push data.
* A StreamOutSession created in response to a stream request (still) uses the sessionID generated remotely.

One node may now have multiple stream out sessions to the same ip with the same session id:

IP | source_flag | session id
1.1.1.1 | 0 (started remotely) | 6
1.1.1.1 | 1 (started locally) | 6   ;;;","10/May/12 16:49;yukim;Isn't it enough to just increment AtomicLong counter until there is no collision when creating locally?;;;","10/May/12 20:15;amorton;There are two path ways for a StreamOutSession to be added to the SOS.streams map:

1) Push / transfer. When node A wants to push data to node B using StreamOut.transferRanges(). In this case the SOS context using the target ip address - <node_b_ip, node_a_sos_counter>

2) Pull / request. When node B wants node A to send it data, using StreamIn.requestRanges(). Node B creates a StreamInSession that uses the target ip address - <node_a_ip, node_b_sis_counter>. When node A gets the stream request from node b it creates a StreamOutSession with context <node_b_ip, node_b_sis_counter>

So we can end up with these two sos contexts on node A:
* <node_b_ip, node_a_sos_counter>
* <node_b_ip, node_b_sis_counter>

We need to avoid collisions between node_a_sos_counter and node_b_sis_counter. It's an unlikely event but we need to be safe.

Adding the source_flag means we end up with this on node A:

* <node_b_ip, 1 ""push"", node_a_sos_counter>
* <node_b_ip, 0 ""pull"", node_b_sis_counter>


bq. Isn't it enough to just increment AtomicLong counter until there is no collision when creating locally?

Are you saying ""lock around creating the StreamOutSessions from both code paths and create session ID's that do not collide with known session ids ?""
;;;","10/May/12 22:25;amorton;4223_counter_session_id-V2.diff 

Uses stream source flag as discussed. Added the flags to StreamHeader so they were together. 
;;;","14/May/12 17:12;yukim;I'm fine with Aaron's approach. Session Id counter will overflow if > Integer.MAX_VALUE but I think thats enough for streaming. Regarding the patch, doing 0 << 32 for StreamOutSession feels redundant to me. I think it's better to just increment the counter inside StreamOutSession constructor.;;;","14/May/12 22:34;amorton;bq.  Regarding the patch, doing 0 << 32 for StreamOutSession feels redundant to me. I think it's better to just increment the counter inside StreamOutSession constructor.

Always doing the shift makes it explicit that the session ID is a composite value constructed from two smaller values. And that StreamInSession and StreamOutSession have the same sessionID construction. Remember the StreamInSession ID becomes a StreamOutSession ID. 

For the same reasons I pulled the logic into nextSessionID(), it's easier to document and make explicit.

Can change if you think it's poor style.;;;","15/May/12 15:51;yukim;OK, +1.;;;","17/May/12 10:59;amorton;Yuki were you going to commit this or do you want me to?;;;","17/May/12 15:19;yukim;Aaron,

Go ahead. :);;;","18/May/12 18:39;amorton;committed to cassandra-1.0, cassandra-1.1 and trunk.

thanks. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error while deleting a columnfamily that is being compacted.,CASSANDRA-4221,12553932,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/May/12 13:23,16/Apr/19 09:32,14/Jul/23 05:52,28/May/12 18:26,1.1.1,,,,,,0,,,,,,,"The following dtest command produces an error:
{code}export CASSANDRA_VERSION=git:cassandra-1.1; nosetests --nocapture --nologcapture concurrent_schema_changes_test.py:TestConcurrentSchemaChanges.load_test{code}

Here is the error:
{code}
Error occured during compaction
java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:239)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1580)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1770)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:61)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:839)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:851)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:142)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:148)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:121)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:264)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
Caused by: java.io.FileNotFoundException: /tmp/dtest-6ECMgy/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-47-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:102)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:985)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
	... 13 more
{code}

For reference, here is the dtest function that causes the failure. The error happens on the line near the bottom that drops the columnfamily:
{code}
    def load_test(self):                                                        
        """"""                                                                     
        apply schema changes while the cluster is under load.                   
        """"""                                                                     
        debug(""load_test()"")                                                    
                                                                                
        cluster = self.cluster                                                  
        cluster.populate(1).start()                                             
        node1 = cluster.nodelist()[0]                                           
        wait(2)                                                                 
        cursor = self.cql_connection(node1).cursor()                            
                                                                                
        def stress(args=[]):                                                    
            debug(""Stressing"")                                                  
            node1.stress(args)                                                  
            debug(""Done Stressing"")                                             
                                                                                
        def compact():                                                          
            debug(""Compacting..."")                                              
            node1.nodetool('compact')                                           
            debug(""Done Compacting."")                                           
                                                                                
        # put some data into the cluster                                        
        stress(['--num-keys=1000000'])                                          
                                                                                
        # now start compacting...                   
        tcompact = Thread(target=compact)                                       
        tcompact.start()                                                        
        wait(1)                                                                 
                                                                                
        # now the cluster is under a lot of load. Make some schema changes.     
        cursor.execute(""USE Keyspace1"")                                         
        wait(1)                                                                 
        cursor.execute(""DROP COLUMNFAMILY Standard1"")                           
                                                                                
        wait(3)                                                                 
                                                                                
        cursor.execute(""CREATE COLUMNFAMILY Standard1 (KEY text PRIMARY KEY)"")  
                                                                                
        tcompact.join()                                                         
 
{code}
Again, the error happens on cassandra-1.1, but not on cassandra-1.0.","ccm, dtest, cassandra-1.1. The error does not happen in cassandra-1.0.",jbellis,mbulman,rcoli,tpatterson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/12 15:33;jbellis;4221-v3.txt;https://issues.apache.org/jira/secure/attachment/12529964/4221-v3.txt","17/May/12 10:27;xedin;CASSANDRA-4221-logging.patch;https://issues.apache.org/jira/secure/attachment/12527820/CASSANDRA-4221-logging.patch","25/May/12 15:02;xedin;CASSANDRA-4221-v2.patch;https://issues.apache.org/jira/secure/attachment/12529726/CASSANDRA-4221-v2.patch","09/May/12 19:35;xedin;CASSANDRA-4221.patch;https://issues.apache.org/jira/secure/attachment/12526198/CASSANDRA-4221.patch","18/May/12 16:49;tpatterson;system.log;https://issues.apache.org/jira/secure/attachment/12528099/system.log",,,,,,,,,,,,,5.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238149,,,Mon May 28 18:26:32 UTC 2012,,,,,,,,,,"0|i0gtef:",96193,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/May/12 18:13;xedin;This one seems to be caused by the same problem as CASSANDRA-4230.;;;","09/May/12 18:16;jbellis;Maybe, but I'm skeptical -- 4230 is complaining about a file existing when it shouldn't, while this one says a file doesn't exist that should :);;;","09/May/12 19:00;xedin;Patch adds a try to stop all running compactions on given Keyspace or ColumnFamily before running a drop command. I have tried the test you have in the description and it ran without failures.;;;","09/May/12 19:12;jbellis;That takes us back to the Bad Old Days pre-CASSANDRA-3116, though.  We should be able to fix w/o resorting to A Big Lock.;;;","09/May/12 19:34;xedin;For the KS or CF drop this seems necessary to try to wait until all running compactions finish otherwise it would end up in errors like one in the description, also other operations - create, update - are not affected by this.;;;","09/May/12 22:54;jbellis;The idea from 3116 was:

- Drop will only delete sstables not actively being compacted
- post-compaction, we check if the CF was dropped, and if so we delete the sstables then;;;","09/May/12 23:06;xedin;I don't know which one is better tho because if compaction fails for some reason which that scenario, wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? We would have to add complexity to the schema merge just to handle that case as well as on the local side...;;;","09/May/12 23:10;xedin;The other way would be to 'mark a CF for delete' and return to the user right way (making CF invisible to users), sending the drop request to the others where they would apply the same thing (try to stop all compactions running, wait until they are done) and drop.;;;","10/May/12 02:32;jbellis;bq. wouldn't that mean that all SSTables that were left behind are staying until somebody manually deletes them (or restart would drop them)? 

We already clean up partially-written sstables after compaction failure, I don't see why we couldn't use similar logic here.;;;","10/May/12 16:14;xedin;The problem I see that that is we need to do a snapshot before start dropping or deleting any CF files so it's probably better to make that drop option 'deferred' until running compactions are stopped so we have a persistent view of the files we would have to operate upon.;;;","10/May/12 16:19;jbellis;DataTracker already makes sstable changes atomic, though.  At any time you can snapshot with that and get a consistent view.;;;","16/May/12 16:40;jbellis;Tyler, can you still reproduce after the recent schema fixes on the 1.1 branch?;;;","17/May/12 04:15;tpatterson;Yes, the error just happened again for me. I did a fresh pull on branch branch cassandra-1.1.;;;","17/May/12 10:27;xedin;Interesting, I can't reproduce it myself. Can you please run it with logging patch attached (and enabled DEBUG logging) and attach debug log from your C* node to this task, so I can check that is happening inside of DataTracker in your case?... ;;;","17/May/12 17:59;tpatterson;This was after applying both patches to the cassandra-1.1 branch, and setting logging to DEBUG.;;;","17/May/12 18:13;tpatterson;Somehow that server.log did not have the debug info. Looking into it now.;;;","17/May/12 20:12;tpatterson;Debug is enabled now; It looks like CCM overwrites the log level. Only the logging patch was applied in this run. ;;;","17/May/12 20:23;xedin;I see debug information I added right now, but there is no IOError in that log described in this task...;;;","18/May/12 16:49;tpatterson;So after some experimentation, the problem is only happening when the log level is set to INFO, but it doesn't happen at DEBUG. Gotta love these ones! I modified the logging patch to do logging.info() rather then logging.debug(), and the problem still happens, so at least you can see those debug messages. I hope this is enough to go on.;;;","18/May/12 19:58;xedin;Hah, now I know what is causing it - it's not a drop problem, the situation is triggered when you re-create ColumnFamily right after drop +(before all SSTables were actually deleted by background task)+ so it reads up all SSTables in the directory back to system and tries to compact them simultaneously with them being deleted in the background. That is why we warn people to *avoid* making any modifications to the active CFs otherwise it could lead to the strange situations like this one.;;;","19/May/12 16:31;jbellis;Would this be fixed by CASSANDRA-3794 then, since old and new CF will have different IDs?;;;","19/May/12 21:45;xedin;Not really because it generates UUID from ksName + cfName to be able make it the same across all machines independent of their state.;;;","19/May/12 22:19;jbellis;Should we just add a call to abort in-progress compactions at drop time (which will help cleanup happen faster) and call that ""as close as we're going to get?"";;;","19/May/12 22:28;xedin;This is what I did in my patch :);;;","23/May/12 21:53;jbellis;- stopCompactionFor should take CFS parameters instead of String
- I don't see any reason to not wait indefinitely here; in fact, if we make sure to wait until compaction finishes, the odds are much better that when we tell the client ""all done"" he won't be able to send a ""create"" quickly enough to hit the bug
- Need to call stopCompactionFor on every replica, not just CompactionServer -- move this to DefsTable.dropColumnFamily?
;;;","23/May/12 22:21;xedin;bq. stopCompactionFor should take CFS parameters instead of String

I don't really follow here, if you want it to have list of CFMetaData instead of String? String is better suited because CompactionInfo.getColumnFamily() returns a String (CF name).

bq. I don't see any reason to not wait indefinitely here; in fact, if we make sure to wait until compaction finishes, the odds are much better that when we tell the client ""all done"" he won't be able to send a ""create"" quickly enough to hit the bug

We don't really try to wait indefinitely here, just for 30 seconds (worst case), if compactions don't finish until then we just move on with delete. do you want it to wait until all compactions  to finish?

bq. Need to call stopCompactionFor on every replica, not just CompactionServer – move this to DefsTable.dropColumnFamily?

I agree, I'm going to move that into dropColumnFamily call so it gets called on the replicas too.;;;","23/May/12 22:24;jbellis;bq. String is better suited because CompactionInfo.getColumnFamily() returns a String 

Feel free to fix that. :)

bq. if compactions don't finish until then we just move on with delete

It throws IOException.

Remember that we check for ability to abort compaction every row; if we're compacting a wide row, it could easily take over 30s w/ throttling.;;;","23/May/12 22:47;xedin;bq. It throws IOException. Remember that we check for ability to abort compaction every row; if we're compacting a wide row, it could easily take over 30s w/ throttling.

Oh yes, sorry. I think we can just remove that exception and move on with drop, or do you want it to until all compactions finish?

;;;","28/May/12 15:33;jbellis;v3 attached.  removes CompactionInfo fields that are redundant w/ the introduction of CFM, and removes the wait from the stop method (it doesn't help clean up the sstables involved any faster, so there is no point in slowing down the drop for it).;;;","28/May/12 18:26;xedin;Committed with nit in CompactionInfo.getColumnFamily() to return cfName instead of ksName as in v3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with creating keyspace after drop,CASSANDRA-4219,12553879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jeffwilliams,jeffwilliams,04/May/12 07:29,16/Apr/19 09:32,14/Jul/23 05:52,15/May/12 14:20,1.1.1,,,,,,0,,,,,,,"Hi,

I'm doing testing and wanted to drop a keyspace (with a column family) to re-add it with a different strategy. So I ran in cqlsh:

DROP KEYSPACE PlayLog;

CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
 AND strategy_options:replication_factor = 2;

And everything seemed to be fine. I ran some inserts, which also seemed to go fine, but then selecting them gave me:

cqlsh:PlayLog> select count(*) from playlog;
TSocket read 0 bytes

I wasn't sure what was wrong, so I tried dropping and creating again, and now when I try to create I get:

cqlsh> CREATE KEYSPACE PlayLog WITH strategy_class = 'SimpleStrategy'
  ...   AND strategy_options:replication_factor = 2;
TSocket read 0 bytes

And the keyspace doesn't get created. In the log it shows:

ERROR [Thrift:4] 2012-05-03 18:23:05,124 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
       at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
       at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:129)
       at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:701)
       at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:875)
       at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1235)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
       at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
       at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
       at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
       at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
       at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
       at java.util.concurrent.FutureTask.get(Unknown Source)
       at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
       ... 13 more
Caused by: java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       ... 3 more
ERROR [MigrationStage:1] 2012-05-03 18:23:05,124 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:441)
       at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:339)
       at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:269)
       at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
       at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
       at java.util.concurrent.FutureTask.run(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)

Any ideas how I can recover from this?

I am running version 1.1.0 and have tried nodetool repair, cleanup, compact. I can create other keyspaces, but still can't create a keyspace called PlayLog even though it is not listed anywhere.

Jeff",Debian 6.0.4 x64,dmuth,jeffwilliams,mhanna,mharris,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/12 09:27;slebresne;0001-Add-debug-logs.txt;https://issues.apache.org/jira/secure/attachment/12525582/0001-Add-debug-logs.txt","11/May/12 18:33;xedin;CASSANDRA-4219.patch;https://issues.apache.org/jira/secure/attachment/12526555/CASSANDRA-4219.patch","25/Dec/17 14:34;mhanna;k2fabric.log.100;https://issues.apache.org/jira/secure/attachment/12903648/k2fabric.log.100","25/Dec/17 14:34;mhanna;k2fabric.log.101;https://issues.apache.org/jira/secure/attachment/12903647/k2fabric.log.101","25/Dec/17 14:34;mhanna;k2fabric.log.102;https://issues.apache.org/jira/secure/attachment/12903646/k2fabric.log.102","25/Dec/17 14:34;mhanna;k2fabric.log.103;https://issues.apache.org/jira/secure/attachment/12903645/k2fabric.log.103","25/Dec/17 14:34;mhanna;k2fabric.log.98;https://issues.apache.org/jira/secure/attachment/12903650/k2fabric.log.98","25/Dec/17 14:34;mhanna;k2fabric.log.99;https://issues.apache.org/jira/secure/attachment/12903649/k2fabric.log.99","25/Dec/17 14:35;mhanna;no-keyspace-ondisk.png;https://issues.apache.org/jira/secure/attachment/12903643/no-keyspace-ondisk.png","25/Dec/17 14:34;mhanna;no_keyspace.png;https://issues.apache.org/jira/secure/attachment/12903644/no_keyspace.png","25/Dec/17 14:35;mhanna;schema_after_ks_creation.png;https://issues.apache.org/jira/secure/attachment/12903642/schema_after_ks_creation.png","25/Dec/17 14:35;mhanna;schema_before_ks_creation.png;https://issues.apache.org/jira/secure/attachment/12903641/schema_before_ks_creation.png","08/May/12 13:44;jeffwilliams;system-91.223.192.26.log.gz;https://issues.apache.org/jira/secure/attachment/12525993/system-91.223.192.26.log.gz","04/May/12 10:25;jeffwilliams;system-debug.log.gz;https://issues.apache.org/jira/secure/attachment/12525590/system-debug.log.gz","05/May/12 12:30;jeffwilliams;system-startup-debug.log.gz;https://issues.apache.org/jira/secure/attachment/12525727/system-startup-debug.log.gz","04/May/12 09:44;jeffwilliams;system.log.gz;https://issues.apache.org/jira/secure/attachment/12525585/system.log.gz",,16.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,238092,,,Mon Dec 25 14:36:27 UTC 2017,,,,,,,,,,"0|i0gtdj:",96189,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/May/12 09:27;slebresne;Would that be possible for you to try applying the attached patch (0001-Add-debug-logs.txt) on one of the machine, switch the log to DEBUG, try recreating said keyspace and send us the resulting log. This should give us more info on what's going on.

Also, do you still have the log of when you first got the 'TSocket read 0 bytes' during a select? Is there any corresponding errors?;;;","04/May/12 09:52;jeffwilliams;I have attached the entire system.log from yesterday for the server I was running cqlsh on (10.20.20.25). The cluster is:

oot@meta01:~# nodetool -h meta01 ring PlayLog3
Address         DC          Rack        Status State   Load            Effective-Owership  Token                                       
                                                                                           113427455640312821154458202477256070485     
10.20.20.25     CPH         R1          Up     Normal  27.88 MB        66.67%              0                                           
10.20.20.26     CPH         R1          Up     Normal  17.5 MB         66.67%              56713727820156410577229101238628035242      
10.20.20.24     CPH         R1          Up     Normal  72.44 MB        66.67%              113427455640312821154458202477256070485     

However, I have switched from SimpleSnitch to PropertyFileSnitch this morning.

I'm not sure about the times I run the commands and where that corresponds to in the log, but I'm guessing you may know.

I was testing fail-over and was shutting down the server 10.20.20.26 during writes as a testing. However, it looks like I took it down at 2012-05-03 16:07:13 and that it was down when I did the drop and create, which could be the cause. I see the first error soon after it came back up. Also, I remember that these 'Couldn't find cfId=1013' errors occurred in the system.log on the other servers at the same time (I can send these if you want).

I'm currently running from the Debian packages. I'll try to apply the patch there and reinstall the packages.;;;","04/May/12 10:25;jeffwilliams;Log for adding Keyspace with debug patch applied and log mode DEBUG.;;;","04/May/12 12:56;slebresne;The weird part here is that this DEBUG log seems to say that the keyspace should exist before you even do the create keyspace (but if it was correctly loaded, you should get a different error, so something is wrong). Could you try restarting the node with DEBUG log and attach that (just wait for the node to be up and running). I want to try to see what's going on with the loading of that keyspace.;;;","05/May/12 12:30;jeffwilliams;Debug log for node startup;;;","08/May/12 13:43;jeffwilliams;Anything useful in that log?

I seems to have replicated the issue somehow. Firstly, I moved the servers onto public IP's, though the last octet is the same:

nodetool -h meta01 ring
Address         DC          Rack        Status State   Load            Owns                Token                                       
                                                                                           113427455640312821154458202477256070485     
91.223.192.25   CPH         R1          Up     Normal  11.2 MB         33.33%              0                                           
91.223.192.26   CPH         R1          Up     Normal  15.16 MB        33.33%              56713727820156410577229101238628035242      
91.223.192.24   CPH         R1          Up     Normal  20.11 MB        33.33%              113427455640312821154458202477256070485   

I created a new keyspace PlayLog2 (PlayLog still does not work), and a column family playlog.

This was available on all nodes. I then ran a few test inserts which worked fine. Then, to test fail-over, I shutdown the node 91.223.192.26 during inserts. The inserts completed fine and a while later I restarted the node 91.223.192.26. Then, when I went to re-run my tests, I see (Hector client):

5710 [Thread-1] DEBUG me.prettyprint.cassandra.connection.client.HThriftClient  - Creating a new thrift connection to meta02.cph.aspiro.com(91.223.192.26):9160
5711 [Thread-0] DEBUG me.prettyprint.cassandra.connection.client.HThriftClient  - keyspace reseting from null to PlayLog2
Exception in thread ""Thread-1"" me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:Keyspace PlayLog2 does not exist)

Sure enough, from command line client on 91.223.192.26, I see no PlayLog2 keyspace, yet it exists on 91.223.192.24 and 91.223.192.25. I have attached the system.log from 91.223.192.26 in the hope that it is useful.;;;","08/May/12 14:58;jeffwilliams;Ok, I can now reproduce this on my cluster.

If I start with all three servers running. And on one of the servers I create a keyspace, create a column family and test, it all works fine. If I then drop the keyspace and re-create it, everything continues to work. However, as soon as one of the nodes is restarted, the keyspace disappears on that node. If I restart every node in the cluster, then the keyspace cannot be seen anyhwere, however, I can still no longer create a keyspace with that name.;;;","11/May/12 08:17;jeffwilliams;Hi Pavel,

Have you been able to reproduce this? I am wanting to use these servers for production traffic and am wondering if this issue is a general bug, or due to a corruption in my cluster.

Regards,
Jeff;;;","11/May/12 10:45;xedin;Hi Jeff, unfortunately I wasn't able to reproduce the situation you are seeing using the following steps: 

   1. run ccm cluster with 3 nodes
   2. create keyspace/cf and added some data
   3. stopped node 2
   4. dropped keyspace (using CLI from node 1)
   5. re-created keyspace and column family (using CLI from node 3 and on the other try from node 1)
   6. added some data to the keyspace
   7. started node 2

I have seen some of ('Couldn't find cfId=X) but this is unavoidable since we use sequential numbering of CFs and you have re-created one with the same name, we have an issue to switch to UUID ids too (CASSANDRA-3794).

Can you try to run 'resetlocalschema' nodetool command on the failing node? It would truncate all of the schema system tables and try to request it again and re-apply, this was designed specially to resolve such weird situations.

;;;","11/May/12 10:51;jeffwilliams;Pavel,

I am able to recreate this with a freshly installed cluster using the debian 1.1 packages. The steps are:

1. Setup the cluster (I only had 2 nodes in my test)
2. create keyspace on node1 (confirm created on node2)
3. drop keyspace on node1 (confirm dropped on node2)
4. re-created keyspace on node1 (confirm re-created on node2)
5. restart node2
6. keyspace no longer exists on node2

Regards,
Jeff



;;;","11/May/12 11:06;xedin;Yeah, I can reproduce this one - that happens because when you re-create keyspace it wouldn't change the UUID version from the original so when migration is send to the node2 originally it wouldn't correctly merge it into the system table. I will try to fix this one asap.;;;","11/May/12 18:33;xedin;The problem was that when KS/CF is deleted row in the system table is marked for delete and all of it's columns are moved, so when it's re-created columns are added but row stays marked for delete, we need to check if given KS/CF doesn't have attributes in it's system table and if it's marked for delete all together, when we do schema version re-generate or load.;;;","11/May/12 18:34;xedin;Jeff, I have done both scenarios you mentioned to check if everything now working as expected, can you please also confirm (just to double-check) if it works on your side too?...;;;","11/May/12 19:38;jeffwilliams;Just did a quick test and it is looking good. The keyspace doesn't disappear! I'll do some more checks to make sure that all of the data inserts works before and after.;;;","11/May/12 22:27;jbellis;do we need to test for row.cf.isEmpty when there was no deletion involved?

do we need a removeDeleted call in there so the row tombstone can supress obsolete columns pre-compaction?;;;","11/May/12 22:35;xedin;No, we only check for row.cf.isEmpty when row was marked for delete which could be that KS/CF was actually deleted (empty but row is still there) or re-created. Don't think that we should worry about removeDeleted because isMarkedForDelete + isEmpty gives us sufficient information.;;;","11/May/12 22:39;jbellis;bq. Don't think that we should worry about removeDeleted because isMarkedForDelete + isEmpty gives us sufficient information

To clarify: what I'm concerned about is, if there is a row-level tombstone against a previously existing CF definition, then row.isEmpty will be false.  so (markedForDelete && isEmpty) will also be false...;;;","11/May/12 22:51;xedin;Ok, I understand your worries, we use CFS.getRangeSlice to fetch data which calls removeDeleted in the process of request processing.;;;","11/May/12 22:57;jbellis;+1;;;","15/May/12 06:44;jeffwilliams;I've done some thorough testing now and this looks fixed. Thanks guys.;;;","15/May/12 14:20;xedin;Committed.;;;","22/May/12 18:55;dmuth;
Good Afternoon,

It turns out that I am having this exact issue, and I found this bug via Google.  As previously stated, I dropped a keyspace during testing, but was unable to re-create it.  Looking in system.schema_keyspaces still shows an entry for that keyspace, but I cannot drop that keyspcae either.

First, is there anything I can do as a workaround, short of deleting all of my data and starting over?

Second, this isn't in a cluster, it's a single machine, and a development machine at that.  So there's no confidential data involved.  If it would be of any help whatsoever, I'd be happy to send copies of the entire /var/lib/cassandra/ directory.  Please let me know!

Thanks for your time,

-- Doug


;;;","22/May/12 19:04;xedin;Hi Doug, I think that the best option would be to simply apply patch from this issue and re-compile Cassandra.;;;","24/Jan/13 02:46;mharris;I am seeing this issue on version 1.1.6 as well upon dropping and recreating a keyspace.  Any idea whether there might have been a regression between the fix of this issue and the release of 1.1.6?

java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:194)
	at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:127)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:992)
	... (redacted)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 64 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.updateKeyspace(DefsTable.java:518)
	at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:415)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:345)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:217)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more;;;","24/Jan/13 03:04;brandon.williams;Before 1.1.7 all bets are off for schema problems.  Jira is not the best support forum, try the mailing list or irc.;;;","24/Jan/13 03:32;mharris;Your bug-tracking system isn't the right place to report that a bug that was supposed to be fixed might have a regression in a later version...?  Ok, I'll mail the list then...;;;","25/Dec/17 14:36;mhanna;I think i have the same issue in C* 3.11.1

i have a multi-dc cluster
6 nodes
2 DCs

DC1
98
99 - seed
100

DC2
101
102
103 - seed

via cqlsh from 102
1. using a fresh cluster
2. create keyspace k2view_functionslu
3. create table
Works!

from node 98
via cqlsh drop keyspace - succeded

now from node 102
via cqlsh from 102
1. create keyspace k2view_functionslu
2. create table => Keyspace k2view_functionslu doesn't exist


- attacged logs from all nodes
- attached schema version from all nodes
- attached ""desc keyspaces"" ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicEndpointSnitch calculates score incorrectly,CASSANDRA-4213,12553700,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,03/May/12 02:59,16/Apr/19 09:32,14/Jul/23 05:52,04/May/12 18:07,1.2.0 beta 1,,,,,,0,,,,,,,"updateScore does double = long/long math which calculates the score wrong 1/3 becomes 0.0 not 0.3333


need 1 to be cast to double",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/12 01:39;dbrosius@apache.org;4213_use_double_math.diff;https://issues.apache.org/jira/secure/attachment/12525547/4213_use_double_math.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237894,,,Fri May 04 18:07:18 UTC 2012,,,,,,,,,,"0|i0gtav:",96177,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"04/May/12 01:39;dbrosius@apache.org;against trunk;;;","04/May/12 15:26;jbellis;Any reason not to put this in 1.0 and 1.1 branches?;;;","04/May/12 15:34;dbrosius@apache.org;I believe this code is new

http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java;h=b0249169601add3e9e222348cfbbc9722676cf39;hp=3b80e67ccc52f6de9e1172a974a8faf490bce3c3;hb=0cc97d91c0cf92cd8476b5a5d0bdf7d3d66a45fc;hpb=5b4a7f29980621a162fdc202a17bd3300c20e298;;;","04/May/12 15:36;jbellis;ah, got it.  marking affects: 1.2;;;","04/May/12 18:07;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError: originally calculated column size of 629444349 but now it is 588008950,CASSANDRA-4206,12553475,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,patrik.modesto,patrik.modesto,01/May/12 11:28,16/Apr/19 09:32,14/Jul/23 05:52,18/Mar/14 14:35,,,,,,,11,,,,,,,"I've 4 node cluster of Cassandra 1.0.9. There is a rfTest3 keyspace with RF=3 and one CF with two secondary indexes. I'm importing data into this CF using Hadoop Mapreduce job, each row has less than 10 colkumns. From JMX:
MaxRowSize:  1597
MeanRowSize: 369

And there are some tens of millions of rows.

It's write-heavy usage and there is a big pressure on each node, there are quite some dropped mutations on each node. After ~12 hours of inserting I see these assertion exceptiona on 3 out of four nodes:

{noformat}
ERROR 06:25:40,124 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of 629444349 but now it is 588008950
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:388)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:256)
       at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:84)
       at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:437)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
629444349 but now it is 588008950
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:384)
       ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size
of 629444349 but now it is 588008950
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:161)
       at org.apache.cassandra.db.compaction.CompactionManager$7.call(CompactionManager.java:380)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
{noformat}

Few lines regarding Hints from the output.log:

{noformat}
 INFO 06:21:26,202 Compacting large row system/HintsColumnFamily:70000000000000000000000000000000 (1712834057 bytes) incrementally
 INFO 06:22:52,610 Compacting large row system/HintsColumnFamily:10000000000000000000000000000000 (2616073981 bytes) incrementally
 INFO 06:22:59,111 flushing high-traffic column family CFS(Keyspace='system', ColumnFamily='HintsColumnFamily') (estimated 305147360 bytes)
 INFO 06:22:59,813 Enqueuing flush of Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
 INFO 06:22:59,814 Writing Memtable-HintsColumnFamily@833933926(3814342/305147360 serialized/live bytes, 7452 ops)
{noformat}

I think the problem may be somehow connected to an IntegerType secondary index. I had a different problem with CF with two secondary indexes, the first UTF8Type, the second IntegerType. After a few hours of inserting data in the afternoon and midnight repair+compact, the next day I couldn't find any row using the IntegerType secondary index. The output was like this:

{noformat}
[default@rfTest3] get IndexTest where col1 = '3230727:http://zaskolak.cz/download.php';
-------------------
RowKey: 3230727:8383582:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348630332000)
=> (column=col2, value=8383582, timestamp=1335348630332000)
-------------------
RowKey: 3230727:8383583:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348449078000)
=> (column=col2, value=8383583, timestamp=1335348449078000)
-------------------
RowKey: 3230727:8383579:http://zaskolak.cz/download.php
=> (column=col1, value=3230727:http://zaskolak.cz/download.php, timestamp=1335348778577000)
=> (column=col2, value=8383579, timestamp=1335348778577000)

3 Rows Returned.
Elapsed time: 292 msec(s).

[default@rfTest3] get IndexTest where col2 = 8383583;

0 Row Returned.
Elapsed time: 7 msec(s
{noformat}

You can see there really is an 8383583 in col2 in on of the listed rows, but the search by secondary index returns nothing.

The Assert Exception also happend only on CF with the secondary index of IntegerType. There were also secondary indexes of UTF8Type and
LongType types. It's the first time I've tried secondary indexes of other type than UTF8Type.

Regards,
Patrik","Debian Squeeze Linux, kernel 2.6.32, sun-java6-bin 6.26-0squeeze1",arya,cburroughs,christianmovi,colinkuo,dankogan,dhendry,dkador,edevil,efalcao,elubow,exabytes18,jeromatron,jjordan,joeyi,krieb,mat.gomes,mishail,patricioe,patrik.modesto,psanford,ravilr,rbranson,rcoli,rlow,thobbs,vijay2win@yahoo.com,vongocminh,yarin,zznate,,,,,,,,,,,,,,,,,,CASSANDRA-5359,CASSANDRA-5720,,CASSANDRA-7808,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237639,,,Fri Aug 22 16:15:15 UTC 2014,,,,,,,,,,"0|i0dcnr:",75980,,,,,Normal,,,,,,,,,,,,,,,,,"02/May/12 20:53;jbellis;Do you have {{multithreaded_compaction}} enabled?;;;","02/May/12 20:55;jbellis;bq. It's the first time I've tried secondary indexes of other type than UTF8Type

I think that's a red herring; the error occurs when compacting the internal hints columnfamily, which has no indexes on it.;;;","02/May/12 20:56;jbellis;bq. After a few hours of inserting data in the afternoon and midnight repair+compact, the next day I couldn't find any row using the IntegerType secondary index

That part is definitely a [separate] bug, assuming the index completed building.;;;","31/Aug/12 02:02;thobbs;I'm not sure how useful it is at this point, but just for the record I'm seeing what appears to be the same issue with 1.0.7 (which means it shouldn't be CASSANDRA-3579):

{noformat}
 INFO [GossipTasks:1] 2012-08-27 14:22:50,242 Gossiper.java (line 818) InetAddress /xx.xx.178.59 is now dead.
 INFO [GossipStage:1] 2012-08-27 14:22:59,090 Gossiper.java (line 804) InetAddress /xx.xx.178.59 is now UP
 INFO [HintedHandoff:1] 2012-08-27 14:23:41,548 HintedHandOffManager.java (line 296) Started hinted handoff for token: 132332031580364958013534569556798748899 with IP: /xx.xx.178.59
 INFO [HintedHandoff:1] 2012-08-27 14:23:41,870 ColumnFamilyStore.java (line 704) Enqueuing flush of Memtable-HintsColumnFamily@2081164539(597050/47764000 serialized/live bytes, 857 ops)
 INFO [FlushWriter:181] 2012-08-27 14:23:41,870 Memtable.java (line 246) Writing Memtable-HintsColumnFamily@2081164539(597050/47764000 serialized/live bytes, 857 ops)
 INFO [FlushWriter:181] 2012-08-27 14:23:41,959 Memtable.java (line 283) Completed flushing /xx/xx/xx/cassandra/datafile/system/HintsColumnFamily-hc-6730-Data.db (624946 bytes)
 INFO [CompactionExecutor:884] 2012-08-27 14:23:41,961 CompactionTask.java (line 113) Compacting [SSTableReader(path='/xx/xx/xx/cassandra/datafile/system/HintsColumnFamily-hc-6729-Data.db'), SSTableReader(path='/ngs/app/xcardp/cassandra/datafile/system/HintsColumnFamily-hc-6730-Data.db')]
 INFO [CompactionExecutor:884] 2012-08-27 14:23:41,987 CompactionController.java (line 133) Compacting large row system/HintsColumnFamily:31372e33342e3137382e3534 (274816343 bytes) incrementally
ERROR [CompactionExecutor:884] 2012-08-27 14:23:56,322 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:884,1,main]
java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
ERROR [HintedHandoff:1] 2012-08-27 14:23:56,323 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:369)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
    at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
    at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:365)
    ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
ERROR [HintedHandoff:1] 2012-08-27 14:23:56,345 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:369)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
    at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
    at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:365)
    ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 197713629 but now it is 197711561
    at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
    at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
    at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    ... 3 more
 INFO [COMMIT-LOG-WRITER] 2012-08-27 14:48:20,812 CommitLogSegment.java (line 60) Creating new commitlog segment /xxx/xxx/xxx/commitlog/CommitLog-1346078900812.log
{noformat}

I should note that this node was upgraded from 0.7.4 about three days prior.;;;","30/Oct/12 23:38;rbranson;Just had this pop up on a 1.1.6 node after doing a 'nodetool removetoken' for a node that had collected a lot of hints.;;;","01/Mar/13 14:32;joeyi;I am seeing what seems like the same issue on 1.2.2 on nodes that had collected a lot of hints after a node had been down for a couple hours.

{noformat}
ERROR [CompactionExecutor:104] 2013-03-01 00:29:54,211 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:104,1,main]
java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [HintedHandoff:1] 2013-03-01 00:29:54,211 CassandraDaemon.java (line 132) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:406)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
        at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
        at java.util.concurrent.FutureTask.get(Unknown Source)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:402)
        ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 350273328 but now it is 350297055
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        ... 3 more
{noformat};;;","01/Mar/13 15:39;joeyi;Version: 1.2.2 - 6 nodes, RF3
OS: Centos 6.3

Looking closer at the errors, I'm only seeing this for the system/hints:

{noformat}
 INFO [CompactionExecutor:555] 2013-03-01 15:37:10,737 CompactionController.java (line 158) Compacting large row system/hints:384ac791
-d0d7-4f5e-b66b-57af885341d5 (350297131 bytes) incrementally
ERROR [CompactionExecutor:555] 2013-03-01 15:37:28,377 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:5
55,1,main]
{noformat};;;","02/May/13 20:44;arya;I have 1600 compactions pending and no compaction is running, so they are all stuck. I also got this in the logs. And Jonathan, I have multi_threaded_compaction disabled. 

ERROR [CompactionExecutor:111] 2013-05-02 13:54:05,913 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:111,1,main]java.lang.AssertionError: originally calculated column size of 1337269150 but now it is 1337269195        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)

;;;","05/Jun/13 08:43;arodrime;Similar thing over here.

C*1.2.2

I have a lot of hints (3 & 4 GB) on 2 nodes out of 12 due to an unknown issue while growing to multiple datacenter (all the nodes of the new datacenter saw themselves as UNREACHABLE from the cassandra-cli - nodetool ring is ok...).

On these 2 nodes I see now :

{code}
ERROR 08:37:16,878 Exception in thread Thread[CompactionExecutor:1540,1,main]
java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:59)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:422)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 08:37:16,878 Exception in thread Thread[HintedHandoff:176,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:406)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:402)
        ... 7 more
Caused by: java.lang.AssertionError: originally calculated column size of 5469343266 but now it is 5469343506
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:163)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
{code};;;","17/Jun/13 15:58;efalcao;Seeing this on my 1.2.5 cluster since upgrading:

ubuntu@c2-1d:~$ nodetool -h localhost compact TRProd Timelines                                                                                                                               
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 91281671 but now it is 91281729
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:334)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1657)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:2146)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:250)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:791)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1447)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:89)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1292)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1380)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:812)
        at sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.AssertionError: originally calculated column size of 91281671 but now it is 91281729
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334);;;","17/Jun/13 17:19;efalcao;I should add that I was upgrading from a 1.1 cluster.;;;","26/Jun/13 17:33;arya;Any clues? We are not able to get one of our critical CFs in good shape as we cannot run compactions on it and its performance is deteriorating. ;;;","26/Jun/13 17:44;joeyi;I think these issues are the same, just different versions of C*.;;;","16/Jul/13 23:15;arya;Here is our exact scenario:

Some bad code, resulted in a 600Mb row in one of our CFs. It is spanned across 2 SSTables. We've deleted it but it does not go away and nodetool cfstats still reports the max row size being the size of this row. It is causing problems because anytime this SSTable is part of a compaction or we do range queries on this CF, we end up with slow 16 sec GCs. I have tried the following to try and clean it up, but it does not go away:

1. nodetool compact with sizeTiered;
2. LCS with large SSTables and wait till it decides to promote the 2 SSTables and merge them;
3. sizeTiered with forceUserDefined compaction trying to compact the 2 SSTable with the deleted key.

Every time we end up getting the exception complaining about calculated columns size like above. scrub, repair, rebuild, upgradesstable, no luck!

Any love? ;;;","21/Aug/13 21:11;arya;I don't remember which bug but there was one bug where Johnathan and Sylvain were arguing about discrepancy of column calculation logic in various places in the code which depended on gc_grace, deleted columns, etc. I took that as a hint and applied it to my scenario. I increased the gc_grace for the CF in question to a large number like 1 year. Then ran user defined compactions and they happily ran. Afther that I change the gc_grace setting back to 10 days and ran compactions again and my tombsones got cleaned up without seeing that exception. So, although, it didn't make sense to me what happened exactly, but at least this could be worth a try for those who are stuck.;;;","27/Aug/13 16:46;zznate;FTR - seeing this currently in 1.2.8 on batchlog compaction attempt (unfortunately I can't easily modify the gc_grace for this). Stack trace:
{code}
ERROR [CompactionExecutor:105] 2013-08-27 17:54:39,942 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:105,1,main]
java.lang.AssertionError: originally calculated column size of 17391408 but now it is 17391426
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:445)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR [OptionalTasks:1] 2013-08-27 17:54:39,942 CassandraDaemon.java (line 192) Exception in thread Thread[OptionalTasks:1,5,main]
{code};;;","27/Aug/13 17:17;jbellis;TBH I'm not sure we're going to fix this in 1.2.x.  If you have a snapshot set of sstables that can reproduce it, then we can dig in, but eyeballing the code hasn't fixed it yet (despite multiple efforts) and probably won't.

The good news is that 2.0 fixed it by always doing single-pass compaction.;;;","26/Sep/13 14:45;dankogan;We are also seeing the same error during compaction.  We can provide the sstables if that helps resolving the issues.

INFO [CompactionExecutor:74953] 2013-09-26 14:23:53,978 CompactionController.java (line 166) Compacting large row iqtell/mail_folder_data_subject_withdate_asc:97995 (131986528 bytes) incrementally
ERROR [CompactionExecutor:74953] 2013-09-26 14:24:01,126 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:74953,1,main]
java.lang.AssertionError: originally calculated column size of 131986437 but now it is 131986500
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:159)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
;;;","24/Nov/13 20:20;patricioe;Seen the same error in 1.2.10 with multithreaded_compaction disabled. It causes ""nodetool rebuild"" to fail making it really hard to rebuild a DC fully.;;;","06/Dec/13 19:31;christianmovi;Seen with 1.2.11:

{code}
java.lang.AssertionError: originally calculated column size of 44470356 but now it is 44470410
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code};;;","06/Dec/13 19:49;christianmovi;For starters, could we add the SSTable filenames to the Exception? That way users could submit broken SStable files to this ticket in the future.;;;","21/Dec/13 20:43;dkador;We're seeing the AssertionError in LazilyCompactedRow a lot too on 1.2.8.  Not a problem with hinted handoffs for us.  We just have a large CF (close to 1TB on disk) that has had most of its row keys deleted at this point.  Lots of pending compactions on many nodes and these errors seem to stymie progress.  Hoping to get those compactions to finish to reclaim disk...

Has anybody figured out a decent workaround?  Should we try disabling multithreaded_compaction?  Looks like folks are still seeing the errors with that off (it's on for us).

How stupid would running Cassandra with assertions off be?

Alternatively, has anybody who's had this problem attempted to upgrade to 2.0 and had the problem fixed?;;;","21/Dec/13 20:52;dkador;I'll add that this CF is using leveled compaction in case that's useful.;;;","21/Dec/13 21:12;jbellis;Disabling assertions to ""fix"" this is a great way to corrupt your sstables and get errors at read time later on.

This is definitely fixed in 2.0.;;;","21/Dec/13 21:19;dkador;That's sort of what I figured.  That's why I asked how stupid it would be.  The answer is ""very"", clearly.

I understand that it's fixed in 2.0 but many of us are still on 1.2.x.  Speaking for myself, upgrading to 2.0 is on the roadmap but I'd prefer to do that as part of a staged rollout and not as an attempt to fix what seems like a bug.

Do you think disabling multithreaded_compaction would help?;;;","22/Dec/13 00:21;jbellis;Doubt it.  Suspect only workaround is to increase in_memory_compaction limit.;;;","22/Dec/13 11:06;christianmovi;I can confirm that this error also occurs with multithreaded_compaction=false.;;;","22/Dec/13 18:05;dkador;We got desperate and tried multithreaded_compaction=false yesterday afternoon and it seems to have worked for us.  Pending compactions across the cluster have dropped from over 20,000 to under 600 (we're not quite done yet).  The numbers were so high because we added six new nodes and they were unable to compact quickly until we made this settings change.

No idea why it worked for us but thought I'd share our experience.  If there's any other useful information I can share, let me know.;;;","01/Jan/14 16:52;elubow;We are seeing this as well with 1.2.11.  As was mentioned above, knowing which CF would very useful here.  It seems to be happening to hints.  The only other major action we are seeing that is out of the ordinary is thousands of hint SSTables being transferred at a time.  Here is the Java error:

{quote}
ERROR [HintedHandoff:6] 2014-01-01 16:45:19,914 CassandraDaemon.java (line 191) Exception in thread Thread[HintedHandoff:6,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:436)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:282)
	at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:90)
	at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:502)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:432)
	... 6 more
Caused by: java.lang.AssertionError: originally calculated column size of 1028119265 but now it is 1028119453
	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:442)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
{quote}

Multithreaded compactions are set to false in our cluster on all nodes.  We also don't have any pending compactions in the cluster.  Just seeing this error a lot in the logs.  The error seems to happen more frequently during bootstraps or repairs that have a lot of work to do.;;;","15/Jan/14 17:49;dhendry;I am also seeing this happening during compaction with Cassandra 1.2.13 on a non-hints column family. I have multithreaded compaction disabled. 

Here is one new piece of information: based on the thread name the exception occurred on, I searched back through the Cassandra log and found a 'Compacting large row' message. I recognized the row key from *that* message because it has come up before within the context of a bug in our client code which resulted in runaway writes to Cassandra.  Its entirely possible that that particular row had more than 2^31 columns written to it. 

This smells like an integer overflow bug to me although I have not had a chance to dig into the Cassandra code yet.;;;","18/Mar/14 14:12;edevil;We're seeing this with 1.2.15, and seems related with large rows being compacted incrementally and being changed at the same time.

We upgraded from 1.1.5 and did not have this assertion error before.;;;","18/Mar/14 14:35;jbellis;This was present in 1.1 and earlier releases as well.  The fix is to upgrade to 2.0.x.;;;","18/Mar/14 14:38;efalcao;+1 just recently upgraded to 2.0.6 and all the errors went away.;;;","18/Mar/14 15:09;rcoli;{quote}
This was present in 1.1 and earlier releases as well. The fix is to upgrade to 2.0.x.
{quote}
Which change in the 2.0 series fixes this, and how? Do you have a JIRA number we could refer to?;;;","18/Mar/14 15:13;edevil;I think the fix that is mentioned is the fact that 2.0 does only single-pass compactions, so it will never be in the situation of having calculated a previous value that has changed.

It seems an architectural change, and not something easily backported.;;;","24/Jul/14 14:56;mat.gomes;Same issues on v 1.2.12 and 1.2.18
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 116397997 but now it is 116398382
...;;;","22/Aug/14 16:15;rlow;The root cause of this in 1.2 is CASSANDRA-7808.;;;",,,,,,,,,,,,,,,,,,,,,,
SSTables are not updated with max timestamp on upgradesstables/compaction leading to non-optimal performance.,CASSANDRA-4205,12553450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,thorkild,thorkild,01/May/12 04:26,16/Apr/19 09:32,14/Jul/23 05:52,03/May/12 23:04,1.0.10,1.1.1,,,,,0,,,,,,,"We upgraded from 0.7.9 to 1.0.7 on a cluster with a heavy update load. After converting all the reads to named column reads instead of get_slice calls, we noticed that we still weren't getting the performance improvements implemented in CASSANDRA-2498. A single named column read was still touching multiple SSTables according to nodetool cfhistograms. 

To verify whether or not this was a reporting issue or a real issue, we ran multiple tests with stress and noticed that it worked as expected. After changing stress so that it ran the read/write test directly in the CF having issues (3 times stress & flush), we noticed that stress also touched multiple SSTables (according to cfhistograms).

So, the root of the problem is ""something"" left over from our pre-1.0 days. All SSTables were upgraded with upgradesstables, and have been written and compacted many times since the upgrade (4 months ago). The usage pattern for this CF is that it is constantly read and updated (overwritten), but no deletes. 

After discussing the problem with Brandon Williams on #cassandra, it seems the problem might be because a max timestamp has never been written for the old SSTables that were upgraded from pre 1.0. They have only been compacted, and the max timestamp is not recorded during compactions. 

A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

{panel}
06:08 < driftx> thorkild_: tx.  The thing is we don't record the max timestamp on compactions, but we can do it specially for upgradesstables.
06:08 < driftx> so, nothing in... nothing out.
06:10 < thorkild_> driftx: ah, so when you upgrade from before the metadata was written, and that data is only feed through upgradesstables and compactions -> never properly written?
06:10 < thorkild_> that makes sense.
06:11 < driftx> right, we never create it, we just reuse it :(
{panel}
",,brandon.williams,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/12 17:06;jbellis;4205.txt;https://issues.apache.org/jira/secure/attachment/12525327/4205.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237612,,,Fri May 04 15:16:38 UTC 2012,,,,,,,,,,"0|i0gt7z:",96164,,yukim,,yukim,Critical,,,,,,,,,,,,,,,,,"02/May/12 15:05;jbellis;We have a related but more serious bug, as noted by Sam Tunnicliffe in CASSANDRA-4116: CF.maxTimestamp isn't including row tombstones.  This can result in incorrect results being returned by the collationcontroller code.;;;","02/May/12 16:55;jbellis;bq. A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

Looking at the code, scrub and upgradesstables and user-defined compactions all force deserialize + maxtimestamp computation.  The only operation that does not is cleanup.;;;","02/May/12 17:06;jbellis;patch to add version hd indicating row tombstones have been computed correctly;;;","02/May/12 17:36;brandon.williams;I can reproduce, upgrading from 0.7 to 1.0 and running upgradesstables afterward.;;;","02/May/12 19:38;jbellis;Checked Brandon's sstables with CASSANDRA-4211.  One had a max timestamp of 1335979912016 after upgradesstables, the other 1335979951175.  cfhistograms showed about 20% of reads hit both sstables.

This sounds about right; it's reasonable that the sstable w/ higher max timestamp, will contain *some* rows w/ actually an older version than the other sstable's max.  The CASSANDRA-2498 approach will always start with the newer sstable, but it will only skip the older one if the first-seen version of the columns requested have a timestamp newer than the max on the older sstable.

So, I think there is no bug here related to upgraded timestamps, it just isn't a magic bullet to prevent all multiple sstable reads.

The patch for creating a new version to represent ""we have *correct* timestamps including row tombstones"" is still relevant, though.;;;","03/May/12 18:58;yukim;Jonathan,

Re: patch, you have to bump up CURRENT_VERSION to ""hd"" but otherwise looking good to me.;;;","03/May/12 23:04;jbellis;good catch, committed w/ current version fix;;;","04/May/12 04:24;thorkild;The original description is wrong (as we now know). Backporting the tool from 4211 and testing shows that the SSTables do indeed have the correct settings, and everything is updated correctly during the various compaction/upgradesstables etc.

After further debugging I found that the reason why I was hitting all the SSTables was due to the CollationController falling back to calling collectAllData(). This was due to the filter not being an instance of NamesQueryFilter:


CollationController.java:
{noformat}

public ColumnFamily getTopLevelColumns()
    {
        return filter.filter instanceof NamesQueryFilter
               && (cfs.metadata.cfType == ColumnFamilyType.Standard || filter.path.superColumnName != null)
               && cfs.metadata.getDefaultValidator() != CounterColumnType.instance
               ? collectTimeOrderedData()
               : collectAllData();
    }
{noformat}

This makes sense, but I struggled to find out why that happened on some CFs, and not on other. After tracing it, I found that the trigger is whether or not the CF has the row-cache enabled. If the row-cache is enabled, ColumnFamilyStore.cacheRow(..) is called, and if you then get a cache-miss, it'll end up calling CoumnFamilyStore.getTopLevelColumns(...), which again creates a new CollationController with an IdentityFilter filter as argument, and thus end up using collectAllData().

In our case, we have a lot of cache-misses since the update frequency is so high and often we only read an entry once before it is replaced. 

Testing with 'stress' before and after showed it moved from using all sstables for every query, to 100% reads satisfied with a single SSTable read (according to cfhistograms).

I tested a preliminary configuration with row-cache in production, and it looked like a lot more of the queries were served by a single sstable. We still have stragglers using more, but that can easily be because of the randomization size-tiered compaction will lead to in this case. 

I don't see an easy way to fix this properly, though, without changing the row cache to be a row+filter cache (covered by CASSANDRA-1956 it seems). Since you need to load the whole row into the row cache, you can't use named columns. ;;;","04/May/12 15:16;jbellis;Thanks for following up, Thorkild.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig does not work on DateType,CASSANDRA-4204,12553443,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,cywjackson,cywjackson,01/May/12 00:20,16/Apr/19 09:32,14/Jul/23 05:52,01/May/12 17:07,1.0.10,,,,,,0,,,,,,,"cqlsh:PigDemo> describe columnfamily test1897;

CREATE COLUMNFAMILY test1897 (
KEY text PRIMARY KEY,
testcol timestamp
) WITH
comment='' AND
comparator=text AND
row_cache_provider='SerializingCacheProvider' AND
key_cache_size=200000.000000 AND
row_cache_size=0.000000 AND
read_repair_chance=1.000000 AND
gc_grace_seconds=864000 AND
default_validation=blob AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
row_cache_save_period_in_seconds=0 AND
key_cache_save_period_in_seconds=14400 AND
replicate_on_write=True;

cqlsh:PigDemo> select * from test1897;
KEY | testcol
-----+-------------------------
akey | 2012-01-21 00:14:12+0000

$ cat test1897.pig
cassandra_data = LOAD 'cassandra://PigDemo/test1897' USING CassandraStorage() AS (name, columns: bag {T: tuple()});
dump cassandra_data;

there seems problem with the DateType. the above simple pig script fail with the attached err
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/12 16:54;brandon.williams;4204.txt;https://issues.apache.org/jira/secure/attachment/12525185/4204.txt","01/May/12 00:21;cywjackson;pig_1335816404547.log;https://issues.apache.org/jira/secure/attachment/12525144/pig_1335816404547.log",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237605,,,Tue May 01 19:58:37 UTC 2012,,,,,,,,,,"0|i0gt7j:",96162,,rbranson,,rbranson,Normal,,,,,,,,,,,,,,,,,"01/May/12 16:54;brandon.williams;Sad patch w/test to cast DateType to a long, since pig has no native date type, and DateType is just a long under the hood anyway.;;;","01/May/12 17:02;rbranson;Reviewed, looks good.;;;","01/May/12 17:07;brandon.williams;Committed.;;;","01/May/12 19:58;brandon.williams;Just noticed we're actually returning a byte array here, so I changed this to a long in 2b7672fa11efa6e8c3c5cbc6fa857b3bc8947092;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix descriptor versioning for bloom filter changes,CASSANDRA-4203,12553435,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,30/Apr/12 22:33,16/Apr/19 09:32,14/Jul/23 05:52,02/May/12 05:27,1.2.0 beta 1,,,,,,0,,,,,,,"CASSANDRA-2975 introduced changes to the Data component, breaking stream compatibility",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/12 01:17;vijay2win@yahoo.com;0001-CASSANDRA-4203-v2.patch;https://issues.apache.org/jira/secure/attachment/12525148/0001-CASSANDRA-4203-v2.patch","30/Apr/12 22:37;jbellis;4203.txt;https://issues.apache.org/jira/secure/attachment/12525128/4203.txt",,,,,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237595,,,Wed May 02 07:36:16 UTC 2012,,,,,,,,,,"0|i0gt73:",96160,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"30/Apr/12 22:37;jbellis;2975 introduced version ""ib"", but as noted in Descriptor comments, ""Minor versions must be forward-compatible"" (which in practice means, ""only additions to the metadata are allowed"").

We'll run out of version letters if we keep bumping the major version in trunk, so rather than turn this into version j, I suggest we stick with ia until 1.2.0 is released.

This patch also updates the streaming compatibility version to i.;;;","30/Apr/12 23:00;yukim;I think you mean CASSANDRA-2319 broke compatibility. +1 on change in {{isStreamingCompatible}}.

And I'm ok with sticking with ""ia"", but you also need to fix here: https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L113;;;","01/May/12 01:17;vijay2win@yahoo.com;Attached patch adds yuki's comments and fixes the test accordingly. 
Attachement also makes streaming to be always be the latest version.
One thing to note is that folks who build from trunk are out of luck they have to wipe the data... 

Not sure if we need CASSANDRA-4196 anymore.;;;","01/May/12 02:06;jbellis;bq. One thing to note is that folks who build from trunk are out of luck they have to wipe the data... 

That would be a problem, but hopefully it's blazingly obvious that running trunk in production is insane.  Especially at the beginning of a dev cycle.

I don't see a good alternative, if we continue chewing through major revisions the way we did in 0.7 we're kind of screwed.  We started at the wrong end of the ascii table; we don't even have the luxury of using upper case characters. :)  (""A"" < ""a"");;;","02/May/12 04:41;jbellis;+1 on v2;;;","02/May/12 05:13;vijay2win@yahoo.com;Committed Thanks!;;;","02/May/12 07:36;slebresne;Agreed on not burning versions during development. If we start trying to keep compatibility between changes to trunk it'll be a mess. I'm fine with starting being careful about such things once we release a first beta (even though the point of doing betas and rcs is that this shouldn't really be a requirement even then) but before that all bets are off imo.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL 3.0 prepare_cql_query fails on ""BEGIN BATCH""",CASSANDRA-4202,12553427,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,sbillig,sbillig,30/Apr/12 22:03,16/Apr/19 09:32,14/Jul/23 05:52,03/May/12 07:30,1.1.1,,,Legacy/CQL,,,0,c++,cql3,thrift,,,,"Preparing the following (contrived) statement with the C++ Thrift bindings 
throws a TTransportException (""No more data to read."" from TTransport.h:41)

q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";
client.prepare_cql_query(pr, q, Compression::NONE);

{code:title=crashtest.cpp}
#include <protocol/TBinaryProtocol.h>
#include <thrift/transport/TSocket.h>
#include <thrift/transport/TTransportUtils.h>
#include ""Cassandra.h""

using namespace std;
using namespace apache::thrift;
using namespace apache::thrift::protocol;
using namespace apache::thrift::transport;
using namespace org::apache::cassandra;
using namespace boost;

int main(int argc, char **argv) {
    shared_ptr<TTransport> socket(new TSocket(""127.0.0.1"", 9160));
    shared_ptr<TTransport> transport(new TFramedTransport(socket));
    shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));

    CassandraClient client(protocol);

    try {
        transport->open();
        client.set_keyspace(""test1"");
        client.set_cql_version(""3.0.0"");

        CqlResult cr;
        CqlPreparedResult pr;

        // In cqlsh: create table crashtest (id int primary key, val text);
        const char *q;
        // q = ""insert into crashtest (id, val) values (?, ?)""; // This works fine
        q = ""begin batch insert into crashtest (id, val) values (?, ?); apply batch"";

        client.prepare_cql_query(pr,  q, Compression::NONE);

        vector<string> vtypes = pr.variable_types;
        vector<string>::iterator it;

        for (it = vtypes.begin(); it != vtypes.end(); it++) {
            cout << *it << endl;
        }
    } catch (TException &tx) {
        cerr << ""TException ERROR: "" << tx.what() << endl;
    }
}
{code}

{code:title=backtrace}
#0  0x00007fff901800e9 in __cxa_throw ()
#1  0x0000000100009ab9 in apache::thrift::transport::readAll<apache::thrift::transport::TBufferBase> (trans=@0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:41
#2  0x0000000100009c1d in apache::thrift::transport::TBufferBase::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:82
#3  0x0000000100009c5b in apache::thrift::transport::TFramedTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TBufferTransports.h:390
#4  0x0000000100004b45 in apache::thrift::transport::TVirtualTransport<apache::thrift::transport::TFramedTransport, apache::thrift::transport::TBufferBase>::readAll_virt (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TVirtualTransport.h:99
#5  0x00000001000034c1 in apache::thrift::transport::TTransport::readAll (this=0x100401100, buf=0x7fff5fbfefc0 ""??_\001"", len=4) at TTransport.h:126
#6  0x0000000100009f4c in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readI32 (this=0x100401370, i32=@0x7fff5fbff020) at TBinaryProtocol.h:372
#7  0x000000010000b5bf in apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TBinaryProtocol.h:203
#8  0x0000000100006b07 in apache::thrift::protocol::TVirtualProtocol<apache::thrift::protocol::TBinaryProtocolT<apache::thrift::transport::TTransport>, apache::thrift::protocol::TProtocolDefaults>::readMessageBegin_virt (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TVirtualProtocol.h:432
#9  0x00000001000abe78 in apache::thrift::protocol::TProtocol::readMessageBegin (this=0x100401370, name=@0x7fff5fbff228, messageType=@0x7fff5fbff224, seqid=@0x7fff5fbff234) at TProtocol.h:518
#10 0x0000000100069a98 in org::apache::cassandra::CassandraClient::recv_prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0) at Cassandra.cpp:10231
#11 0x000000010003bf3f in org::apache::cassandra::CassandraClient::prepare_cql_query (this=0x7fff5fbff5b0, _return=@0x7fff5fbff4c0, query=@0x7fff5fbff6b0, compression=org::apache::cassandra::Compression::NONE) at Cassandra.cpp:10206
#12 0x00000001000020ea in main (argc=1, argv=0x7fff5fbff8c8) at crashtest.cpp:36
{code}

{code:title=server error message}

ERROR 17:13:55,089 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.cassandra.cql3.statements.UpdateStatement.prepare(UpdateStatement.java:278)
	at org.apache.cassandra.cql3.statements.BatchStatement.prepare(BatchStatement.java:157)
	at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:207)
	at org.apache.cassandra.cql3.QueryProcessor.prepare(QueryProcessor.java:158)
	at org.apache.cassandra.thrift.CassandraServer.prepare_cql_query(CassandraServer.java:1260)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3484)
	at org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query.getResult(Cassandra.java:3472)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
{code}","OSX 10.7.2, Cassandra 1.1.0, Thrift 0.7",sbillig,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/12 11:36;slebresne;4202.txt;https://issues.apache.org/jira/secure/attachment/12525277/4202.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237587,,,Thu May 03 07:30:59 UTC 2012,,,,,,,,,,"0|i0gt6v:",96159,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"30/Apr/12 22:06;jbellis;Does the server log any errors?;;;","30/Apr/12 22:16;sbillig;Oops, forgot the most important part.  Added it to the description.;;;","02/May/12 11:36;slebresne;Patch attached to fix (we need the statements inside the batch to use the bounded variables from the batch itself).;;;","02/May/12 13:45;jbellis;Is this an artifact of ""everything is a prepared statement?""  Does that really make sense for batches?;;;","02/May/12 13:58;slebresne;Depends what you mean by that :). To some extent it's an artifact of how we index bound variables in the parser. What happened is that the parser computes the index of the prepared variables (incrementing the index while parsing each time it sees a bound variable) and only the top-level statement knows how many variables there is at the end. For a batch, it means that individual inserts/deletes inside a batch needs to refer to the batch to know how many variables there is in total, which is what was not done. I don't know how clear that is and if I have answered your question though.  ;;;","02/May/12 20:16;xedin;+1;;;","03/May/12 07:30;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preserve commitlog size cap when recycling segments at startup,CASSANDRA-4201,12553420,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,pchalamet,pchalamet,30/Apr/12 21:28,16/Apr/19 09:32,14/Jul/23 05:52,23/May/12 21:34,1.1.1,,,,,,0,commitlog,,,,,,"1. Create a single node cluster, use default configuration, use cassandra.bat to start the server:

2. run the following commands in cli:
{code}
create keyspace toto;
use toto;
create column family titi;
truncate titi;
{code}

3. the node dies with this error:
{code}
ERROR 23:23:02,118 Exception in thread Thread[COMMIT-LOG-ALLOCATOR,5,main]
java.io.IOError: java.io.IOException: Map failed
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:127)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.recycle(CommitLogSegment.java:202)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$2.run(CommitLogAllocator.java:159)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator$1.runMayThrow(CommitLogAllocator.java:95)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Map failed
        at sun.nio.ch.FileChannelImpl.map(Unknown Source)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:119)
        ... 5 more
Caused by: java.lang.OutOfMemoryError: Map failed
        at sun.nio.ch.FileChannelImpl.map0(Native Method)
        ... 7 more
 INFO 23:23:02,122 Stop listening to thrift clients
 INFO 23:23:02,123 Waiting for messaging service to quiesce
 INFO 23:23:02,125 MessagingService shutting down server thread.
{code}","Windows 7 x64, 4Gb, JRE 1.6.0_31 (x86)",vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/12 22:01;jbellis;4201.txt;https://issues.apache.org/jira/secure/attachment/12525229/4201.txt","30/Apr/12 21:28;pchalamet;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12525111/cassandra.yaml","01/May/12 21:50;pchalamet;log.txt;https://issues.apache.org/jira/secure/attachment/12525227/log.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237579,,,Wed May 23 21:34:12 UTC 2012,,,,,,,,,,"0|i0gt6f:",96157,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"30/Apr/12 21:47;jbellis;I can't reproduce.  Are you using a 32bit jvm?;;;","01/May/12 07:52;pchalamet;Yes it is a 32 bits jvm running on x64 system.;;;","01/May/12 14:01;jbellis;How big is your heap?;;;","01/May/12 14:32;pchalamet;1 gig. Using jvm options in cassandra.bat :

set JAVA_OPTS=-ea^
 -javaagent:""%CASSANDRA_HOME%\lib\jamm-0.2.5.jar""^
 -Xms1G^
 -Xmx1G^
 -XX:+HeapDumpOnOutOfMemoryError^
 -XX:+UseParNewGC^
 -XX:+UseConcMarkSweepGC^
 -XX:+CMSParallelRemarkEnabled^
 -XX:SurvivorRatio=8^
 -XX:MaxTenuringThreshold=1^
 -XX:CMSInitiatingOccupancyFraction=75^
 -XX:+UseCMSInitiatingOccupancyOnly^
 -Dcom.sun.management.jmxremote.port=7199^
 -Dcom.sun.management.jmxremote.ssl=false^
 -Dcom.sun.management.jmxremote.authenticate=false^
 -Dlog4j.configuration=log4j-server.properties^
 -Dlog4j.defaultInitOverride=true;;;","01/May/12 21:50;pchalamet;The first time truncate is called, CommitLogAllocator (line 104) calls createFreshSegment() : a new CommitLogSegment is created. It is working fine.

It seems that everything collapes when the segment is recycled. CommitLogSegment failed line 119
{code}
     buffer = logFileAccessor.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, CommitLog.SEGMENT_SIZE);
{code}
The map() function is failing with ""java.io.IOException: Map failed"" with an OutOfMemory as inner exception.

I've also attached a log in debug mode.;;;","01/May/12 22:01;jbellis;The problem is you don't have enough contiguous address space to mmap the commitlog segements.

1.1.1 already has a configuration parameter to change the commitlog segment size; dropping that to 16, and setting commitlog_total_space_in_mb to the same, works for me.

I did find a related bug, that Cassandra keeps extra commitlog segments around at startup time.  Patch attached to fix that.;;;","18/May/12 00:29;vijay2win@yahoo.com;+1 ;;;","23/May/12 21:34;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move error on 1 node cluster,CASSANDRA-4200,12553413,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,nickmbailey,nickmbailey,30/Apr/12 20:27,16/Apr/19 09:32,14/Jul/23 05:52,04/May/12 08:19,1.1.1,,,,,,0,,,,,,,"Attempting to move a node in a 1 node cluster with a keyspace using NTS produces an error:

{noformat}
bin/nodetool -h localhost move 0
Exception in thread ""main"" java.lang.IllegalStateException: unable to find sufficient sources for streaming range (0,129685538820263942208828358218513421652]
at org.apache.cassandra.dht.RangeStreamer.getRangeFetchMap(RangeStreamer.java:197)
	at org.apache.cassandra.dht.RangeStreamer.getWorkMap(RangeStreamer.java:205)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2419)
	at org.apache.cassandra.service.StorageService.move(StorageService.java:2327)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/12 09:40;slebresne;4200-v2.txt;https://issues.apache.org/jira/secure/attachment/12525422/4200-v2.txt","02/May/12 10:08;slebresne;4200.txt;https://issues.apache.org/jira/secure/attachment/12525271/4200.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237572,,,Fri May 04 08:19:30 UTC 2012,,,,,,,,,,"0|i0gt5z:",96155,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"02/May/12 10:08;slebresne;The problem is that RangeStreamer excludes localhost by default, but it shouldn't do that for move.

Attached patch to fix (it moves the locahost exclusion out of RangeStreamer, but I think that is cleaner like that).;;;","02/May/12 14:16;slebresne;Note: I've also added a test for this in the dtests;;;","02/May/12 17:57;vijay2win@yahoo.com;attached patch works but it streams the ranges back to the localhost, which i am not sure is intended.


 INFO [RMI TCP Connection(8)-10.2.179.47] 2012-05-02 10:50:17,366 StorageService.java (line 752) MOVING: fetching new ranges and streaming old ranges
 INFO [StreamStage:1] 2012-05-02 10:50:17,367 StreamOut.java (line 113) Beginning transfer to localhost/127.0.0.1
 INFO [StreamStage:1] 2012-05-02 10:50:17,367 StreamOut.java (line 94) Flushing memtables for [CFS(Keyspace='Keyspace1', ColumnFamily='Counter1'), CFS(Keyspace='Keyspace1', ColumnFamily='Super1'), CFS(Keyspace='Keyspace1', ColumnFamily='SuperCounter1'), CFS(Keyspace='Keyspace1', ColumnFamily='Standard1')]...
 INFO [StreamStage:1] 2012-05-02 10:50:17,368 StreamOut.java (line 159) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-69-Data.db sections=1 progress=0/279349056 - 0%, /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-86-Data.db sections=1 progress=0/279349056 - 0%, /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-89-Data.db sections=1 progress=0/59903136 - 0%], 3 sstables.
 INFO [StreamStage:1] 2012-05-02 10:50:17,368 StreamOutSession.java (line 161) Streaming to localhost/127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:25,211 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-69-Data.db to /127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:32,983 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-86-Data.db to /127.0.0.1
 INFO [Streaming to localhost/127.0.0.1:2] 2012-05-02 10:50:34,760 StreamReplyVerbHandler.java (line 55) Successfully sent /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-89-Data.db to /127.0.0.1
 INFO [Thread-15] 2012-05-02 10:50:34,761 StreamInSession.java (line 190) Finished streaming session 1335981017366948000 from localhost/127.0.0.1
;;;","03/May/12 09:40;slebresne;Oups, you're right, I don't know what I was thinking. Attaching v2 that simply recognize that localhost is a valid source (but don't stream from it).;;;","03/May/12 19:15;vijay2win@yahoo.com;+1;;;","04/May/12 08:19;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in log when upgrading multi-node cluster to 1.1,CASSANDRA-4195,12553369,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,30/Apr/12 14:17,16/Apr/19 09:32,14/Jul/23 05:52,11/Jul/12 22:00,1.0.11,,,,,,0,,,,,,,"I upgraded a cluster from 1.0.9 to 1.1.0. The following message shows up in the logs for all but the first node.
{code}
ERROR [GossipStage:1] 2012-04-30 07:37:06,986 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID                  
    at java.util.UUID.timestamp(UUID.java:331)                                  
    at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
    at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
    at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
    at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)           
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)   
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
{code}

this dtest demonstrates the issue. It was added to the cassandra-dtest repository as upgrade_to_11_test:
{code}
from dtest import Tester, debug 
from tools import * 
 
class TestUpgradeTo1_1(Tester): 
 
    def upgrade_test(self): 
        self.num_rows = 0 
        cluster = self.cluster 
 
        # Forcing cluster version on purpose 
        cluster.set_cassandra_dir(cassandra_version='1.0.9') 
 
        cluster.populate(3).start() 
        time.sleep(1) 
 
        for node in cluster.nodelist():     
            node.flush() 
            time.sleep(.5) 
            node.stop(wait_other_notice=True) 
            node.set_cassandra_dir(cassandra_version='1.1.0') 
            node.start(wait_other_notice=True) 
            time.sleep(.5)
{code}","ccm, dtest. Ubuntu",xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/12 20:33;xedin;CASSANDRA-4195.patch;https://issues.apache.org/jira/secure/attachment/12535737/CASSANDRA-4195.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,237523,,,Wed Jul 11 22:00:53 UTC 2012,,,,,,,,,,"0|i0gt3r:",96145,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Apr/12 14:34;jbellis;I thought we added logic to not send incompatible migration messages to 1.0.x nodes?;;;","01/May/12 00:12;xedin;Yes, we don't send it and the problem is not about it - here we see the same problem I was talking about at CASSANDRA-3804 - as we no longer use time-based UUID to represent schema version, the ""old"" nodes (version < 1.1) would fail to join into the ring because they would be stuck on the ""highest"" version resolution as they assume that all nodes have time-based UUID for version which is no longer the case. We should make a strict regulation about mixed version C* ring otherwise things would continue to break in the different ways, probably to forbid the older node to join if it detects nodes with version >= 1.1, because we can't rely just skip schema agreement checks on startup or on ring changes...;;;","01/May/12 03:06;jbellis;bq. probably to forbid the older node to join if it detects nodes with version >= 1.1, because we can't rely just skip schema agreement checks on startup or on ring changes

Doesn't that kill node-at-a-time upgrading dead in the water?;;;","01/May/12 10:48;xedin;It would, but on the other hand one can't upgrade node-at-a-time to 1.1 because of incompatible schema changes... We of course can report something like ""Nodes with C* version >= 1.1 detected, please upgrade!"" instead of failing in MM.rectify(...) but if we continue to operate on that node we can't guarantee stability anyway... ;;;","09/Jul/12 16:23;jbellis;We *need* to support read/write ops in a mixed cluster.  We don't need to support schema changes.;;;","09/Jul/12 16:31;xedin;bq. We need to support read/write ops in a mixed cluster. We don't need to support schema changes.

See my first comment, old nodes wouldn't be able to join the ring because they can't resolve the schema version correctly and as we can't know what is the difference between old/new schemas they wouldn't be able to properly handle read/write workloads. We can probably add special mechanism to the 1.0 that would ask for schema in Avro format if it detects that schema version is not a TimeUUID, load it and try to operate but that would require from uses to update their nodes to the latest 1.0 version before they can do upgrade to 1.1.;;;","09/Jul/12 16:35;jbellis;Ugh, how did we miss this in testing?

How I think it should work:

# if node can't parse schema version, it should join ring w/ whatever schema it currently has
# nodes should make best-effort to fulfil requests w/ current schema instead of blocking for schema reconciliation

That way as long as users don't try to modify schema during the upgrade they will be fine.

I think #2 works as desired, which leaves #1 as a fairly easy update to make in 1.0.11.;;;","09/Jul/12 16:49;xedin;Ok, I can do #1 and it would get ring into disagreement until upgrade to 1.1 is finished with is a good thing in our case because users won't be able to mutate it. I was warning about from the time we have decided to change the way version is handled... ;;;","09/Jul/12 20:33;xedin;patch against cassandra-1.0 to make it skip trying to migrate the schema if it detects that it's running in mixed node cluster and verbose warning.;;;","09/Jul/12 21:59;brandon.williams;Worked for me.;;;","11/Jul/12 21:39;jbellis;+1;;;","11/Jul/12 22:00;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql delete does not delete,CASSANDRA-4193,12552933,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,cywjackson,cywjackson,26/Apr/12 19:11,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/12 08:07,1.1.2,,,,,,0,cql3,,,,,,"tested in 1.1 and trunk branch on a single node:
{panel}
cqlsh:test> create table testcf_old ( username varchar , id int , name varchar , stuff varchar, primary key(username,id,name)) with compact storage;
cqlsh:test> insert into testcf_old ( username , id , name , stuff ) values ('abc', 2, 'rst', 'some other bunch of craps');
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps

cqlsh:test> delete from testcf_old where username = 'abc' and id =2;
cqlsh:test> select * from testcf_old;
 username | id | name | stuff
----------+----+------+---------------------------
      abc |  2 |  rst | some other bunch of craps
      abc |  4 |  xyz |          a bunch of craps
{panel}

same also when not using compact:
{panel}
cqlsh:test> create table testcf ( username varchar , id int , name varchar , stuff varchar, primary key(username,id));
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps

cqlsh:test> delete from testcf where username = 'abc' and id =2;
cqlsh:test> select * from testcf;
 username | id | name                      | stuff
----------+----+---------------------------+------------------
      abc |  2 | some other bunch of craps |              rst
      abc |  4 |                       xyz | a bunch of craps
{panel}",,slebresne,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/12 13:15;slebresne;4193.txt;https://issues.apache.org/jira/secure/attachment/12524854/4193.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236704,,,Fri Jun 29 08:07:12 UTC 2012,,,,,,,,,,"0|i0gt2v:",96141,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Apr/12 13:14;slebresne;So for the compact case, this is a dupe of CASSANDRA-3708, it in fact requires a range tombstone (they could be millions of record having the 'abc' and 2 as first components). The reason for the non-compact case is very similar, this amount internally to remove multiple columns. However in that second we could implement a workaround as we know which columns are defined for the table. However, for this too CASSANDRA-3708 will offer a better fix, as it will be more efficient to have 1 (range) tombstone rather than n where n is the number of columns in the table and less special code once CASSANDRA-3708 is in.

So what I propose is for now to throw an error on the compact case and support the second one by deleting each column individually. Once CASSANDRA-3708 is in, we'll use it to replace the second part. Patch attached to do that.;;;","27/Jun/12 16:56;jbellis;With CASSANDRA-3708 committed, is this still relevant?;;;","28/Jun/12 09:59;slebresne;It is relevant for the 1.1 branch (for which the patch is targeted) since CASSANDRA-3708 is 1.2 only.;;;","28/Jun/12 22:16;jbellis;+1

nit: comment above ""cfDef.isComposite && builder.componentCount() != 0"" needs to be updated;;;","29/Jun/12 08:07;slebresne;Committed (fix comment fixed) to 1.1. Since it's not need for trunk I've merge the commit but with '-s ours'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3: fix index dropping and assign default name if none provided at index creation,CASSANDRA-4192,12552832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Apr/12 10:35,16/Apr/19 09:32,14/Jul/23 05:52,14/May/12 06:52,1.1.1,,,Legacy/CQL,,,0,cql3,,,,,,"This ticket proposes to fix two problems of CQL3 index handling:
# DROP INDEX is broken (because the code forgot to clone the metadata before doing modification which break the schema update path)
# If an index is created with a name (which CREATE INDEX allow), there is no way to drop the index (note that we will internally assign a name to the index ColumnFamilyStore, but we don't assign a name in the ColumnDefinition object, which is the only one checked by DROP INDEX).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/12 08:48;slebresne;4192-v2.txt;https://issues.apache.org/jira/secure/attachment/12525415/4192-v2.txt","26/Apr/12 10:40;slebresne;4192.txt;https://issues.apache.org/jira/secure/attachment/12524420/4192.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236785,,,Mon May 14 06:52:34 UTC 2012,,,,,,,,,,"0|i0gt2f:",96139,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Apr/12 10:40;slebresne;Patch attached fixes the two issues above. For the second one, it makes sure CREATE INDEX always assign a name to the index. If none is provided, it assign a default one based on the column family name and column name (making sure that 1) the index name is valid and 2) the name is unique). The patch also make sure one cannot create 2 index with the same name, as DROP would have a random behavior in that case.

Note that is probably other way to deal with those problems, yet I'm not sure it's worth getting much more fancy than that.;;;","02/May/12 17:40;jbellis;Can we leverage CFMetadata.validate instead of rewriting the code here?;;;","03/May/12 08:48;slebresne;Right, I looked too quickly at that code and missed that we already validate the name is not unique and we already do assign a name to index that don't have one, sorry. What we could do though is to make sure we pick a name that is unique when choosing a default one (it's not a big deal to fail in that case, the user can always pick a name itself, but it's not very nice).

Anyway, attaching v2, that only includes the fix to DROP INDEX and make sure we pick a unique name.;;;","11/May/12 22:49;jbellis;+1;;;","14/May/12 06:52;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apparent data loss using super columns and row cache via ConcurrentLinkedHashCacheProvider,CASSANDRA-4190,12552706,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,minaguib,minaguib,25/Apr/12 19:01,16/Apr/19 09:32,14/Jul/23 05:52,26/Apr/12 14:35,1.0.10,1.1.1,,,,,0,cache,ConcurrentLinkedHashCacheProvider,supercolumns,,,,"Tested on a vanilla single-node cassandra 1.0.9 installation.

When using super columns along with row caching via ConcurrentLinkedHashCacheProvider (default if no JNA available, or explicitly configured even if JNA available), there's what appears as transient data loss.

Given this script executed in cassandra-cli:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and rows_cached=75000 and row_cache_provider='ConcurrentLinkedHashCacheProvider';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The output from the 3 gets above is as follows:

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

{quote}
=> (super_column=attrs,
     (column=name, value=Mina, timestamp=1335377788441000))
Returned 1 results.
{quote}

It's clear that the second and third set commands (country, region) are missing in the returned results.

If the row cache is explicitly invalidated (in a second terminal, via `nodetool -h localhost invalidaterowcache Test Users`), the missing data springs to life on next 'get':
{quote}
[default@Test] get Users['mina'];
=> (super_column=attrs,
     (column=country, value=Canada, timestamp=1335377839592000)
     (column=name, value=Mina, timestamp=1335377788441000)
     (column=region, value=Quebec, timestamp=1335377871353000))
Returned 1 results.
{quote}

From cursory checks, this does not to appear to happen with regular columns, nor with JNA enabled + SerializingCacheProvider.

",Linux 2.6.27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/12 13:05;slebresne;4190.txt;https://issues.apache.org/jira/secure/attachment/12524435/4190.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236784,,,Thu Apr 26 14:35:30 UTC 2012,,,,,,,,,,"0|i0gt1j:",96135,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Apr/12 19:52;minaguib;I can confirm that testing with cassandra 1.1.0 has the same conclusion.

To reproduce against cassandra 1.1.0, edit cassandra.yaml and set:
{quote}
row_cache_provider: ConcurrentLinkedHashCacheProvider                                                                                                                                                     
row_cache_size_in_mb: 200                                                                                                                                                                                 
{quote}

And use this slightly updated script to accomodate for 1.1.0 changes:
{quote}
create keyspace Test;
use Test;

create column family Users with column_type='Super' and key_validation_class='UTF8Type' and comparator='UTF8Type' and subcomparator='UTF8Type' and default_validation_class='UTF8Type' and caching='ALL';

set Users['mina']['attrs']['name'] = 'Mina';
get Users['mina'];

set Users['mina']['attrs']['country'] = 'Canada';
get Users['mina'];

set Users['mina']['attrs']['region'] = 'Quebec';
get Users['mina'];
{quote}

The rest of the observations are the same as with the cassandra 1.0.9 test.;;;","26/Apr/12 13:05;slebresne;Turns out the patch for CASSANDRA-3957 had a stupid typo. Attaching patch to fix and I've pushed a test in the dtests.;;;","26/Apr/12 14:26;jbellis;Javadoc also needs update, otherwise +1;;;","26/Apr/12 14:35;slebresne;Committed (with javadoc update), thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor CQL3 fixes,CASSANDRA-4185,12552645,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Apr/12 13:08,16/Apr/19 09:32,14/Jul/23 05:52,03/May/12 07:30,1.1.1,,,Legacy/CQL,,,0,cql3,,,,,,"The goal of this ticket is to be the home for a number of minor fixes/improvements in CQL3 that I didn't felt warranted a ticket each. It includes 4 patches:
* The first one fixes the grammar for float constants, so as to not recognize 3.-3, but to actually allow 3. (i.e, with radix point but with the fractional part left blank)
* The second one correctly detect the (invalid) case where a table is created with COMPACT STORAGE but without any 'clustering keys'.
* The third one fixes COUNT, first by making sure both COUNT(*) and COUNT(1) are correctly recognized and also by ""processing"" the internal row before counting, are there isn't a 1-to-1 correspondence between internal rows and CQL rows in CQL3. The grammar change in this patch actually rely on CASSANDRA-4184
* The fourth and last patch disallows the counter type for keys (i.e. any column part of the PRIMARY KEY) as it is completely non-sensical and will only led to confusion.
",,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4184,"25/Apr/12 13:08;slebresne;0001-Fix-float-parsing.txt;https://issues.apache.org/jira/secure/attachment/12524265/0001-Fix-float-parsing.txt","25/Apr/12 13:08;slebresne;0002-Fix-compact-storage-validation.txt;https://issues.apache.org/jira/secure/attachment/12524266/0002-Fix-compact-storage-validation.txt","25/Apr/12 13:08;slebresne;0003-Fix-COUNT-in-select.txt;https://issues.apache.org/jira/secure/attachment/12524267/0003-Fix-COUNT-in-select.txt","25/Apr/12 14:11;slebresne;0004-Disallow-counters-for-PRIMARY-KEY-part-v2.txt;https://issues.apache.org/jira/secure/attachment/12524273/0004-Disallow-counters-for-PRIMARY-KEY-part-v2.txt",,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236791,,,Thu May 03 07:30:41 UTC 2012,,,,,,,,,,"0|i0gsz3:",96124,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Apr/12 14:11;slebresne;Realized the last patch was missing some parts. v2 attached with those missing parts.;;;","02/May/12 17:45;jbellis;+1;;;","03/May/12 07:30;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dependency versions in generated pos,CASSANDRA-4183,12552532,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stephenc,stephenc,stephenc,25/Apr/12 08:12,16/Apr/19 09:32,14/Jul/23 05:52,04/May/12 13:06,1.1.1,,,Packaging,,,0,,,,,,,Some of the versions of dependencies have fallen out of sync,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5126,,,,,,,,,,,"08/Jan/13 21:43;amorton;5126-2.txt;https://issues.apache.org/jira/secure/attachment/12563831/5126-2.txt","25/Apr/12 08:12;stephenc;CASSANDRA-4183.diff;https://issues.apache.org/jira/secure/attachment/12524162/CASSANDRA-4183.diff",,,,,,,,,,,,,,,,2.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236792,,,Fri May 04 13:06:46 UTC 2012,,,,,,,,,,"0|i0gsy7:",96120,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"25/Apr/12 08:13;stephenc;Ready to be applied to the cassandra-1.1 branch;;;","25/Apr/12 08:17;stephenc;Patch should also be applied to trunk;;;","04/May/12 13:06;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop on CF with ColumnCounter columns fails,CASSANDRA-4181,12552177,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcocova,marcocova,marcocova,23/Apr/12 18:22,16/Apr/19 09:32,14/Jul/23 05:52,23/Apr/12 19:06,1.1.1,,,,,,0,,,,,,,"Accessing CounterColumn from Hadoop fails with an exception:

{noformat} 
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:456)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:462)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:409)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:184)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:500)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator$WideColumnIterator.computeNext(ColumnFamilyRecordReader.java:472)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at com.google.common.collect.Iterators$PeekingImpl.hasNext(Iterators.java:1080)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:449)
        ... 11 more
{noformat}",,marcocova,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/12 18:36;marcocova;cassandra-4181.patch;https://issues.apache.org/jira/secure/attachment/12523836/cassandra-4181.patch",,,,,,,,,,,,,,,,,1.0,marcocova,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236793,,,Mon Apr 23 19:06:40 UTC 2012,,,,,,,,,,"0|i0gsxb:",96116,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"23/Apr/12 18:36;marcocova;Small fix: unthriftify the column before accessing its name;;;","23/Apr/12 19:06;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ALTER TABLE foo WITH default_validation=int has no effect,CASSANDRA-4171,12551531,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,18/Apr/12 21:01,16/Apr/19 09:32,14/Jul/23 05:52,20/Apr/12 15:36,1.1.0,,,Legacy/CQL,,,0,cql3,,,,,,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY) WITH default_validation=timestamp;
ALTER TABLE test WITH default_validation=int;
{noformat}

does not actually change the default validation type of the CF. It does under cql2.

No error is thrown. Some properties *can* be successfully changed using ALTER WITH, such as comment and gc_grace_seconds, but I haven't tested all of them. It seems probable that default_validation is the only problematic one, since it's the only (changeable) property which accepts CQL typenames.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/12 10:25;slebresne;4171.txt;https://issues.apache.org/jira/secure/attachment/12523476/4171.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236339,,,Fri Apr 20 15:36:00 UTC 2012,,,,,,,,,,"0|i0gstr:",96100,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Apr/12 21:15;jbellis;We should raise an error for trying to use default_validation under cql3.  The right way to model this would be something like:

{noformat}
CREATE TABLE test (
    foo text,
    i   int,
    PRIMARY KEY (foo, i)
) WITH COMPACT STORAGE;
{noformat}
;;;","18/Apr/12 21:17;jbellis;(Re-opening, didn't mean to resolve.);;;","20/Apr/12 10:25;slebresne;I believe the correct fix is to reset the set of obsolete keywords for CQL3. We've used that in CLQ2 for keywords to ignore (by opposition to reject) that became obsolete in order to ensure backward compatibility. But CQL3 is not backward compatible anyway so we should start afresh.;;;","20/Apr/12 15:30;jbellis;+1;;;","20/Apr/12 15:36;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 ALTER TABLE ALTER TYPE has no effect,CASSANDRA-4170,12551529,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,18/Apr/12 20:57,16/Apr/19 09:32,14/Jul/23 05:52,20/Apr/12 15:27,1.1.0,,,Legacy/CQL,,,0,cql3,,,,,,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY, bar int);
ALTER TABLE test ALTER bar TYPE float;
{noformat}

does not actually change the column type of bar. It does under cql2.

Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by CASSANDRA-4163. But even with that applied, the ALTER shown here has no effect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/12 10:15;slebresne;4170.txt;https://issues.apache.org/jira/secure/attachment/12523474/4170.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236337,,,Fri Apr 20 15:27:46 UTC 2012,,,,,,,,,,"0|i0gstb:",96098,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"20/Apr/12 10:15;slebresne;Due to a small typo. Patch attached. I've push a simple test for that in the dtests.;;;","20/Apr/12 14:00;jbellis;+1;;;","20/Apr/12 15:27;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Setup"" section of tools/stress/README.txt needs update",CASSANDRA-4168,12551487,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,tpatterson,tpatterson,18/Apr/12 16:14,16/Apr/19 09:32,14/Jul/23 05:52,01/May/12 19:00,1.0.10,1.1.1,,Legacy/Tools,,,0,stress,,,,,,"The README.txt file states ""Run `ant` from the Cassandra source directory, then Run `ant` from the contrib/stress directory.""

The file needs to reflect the changes in the way stress is built.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236295,,,Tue May 01 19:00:22 UTC 2012,,,,,,,,,,"0|i0gssf:",96094,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"18/Apr/12 16:45;brandon.williams;Done in 2d029e86d99e74c8d0eeb321d821daeaa27b1b52;;;","23/Apr/12 17:42;tpatterson;I vote that the readme should mention the need to run 'ant jar' or 'ant stress-build';;;","23/Apr/12 18:15;brandon.williams;I suggest that it's time we made 'ant jar' the default target instead.;;;","01/May/12 02:03;bcoverston;+1 ant jar should be the default target.;;;","01/May/12 19:00;brandon.williams;Changed all current branches to 'jar' by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter constructor crashes on wrong default parameter in conf.get() which is letter instead of number,CASSANDRA-4166,12551454,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,michalm,michalm,18/Apr/12 11:42,16/Apr/19 09:32,14/Jul/23 05:52,18/Apr/12 14:11,1.1.1,,,,,,0,,,,,,,"I've pulled the newest code and my bulkloading started to crash on:

{noformat}2012-04-18 12:34:09,620 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.NumberFormatException: For input string: ""O""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48){noformat}

The problem is this line and default parameter of conf.get() which was accidentally set to ""O"" (letter O) instead of 0 (zero).

{noformat}maxFailures = Integer.valueOf(conf.get(MAX_FAILED_HOSTS, ""O""));{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/12 11:44;michalm;trunk-4166.txt;https://issues.apache.org/jira/secure/attachment/12523177/trunk-4166.txt",,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236262,,,Wed Apr 18 14:11:11 UTC 2012,,,,,,,,,,"0|i0gsrb:",96089,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"18/Apr/12 11:44;michalm;Patch. Kind of obvious ones ;);;;","18/Apr/12 11:45;michalm;Changing ""O"" to ""0"", proudly called ""a patch"" ;);;;","18/Apr/12 14:11;brandon.williams;Good catch.  Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cqlsh should support DESCRIBE on cql3-style composite CFs,CASSANDRA-4164,12551347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,nickmbailey,nickmbailey,18/Apr/12 03:50,16/Apr/19 09:32,14/Jul/23 05:52,04/May/12 17:47,1.1.1,,,,,,0,cql3,,,,,,"There is a discrepancy between create column family commands and then the output of the describe command:

{noformat}
cqlsh:test> CREATE TABLE timeline (
        ...     user_id varchar,
        ...     tweet_id uuid,
        ...     author varchar,
        ...     body varchar,
        ...     PRIMARY KEY (user_id, tweet_id)
        ... );
cqlsh:test> describe columnfamily timeline;

CREATE COLUMNFAMILY timeline (
  user_id text PRIMARY KEY
) WITH
  comment='' AND
  comparator='CompositeType(org.apache.cassandra.db.marshal.UUIDType,org.apache.cassandra.db.marshal.UTF8Type)' AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write=True AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='org.apache.cassandra.io.compress.SnappyCompressor';
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/12 17:38;thepaul;4164-fix.patch.txt;https://issues.apache.org/jira/secure/attachment/12525652/4164-fix.patch.txt","27/Apr/12 18:25;thepaul;4164.patch-2.txt;https://issues.apache.org/jira/secure/attachment/12524901/4164.patch-2.txt",,,,,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236211,,,Fri May 04 19:05:55 UTC 2012,,,,,,,,,,"0|i0gsqf:",96085,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"25/Apr/12 20:18;thepaul;teach cqlsh how to understand system.schema_columnfamilies so that it can get information on composite primary key tables.

patch attached, or, as usual, in my 4164 branch on my github. tagged here:

https://github.com/thepaul/cassandra/tree/pending/4164

Note that this still won't work for describing composite CFs in the system keyspace; system CFs don't show up in schema_columnfamilies, and there's no other way to get key-component column info.;;;","26/Apr/12 12:54;thepaul;Was just pointed out to me that this doesn't identify tables with COMPACT STORAGE, and it should. I'll add a fix (assuming its's possible to distinguish them over thrift or through system.schema_columnfamilies).;;;","26/Apr/12 13:11;slebresne;The current way the code distinguishes COMPACT STORAGE is by checking if column_metadata is empty (i.e. if system.schema_columns is empty).;;;","27/Apr/12 18:25;thepaul;Thanks, Sylvain. Branch is updated; new tag is here:

https://github.com/thepaul/cassandra/tree/pending/4164-2

New comprehensive patch attached.;;;","27/Apr/12 21:11;brandon.williams;+1, committed.;;;","04/May/12 17:38;thepaul;The rebased updated fix here introduced a bug; DESCRIBE COLUMNFAMILY does not work with this change (user only sees ""global name 'ksname' is not defined"").;;;","04/May/12 17:38;thepaul;One-line fix for that attached.;;;","04/May/12 17:47;brandon.williams;Committed, but I'm wondering if it's time that cqlsh had some kind of tests?  Even something ghetto like we do for pig could catch this sort of thing.;;;","04/May/12 19:05;thepaul;Absolutely it's time. That's CASSANDRA-3920, which is in progress. Just need cycles..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 ALTER TABLE command causes NPE,CASSANDRA-4163,12551325,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,khahn,khahn,17/Apr/12 23:43,16/Apr/19 09:32,14/Jul/23 05:52,19/Apr/12 20:54,1.1.0,,,,,,0,cql3,,,,,,"To reproduce the problem:

./cqlsh --cql3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.0-rc1-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.30.0]
Use HELP for help.

cqlsh> CREATE KEYSPACE test34 WITH strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor='1';

cqlsh> USE test34;

cqlsh:test34> CREATE TABLE users (
          ... password varchar,
          ... gender varchar,
          ... session_token varchar,
          ... state varchar,
          ... birth_year bigint,
          ... pk varchar,
          ... PRIMARY KEY (pk)
          ... );

cqlsh:test34> ALTER TABLE users ADD coupon_code varchar;
TSocket read 0 bytes
","INFO 16:07:11,757 Cassandra version: 1.1.0-rc1-SNAPSHOT
INFO 16:07:11,757 Thrift API version: 19.30.0
INFO 16:07:11,758 CQL supported versions: 2.0.0,3.0.0-beta1 (default: 2.0.0)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/12 18:36;thepaul;4163.patch-2.txt;https://issues.apache.org/jira/secure/attachment/12523384/4163.patch-2.txt","18/Apr/12 20:51;thepaul;4163.patch.txt;https://issues.apache.org/jira/secure/attachment/12523254/4163.patch.txt",,,,,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236189,,,Thu Apr 19 20:54:22 UTC 2012,,,,,,,,,,"0|i0gspz:",96083,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Apr/12 02:55;thepaul;Sorry, should have been more specific. This is not cqlsh related; it causes an NPE in the Cassandra server:

{noformat}
ERROR [Thrift:2] 2012-04-17 15:13:28,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.cql3.CFPropDefs.addAll(CFPropDefs.java:192)
        at org.apache.cassandra.cql3.statements.AlterTableStatement.<init>(AlterTableStatement.java:52)
        at org.apache.cassandra.cql3.CqlParser.alterTableStatement(CqlParser.java:2286)
        at org.apache.cassandra.cql3.CqlParser.cqlStatement(CqlParser.java:428)
        at org.apache.cassandra.cql3.CqlParser.query(CqlParser.java:183)
        at org.apache.cassandra.cql3.QueryProcessor.parseStatement(QueryProcessor.java:219)
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:201)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat};;;","18/Apr/12 20:51;thepaul;Fix attached here; also broken up into 3 more concise chunks on the 4163 branch in my github. Link:

http://github.com/thepaul/cassandra/tree/pending/4163

However, in testing, I found that ALTER TABLE  ALTER TYPE still doesn't work, and ALTER TABLE WITH only works in some cases. Will need to make new tickets for those.;;;","18/Apr/12 20:58;jbellis;LGTM.

Nit: could we initialize properties to an empty map to avoid having to null-check it?;;;","18/Apr/12 21:03;thepaul;We could, in the grammar definition, but that attribute is only ever updated with an assignment, so the empty map would be consed for nothing. Hardly a big deal either way though.;;;","18/Apr/12 21:17;jbellis;Yeah, I don't see this as performance critical so I'd rather go for cleanliness.;;;","19/Apr/12 18:36;thepaul;K, attached and updated in github.;;;","19/Apr/12 18:37;thepaul;Link for the new tag (it's on the same branch, though):

http://github.com/thepaul/cassandra/tree/pending/4163-2;;;","19/Apr/12 20:54;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 3.0 does not work in cqlsh with uppercase SELECT,CASSANDRA-4161,12551208,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dohse,dohse,dohse,17/Apr/12 12:20,16/Apr/19 09:32,14/Jul/23 05:52,19/Apr/12 19:02,1.1.0,,,Legacy/Tools,,,0,cql3,cqlsh,,,,,"Uppercase SELECT prevents usage of CQL 3.0 features like ORDER BY

Example:

select * from test ORDER BY number; # works
SELECT * from test ORDER BY number; # fails",cqlsh,ctavan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/12 12:22;dohse;0001-Allow-CQL-3.0-with-uppercase-SELECT-statement.patch;https://issues.apache.org/jira/secure/attachment/12522949/0001-Allow-CQL-3.0-with-uppercase-SELECT-statement.patch","17/Apr/12 21:03;thepaul;4161.patch.txt;https://issues.apache.org/jira/secure/attachment/12523021/4161.patch.txt",,,,,,,,,,,,,,,,2.0,dohse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236072,,,Thu Apr 19 19:02:53 UTC 2012,,,,,,,,,,"0|i0gsp3:",96079,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"17/Apr/12 12:22;dohse;Fix problem by converting command to lower case;;;","17/Apr/12 21:03;thepaul;Good catch, thanks. I do think that a slightly better solution is to do the downcasing in a more specific place, though, as attached. I would like to keep original case intact in cmdword, even if it's just for error messages right now.

Does this work for you?

(also available in my 4161 branch in github: https://github.com/thepaul/cassandra/tree/4161);;;","19/Apr/12 08:30;ctavan;I'm wondering why CQL is being parsed in the client at all? Couldn't we just handle the exceptions thrown by cassandra? That way we wouldn't have to keep cqlsh in sync with CQL development on the C*-side.;;;","19/Apr/12 09:06;dohse;Works for me™;;;","19/Apr/12 15:39;thepaul;bq. I'm wondering why CQL is being parsed in the client at all? Couldn't we just handle the exceptions thrown by cassandra? That way we wouldn't have to keep cqlsh in sync with CQL development on the C*-side.

cqlsh has to attempt to parse input in order to recognize keyspace switches, provide tab-completion, implement the cqlsh-specific commands, separate multiple statements, and (in the future) to allow things like CASSANDRA-3799.

Yes, of course, if cqlsh can identify a CQL statement but can't parse it, and it doesn't recognize the command word as being cqlsh-specific, it should pass the CQL on untouched to Cassandra. The problem in this ticket was with cqlsh deciding incorrectly that the user intended to give a cqlsh-only command.;;;","19/Apr/12 15:39;thepaul;+1 for 4161.patch.txt.;;;","19/Apr/12 16:40;dohse;+1 for 4161.patch.txt;;;","19/Apr/12 19:02;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY ... DESC reverses comparrison predicates in WHERE,CASSANDRA-4160,12551206,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dohse,dohse,17/Apr/12 12:06,16/Apr/19 09:32,14/Jul/23 05:52,02/May/12 15:47,1.1.1,,,Legacy/CQL,,,1,cql3,,,,,,"When issuing a cql select statement with an ORDER BY ... DESC clause the comparison predicates in the WHERE clause gets reversed. 

Example: (see also attached)

SELECT number FROM test WHERE number < 3 ORDER BY number DESC

returns the results expected of WHERE number > 3",cqlsh,ctavan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/12 14:44;slebresne;4160.txt;https://issues.apache.org/jira/secure/attachment/12525298/4160.txt","17/Apr/12 12:08;dohse;test.cql;https://issues.apache.org/jira/secure/attachment/12522947/test.cql",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236070,,,Wed May 02 15:47:57 UTC 2012,,,,,,,,,,"0|i0gson:",96077,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"17/Apr/12 12:08;dohse;Example schema and data to demonstrate bug;;;","02/May/12 14:44;slebresne;Yep, we were forgetting to change the start and end when giving them to getSlice() on reversed queries. Fix attached.;;;","02/May/12 15:03;jbellis;+1;;;","02/May/12 15:31;dohse;Works;;;","02/May/12 15:47;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isReadyForBootstrap doesn't compare schema UUID by timestamp as it should,CASSANDRA-4159,12551182,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Apr/12 08:02,16/Apr/19 09:32,14/Jul/23 05:52,17/Apr/12 16:11,1.0.10,,,,,,0,,,,,,,"CASSANDRA-3629 introduced a wait to be sure the node is up to date on the schema before starting bootstrap. However, the isReadyForBootsrap() method compares schema version using UUID.compareTo(), which doesn't compare UUID by timestamp, while the rest of the code does compare using timestamp (MigrationManager.updateHighestKnown).

During a test where lots of node were boostrapped simultaneously (and some schema change were done), we ended up having some node stuck in the isReadyForBoostrap loop. Restarting the node fixed it, so while I can't confirm it, I suspect this was the source of that problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/12 08:05;slebresne;4159.txt;https://issues.apache.org/jira/secure/attachment/12522924/4159.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,236046,,,Tue Apr 17 16:11:11 UTC 2012,,,,,,,,,,"0|i0gso7:",96075,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"17/Apr/12 15:37;brandon.williams;+1;;;","17/Apr/12 16:11;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL should support CL.TWO,CASSANDRA-4156,12551084,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,thepaul,thepaul,16/Apr/12 16:44,16/Apr/19 09:32,14/Jul/23 05:52,17/Apr/12 07:15,1.1.0,,,Legacy/CQL,,,0,cql,cql3,,,,,CL.TWO was overlooked in creating the CQL language spec. It should probably be added.,,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/12 19:23;mdennis;cassandra-trunk-4156.txt;https://issues.apache.org/jira/secure/attachment/12522839/cassandra-trunk-4156.txt",,,,,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235948,,,Tue Apr 17 07:15:43 UTC 2012,,,,,,,,,,"0|i0gsn3:",96070,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"16/Apr/12 19:24;mdennis;tested on EC2 with multiple nodes;;;","17/Apr/12 07:15;slebresne;+1. Committed for 1.1.0, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFRR wide row iterator does not handle tombstones well,CASSANDRA-4154,12551062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,16/Apr/12 13:35,16/Apr/19 09:32,14/Jul/23 05:52,16/Apr/12 21:24,1.1.0,,,,,,0,,,,,,,"If the last row is a tombstone, CFRR's wide row iterator will throw an exception:

{noformat}

java.util.NoSuchElementException
        at com.google.common.collect.Iterables.getLast(Iterables.java:663)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:441)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:467)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:413)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:137)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:188)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:140)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:199)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:187)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:423)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/12 20:05;jbellis;4154.txt;https://issues.apache.org/jira/secure/attachment/12522850/4154.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235926,,,Mon Apr 16 21:24:38 UTC 2012,,,,,,,,,,"0|i0gsm7:",96066,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"16/Apr/12 20:31;brandon.williams;+1;;;","16/Apr/12 21:24;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow larger cache capacities than 2GB,CASSANDRA-4150,12550874,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,13/Apr/12 20:08,16/Apr/19 09:32,14/Jul/23 05:52,07/Jun/12 04:26,1.1.1,,,,,,0,,,,,,,"The problem is that capacity is a Integer which can maximum hold 2 GB,
I will post a fix to CLHM in the mean time we might want to remove the maximumWeightedCapacity code path (atleast for Serializing cache) and implement it in our code.",,cburroughs,jbellis,ssmith,tooda01,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/12 01:59;vijay2win@yahoo.com;0001-CASSANDRA-4139-v2.patch;https://issues.apache.org/jira/secure/attachment/12527553/0001-CASSANDRA-4139-v2.patch","09/May/12 00:40;vijay2win@yahoo.com;0001-CASSANDRA-4150.patch;https://issues.apache.org/jira/secure/attachment/12526080/0001-CASSANDRA-4150.patch","09/May/12 00:40;vijay2win@yahoo.com;0002-Use-EntryWeigher-for-HeapCache.patch;https://issues.apache.org/jira/secure/attachment/12526081/0002-Use-EntryWeigher-for-HeapCache.patch","16/May/12 01:59;vijay2win@yahoo.com;0002-add-bytes-written-metric-v2.patch;https://issues.apache.org/jira/secure/attachment/12527554/0002-add-bytes-written-metric-v2.patch","09/May/12 00:40;vijay2win@yahoo.com;concurrentlinkedhashmap-lru-1.3.jar;https://issues.apache.org/jira/secure/attachment/12526079/concurrentlinkedhashmap-lru-1.3.jar",,,,,,,,,,,,,5.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235738,,,Thu Dec 13 22:44:37 UTC 2012,,,,,,,,,,"0|i0gskf:",96058,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Apr/12 20:30;vijay2win@yahoo.com;issue: http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=33 created.;;;","14/Apr/12 20:17;vijay2win@yahoo.com;For the records the comments from issue in CLHM library:
{quote}
I think a long capacity is fine, but I'm not actively working on a next release to roll this into soon. If this is critical than it could be a patch release. You are of course welcome to fork if neither of those options are okay.

I helped my former colleagues at Google with Guava's CacheBuilder (formerly MapMaker), which could be considered the successor to this project. There the maximum weight is a long.
{quote}

IRC: Guava doesnt support descendingKeySetWithLimit
Possibly fork the CLHM code into Cassandra code base or drop the hotkey's method and use guava (Thats the only limitation which i see for now).;;;","09/May/12 00:40;vijay2win@yahoo.com;CLHM (Ben) fixed the issue:

{quote}
Fixed in v1.3. I plan on releasing this tonight.

Also introduced EntryWeigher<K, V> to allow key/value weighing. We fixed this oversight in Guava's CacheBuilder from the get-go. 

I believe Cassandra wanted entry weighers too, but it wasn't high priority (no bug filed). Please consider adopting it when you upgrade the library.
{quote};;;","15/May/12 03:58;jbellis;Using MemoryMeter on a hot path like cache updates scares me a bit -- it's pretty slow.  Might want to switch to using the dataSize * liveRatio measurement that memtables use.

Aside #1: So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?

Aside #2: I don't see any use of IRCP with useMemoryWeigher=false, looks like we can remove that parameter;;;","15/May/12 04:00;jbellis;Aside #3: would prefer to assert value.size() < Integer.MAX_VALUE in SC.Weigher, instead of having a Math.min call in there.  Strongly doubt the rest of the code would support serialized values larger than that anyway.;;;","15/May/12 04:06;jbellis;bq. So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?

(If that's the case, we should target this for 1.1.1.);;;","15/May/12 04:41;vijay2win@yahoo.com;>>> Using MemoryMeter on a hot path like cache updates scares me a bit – it's pretty slow. Might want to switch to using the dataSize * liveRatio measurement that memtables use.
Will do.

>>> So basically current 1.1 CLHM is totally broken since it has a capacity of X MB but weighs each item as one byte?
No we assume that the KeyCache to be 48 byte long average size (CacheService.AVERAGE_KEY_CACHE_ROW_SIZE)... 
basically, cache capacity = keyCacheInMemoryCapacity / AVERAGE_KEY_CACHE_ROW_SIZE
hence each entry is considered to be 1 byte/one entry. (Kind of hackie may be we didn't have EntryWeigher i guess, now that we have it we can get rid of that logic).

>>> I don't see any use of IRCP with useMemoryWeigher=false, looks like we can remove that parameter
We still need it if we dont have Jamm Enabled.

>>> would prefer to assert value.size() < Integer.MAX_VALUE in SC.Weigher
Will do, Plz let me know if everything else is fine...

>>>If that's the case, we should target this for 1.1.1
Either ways we have to target for 1.1.1 because the serialized cache is broken :( 
;;;","16/May/12 01:59;vijay2win@yahoo.com;The problem with the liveRatio is that it is variable and we might have a leak if we use the ratio... 
- Lets say when we add the entry we have a ratio of 1.2 and when the entry is removed we will have the ratio of 1.1 then we will leak some space for cache utilization. So it will be better if this number doesn't change.

0001 removes memory meter and does changes to fix the issue.
0002 adds entry weight and small refactor to change the Provider interface as suggested.

Thanks!;;;","16/May/12 16:55;jbellis;{code}
.       EntryWeigher<KeyCacheKey, RowIndexEntry> weigher = new EntryWeigher<KeyCacheKey, RowIndexEntry>()
        {
            public int weightOf(KeyCacheKey key, RowIndexEntry value)
            {
                return key.serializedSize() + value.serializedSize();
            }
        };
        ICache<KeyCacheKey, RowIndexEntry> kc = ConcurrentLinkedHashCache.create(keyCacheInMemoryCapacity, weigher);
{code}

Using the serialized size for a non-serialized cache looks fishy to me.;;;","21/May/12 21:02;vijay2win@yahoo.com;Partially committed 0001 d to 1.1.1 and trunk, which will remove the limitation of 2 GB which this ticket was originally ment to do.
for 0002 i am working on calculating the object overhead + size without using reflection. Thanks!;;;","23/May/12 17:40;vijay2win@yahoo.com;Before i go ahead with the complicated implementation (touching most of the cache values), i did a smoke test and thought of sharing the results.

lgmac-vparthsarathy:cass vparthasarathy$ java -javaagent:/Users/vparthasarathy/Documents/workspace/cassandraT11/lib/jamm-0.2.5.jar -jar ~/Desktop/TestJamm.jar 100000000
Using reflection took: 25954
Using NativeCalculation took: 178
Using MemoryMeter took: 992
lgmac-vparthsarathy:cass vparthasarathy$ 

I used  https://github.com/twitter/commons/blob/master/src/java/com/twitter/common/objectsize/ObjectSizeCalculator.java for reflection test.;;;","06/Jun/12 22:43;jbellis;bq. Lets say when we add the entry we have a ratio of 1.2 and when the entry is removed we will have the ratio of 1.1 then we will leak some space for cache utilization. So it will be better if this number doesn't change

I did some code diving into CLHM -- it looks like it annotates the cache entry with the weight-at-put-time, and uses that in remove or replace.  If so, I think MemoryMeter might be fine after all.

OTOH if the cache isn't churning maybe MemoryMeter is just fine the way it is.

I'm inclined to close this and just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys in our weights.  Thoughts?;;;","07/Jun/12 04:26;vijay2win@yahoo.com;{quote}
just open a new ticket for updating to CLHM 1.3 in 1.2 and including keys in our weights. Thoughts?
{quote}
Done! opened CASSANDRA-4315... Thanks!;;;","07/Jun/12 12:36;cburroughs;I'm having trouble following what happened from this ticket and 4315 (""just open a new ticket for updating to CLHM 1.3 in 1.2... "").  Isn't 02672936f635c93e84ed6625bb994e1628da5a9b in the 1.1 branch and we already upgraded to CLHM 1.3?

 https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=02672936f635c93e84ed6625bb994e1628da5a9b;;;","07/Jun/12 16:14;vijay2win@yahoo.com;Yes we did upgrade it to CLHM 1.3 in both Cassandra 1.1 and Cassandra 1.2 to fix the original issue mentioned in the ticket.

Ben (author of CLHM) also added EntryWeight<K, V> to CLHM 1.3 on our request. Which is 0002 part of the patch originally submitted. 
To use EntryWeights we needed to calculate the memory size's (We where trying out multiple options using MemoryMeter or using other mechanisms) of the Key and Value, we where trying that in this ticket.

Hence CASSANDRA-4315 was opened to continue the work of updating CLHM Weigher<V> to CLHM EntryWeight<K, V>.;;;","12/Jun/12 16:18;ssmith;It looks like the Maven dependencies in build.xml weren't updated to reference CLHM 1.3:

{noformat}
...
<dependency groupId=""com.googlecode.concurrentlinkedhashmap"" artifactId=""concurrentlinkedhashmap-lru"" version=""1.2""/>
...
{noformat}

As a result, the [Cassandra Maven Plugin|http://mojo.codehaus.org/cassandra-maven-plugin/] blows up if you try to make it use Cassandra 1.1.1.

{noformat}
...
[ERROR] 10:59:07,698 Exception encountered during startup
[INFO] java.lang.NoSuchMethodError: com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Builder.maximumWeightedCapacity(J)Lcom/googlecode/concurrentlinkedhashmap/ConcurrentLinkedHashMap$Builder;
[INFO]     at org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)
[INFO]     at org.apache.cassandra.cache.ConcurrentLinkedHashCache.create(ConcurrentLinkedHashCache.java:70)
...
{noformat};;;","12/Jun/12 18:41;vijay2win@yahoo.com;Thanks! committed to 1.1 and trunk.;;;","13/Dec/12 21:16;tooda01;The Maven plugin issue still exists with Cassandra 1.1.5;;;","13/Dec/12 22:44;jbellis;Feel free to attach a patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when using sstableloader with PropertyFileSnitch configured,CASSANDRA-4145,12550785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jicheng,jicheng,jicheng,13/Apr/12 07:08,16/Apr/19 09:32,14/Jul/23 05:52,13/Apr/12 15:33,1.0.10,1.1.0,,Legacy/Tools,,,0,bulkloader,,,,,,"I got a NullPointerException when using sstableloader on 1.0.6. The cluster is using PropertyFileSnitch. The same configuration file is used for sstableloader. 

The problem is if StorageService is initialized before DatabaseDescriptor, PropertyFileSnitch will try to access StorageService.instance before it finishes initialization.


{code}
 ERROR 01:14:05,601 Fatal configuration error
org.apache.cassandra.config.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:607)
        at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:454)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:306)
        at org.apache.cassandra.service.StorageService.<init>(StorageService.java:187)
        at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:190)
        at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:183)
        at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:106)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:62)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:589)
        ... 7 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.locator.PropertyFileSnitch.reloadConfiguration(PropertyFileSnitch.java:170)
        at org.apache.cassandra.locator.PropertyFileSnitch.<init>(PropertyFileSnitch.java:60)
        ... 12 more
Error instantiating snitch class 'org.apache.cassandra.locator.PropertyFileSnitch'.
Fatal configuration error; unable to start server.  See log for stacktrace.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/12 07:13;jicheng;4145.txt;https://issues.apache.org/jira/secure/attachment/12522536/4145.txt",,,,,,,,,,,,,,,,,1.0,jicheng,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235649,,,Fri Apr 13 15:33:25 UTC 2012,,,,,,,,,,"0|i0gsif:",96049,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Apr/12 15:33;jbellis;lgtm, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Looks like Serializing cache broken in 1.1,CASSANDRA-4141,12550607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,12/Apr/12 00:00,16/Apr/19 09:32,14/Jul/23 05:52,12/Apr/12 15:39,1.1.0,1.2.0 beta 1,,,,,0,,,,,,,"I get the following error while setting the row cache to be 1500 MB

INFO 23:27:25,416 Initializing row cache with capacity of 1500 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid26402.hprof ...

havent spend a lot of time looking into the issue but looks like SC constructor has 

.initialCapacity(capacity)
.maximumWeightedCapacity(capacity)

 which 1500Mb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/12 00:22;vijay2win@yahoo.com;0001-CASSANDRA-4141.patch;https://issues.apache.org/jira/secure/attachment/12522351/0001-CASSANDRA-4141.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235471,,,Thu Apr 12 15:39:24 UTC 2012,,,,,,,,,,"0|i0gsgn:",96041,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"12/Apr/12 00:22;vijay2win@yahoo.com;attached fixes this issue... changes to ConcurrentLinkedHashCache is not needed but thought the default was good instead of setting it to 0.;;;","12/Apr/12 09:51;xedin;Wouldn't that instead crash with OOM later in some unpredictable moment (e.g. when cache is read from disk) or when cache resizing is done?;;;","12/Apr/12 14:23;vijay2win@yahoo.com;initialCapacity here is the number of elements in the hashmap and not the size. Size should be controlled by maximumWeightedCapacity (yes the names are confusing through)
We are running OOM because we where trying to allocate 1500 * 1024 *1024 elements.

the intial capacity is used to create the ConcurrentHashMap
{code}
new ConcurrentHashMap<K, Node>(builder.initialCapacity, 0.75f, concurrencyLevel);
{code};;;","12/Apr/12 14:37;xedin;+1;;;","12/Apr/12 15:39;vijay2win@yahoo.com;Committed thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_paged_slices doesn't reset startColumn after first row,CASSANDRA-4136,12550441,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,jbellis,jbellis,10/Apr/12 21:15,16/Apr/19 09:32,14/Jul/23 05:52,11/Apr/12 14:34,1.1.0,,,,,,0,,,,,,,"As an example, consider the WordCount example (see CASSANDRA-3883).  WordCountSetup inserts 1000 rows, each with three columns: text3, text4, int1.  (Some other miscellaneous columns are inserted in a few rows, but we can ignore them here.)

Paging through with get_paged_slice calls with a count of 99, CFRecordReader will first retrieve 33 rows, the last of which we will call K.  Then it will attempt to fetch 99 more columns, starting with row K column text4.

The bug is that it will only fetch text4 for *each* subsequent row K+i, instead of returning (K, text4), (K+1, int1), (K+1, int3), (K+1, text4), etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/12 14:12;slebresne;4136.txt;https://issues.apache.org/jira/secure/attachment/12522249/4136.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,235305,,,Wed Apr 11 14:34:51 UTC 2012,,,,,,,,,,"0|i0gsef:",96031,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"11/Apr/12 14:12;slebresne;Attached patch adds the ability to do paging through multiple rows. The support added by the patch is limited to what get_paged_slices requires (in particular using a SliceQueryFilter where finish != """" is not supported with that new option). The patch contains a unit test.;;;","11/Apr/12 14:24;jbellis;With this and the 3883 patches I get

{noformat}
$ cat /tmp/word_count5/part-r-00000
0       250
1       250
2       250
3       250
word1   2002
word2   1
{noformat}

which is the expected result.

+1;;;","11/Apr/12 14:34;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot create keyspace with specific keywords through cli,CASSANDRA-4129,12549881,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,mkmainali,mkmainali,06/Apr/12 13:42,16/Apr/19 09:32,14/Jul/23 05:52,06/Apr/12 16:46,1.0.10,,,,,,0,,,,,,,"Keyspaces cannot be create when the keyspace name which are used as keywords in the cli, such as 'keyspace', 'family' etc., through CLI. Even when surrounding the keyspace with quotation does not solve the problem. However, such keyspaces can be created through other client such as Hector.

This is similar to the issue CASSANDRA-3195, in which the column families could not be created. Similar to the solution of CASSANDRA-3195, using String keyspaceName = CliUtil.unescapeSQLString(statement.getChild(0).getText()) in executeAddKeySpace would solve the problem. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/12 14:46;xedin;CASSANDRA-4129.patch;https://issues.apache.org/jira/secure/attachment/12521680/CASSANDRA-4129.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234872,,,Fri Apr 06 16:46:49 UTC 2012,,,,,,,,,,"0|i0gsbj:",96018,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Apr/12 13:58;mkmainali;To be more specific, creating keyspaces like 'create keyspace keyspace' or 'create keyspace 'keyspace'' or 'create keyspace family' would fail and raise syntax error error.;;;","06/Apr/12 14:12;jbellis;cli keywords must be quoted;;;","06/Apr/12 14:27;mkmainali;For keyspace creation quotation, both single and double, does not work.

Here is a sample of error I get when creating keyspace

[default@unknown]create keyspace 'column';
Invalid keyspace name: 'column'

[default@unknown]create keyspace ""column"";
Syntax error at position 16: unexpected """""" for `create keyspace ""column"";`.

Using quotation works for cf, but not for ks. Anything specific I need to do?;;;","06/Apr/12 14:46;xedin;patch against cassandra-1.0;;;","06/Apr/12 16:35;jbellis;+1;;;","06/Apr/12 16:46;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress tool hangs forever on timeout or error,CASSANDRA-4128,12549798,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,tpatterson,tpatterson,05/Apr/12 21:42,16/Apr/19 09:32,14/Jul/23 05:52,06/Apr/12 20:35,1.1.0,,,Legacy/Tools,,,0,stress,,,,,,"The stress tool hangs forever if it encounters a timeout or exception. CTRL-C will kill it if run from a terminal, but when running it from a script (like a dtest) it hangs the script forever. It would be great for scripting it if a reasonable error code was returned when things go wrong.

To duplicate, clear out /var/lib/cassandra and then run ""stress --operation=READ"".","This happens in every version of the stress tool, that I know of, including calling it from the dtests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/12 19:32;xedin;CASSANDRA-4128.patch;https://issues.apache.org/jira/secure/attachment/12521715/CASSANDRA-4128.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234789,,,Fri Apr 06 20:35:02 UTC 2012,,,,,,,,,,"0|i0gsb3:",96016,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"06/Apr/12 20:14;brandon.williams;+1;;;","06/Apr/12 20:35;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
check most recent TS values in SSTables when a row tombstone has already been encountered,CASSANDRA-4116,12549563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,mdennis,mdennis,04/Apr/12 20:27,16/Apr/19 09:32,14/Jul/23 05:52,02/May/12 15:02,1.1.1,,,,,,0,,,,,,,"once C* comes across a row tombstone, C* should check the TS on the tombstone against all SSTables.  If the most recent TS in an SST is older than the row tombstone, that entire SST (or the remainder of it) can be safely ignored.

There are two drivers for this.

* avoid checking column values that could not possibly be in the result set

* avoid OOMing because all the tombstones are temporarily kept in memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/12 16:03;samt;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4116-After-Row-tombstone-skip-earlier-SSTabl.txt;https://issues.apache.org/jira/secure/attachment/12525178/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-4116-After-Row-tombstone-skip-earlier-SSTabl.txt","01/May/12 16:03;samt;ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-for-.txt;https://issues.apache.org/jira/secure/attachment/12525179/ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-for-.txt","02/May/12 09:32;samt;CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-1.0.txt;https://issues.apache.org/jira/secure/attachment/12525270/CASSANDRA-4116-Set-SSTable-maxTimestamp-correctly-1.0.txt",,,,,,,,,,,,,,,3.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234554,,,Wed May 02 15:02:10 UTC 2012,,,,,,,,,,"0|i0gs6f:",95995,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Apr/12 19:37;samt;While I was working through this & testing with tiny amounts of data, I found that if the last (or only) update to a CF before a flush is a Row level delete then the maxTimeStamp on the resulting SSTable is incorrect. The second patch includes a fix + test for that - I included it as a separate patch as I was a bit less confident about screwing around with the SSTable metadata although all existing tests are still passing.;;;","29/Apr/12 12:12;samt;Updated patches because I realised the license header was missing from the new test source;;;","01/May/12 04:03;jbellis;{code}
.               if (sstable.getMaxTimestamp() < mostRecentRowTombstone)
                    continue;
{code}

For {{collectTimeOrderedData}}, I think that can be a ""break"" since we're scanning in reverse chronological order.

For {{collectAllData}}, I think it might be worth making (potentially) two passes: if we find a deletion marker on the first pass, we should do a second pass, so we can strip out sstables that we saw the first time before we knew about the marker.  This would improve the deletion case, without imposing the overhead of a sort or two passes on workloads with no deletes.;;;","01/May/12 16:11;samt;Updated patches as per comments. In {{collectAllData}}, collect the eligible iterators in a multimap with key = maxTimestamp (I guess that its pretty unlikely for two sstables to have the exact same timestamp, but using a Multimap doesn't really add any cost). Then, if we did find a row tombstone, only keep the iterators from younger sstables. Otherwise, keep all that we collected.;;;","02/May/12 04:30;jbellis;Committed patch 0001 to 1.1.1, with some cleanup.  (Main one was to use a Map<iterator, timestamp> instead of a multimap the other direction.)

I suspect 0002 is needed for earlier releases as well, can you post a version against the 1.0 branch?;;;","02/May/12 09:32;samt;Added version of patch 0002 with fix to sstable maxTimeStamp against 1.0 branch;;;","02/May/12 15:02;jbellis;thanks, committed.  (moved the fix into CF.maxTimestamp so the other code paths using that will benefit as well.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNREACHABLE schema after decommissioning a non-seed node,CASSANDRA-4115,12549543,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,tpatterson,tpatterson,04/Apr/12 17:43,16/Apr/19 09:32,14/Jul/23 05:52,22/May/12 16:29,1.1.1,,,,,,0,,,,,,,"decommission a non-seed node, sleep 30 seconds, then use thrift to check the schema. UNREACHABLE is listed:

{'75dc4c07-3c1a-3013-ad7d-11fb34208465': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}",ccm using the following unavailable_schema_test.py dtest.,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/12 15:07;brandon.williams;4115-v2.txt;https://issues.apache.org/jira/secure/attachment/12528602/4115-v2.txt","06/Apr/12 21:13;brandon.williams;4115.txt;https://issues.apache.org/jira/secure/attachment/12521734/4115.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234534,,,Tue May 22 16:29:31 UTC 2012,,,,,,,,,,"0|i0gs5z:",95993,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Critical,,,,,,,,,,,,,,,,,"04/Apr/12 17:47;jbellis;Tyler, what version(s) did you observer this against?;;;","04/Apr/12 17:48;jbellis;Also, did you mean to attach unavailable_schema_test.py somewhere?;;;","05/Apr/12 01:13;tpatterson;I tested and got it on branch 1.0.9-tentative and release 1.0.8.

I added unavailable_schema_test.py to the master branch of dtest.;;;","06/Apr/12 21:13;brandon.williams;The only way I can see this happening on 1.0 is if ring delay is set to something lower than the gossip interval *2, otherwise it might not actually send the 'left' message.  That would result in the node being stuck in a 'leaving' and down state keeping it in the unreachable map.

For 1.1 and trunk, this is caused by CASSANDRA-3936, forcing a conviction when the decom node shuts the gossiper down, after the others have already removed it.  Instead, markAlive and markDead should have a dead state check and thus ignore it, since it's never valid to do either with a dead state.;;;","06/Apr/12 21:28;jbellis;bq. The only way I can see this happening on 1.0 is if ring delay is set to something lower than the gossip interval *2

Pretty sure Tyler didn't mess with ring delay.;;;","06/Apr/12 21:35;tpatterson;After testing a little more I found the following:

On 1.0.9 I get the following schema when sleeping 30 seconds after the decommission:
{'00000000-0000-1000-0000-000000000000': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}

On 1.1, this is the schema 30 seconds after the decommission:
{'59adb24e-f3cd-3e02-97f0-5b395827453f': ['127.0.0.1'],
 'UNREACHABLE': ['127.0.0.2']}

If I increase the sleep after decommissioning from 30 to 90 seconds, then 1.0.9 now returns only one schema, while 1.1 still fails with the UNREACHABLE schema. 

None of these tests were done with the recent patch.;;;","12/Apr/12 16:36;brandon.williams;This must be something with the environment, on 1.0 I see the node removed as soon as the decom node begins to announce it has left (before it has completed announcing, and thus before nodetool would even return) and it never reappears.;;;","12/Apr/12 16:37;brandon.williams;What does strike me as odd though is that your 1.0 test has no schema at all, hence the 00000000-0000-1000-0000-000000000000 uuid.;;;","01/May/12 21:06;brandon.williams;Setting this as blocker for 1.1.1 so it doesn't slip by, we definitely need this patch in 1.1.;;;","21/May/12 22:24;vijay2win@yahoo.com;I think we will have a problem in handleMajorStateChange

{code}
        if (!isDeadState(epState))
            markAlive(ep, epState);
        else
        {
            logger.debug(""Not marking "" + ep + "" alive due to dead state"");
            markDead(ep, epState);
        }
{code}

but in markDead we have the following so it will never be marked dead.

{code}

        if (isDeadState(localState))
            return;
{code};;;","22/May/12 15:07;brandon.williams;Hmm, you're right.  v2 instead filters dead states out of convict();;;","22/May/12 16:23;vijay2win@yahoo.com;+1;;;","22/May/12 16:29;brandon.williams;Committed; resolving since I can't repro on 1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default read_repair_chance value is wrong,CASSANDRA-4114,12549455,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,mkmainali,mkmainali,04/Apr/12 07:20,16/Apr/19 09:32,14/Jul/23 05:52,04/Apr/12 17:06,1.1.0,,,Legacy/Tools,,,0,,,,,,,"The documents says that the default read_repair_chance value is 0.1, and it is also declared so in CFMetaDeta. However, when creating a column family with ""create column family foo"" via cli and checking with ""show keyspaces"" shows that the read_repair_chance=1.0. This also happens when creating the column family through Hector.

Going through the code, I find that in CfDef class, the constructor without any parameters sets the read_repair_chance to 1. Changing this value to 0.1 seems to create a column family with the 0.1 read_repair_chance. The best might be to remove it from the CfDef as the read_repair_chance is set to the default value in CFMetaDeta.",,mkmainali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/12 16:19;jbellis;4114.txt;https://issues.apache.org/jira/secure/attachment/12521329/4114.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234446,,,Wed Apr 04 17:06:56 UTC 2012,,,,,,,,,,"0|i0gs5j:",95991,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"04/Apr/12 16:19;jbellis;I think that since the cli has behaved this way since 1.0.0, changing it now might surprise people who didn't read NEWS (and thus don't know that it was supposed to change to default of 0.1).  So I propose fixing this in 1.1.0 instead. 

(For completeness, I note that cql {{CREATE COLUMNFAMILY}} does default to 0.1 correctly since it does not build its CFMetadata objects from Thrift.)
;;;","04/Apr/12 16:38;vijay2win@yahoo.com;+1 and tested by creating a cf via cli. (  and read_repair_chance = 0.1 );;;","04/Apr/12 17:06;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh does not work with piping,CASSANDRA-4113,12549357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,pavel1,pavel1,03/Apr/12 16:18,16/Apr/19 09:32,14/Jul/23 05:52,15/May/12 22:25,1.0.11,,,,,,0,cqlsh,,,,,,"When I run cqlsh under ant like this
	<target name=""create-test-db"">
		<exec executable=""../dsc-cassandra-1.0.8/bin/cqlsh"" input=""./resource/test/test.cql"">  
		</exec>
	</target>
I got: 
     [exec] Traceback (most recent call last):
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1891, in <module>
     [exec]     main(*read_options(sys.argv[1:], os.environ))
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 1863, in main
     [exec]     completekey=options.completekey)
     [exec]   File ""../dsc-cassandra-1.0.8/bin/cqlsh"", line 384, in __init__
     [exec]     self.output_codec = codecs.lookup(encoding)
     [exec] TypeError: must be string, not None
     [exec] Result: 1

I suggest replacement of:
        if encoding is None:
            encoding = sys.stdout.encoding
to: 
       if encoding is None:
           encoding=locale.getpreferredencoding()
",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234348,,,Tue May 15 22:25:29 UTC 2012,,,,,,,,,,"0|i0gs53:",95989,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"23/Apr/12 21:31;thepaul;Fixed in my 4113 branch (current tag pending/4113) in github:

https://github.com/thepaul/cassandra/tree/pending/4113 . Defaults encoding to ascii when python doesn't provide any useful sys.stdout.encoding (as happens when stdout is not a tty).

1.1 branch already had this fix as part of CASSANDRA-3479.;;;","23/Apr/12 21:36;thepaul;Oops, I didn't even read the last part of the report. Didn't know about locale.getpreferredencoding()- tested it out, and you're right, that's a perfect fit. Redoing fix.;;;","23/Apr/12 21:47;thepaul;New fix in same branch (4113), now tagged pending/4113-2:

https://github.com/thepaul/cassandra/tree/pending/4113-2

And adjusted for the 1.1 branch, since it doesn't cherry-pick or merge cleanly:

https://github.com/thepaul/cassandra/tree/pending/4113-for-1.1;;;","15/May/12 22:25;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cleanup giving exception,CASSANDRA-4112,12549262,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,shoaibmir,shoaibmir,03/Apr/12 00:18,16/Apr/19 09:32,14/Jul/23 05:52,03/Apr/12 14:35,1.0.9,,,,,,0,compaction,,,,,,"We just recently started using version 1.0.9, previously we were using tiered compaction because of a bug in 1.0.8 (not letting us use leveled compaction) and now since moving to 1.0.9 we have started using leveled compaction.

Trying to do a cleanup we are getting the following exception:

root@test:~# nodetool -h localhost cleanup 
Error occured during cleanup
java.util.concurrent.ExecutionException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:204)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:240)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:988)
        at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1639)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.util.NoSuchElementException
        at java.util.ArrayList$Itr.next(ArrayList.java:757)
        at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:196)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:147)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1010)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:802)
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:244)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:183)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)

cheers,
Shoaib","Ubuntu LTS 10.04, OpenJDK 1.6.0_20",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/12 02:23;jbellis;4112.txt;https://issues.apache.org/jira/secure/attachment/12521096/4112.txt","03/Apr/12 07:46;slebresne;4112_v2.txt;https://issues.apache.org/jira/secure/attachment/12521114/4112_v2.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234253,,,Tue Apr 03 14:35:38 UTC 2012,,,,,,,,,,"0|i0gs4n:",95987,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"03/Apr/12 00:55;jbellis;Are you running with assertions disabled?;;;","03/Apr/12 01:04;shoaibmir;Yes, we have assertions disabled.;;;","03/Apr/12 02:23;jbellis;LCS incorrectly assumed that cleanup would always result in the same number of sstables as before, which is not the case (if no keys in an sstable belong post-cleanup, it will be left out entirely).

fix attached.;;;","03/Apr/12 07:46;slebresne;Agreed on the diagnostic, but the patch changes the behavior a bit in that it adds the new sstable to level 0 (which is probably ok in the sense of not introducing a bug, but is less efficient for no good reason that I can see). Attaching v2 that correct that.;;;","03/Apr/12 14:23;jbellis;+1 on v2;;;","03/Apr/12 14:35;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializing cache can cause Segfault in 1.1,CASSANDRA-4111,12549251,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Apr/12 23:29,16/Apr/19 09:32,14/Jul/23 05:52,03/Apr/12 19:35,1.1.0,,,,,,0,,,,,,,"Rare but this can happen per sure, looks like this issue is after CASSANDRA-3862 hence affectes only 1.1

        FreeableMemory old = map.get(key);
        if (old == null)
            return false;

        // see if the old value matches the one we want to replace
        FreeableMemory mem = serialize(value);
        if (mem == null)
            return false; // out of memory.  never mind.
        V oldValue = deserialize(old);
        boolean success = oldValue.equals(oldToReplace) && map.replace(key, old, mem);

        if (success)
            old.unreference();
        else
            mem.unreference();
        return success;

in the above code block we deserialize(old) without taking reference to the old memory, this can case seg faults when the old is reclaimed (free is called)
Fix is to get the reference just for deserialization

        V oldValue;
        // reference old guy before de-serializing
        old.reference();
        try
        {
             oldValue = deserialize(old);
        }
        finally
        {
            old.unreference();
        }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/12 17:06;vijay2win@yahoo.com;0001-CASSANDRA-4111-v2.patch;https://issues.apache.org/jira/secure/attachment/12521171/0001-CASSANDRA-4111-v2.patch","02/Apr/12 23:42;vijay2win@yahoo.com;0001-CASSANDRA-4111.patch;https://issues.apache.org/jira/secure/attachment/12521079/0001-CASSANDRA-4111.patch",,,,,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234242,,,Tue Apr 03 19:35:35 UTC 2012,,,,,,,,,,"0|i0gs47:",95985,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Apr/12 23:47;vijay2win@yahoo.com;Noticed it when compaction was serializing it and update was freeing it.;;;","03/Apr/12 02:09;jbellis;good catch.  also need to handle old.reference() returning false tho.
;;;","03/Apr/12 17:06;vijay2win@yahoo.com;ahaaa missed that.... V2 fixes it. Thanks!;;;","03/Apr/12 17:28;jbellis;+1;;;","03/Apr/12 19:35;vijay2win@yahoo.com;Committed to 1.1.0, 1.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
http link is broken in cassandra-env.sh,CASSANDRA-4106,12549178,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,chipitsine,chipitsine,02/Apr/12 16:21,16/Apr/19 09:32,14/Jul/23 05:52,02/Apr/12 16:23,,,,Packaging,,,0,documentation,,,,,,"# jmx: metrics and administration interface
#
# add this if you're having trouble connecting:
# JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=<public name>""
#
# see
# http://blogs.sun.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole
# for more on configuring JMX through firewalls, etc. (Short version:
# get it working with no firewall first.)


link to https://blogs.sun.com leads to 404",any,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,234169,,,Mon Apr 02 16:23:52 UTC 2012,,,,,,,,,,"0|i0gs1z:",95975,,,,,Low,,,,,,,,,,,,,,,,,"02/Apr/12 16:23;chipitsine;please change link with https://blogs.oracle.com/jmxetc/entry/troubleshooting_connection_problems_in_jconsole;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make scrub and cleanup operations throttled,CASSANDRA-4100,12548569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,28/Mar/12 22:42,16/Apr/19 09:32,14/Jul/23 05:52,11/Apr/12 00:49,1.1.1,1.2.0 beta 1,,,,,0,compaction,,,,,,Looks like scrub and cleanup operations are not throttled and it will be nice to throttle else we are likely to run into IO issues while running it on live cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/12 23:38;vijay2win@yahoo.com;0001-CASSANDRA-4100-v2.patch;https://issues.apache.org/jira/secure/attachment/12522040/0001-CASSANDRA-4100-v2.patch","10/Apr/12 17:53;vijay2win@yahoo.com;0001-CASSANDRA-4100-v3.patch;https://issues.apache.org/jira/secure/attachment/12522138/0001-CASSANDRA-4100-v3.patch","03/Apr/12 19:15;vijay2win@yahoo.com;0001-CASSANDRA-4100.patch;https://issues.apache.org/jira/secure/attachment/12521194/0001-CASSANDRA-4100.patch",,,,,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233666,,,Wed Apr 11 00:49:10 UTC 2012,,,,,,,,,,"0|i0grz3:",95962,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"03/Apr/12 19:15;vijay2win@yahoo.com;Attached patch adds the throttle for cleanup and scrub. 
Note: compaction_throughput_mb_per_sec is used for the throttling, not sure if making a seperate property for cleanup/scrub is better....;;;","04/Apr/12 15:01;yukim;Looks like patch is against trunk, though fix version is marked as 1.0.10.
Anyway, I think Throttle object in CompcationController should be non-static since compactions may run in parallel.;;;","04/Apr/12 15:36;vijay2win@yahoo.com;>>> I think Throttle object in CompcationController should be non-static since compactions may run in parallel.
Exactly thats why static is better, Parallel compaction is not a problem per say (ParallelCompactionIterable.getReduced() will take care of it), but compaction running one after the other (lot of small compactions).

Let me know if everything else is ok i will rebase to 1.0.10 and move away from static (I am ok either ways), if needed. Thanks!;;;","09/Apr/12 21:18;yukim;OK, so static Throttle is fine here since one compaction_throughput_mb_per_sec is used for all compactions. Then, do we need to divide that by number of active compactions? I'm referring the code inside the implementation of ThroughputFunction:

{code}
totalBytesPerMS / Math.max(1, CompactionManager.instance.getActiveCompactions());
{code};;;","09/Apr/12 23:38;vijay2win@yahoo.com;Fixed. Thanks!;;;","10/Apr/12 07:27;slebresne;bq. OK, so static Throttle is fine

I'll have to disagree. I'm pretty sure this patch break throttling. If more than one compaction share the Throttle object, they also share the Throttle.timeAtLastDelay field. Which means that as soon as there is more than 1 compaction running at any given time, the interval on which throttling is computing is bogus, and thus throttling will be bogus.

More generally, I'm -1 on changing code that does not have any known problem on the 1.0 branch (and as far as I know, current throttling works well) as 1.0 is getting really stable and we should start being conservative there (but I'd be fine with a patch that just add throttling to scrub and cleanup for 1.0).;;;","10/Apr/12 07:37;slebresne;And to be clear, I'm not saying that throttling cannot be improved (it is true that currently it is too conservative and could throttle a thread even though the total throughput is below the threshold), but I'm saying that 1) this patch does not fix that correctly and 2) in any case this should be another ticket that shouldn't be targeted at 1.0.;;;","10/Apr/12 17:53;vijay2win@yahoo.com;Alright, v3 simply adds throttle to scrub and cleanup... Simple refactor to move the throttle to compaction controller.;;;","10/Apr/12 21:59;yukim;v3 looks good to me, but as Sylvain said, I'm +1 to put this to version 1.1.1 instead of 1.0.10.;;;","11/Apr/12 00:49;vijay2win@yahoo.com;Committed to 1.1 and trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingTCPConnection recognizes from by doing socket.getInetAddress() instead of BroadCastAddress,CASSANDRA-4099,12548549,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,28/Mar/12 20:54,16/Apr/19 09:32,14/Jul/23 05:52,06/Jun/12 02:23,1.0.9,1.1.0,,,,,0,,,,,,,"change ""this.from = socket.getInetAddress()"" to understand the broad cast IP, but the problem is we dont know until the first packet is received, this ticket is to work around the problem until it reads the first packet.",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/12 23:48;vijay2win@yahoo.com;0001-CASSANDRA-4099-v2.patch;https://issues.apache.org/jira/secure/attachment/12520343/0001-CASSANDRA-4099-v2.patch","29/Mar/12 18:39;vijay2win@yahoo.com;0001-CASSANDRA-4099-v3.patch;https://issues.apache.org/jira/secure/attachment/12520479/0001-CASSANDRA-4099-v3.patch","30/Mar/12 01:33;vijay2win@yahoo.com;0001-CASSANDRA-4099-v4.patch;https://issues.apache.org/jira/secure/attachment/12520546/0001-CASSANDRA-4099-v4.patch","28/Mar/12 21:59;vijay2win@yahoo.com;0001-CASSANDRA-4099.patch;https://issues.apache.org/jira/secure/attachment/12520332/0001-CASSANDRA-4099.patch",,,,,,,,,,,,,,4.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233646,,,Wed Jun 06 02:23:36 UTC 2012,,,,,,,,,,"0|i0gryn:",95960,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"28/Mar/12 23:00;brandon.williams;This doesn't look like a perfect solution since all nodes will have to stream to all other nodes in order to learn the correct version, and thus be able to use newer-version features.  I'm not sure there's currently a way around this, though.  I created CASSANDRA-4101 to get us started there, but I'll look more closely here tomorrow.;;;","28/Mar/12 23:06;vijay2win@yahoo.com;Thanks Brandon, CASSANDRA-4101 looks like a better solution but not only does the Streaming sets the version Gossip or any connunication does set it, the following does it

{code} 
            from = msg.getFrom(); // why? see => CASSANDRA-4099
            if (version > MessagingService.current_version)
            {
                // save the endpoint so gossip will reconnect to it
                Gossiper.instance.addSavedEndpoint(from);
                logger.info(""Received "" + (isStream ? ""streaming "" : """") + ""connection from newer protocol version. Ignoring"");
            }
            else if (msg != null)
            {
                Gossiper.instance.setVersion(from, version);
                logger.debug(""set version for {} to {}"", from, version);
            }
{code} ;;;","28/Mar/12 23:11;brandon.williams;Won't 'from' still always be wrong in your configuration unless streaming occurs?;;;","28/Mar/12 23:16;vijay2win@yahoo.com;Nope there is 2 places where we set the from one is during the stream just before we start. the second place is when it is not a stream and when we recive the first message so we should be ok and should never be null.

1)

{code}
                    // why? see => CASSANDRA-4099
                    from = streamHeader.broadcastAddress;
                    stream(streamHeader, input);
{code}

2)

{code}

Message msg = receiveMessage(input, version);
            from = msg.getFrom(); // why? see => CASSANDRA-4099
{code}

Hope it makes sense.;;;","28/Mar/12 23:22;brandon.williams;I think I see, in your situation the version is correct for everything except streaming, hence 1)?  It seems like the problem here is it will still accept streams from a lesser version, which is always version-specific.;;;","28/Mar/12 23:28;vijay2win@yahoo.com;I think we can remove

{code}
                if (version <= MessagingService.current_version)
                {
                    int size = input.readInt();
                    byte[] headerBytes = new byte[size];
                    input.readFully(headerBytes);
                    StreamHeader streamHeader = StreamHeader.serializer().deserialize(new DataInputStream(new FastByteArrayInputStream(headerBytes)), version);
                    // why? see => CASSANDRA-4099
                    from = streamHeader.broadcastAddress;
{code}

Because the version will be already set by the Gossip hopefully :) but I still dont understand why we streaming needs to be version specific though.;;;","28/Mar/12 23:48;vijay2win@yahoo.com;IRC Brandon explained the changes in flowcontrol which will not allow us to stream data from other versions, v2 removes those changes from v1;;;","29/Mar/12 14:22;brandon.williams;I'm confused, how does 'from' differ from 'msg.getFrom' in this patch?  It seems like a no-op.;;;","29/Mar/12 15:25;vijay2win@yahoo.com;getFrom gets the IP from the message header and the message header is set by the caller, which uses FB.BCA and the receiving machine sets it. Plz look at  Message.getInternalReply for example. Thanks!;;;","29/Mar/12 16:13;brandon.williams;I see, it's injecting another getFrom call.  +1 (though this version only applies to trunk);;;","29/Mar/12 17:56;jbellis;I still don't get it.

{noformat}
+            from = msg.getFrom(); // why? see => CASSANDRA-4099
-                Gossiper.instance.setVersion(msg.getFrom(), version);
+                Gossiper.instance.setVersion(from, version);
{noformat}

How is the first getFrom, not the same as the one we've de-inlined?;;;","29/Mar/12 18:19;brandon.williams;It overrides where from is set in the constructor:

{code}
this.from = socket.getInetAddress();
{code};;;","29/Mar/12 18:22;jbellis;I see.

Looking at the usages of ITC.from, it looks like we can drop that constructor initialization entirely...;;;","29/Mar/12 18:27;brandon.williams;I agree, we should get out of the habit of examining sockets directly due to broadcast_address.;;;","29/Mar/12 18:39;vijay2win@yahoo.com;Attached version incorporate's the comments... Note that the streaming a file will not remove or add the version, which i think is a better option IMHO. Plz let me know if you think otherwise. Thanks!;;;","29/Mar/12 18:50;jbellis;+1 on v3 for 1.0+

I'm fine w/ continuing to ignore version on streaming for now;;;","29/Mar/12 23:51;vijay2win@yahoo.com;Committed to 1.0 and trunk (as CASSANDRA-4101 is closed).;;;","30/Mar/12 01:19;vijay2win@yahoo.com;Looks like there is a case we will run into NPE when the other node which is sending a higher version we ignore it but the problem the msg object is not available.;;;","30/Mar/12 01:26;vijay2win@yahoo.com;v4 is on top of the v3 and it fixes the NPE;;;","30/Mar/12 16:20;brandon.williams;+1;;;","30/Mar/12 18:51;vijay2win@yahoo.com;Committed back Thanks!;;;","05/Jun/12 22:50;jbellis;I think this is still broken.  if A forwards a message from B to C, then {{Gossiper.instance.setVersion(from, version)}} C will mark B's version (B == from) to the version that A sent.  But A sends its own version, not B's.  (Which is correct for the purpose of message forwarding, since A re-serializes instead of passing what B sent verbatim.)

I don't think we can accommodate both message forwarding, and broadcast address != socket address, without a protocol change to include a ""here is my reply-to broadcast_address"" piece of information when the connection is first established, distinct from Message.from.;;;","05/Jun/12 23:37;vijay2win@yahoo.com;Hi Jonathan, IMO the right fix for it with a similar approach as in the ticket CASSANDRA-4101 (we can also remove the ConcurrentMap.get() to compare the versions), Agree?;;;","05/Jun/12 23:45;brandon.williams;I agree that something like CASSANDRA-4101 is the right way to do this;;;","06/Jun/12 02:23;jbellis;I kind of think we've reached the limits of what we can do by band-aiding this.  Opened CASSANDRA-4311 for a deeper fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mlockall() returned code is ignored w/o assertions,CASSANDRA-4096,12548428,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,scode,scode,28/Mar/12 03:13,16/Apr/19 09:32,14/Jul/23 05:52,30/Mar/12 15:41,1.0.9,1.1.0,,,,,0,jna,,,,,,"We log that mlockall() was successful only based on the lack of an assertion failure, so for anyone running w/o {{-ea}} we are lying about mlockall() succeeding.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/12 18:08;jbellis;4096.txt;https://issues.apache.org/jira/secure/attachment/12520463/4096.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233525,,,Fri Mar 30 15:41:53 UTC 2012,,,,,,,,,,"0|i0grxj:",95955,,scode,,scode,Low,,,,,,,,,,,,,,,,,"29/Mar/12 18:08;jbellis;The assert is redundant anyway since JNA will check return value and errno for us. Patch attached.;;;","30/Mar/12 02:42;scode;Good point. If that's the behavior of JNA, then definitely +1.;;;","30/Mar/12 15:41;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing get_slice (NullPointerException),CASSANDRA-4095,12548427,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jlaban,jlaban,28/Mar/12 03:09,16/Apr/19 09:32,14/Jul/23 05:52,30/Mar/12 15:24,1.0.9,1.1.0,,,,,0,,,,,,,"I get this pretty regularly.  It seems to happen transiently on multiple nodes in my cluster, every so often, and goes away.


ERROR [Thrift:45] 2012-03-26 19:59:12,024 Cassandra.java (line 3041) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:76)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:724)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:283)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:365)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:326)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



The line in question is (I think) the one below, so it looks like the column family reference for a row can sometimes be null?

    int liveColumnsInRow = row != null ? row.cf.getLiveColumnCount() : 0;



Here is my column family (on 1.0.8):

    ColumnFamily: WorkQueue (Super)
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type/org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
","Java(TM) SE Runtime Environment (build 1.6.0_30-b12)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/12 18:45;jbellis;4095.txt;https://issues.apache.org/jira/secure/attachment/12520481/4095.txt","30/Mar/12 08:28;slebresne;4095_v2.txt;https://issues.apache.org/jira/secure/attachment/12520581/4095_v2.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233524,,,Fri Mar 30 15:24:21 UTC 2012,,,,,,,,,,"0|i0grx3:",95953,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"29/Mar/12 18:45;jbellis;Thanks for the report, John.  I think your analysis is spot on.  Patch attached that does not assume row.cf is non-null.;;;","29/Mar/12 18:56;vijay2win@yahoo.com;+1;;;","30/Mar/12 08:28;slebresne;The patch doesn't compile, it's calling a method that doesn't exist. Attaching v2 that I think adds the intended method.;;;","30/Mar/12 14:06;jbellis;I wrote the patch against 1.1, which already has that method.  Sorry for not being clear.;;;","30/Mar/12 14:52;vijay2win@yahoo.com;I Should have also verified against 1.0, thanks!;;;","30/Mar/12 15:24;slebresne;Alright, I've committed v2 to 1.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
schema_* CFs do not respect column comparator which leads to CLI commands failure.,CASSANDRA-4093,12548388,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dbrosius@apache.org,dbrosius@apache.org,27/Mar/12 20:59,16/Apr/19 09:32,14/Jul/23 05:52,09/Apr/12 16:58,1.1.0,,,Legacy/Tools,,,0,,,,,,,"ColumnDefinition.{ascii, utf8, bool, ...} static methods used to initialize schema_* CFs column_metadata do not respect CF comparator and use ByteBufferUtil.bytes(...) for column names which creates problems in CLI and probably in other places.

The CompositeType validator throws exception on first column

String columnName = columnNameValidator.getString(columnDef.name);

Because it appears the composite type length header is wrong (25455)

AbstractCompositeType.getWithShortLength

java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:247)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:50)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:59)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:139)
	at org.apache.cassandra.cli.CliClient.describeColumnFamily(CliClient.java:2046)
	at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1969)
	at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1574)

(seen in trunk)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/12 19:02;slebresne;4093.txt;https://issues.apache.org/jira/secure/attachment/12520482/4093.txt","06/Apr/12 16:15;slebresne;4093_v2.txt;https://issues.apache.org/jira/secure/attachment/12521691/4093_v2.txt","28/Mar/12 00:26;xedin;CASSANDRA-4093-CD-changes.patch;https://issues.apache.org/jira/secure/attachment/12520216/CASSANDRA-4093-CD-changes.patch",,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233485,,,Mon Apr 09 16:58:41 UTC 2012,,,,,,,,,,"0|i0grvz:",95948,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Mar/12 21:11;xedin;Can you please provide a simple test-case for this problem? It doesn't seem to be CLI related but rather CompositeType related problem...;;;","27/Mar/12 23:28;dbrosius@apache.org;I just created a brand new database directory, went into cli and did

show keyspaces;

the output finishes with this: Notice the trailing 'null'. That is caused by the exception.

    ColumnFamily: schema_columnfamilies
    ""ColumnFamily definitions""
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.AsciiType,org.apache.cassandra.db.marshal.AsciiType)
      GC grace seconds: 10368000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Column Metadata:
null


;;;","28/Mar/12 00:26;xedin;patch makes changes to ColumnDefinition to make it CF comparator aware, CLI `show` and `describe` commands works just as expected but it breaks ColumnFamily related migrations with some weird NPE exceptions.;;;","29/Mar/12 18:37;jbellis;Hmm, I'm pretty sure that giving CFMetdata column names a composite type header is the wrong solution. I think instead we need to teach cliclient that when you have a composite type comparator, you need to look at column aliases as well as column names to form the composite in question.;;;","29/Mar/12 18:45;xedin;I don't get it, if the column names in column_metadata were not serialized using given CF comparator shouldn't we fix that instead of changing CLI?;;;","29/Mar/12 18:56;jbellis;No, because when you have column aliases the name in the metadata is a ""leaf"" name, it's not supposed to be a full composite name. In other words,

composite name = (alias name, metadata name);;;","29/Mar/12 19:02;slebresne;So the problem is the following: for CQL3, we've introduced the fact that when we have a composite comparator, the 'name' of a ColumnDefinition refers to the last component of the composite comparator. In other words, one should use the last comparator of the composite comparator to parse the ColumnDefinition name, but the CLI doesn't know that. So we do need to fix the CLI.

However, now that I think about this, I realize we may have been a little bit careless when doing that, because the current code basically *assumes* that if you have a composite comparator, then ColumnDefinition refers to the last component of those. That may break compatibility for user using composite type today. I'm not sure how many people use composite comparators *and* have ColumnDefinition on them, but we probably shouldn't assume nobody does that.

So I think to fix that we need to add a new information to ColumnDefinition, which if the comparator of the CF is a composite type, to which component the definition apply (or if it apply to the column name as a whole). Attached a patch that does just that. I'll note that while we could probably do with a boolean in that case (since currently we only have 2 cases: either the definition apply to the full column name or it applies to the last component), but the full generality of specifying which component exactly the definition apply to will likely be useful if we allow defining secondary index of other parts of the composite (CASSANDRA-3680). Note that the patch does fix the CLI following that change.
;;;","29/Mar/12 19:16;jbellis;bq. I'm not sure how many people use composite comparators and have ColumnDefinition on them, but we probably shouldn't assume nobody does that

given that you can't do that with the cli (as shown here) I think that's actually a pretty safe assumption.;;;","29/Mar/12 19:40;slebresne;bq. given that you can't do that with the cli

Yes you can. What the cli doesn't support is the new-in-1.1 way of interpreting ColumnDefinition for CF having a composite comparator as refering to the last component. It is perfectly possible in pre-1.1 to have a composite comparator and declare a ColumnDefinition. What I'm saying is that currently (i.e without the attached patch), 1.1 breaks compatibility with pre-1.1 for that use case, which is a bad thing.;;;","29/Mar/12 20:05;xedin;I confirm that CLI works as expected with the patch but I'm not sure that adding new option to the ColumnDef especially because users can simply ignore it without even given it a thought. Feels like we are adding complexity from pure air...;;;","29/Mar/12 21:21;jbellis;I think we'll regret the precedent in trying to preserve all the arbitrarily complex things you could do with the old CFMetadata.  It really doesn't make sense to have a CompositeType as a column name.  Given that, and given that CFMetadata names under <= 1.0 were only useful for validation (and indexes, but we don't index CTs yet either), I'd rather say ""we don't support that in 1.1+, and if you were doing that, drop the column definition before you upgrade.""

Optionally, we could auto-drop it for them and log a warning.;;;","29/Mar/12 23:19;xedin;I like that more than adding ""component_index"" option which would be easily misconfigured and create a confusion. Right now we should change comparators of the schema_* CFs for that to work and add a check to the ThriftValidation and CQL validations to prevent users from setting CompositeType comparators to the new CFs along side with check when schema is converted to 1.1...;;;","30/Mar/12 07:51;slebresne;bq. but we don't index CTs yet either

That's not true. We do support indexes on full CT currently (by the mere fact that CompositeType is a fully functional AbstractType). What we don't support (yet) is indexing on a specific component of a CT.

Now, we agreed on CASSANDRA-3782 that it could sometimes be usefull to index a specific column in a wide row as a way of 'tagging' such wide rows (and there is probably other useful use cases). Why would that suddently stop making sense because the name is a composite one?

So this is supported and this does have some reasonable use cases. As such, I *strongly* think that it would be unreasonable to break it (and in that case, auto-dropping the index for people (or even forcing them to do it) would be the worst possible user experience ever). And I even think we should seriously give some though about maintaining that possibility (the ability to index on a full column name) even in CQL3 (which will likely be a simple and logical extension of CASSANDRA-3792).

bq. trying to preserve all the arbitrarily complex things you could do with the old CFMetadata

I disagree that this is arbitrary complex. The patch adds the ability to specify for a ColumnDefinition to which component it refers to. If we want to support secondary indexes on specific component of a composite column (which I think is the goal of CASSANDRA-3680), we'll have to add that exact information to ColumnDefinition anyway. So I think it's really adding new and useful flexibility.
;;;","30/Mar/12 14:17;jbellis;bq. we agreed on CASSANDRA-3782 that it could sometimes be usefull to index a specific column in a wide row as a way of 'tagging' such wide rows 

But you don't need to support indexing the CT column itself, to support that use case.  More consistent to have a ""tag"" sparse column and index that instead.

bq. really adding new and useful flexibility

New, yes; useful? dubious.

We've barely educated people on CQL3 wide rows as it is, now is not a good time to complicate things further.  Let's take an incremental approach and add such things in response to user demand, should it materialize, instead of adding features ""because we can."";;;","30/Mar/12 14:40;slebresne;bq. But you don't need to support indexing the CT column itself, to support that use case. More consistent to have a ""tag"" sparse column and index that instead.

But my concern is for backward compatibility. Currently (with 0.8-1.0), you *cannot* support this with a tag sparse column that you index because we *don't* support indexing on a component of a CT (even 1.1 doesn't support it yet). I'm not talking of adding some new possibility that may or may not be useful. I'm saying that if someone is indexing on the CT column itself, which again is currently the *only* solution and btw wcan be done with pure thrift that we have swear not to break, then we'll completely break these users application (and as if that wasn't bad enough, they won't even have a correct replacement at the moment).;;;","30/Mar/12 15:50;jbellis;The thing about backwards-compatibility hacks is you tend to get stuck with them.  I'm okay with any of these alternatives:

# Add a big huge warning to NEWS that this corner case is not supported to 1.1, but an alternative solution for ""tagging"" will be supported eventually.  Fail 1.1 startup if such a schema startup is detected and explain the new limitation
# Wait until we actually have indexes on composite PKs supported to release 1.1
# Ask users@ if anyone actually has such a use case in production, and go with 1 or 2 based on the responses

Here is why I'm willing to go scorched earth on this:

- Schema design is *the* most difficult thing to explain to new Cassandra users. We've made *huge* strides towards simplifying this in CQL3 and composite PKs. I don't think we can afford to dilute this with footnotes about how you can escape hatch back to the old world, dragging in all our old legacy baggage again. Put another way: it's time to bury ""wtf is a supercolumn"" once and for all.
- Of our hundreds of deployments of 0.8 and 1.0, I can live with one or two needing to wait for 1.1.2 or whatever to upgrade.  We have a pretty good feel at this point for commonly used features and this isn't one of those.

TLDR: backwards compatibility in this specific case is high cost, low benefit.;;;","30/Mar/12 16:59;slebresne;bq. The thing about backwards-compatibility hacks is you tend to get stuck with them.

I suppose it depends of ones own definition of a hack, but I really don't consider this patch as a hack. As soon as we consider CT to be first class citizen, adding the ability to specify to which component a ColumnDefinition applies (which is all the patch does) feels like a rather natural extension to me. And as said, if we decide to allow secondary indexes on any component of a CT, which I though was something we wanted ultimately, we'll have to add that componentIndex information. So breaking compatibility to avoid adding a simple info to ColumnDefinition that we'll very likely end up adding anyway later feels like the wrong choice to me.

bq. We've made huge strides towards simplifying this in CQL3 and composite PKs. I don't think we can afford to dilute this with footnotes about how you can escape hatch back to the old world

I don't understand. As of the attached patch, there is 0 footnotes to add to CQL3. Currently it's pretty much *exclusively* a thrift backward-compatibility patch. If you're referring to me saying that we could later consider allowing index on a full composite in CQL3, then I'm sorry, let's forget about that for now. This patch does not force us at all to do that.

bq. dragging in all our old legacy baggage again. Put another way: it's time to bury ""wtf is a supercolumn"" once and for all.

I don't understand what you're referring to. Again, what this patch does is recognizing that CT are now first class citizen and thus that it makes sense that a ColumnDefinition may refer to any of the component. And while it is true that we don't *yet* use that generalization, it *trivially* allows us to support backward compatibility for thrift, hence the idea of doing that generalization now. I don't see where this patch drag *any* old legacy baggage so I'd love to get more precision on what you're referring to exactly. 

bq. Of our hundreds of deployments of 0.8 and 1.0, I can live with one or two needing to wait for 1.1.2 or whatever to upgrade.

But that is *not* what will happen. Anyone that use an index on a CT currently won't be able to upgrade *ever*. Because their index will just be not be supported anymore. If they upgrade, C* will crash. Seriously, they will be completely screwed. Their only option will be some complex manual data model migration which will be impossible to do without any downtime for the application (since no version will both support their existing index *and* the new possibly-introduced-in-1.2 indexing of the the last component of a CT).

bq. TLDR: backwards compatibility in this specific case is high cost, low benefit.

I couldn't disagree more. I still don't see what this high cost is. I wrote the patch and I just don't see at all the high cost. The patch is fairly trivial (it's not small in size but for a good part because it include thrift generated code) and has barely any user side visibility. More precisely, the only visibility it has is on the thrift side, and even then it's an optional field whose default is the right one for the thrift side. On the other side, the benefit is huge imo because it means we don't screw up the people that use C* in original ways.;;;","30/Mar/12 17:35;dbrosius@apache.org;>> Ask users@ if anyone actually has such a use case in production, and go with 1 or 2 based on the responses

This makes sense to me... it's possible that this isn't really an issue.;;;","30/Mar/12 23:07;xedin;Would users be required to manually set ""component_index"" at every metadata column they have after update to 1.1 or to rephrase that - when users are going to be *required* to set ""composite_index"" field?;;;","31/Mar/12 12:07;slebresne;bq. Meaning there will be C* versions which support both?

Only if 1) they use thrift and 2) they ever want to define a ColumnDef that apply to one of the component of a CT. Currently, 2) doesn't have any interest on the thrift side, except maybe as a preparation if you're going to switch to cql3.

In other word, you will never be *required* to set the field. ;;;","31/Mar/12 13:23;xedin;If users are never required to set it for everything to work, what is the benefit of adding new field at the first place? ;;;","31/Mar/12 13:47;slebresne;Because the system CFs are using CQL3 style metadata so that we can display them nicely when queried with CQL3. So typically the CLI (or anyone else) needs that info to be able to correctly decode and display the system table definitions.

Besides, when we add the ability to index the components of a CT, the fact that component_index is exposed through thrift means that the thrift user will be able to use that new functionality, which while probably not a priori is still nice.

Lastly, the field is useful internally in ColumnDefinition so not adding it to ColumnDef (to say avoid confusing thrift user) would imo be a bit dangerous (it would make using both thrift and CQL3 at the same time difficult, which may be useful for people transitioning to CQL3). Overall I really don't think adding the new field is such a big deal. It is true that is is currently mildly useful for pure thrift user, but it's not like it's something new. All the aliases (key_alias, column_aliases, value_alias) are only useful to CQL3 but yet have been added on the thrift side. This is not really different.;;;","31/Mar/12 14:17;xedin;Sounds like it wasn't a good time to make schema_* CFs to use CQL3 style metadata which breaks all other parties and causes half-hacky field (dispute the fact that it is not even useful yet) to be added to the thrift structure just to support correct data display even if that is ambiguous for users how/when to correctly use it...;;;","31/Mar/12 14:30;jbellis;bq. Sounds like it wasn't a good time to make schema_* CFs to use CQL3 style metadata 

I'm still convinced that part is worth it to be able to query schema information without thrift describe_ methods.;;;","31/Mar/12 14:32;xedin;bq. I'm still convinced that part is worth it to be able to query schema information without thrift describe_ methods.

Ok but it's only useful with CQL3 because all others wouldn't be able to query that correctly like CLI because of the way column metadata is stored right now.;;;","31/Mar/12 14:33;jbellis;bq. the field is useful internally in ColumnDefinition so not adding it to ColumnDef (to say avoid confusing thrift user) would imo be a bit dangerous 

Here's what I don't get.  We're adding this to support backwards compatibility, and yet we need to change the interface to do it?  Clearly a 0.8 or 1.0 client isn't going to be aware of this new field.

bq. All the aliases (key_alias, column_aliases, value_alias) are only useful to CQL3 but yet have been added on the thrift side

Because we added those before we had cqlsh, so the cli was the only way to configure them.  In retrospect, not a great idea.;;;","31/Mar/12 14:35;jbellis;bq. Ok but it's only useful with CQL3 because all others wouldn't be able to query that correctly like CLI because of the way column metadata is stored right now.

Granted, which is why we need to make the cli aware of column aliases.;;;","31/Mar/12 14:43;xedin;bq. Granted, which is why we need to make the cli aware of column aliases.

The problem with that it's not only about CLI it is also about all other possible clients too because users expect comparator to be able to {de-}serialize column names correctly. So we should make it very clear how to work with this type of situation without making any special cases (e.g. for CT).

bq. Because we added those before we had cqlsh, so the cli was the only way to configure them. In retrospect, not a great idea.

I also don't think that having aliases in Thrift really justifies this situation.;;;","31/Mar/12 14:52;jbellis;bq. If you're referring to me saying that we could later consider allowing index on a full composite in CQL3, then I'm sorry, let's forget about that for now. This patch does not force us at all to do that.

My problem is that by supporting this misfeature from thrift in the name of backwards compatibility, we're only pushing the problem into the future: the next step is for someone to say, ""you support this in Thrift, so I should be able to do it in CQL too.""  I'd rather just draw a line and say, ""allowing this was a bad idea and we don't support it anymore.""

bq. while it is true that we don't yet use that generalization

My point is that I'm pretty sure this is a generalization I don't want to support at all.  This is a case where exposing every detail of your storage engine is a bad idea.

bq. Anyone that use an index on a CT currently won't be able to upgrade ever

The upgrade path requires some effort but is conceptually simple:

# update your application to no longer use the CT column index
# upgrade

(If you respond that updating your application is not acceptable, then why are you bothering to upgrade?  Stay on the stable version that you built against in the first place, there is nothing wrong with that.)

My claim is that inflicting this on a small handful of users (possibly as small as zero, and I would bet money not more than one) is worth the upside of getting to a clean data model in 1.2 without the distractions of legacy features like this.  The danger is that the longer we preserve features like this, the more potential there is for new users to start using them which makes it more difficult for us to drop them later.  So I'd rather make a clean break now, than drag it out.

That said, if you are fundamentally opposed to not dropping any feature no matter how obscure without some warning, which I admit is at least a consistent position, I would be okay with supporting this in 1.1 with a clear intention to drop it in 1.2.  (Which would imply to me that we leave the thrift interface alone.)  This would give us a full release to make sure we have alternatives to whatever use cases people may have for the CT index (e.g. indexing the sparse columns for the tag scenario).;;;","31/Mar/12 15:06;jbellis;bq. The problem with that it's not only about CLI it is also about all other possible clients too because users expect comparator to be able to {de-}serialize column names correctly

But this is only a problem if you're trying to show the schema for the System keyspace.  (Presumably if you're using column aliases in your own CFs, you're using a sufficiently up to date client already.)

I'm willing to accept that ""to introspect the system keyspace, you need to be using a 1.1-aware cli."";;;","31/Mar/12 15:13;xedin;Would that assumption still hold when users are going to start creating CFs using CQL3?;;;","31/Mar/12 15:42;jbellis;""Presumably if you're using column aliases in your own CFs, you're using a sufficiently up to date client already.""

In other words, your upgrade path is

# upgrade C* to 1.1
# upgrade cli to 1.1
# start creating CQL3 CFs

In particular, 3 doesn't work before 1, at which point 2 is trivial.;;;","31/Mar/12 15:53;xedin;I'm fine fixing it just for CLI then.;;;","31/Mar/12 17:41;slebresne;Wow, that seems to get so out of proportion and I so don't understand why.

What happpened is this: currently (i.e. in all released version), a ColumnDefinition name refers to the CF column name, i.e. one must use the column comparator to decode it. In all currently released version, CompositeType is a normal comparator and that rule applies to it, and for anyone that will use thrift in 1.1+, for all intent and purposes CompositeType will still be a normal comparator like all the other ones and so the natural thing will still be that the ColumDefinition name applies to CT comparator entirely.

Now when I wrote the CQL3 patch (the initial one), I realised that we needed a new feature, the need to be able to have ColumnDefinitions whose name refers to only one of the component of the CT comparator. And while  we currently only need to refer to the last component (because we don't yet support secondary indexes on CT), we *will* need to have ColumnDefinition whose name refer to *any* of the CT components as soon as we support secondary indexes on CT (CASSANDRA-3680). In other words, the introduction of composite_index (this patch) is just the first part CASSANDRA-3680. We *will* have to add it soon enough. It is *hardly* something added *just* for backward compatibility.

Anyway while I was writting the CQL3 patch, instead of properly handling that new case, I introduced a *bug* consisting of changing the behavior of ColumnDefinition for CT so that by default they refer to the last component of the CT, not the comparator itself anymore. I.e. I changed the default behavior to an incompatible one.  I'm sorry I did that, I shouldn't have and the goal of that patch is to restore the default behavior and introduce a new info to ColumnDefinition to allow switching to the new behavior. Again, I think it is incorrect to say that this patch is just 'in the name of backward compatibility'. If I had mistakefully changed the subcolumn comparator of super column to apply to column names instead of super columns names, we wouldn't call fixing that 'supporting a misfeature in the name of backward compatibilty'. But yes, this bug happens to break backward compatibility and I do have a big problem with that part.

On the thrift side. Yes this does add a new field to ColumnDef. But:
# It's untrue that column_aliases and value_alias were 'added before we had cqlsh'. They would be added in the exact same version as this patch, so I think that using that argument against component_index is unfair.
# I'm not sure this will be so very confusing to users. Again, for thrift users, CT is a comparator like any other. The confusing thing would be for a ColumnDef to apply to the last component rather than the full CT. Why would CT be suddenly special (again, on the thrift side) and why the last component? It would random and thus confusing. On the contrary, if you have component_index, then you can say that the default is the one you'd expect, i.e the one that apply to all other comparator, but that we've added the new ability to make ColumnDef apply to other component.
# As soon as CASSANDRA-3680 is done, it will be usefull to allow creating secondary indexes on CT component on the thrift side. Why wouldn't we allow CASSANDRA-3680 on the thrift if it only cost us the addition of a simple int field in ColumnDef?

{quote}
The upgrade path requires some effort but is conceptually simple:

# update your application to no longer use the CT column index
# upgrade
{quote}

My problem is that step 1 will be *super* painfull. Because before upgrading, you don't have any reasonable alternative for whatever you were doing with the CT column index (tagging rows for instance). So you have to write manual code to do the same manual indexing. And then you have to write a map reduce job to reconstruct this new manual index for existing rows. In real life situations where you must do that without downtime, it's a pain in the ass.

And yes I know, you don't believe anyone uses that. But I have the weakness to think that we do not know what everyone user is doing. And yes, I have a *big* problem with screwing up even 1 user (especially after we've said we were serious about not breaking the thrift API). And yes, even if that force us to write a little bit more code now and/or later.

bq. is worth the upside of getting to a clean data model in 1.2 without the distractions of legacy features like this. The danger is that the longer we preserve features like this, the more potential there is for new users to start using them which makes it more difficult for us to drop them later

Ok, I understand the argument in principle but in practice what is it we're talking about?  There is 0 line of specific code to support index on a full CT column name, this is all handled by the current secondary index code transparently.  So worst case scenario, more people use the feature (which btw would suggest it's vaguely useful). But then adding support in CQL3 would be fairly trivial. The only thing we'd have to add is way to declare those index (again the rest is already handled by generic code), and that would likely be a nature extension of whatever we come up for CASSANDRA-3782. Typically, to reuse one of the proposed syntax on that issue, on top of supporting:
{noformat}
CREATE INDEX index_name ON timeline(0);
{noformat}
we would have to also add support for
{noformat}
CREATE INDEX index_name ON timeline(0, 'foobar', 4);
{noformat}
Again, this is hardly making the data mode in 1.2 dirty. Actually I'm not even sure I understand in what way that would hinder the compression of the data model one bit. And again, we don't *have* to support it if we don't want to. But at least, instead of forcing a painful upgrade on those that were using index on full CT from thrift when *we* decide, they would have the option to stay with thrift until they are ready to make the migration ().

Overall, I completely disagree that this patch will force anything on us in the future and I just don't see where it make the data model for 1.2 less clean.  I also strongly think we should be more considerate about breaking use cases that people may be using, even if we're talking very few people.
;;;","05/Apr/12 19:50;jbellis;bq. I'm not sure this will be so very confusing to users.

What I'm worried about is that it forces us to distinguish between ""logical"" and ""physical"" columns again.  I *love* that with CQL3 all I have to talk about is CQL columns and not have to dig out my diagrams of mapping CT to logical columns until someone starts to actually dig into the engine code.

bq. the introduction of composite_index (this patch) is just the first part CASSANDRA-3680. We will have to add it soon enough. It is hardly something added just for backward compatibility.

I don't follow at all.  3680 just means we want to be able to create an index on a logical column that is part of a CT under the hood: i.e., exactly the same thing we can already represent with the current-as-of-today CFMetadata.

I have virtually zero interest in supporting partial indexes as discussed in 3782; RDBMSes have shown pretty conclusively that this is a very niche feature.  Very much in the category of ""let's take our time and add it if it makes sense, not just because we know how to do it.""

bq. It's untrue that column_aliases and value_alias were 'added before we had cqlsh'. 

You left out key_alias, which is what I was referring to having added in 0.8 well before we had cqlsh.  I can only guess that we added column_aliases and value_aliases to thrift as well for the sake of consistency with that precedent.  As you point out, though, it's not too late to rip those out and we probably should.

bq. As soon as CASSANDRA-3680 is done, it will be usefull to allow creating secondary indexes on CT component on the thrift side. Why wouldn't we allow CASSANDRA-3680 on the thrift if it only cost us the addition of a simple int field in ColumnDef?

Sounds like you're getting a little ahead of yourself.  Why not add it as part of 3680, should that be something we want to do?  (But as I described above, I don't think it is.);;;","05/Apr/12 20:00;jbellis;bq. What I'm worried about is that it forces us to distinguish between ""logical"" and ""physical"" columns again. 

That said, you're probably right that for CQL users this is an implementation detail that they won't care about.  And Thrift users can carry on with CT definitions to the degree they have or haven't previously, I suppose.

But, that doesn't mean we should gratuitously blur the lines between the two.

So here is what I propose:

- Change ColumnDefinition back to its old behavior.  I guess we'll need to add the int to allow us to support CQL3 internally, but we don't need to expose it to thrift yet, if at all.  (Alternatively we could add a cql_column_metadata that supports the new semantics.)
- Remove column_aliases and value_aliases from Thrift.  They serve no purpose there than to give users a gun with which to shoot themselves in the foot.
;;;","06/Apr/12 14:31;slebresne;bq. I don't follow at all. 3680 just means we want to be able to create an index on a logical column that is part of a CT under the hood

Hum, I though that parts of CASSANDRA-3680 was that say you have
{noformat}
CREATE TABLE t (
  rk text,
  k1 text,
  k2 text,
  c1 text,
  c2 text,
  PRIMARY KEY (rk, k1, k2)
);
{noformat}
then we could allow defining something like:
{noformat}
CREATE INDEX ON t(k1);
{noformat}
(or the same with k2).

Now truth is I haven't given a tons of though to this and if that is what you call partial indexes (it's not exactly the same than CASSANDRA-3782), then maybe it does have little convincing use cases. In which case, fair enough.

bq. for CQL users this is an implementation detail that they won't care about. And Thrift users can carry on with CT definitions to the degree they have or haven't previously

That's exactly what I meant :)

bq. Alternatively we could add a cql_column_metadata that supports the new semantics

Not necessarily against the idea but it feels like the integer makes for less special casing internally.

bq. Remove column_aliases and value_aliases from Thrift. They serve no purpose there than to give users a gun with which to shoot themselves in the foot

I'm good with that. There may just be 2 things to be careful with when doing that:
* we probably cannot assume just yet that no user will do a mix of access through thrift and through CQL. We must make sure one doesn't destroy the CQL metadata while doing a simple column family update from thrift. Shouldn't be too hard to do though.
* exposing everything through thrift have the slight advantage that a use may say set a column alias to allow transitioning to CQL3. There is likely better way to probide that though.

Anyway, I'll update the patch so that it doesn't expose those and component_index through thrift, and to not return ColumnDefinition when component_index is not null through thrift (so that the cli don't get confused).
;;;","06/Apr/12 14:43;jbellis;bq. we could allow defining something like {{create index on t(k1)}}

Ah, right.  Yes, I suppose there's no reason not to expose that to Thrift as well.;;;","06/Apr/12 16:15;slebresne;Attaching 4093_v2. Internally, this is roughtly the same as the previous patch (using componentIndex). However, this patch doesn't not expose it through thrift and remove column_aliases and value_alias (from thrift) too.

Note that it completely skip ColumnDefinition whose componentIndex is not null (i.e those created by CQL3) when translating to thrift, which fixes the CLI problem (basically the CLI don't get back the parts he don't know how to interpret).

An additional change is that CFMetaData.apply() make sure that a thrift update won't wrongfully remove CQL3 only metadata (ie. columnAliases, valueAliases and ColumnDefinition with a non-null componentIndex). In other words, it's safe to create a column family through CQL3 and later update it with the cli (to avoid foot shooting for those that would be in the middle of transitioning to CQL3 for instance).

The only small detail is that we *need* to backport CASSANDRA-4037 to 1.1.0 for this to work (since currently CQL do a toThrift(fromThrift()) dance for schema update (in order to use ThriftValidation)).

bq. Yes, I suppose there's no reason not to expose that to Thrift as well.

I'm actually happy with the idea of v2 of not exposing to thrift what's not usefull right now. I guess we can go with v2 and expose componentIndex later when it makes sense on the thrift.
;;;","06/Apr/12 19:10;jbellis;bq. I guess we can go with v2 and expose componentIndex later when it makes sense on the thrift.

That's what I had in mind.

We should keep dclocal_r_r as parameter 37 in the thrift idl to make it compatible w/ beta clients.
Otherwsise, +1 on v2 and backport of 4037.;;;","09/Apr/12 16:58;slebresne;Backported CASSANDRA-4037 and committed v2 (with dc_local_rr back at 37). Thanks.;;;",,,,,,,,,,,,,,,,,,
cqlsh can't handle python being a python3,CASSANDRA-4090,12548273,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ash211,ash211,ash211,27/Mar/12 07:34,16/Apr/19 09:32,14/Jul/23 05:52,03/Apr/12 16:12,1.0.10,1.1.0,,Legacy/Tools,,,0,cqlsh,,,,,,"cqlsh fails to run when {{python}} is a Python 3, with this error message:

{code}
andrew@spite:~/src/cassandra-trunk/bin $ ./cqlsh 
  File ""./cqlsh"", line 97
    except ImportError, e:
                      ^
SyntaxError: invalid syntax
andrew@spite:~/src/cassandra-trunk/bin $ 
{code}

The error occurs because the cqlsh script checks for a default installation of python that is older than a certain version, but not one newer that is incompatible (e.g. Python3).  To fix this, I update the logic to only run {{python}} if it's a version at least 2.5 but before 3.0  If this version of python is in that range then role with it, otherwise try python2.6, python2.7, then python2.5 (no change from before).

This is working on my installation, where {{python}} executes python 3.2.2 and doesn't break backwards compatibility to distributions that haven't made the jump to Python3 as default yet.","On Archlinux, where Python3 installations are default (most distros currently use Python2 as default now)

{code}
$ ls -l `which python` 
lrwxrwxrwx 1 root root 7 Nov 21 09:05 /usr/bin/python -> python3
{code}",ceefour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/12 21:36;thepaul;4090.patch.txt;https://issues.apache.org/jira/secure/attachment/12521052/4090.patch.txt","27/Mar/12 07:35;ash211;python3-fix.patch;https://issues.apache.org/jira/secure/attachment/12520076/python3-fix.patch",,,,,,,,,,,,,,,,2.0,ash211,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233370,,,Tue Apr 03 16:12:34 UTC 2012,,,,,,,,,,"0|i0grun:",95942,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"27/Mar/12 07:37;ash211;Patch attached.;;;","02/Apr/12 21:36;thepaul;I would argue that Archlinux is the broken one, in this respect: nearly everything executable in the Python ecosystem still expects an unqualified ""python"" to be python2.

But oh well. This is an easy improvement to make. The logic in the original patch is backward; you want sys.exit to exit with False when the version is good (since False becomes 0, which shell treats as success). Adjusted a little for brevity and inverted logic.;;;","03/Apr/12 02:14;ash211@gmail.com;Paul's patch looks good to me, and I agree that python being a python3
is nonstandard. An easy fix to support though, and flipping the error
code is right too.

Anything else needed before applying the patch to trunk?

Thanks!

On Apr 2, 2012, at 16:37, ""paul cannon (Updated) (JIRA)""

;;;","03/Apr/12 15:36;thepaul;+1 from me.;;;","03/Apr/12 16:12;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo fix: key_valiation_class -> key_validation_class,CASSANDRA-4089,12548263,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,ash211,ash211,27/Mar/12 06:32,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 22:46,1.0.0,,,Legacy/Documentation and Website,,,0,,,,,,,There is a typo in the Cli help docs for the update column family command.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/12 06:33;ash211;typo-fix-01.txt;https://issues.apache.org/jira/secure/attachment/12520070/typo-fix-01.txt",,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233360,,,Wed Mar 28 01:10:25 UTC 2012,,,,,,,,,,"0|i0gru7:",95940,,,,,Low,,,,,,,,,,,,,,,,,"27/Mar/12 06:33;ash211;diff --git a/src/resources/org/apache/cassandra/cli/CliHelp.
index 9e417dc..017b54e 100644
--- a/src/resources/org/apache/cassandra/cli/CliHelp.yaml
+++ b/src/resources/org/apache/cassandra/cli/CliHelp.yaml
@@ -692,7 +692,7 @@ commands:
           It is also valid to specify the fully-qualified c
           extends org.apache.cassandra.db.marshal.AbstractT
 
-        - key_valiation_class: Validator to use for keys.
+        - key_validation_class: Validator to use for keys.
           Default is BytesType which applies no validation.
 
           Supported values are:;;;","27/Mar/12 22:46;xedin;Committed, thanks!;;;","28/Mar/12 01:01;jbellis;Doesn't affect 1.0+?;;;","28/Mar/12 01:10;ash211;I observed it on 1.0.8

On Tue, Mar 27, 2012 at 6:03 PM, Jonathan Ellis (Commented) (JIRA) <

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
decom should shut thrift down,CASSANDRA-4086,12548228,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,27/Mar/12 02:12,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 17:31,1.0.9,1.1.0,,,,,0,,,,,,,"If you decom a node an then try to use it, you get nothing but timeouts.  Instead let's just kill thrift so intelligent clients can move along.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/12 16:28;brandon.williams;4086.txt;https://issues.apache.org/jira/secure/attachment/12520145/4086.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233325,,,Tue Mar 27 17:31:38 UTC 2012,,,,,,,,,,"0|i0grsv:",95934,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Mar/12 16:28;brandon.williams;Trivial patch to shut thrift down just before gossip/MS.;;;","27/Mar/12 16:32;jbellis;should we just have it call drain() instead of partially reimplementing it?;;;","27/Mar/12 16:56;brandon.williams;I've tried that, but it feels like it makes the logic for decom much less clear, and has a side effect that drain shuts the node down, which we don't want.;;;","27/Mar/12 17:10;jbellis;Why don't we want that?;;;","27/Mar/12 17:16;brandon.williams;Historically I think the reasoning is you may have packaging that automatically restarts the process, which is something you don't really want with decom, but isn't a huge problem for drain.  David apparently ran into this problem on CASSANDRA-1483.;;;","27/Mar/12 17:20;jbellis;WFM then, +1;;;","27/Mar/12 17:31;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Issue with cassandra-cli ""assume"" command and custom types",CASSANDRA-4081,12547942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,drew_kutchar,drew_kutchar,24/Mar/12 07:05,16/Apr/19 09:32,14/Jul/23 05:52,30/Mar/12 08:36,1.0.9,,,,,,1,,,,,,,"There seems to be an issue with cassandra-cli's assume command with a custom type. I get ""Syntax error at position 35: missing EOF at '.'""

To make sure the issue is not with my custom type, I tried it with the built-in BytesType and got the same error:

[default@test] assume UserDetails validator as org.apache.cassandra.db.marshal.BytesType;
Syntax error at position 35: missing EOF at '.'

I also tried it with single and double quotes with no success:
[default@test] assume UserDetails validator as 'org.apache.cassandra.db.marshal.BytesType';
Syntax error at position 32: mismatched input ''org.apache.cassandra.db.marshal.BytesType'' expecting Identifier


Based on the output of ""help assume"" I should be able to just pass a fqn of a class.

> It is also valid to specify the fully-qualified class name to a class that
> extends org.apache.Cassandra.db.marshal.AbstractType.
",Cassandra 1.0.7 on Mac OSX Lion,chengas123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/12 13:22;xedin;CASSANDRA-4081.patch;https://issues.apache.org/jira/secure/attachment/12520419/CASSANDRA-4081.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,233039,,,Fri Mar 30 08:36:18 UTC 2012,,,,,,,,,,"0|i0grqn:",95924,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"24/Mar/12 19:04;dbrosius@apache.org;It appears you need to supply one of these constants from the CliClient.Function

BYTES, INTEGER, LONG, INT, LEXICALUUID, TIMEUUID, UTF8, ASCII, COUNTERCOLUMN

and not class names. Given that it would appear to me that the doc is wrong about being able to specify custom class types.;;;","24/Mar/12 20:40;drew_kutchar;Seems like it, but I'd really prefer if I can pass in a fqn of a class. We are storing Jackson Smile encoded blobs as bytes in Cassandra but we would like to be able to get a human friendly output from cassandra-cli.;;;","30/Mar/12 08:36;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverflowError when upgrading to 1.0.8 from 0.8.10,CASSANDRA-4078,12547796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,wenjun@openf.in,wenjun@openf.in,23/Mar/12 13:44,16/Apr/19 09:32,14/Jul/23 05:52,02/Apr/12 21:36,1.0.10,,,,,,0,,,,,,,"Hello

I am trying to upgrade our 1-node setup from 0.8.10 to 1.0.8 and seeing the following exception when starting up 1.0.8.  We have been running 0.8.10 without any issues.
 
Attached is the entire log file during startup of 1.0.8.  There are 2 exceptions:

1. StackOverflowError (line 2599)
2. InstanceAlreadyExistsException (line 3632)

I tried ""run scrub"" under 0.8.10 first, it did not help.  Also, I tried dropping the column family which caused the exception, it just got the same exceptions from another column family.

Thanks
","OS: Linux xps.openfin 2.6.35.13-91.fc14.i686 #1 SMP Tue May 3 13:36:36 UTC 2011 i686 i686 i386 GNU/Linux

Java: JVM vendor/version: Java HotSpot(TM) Server VM/1.6.0_31
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/12 20:55;jbellis;4078-asserts-v3.txt;https://issues.apache.org/jira/secure/attachment/12521035/4078-asserts-v3.txt","23/Mar/12 20:34;thepaul;4078.add-asserts.txt;https://issues.apache.org/jira/secure/attachment/12519707/4078.add-asserts.txt","02/Apr/12 20:41;thepaul;4078.patch2.txt;https://issues.apache.org/jira/secure/attachment/12521032/4078.patch2.txt","26/Mar/12 22:46;wenjun@openf.in;cassandra.yaml.1.0.8;https://issues.apache.org/jira/secure/attachment/12520027/cassandra.yaml.1.0.8","26/Mar/12 22:46;wenjun@openf.in;cassandra.yaml.8.10;https://issues.apache.org/jira/secure/attachment/12520028/cassandra.yaml.8.10","30/Mar/12 20:33;wenjun@openf.in;keycheck.txt;https://issues.apache.org/jira/secure/attachment/12520664/keycheck.txt","23/Mar/12 13:46;wenjun@openf.in;system.log;https://issues.apache.org/jira/secure/attachment/12519621/system.log","26/Mar/12 16:57;wenjun@openf.in;system.log.0326;https://issues.apache.org/jira/secure/attachment/12519973/system.log.0326","26/Mar/12 18:20;wenjun@openf.in;system.log.0326-02;https://issues.apache.org/jira/secure/attachment/12519987/system.log.0326-02",,,,,,,,,9.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232893,,,Mon Apr 02 21:36:31 UTC 2012,,,,,,,,,,"0|i0grpj:",95919,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Mar/12 13:46;wenjun@openf.in;startup log from version 1.0.8;;;","23/Mar/12 16:14;jbellis;Looks like a problem w/ the intervaltree creation.;;;","23/Mar/12 20:34;thepaul;Wenjun, would it be possible to run your 1.0.8 version again (with DEBUG logging, as before) with this patch applied?

I haven't been able to figure out yet how this could get into an infinite recursion, but if we can identify an invalid assumption somewhere, that ought to help a lot.

If it's easier to get at, this change is also available in my github fork at git@github.com:thepaul/cassandra.git (branch named 4078).;;;","26/Mar/12 16:56;wenjun@openf.in;Paul
when I tried running code from git@github.com:thepaul/cassandra.git and I am getting the following errors.  The only changes I made in cassandra.yaml are path to data files and local IP address.

java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.

I don't really use git so I may have done something wrong to get your code.  The command I used is ""git clone git@github.com:thepaul/cassandra.git"".  I noticed your changes in IntervalNode.java were not included in the cloned code.  I copied the changes from 4078.add-aserts.txt.  ant was able to build without any issues.  How do I make sure the new classe files are included in cassandra runtime?

Attached is the complete log for startup.

Thanks
Wenjun
;;;","26/Mar/12 17:20;thepaul;Ah, you got trunk by default (unstable code for future Cassandra 1.2). You need to do ""git checkout 4078"" in that directory to switch to the 4078 branch.;;;","26/Mar/12 18:19;wenjun@openf.in;I am able to download the branch from github site.  Attached is the log file.

Thanks;;;","26/Mar/12 18:50;thepaul;Ok. It looks like there are indeed 6 places where something is trying to create an Interval where the min is  greater than the max. The following are taken from your log, right before the AssertionError, with some spaces added and other valid intervals cut out:

{noformat}
Interval(DecoratedKey(f3f31fef-9b24-4fe3-be8e-b54a43ccf867, f3f31fef9b244fe3be8eb54a43ccf867),
         DecoratedKey(62476171-4693-4a81-bd7c-01726bfca4c1, 6247617146934a81bd7c01726bfca4c1)),
Interval(DecoratedKey(c60e0fbc-dcc6-472c-8d5c-8b3cd041e4c9, c60e0fbcdcc6472c8d5c8b3cd041e4c9),
         DecoratedKey(7dd77e1f-92e0-4bc7-a417-2f9b0b97170d, 7dd77e1f92e04bc7a4172f9b0b97170d)),
Interval(DecoratedKey(920c1919-98c4-4da8-88da-325067f9ece3, 920c191998c44da888da325067f9ece3),
         DecoratedKey(40d6f31d-4b37-492e-b06c-9de0088de83f, 40d6f31d4b37492eb06c9de0088de83f)),
Interval(DecoratedKey(a5f8d3d8-d26a-47fd-904c-8d55784776fc, a5f8d3d8d26a47fd904c8d55784776fc),
         DecoratedKey(762f9402-e5f6-44bb-824c-f93170c2f508, 762f9402e5f644bb824cf93170c2f508)),
Interval(DecoratedKey(a27d2f3b-8255-42fb-a8d5-5f0fe488dadb, a27d2f3b825542fba8d55f0fe488dadb),
         DecoratedKey(7582809c-34b3-4736-8dfe-19811303bbfa, 7582809c34b347368dfe19811303bbfa)),
Interval(DecoratedKey(d72c5963-ebe8-4e13-a74b-5584ca1d053c, d72c5963ebe84e13a74b5584ca1d053c),
         DecoratedKey(4962307f-261d-469a-b48b-1de7cd7a7700, 4962307f261d469ab48b1de7cd7a7700)),
{noformat}

This shouldn't ever happen, so I'll do some poking around to see where these come from.;;;","26/Mar/12 22:24;thepaul;These values come right from the SSTables. I've been looking through code to see if I can find any possibility of bad values being written to an SSTable, but there might be a simpler explanation:

Is there any possibility that you have configured a different partitioner for Cassandra 1.0.8 from the one you used when writing the SSTables (0.8.10, I suppose)?;;;","26/Mar/12 22:47;wenjun@openf.in;I just attached yaml files for both 0.8.10 and 1.0.8 (from your branch).;;;","27/Mar/12 02:05;thepaul;Ok, looks like both are RandomPartitioner. I'll keep investigating.;;;","29/Mar/12 20:52;wenjun@openf.in;Paul, 

so you know: I just tried 1.1.0-beta2 and seeing the same error.;;;","29/Mar/12 22:24;thepaul;Yeah, the problem is almost certainly either (a) some important difference in the way it's being used now versus the way it was used before, or (b) some sort of bug in 0.8.10. I don't think 1.0.8 or 1.1.0 are acting improperly given the data and configuration here; I'm just hoping I can help you sort out whatever datatype expectation mismatch or invalid sstables you have.;;;","30/Mar/12 00:02;thepaul;Wenjun, are you able to tell from the UUID keys shown which ColumnFamily is having these troubles? I can't quite tell for sure from the logs. If necessary, we can add more asserts to identify the right place, but we can skip that step if you already know which one it is.;;;","30/Mar/12 16:21;wenjun@openf.in;Paul, yes, finally I am able to narrow it down to this one CF.  I truncated data from all other CFs, ran 'cleanp' and 'scrub', and it is still happening.  What else can I do to help debugging this issue?  The followings are definition of this CF:

    ColumnFamily: UserAgreementStatus
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.45468749999999997/1440/97 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: [UserAgreementStatus.userId]
      Column Metadata:
        Column Name: agreementName
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: response
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: userId
          Validation Class: org.apache.cassandra.db.marshal.LexicalUUIDType
          Index Name: userId
          Index Type: KEYS
        Column Name: viewTime
          Validation Class: org.apache.cassandra.db.marshal.LongType

Log right before the exception:
DEBUG [SSTableBatchOpen:2] 2012-03-30 12:09:20,277 SSTableReader.java (line 190) INDEX LOAD TIME for /home/cassandra/var/lib/cassandra/d
ata/Appoji/UserAgreementStatus.userId-f-56: 3 ms.
DEBUG [SSTableBatchOpen:2] 2012-03-30 12:09:20,277 SSTableReader.java (line 193) key cache contains 0/0 keys


;;;","30/Mar/12 19:23;thepaul;Wenjun- it would help at this point to be sure whether or not your sstables really do have keys which are out of order. Could you run the sstablekeys tool on the data file for that UserAgreementStatus CF? It looks like it is at {{/home/cassandra/var/lib/cassandra/data/Appoji/UserAgreementStatus/Appoji-UserAgreementStatus.userId-g-*-Data.db}}. We want to check that the byte sequences are in order:

{noformat}
$ for s in /home/cassandra/var/lib/cassandra/data/Appoji/UserAgreementStatus/Appoji-UserAgreementStatus.userId-g-*-{Data,Index}.db; do
>     echo ""checking $s""
>     bin/sstablekeys ""$s"" | sort -c
> done
{noformat}

If that doesn't give any output, then maybe there really is a bug in the 1.0/1.1 code that we need to look at.;;;","30/Mar/12 20:33;wenjun@openf.in;Paul, attache is results from sstablekeys

Thanks;;;","30/Mar/12 21:33;thepaul;Ok, so there are definitely out-of-order keys in your sstables. I have no good theories as to how that happened- I don't think anyone else has run into this. Were you possibly using any sort of custom types or parameterized types with 0.8.9? Custom anything?

Either way, the thing to be done now is probably to get your data sorted. Maybe we can come up with a one-off tool of some kind to read the values and write them back out in a sorted way.;;;","30/Mar/12 22:35;yukim;I don't have a clue about the cause, but since corrupted files are index column families, I think work around is to remove all those corrupted index sstables, upgrade C*, then rebuild index using 'nodetool rebuild_index'.;;;","02/Apr/12 14:37;thepaul;Yuki- there is an index sstable with out-of-order keys, and that one could be fixed as you suggest, but the actual data sstable also has out-of-order keys.;;;","02/Apr/12 15:34;wenjun@openf.in;I am able to fix the issue by doing the followings:

1. ran original command for creating the CF (replacing create with update) in cassandra-cli, got an error ""cannot modify index name"". (I looked it up in Cassandra mailing list and did not found any reference, so not sure what it means).

2. ran the command again WITHOUT index in column_metadata.

3. ran the command WITH index in column_metadata.

Now I am able to start 1.0.8 and 1.1.0-beta2.  I double-checked that CF and it seems all the data and index are fine.  So, let me know if I need to check anything else.

Thanks


;;;","02/Apr/12 15:46;thepaul;Oh, good. I'm glad that was doable.

Before we close the ticket, is there anything at all from your experience or Cassandra usage that may have been unusual- hardware, platform, storage method, custom data or index types, etc? I would really like at least to get some clues about how this happened, if possible.;;;","02/Apr/12 16:37;wenjun@openf.in;
I found something over the weekend: it seems 0.8.10 does not allow duplicate index names for different CFs.  We do have CFs having same name (such as userId) for their indexes, which means some earlier version must have allowed it (we started from 0.7.4).  Can this be the cause?

;;;","02/Apr/12 16:37;wenjun@openf.in;
I found something over the weekend: it seems 0.8.10 does not allow duplicate index names for different CFs.  We do have CFs having same name (such as userId) for their indexes, which means some earlier version must have allowed it (we started from 0.7.4).  Can this be the cause?

;;;","02/Apr/12 20:41;thepaul;Ok. We'll keep that info around in case we ever see something like this again. Thanks.

I do recommend that these asserts (patch attached, also at my pending/4078-2 tag in github) get committed to mainline Cassandra, though, to help protect against this sort of situation. The asserts in this patch will not catch all possible updates to SSTable.last, but they surround some of the more important (and less frequent) places.

Also, the asserts inside the IntervalTree creation should avoid getting into confusing StackOverflowError situations.;;;","02/Apr/12 20:55;jbellis;alternate patch attached that converts the ordering check in SSTW.beforeAppend to an assert;;;","02/Apr/12 21:03;thepaul;{noformat}
assert decoratedKey == null : ""Keys must not be null"";
{noformat}

(-1);;;","02/Apr/12 21:36;jbellis;committed w/ that fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScrubTest failing on current 1.1.0 branch,CASSANDRA-4077,12547767,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Mar/12 09:03,16/Apr/19 09:32,14/Jul/23 05:52,30/Mar/12 15:32,1.1.0,,,Legacy/Testing,,,0,,,,,,,"I get the following error:
{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 09:53:05,979 Corrupt sstable build/test/cassandra/data/Keyspace1/Super5/Keyspace1-Super5-f-2; skipped
    [junit] java.io.EOFException
    [junit] 	at java.io.DataInputStream.readFully(DataInputStream.java:180)
    [junit] 	at java.io.DataInputStream.readLong(DataInputStream.java:399)
    [junit] 	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.deserialize(ReplayPosition.java:133)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:206)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableMetadata$SSTableMetadataSerializer.deserialize(SSTableMetadata.java:194)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:155)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:483)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:86)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41)
    [junit] 	at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:220)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:518)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1052)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:906)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:87)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/12 15:16;slebresne;4077.txt;https://issues.apache.org/jira/secure/attachment/12520611/4077.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232864,,,Fri Mar 30 15:32:58 UTC 2012,,,,,,,,,,"0|i0grp3:",95917,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Mar/12 15:16;slebresne;This is because loadNewSSTable was incorrectly bumping the version of the sstables to the current version when renaming them. Trivial patch attached.;;;","30/Mar/12 15:28;jbellis;+1;;;","30/Mar/12 15:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: Tab completion should not suggest consistency level ANY for select statements,CASSANDRA-4074,12547710,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,22/Mar/12 23:30,16/Apr/19 09:32,14/Jul/23 05:52,14/Jul/12 21:22,1.1.3,,,,,,0,cql,cqlsh,,,,,consistency level ANY should not be suggested in tab-completion for SELECT statements,,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/12 23:42;thepaul;0001-cqlsh-don-t-suggest-CL.ANY-for-SELECT.patch;https://issues.apache.org/jira/secure/attachment/12536476/0001-cqlsh-don-t-suggest-CL.ANY-for-SELECT.patch",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232807,,,Sat Jul 14 21:22:27 UTC 2012,,,,,,,,,,"0|i0grnr:",95911,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Jul/12 23:42;thepaul;Patch attached; changes also available in the 4074 branch of my github. Current version tagged pending/4074.

https://github.com/thepaul/cassandra/tree/4074;;;","14/Jul/12 21:22;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.setMaxCompactionThreshold doesn't allow 0 unless min is also 0,CASSANDRA-4070,12547350,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Mar/12 08:41,16/Apr/19 09:32,14/Jul/23 05:52,22/Mar/12 12:45,1.0.9,,,,,,0,,,,,,,"Thrift allows to set the max compaction threshold to 0 to disable compaction. However, CFS.setMaxCompactionThreshold throws an exception min > max even if max is 0.

Note that even if someone sets 0 for both the min and max thresholds, we'll can have a problem because SizeTieredCompaction calls CFS.setMaxCompactionThreshold before calling CFS.setMinCompactionThreshold and thus will trigger the RuntimeException when it shouldn't.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/12 08:47;slebresne;4070.patch;https://issues.apache.org/jira/secure/attachment/12519201/4070.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232508,,,Thu Mar 22 12:45:19 UTC 2012,,,,,,,,,,"0|i0grm7:",95904,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Mar/12 08:47;slebresne;Trivial patch attached (against 1.0);;;","21/Mar/12 11:35;jbellis;+1, but for the record using min/max as ""disable compaction"" signal is kind of broken. ;;;","22/Mar/12 12:45;slebresne;Committed, thanks

I agree that we probably should have a better way to disable compaction. Actually given that leveled compaction pretty much ignore the max and min threshold, I think we should think about moving those to the compaction options.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bogus MemoryMeter liveRatio calculations,CASSANDRA-4065,12547162,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,doubleday,doubleday,doubleday,20/Mar/12 08:39,16/Apr/19 09:32,14/Jul/23 05:52,18/Apr/12 23:14,1.1.0,,,,,,0,,,,,,,"I get strange cfs.liveRatios.

A couple of mem meter runs seem to calculate bogus results: 

{noformat}
Tue 09:14:48 dd@blnrzh045:~$ grep 'setting live ratio to maximum of 64 instead of' /var/log/cassandra/system.log
 WARN [MemoryMeter:1] 2012-03-20 08:08:07,253 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:09,160 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:13,274 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:22,032 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:12:41,057 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 67.11787351054079
 WARN [MemoryMeter:1] 2012-03-20 08:13:50,877 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 112.58547951925435
 WARN [MemoryMeter:1] 2012-03-20 08:15:29,021 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 193.36945063589877
 WARN [MemoryMeter:1] 2012-03-20 08:17:50,716 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 348.45008340969434
{noformat}

Because meter runs never decrease liveRatio in Memtable (Which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?):

{noformat}
cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
{noformat}

Memtables are flushed every couple of secs:

{noformat}
ColumnFamilyStore.java (line 712) Enqueuing flush of Memtable-BlobStore@935814661(1874540/149963200 serialized/live bytes, 202 ops)
{noformat}

Even though a saner liveRatio has been calculated after the bogus runs:

{noformat}
INFO [MemoryMeter:1] 2012-03-20 08:19:55,934 Memtable.java (line 198) CFS(Keyspace='SmeetBlob', ColumnFamily='BlobStore') 
   liveRatio is 64.0 (just-counted was 2.97165811895841).  calculation took 124ms for 58 columns
{noformat}",,doubleday,thepaul,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,doubleday,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232320,,,Wed Apr 18 23:14:34 UTC 2012,,,,,,,,,,"0|i0grjz:",95894,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Mar/12 15:43;jbellis;bq. meter runs never decrease liveRatio in Memtable, which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?

I'm open to ways to improve this, but the idea is that the penalty for overestimating how big the memtable is (frequent flushes) is less severe than the penalty for underestimating (running out of memory and dying).;;;","20/Mar/12 15:44;jbellis;I wonder if we could use some kind of constantly-updated estimate instead of scanning the whole memtable periodically...;;;","20/Mar/12 17:02;doubleday;Maybe a simple solution would suffice: 

Don't use jamm but do some simplified estimation as 

estimate_size = $raw_size + $row_count * ROW_OVERHEAD + $column_count * COL_OVERHEAD

Since we know the used data structures ... and to keep it easy use 64b overhead since this will be the usual case anyway.

;;;","20/Mar/12 17:30;jbellis;That's probably possible in theory, but there isn't a fixed-size overhead for structures like CLHM.;;;","20/Mar/12 21:55;doubleday;bq. That's probably possible in theory, but there isn't a fixed-size overhead for structures like CLHM.

Well yes - I though of an approximation.

But maybe even easier (still conservative but able to heal):

{noformat}
if (newRatio > cfs.liveRatio) {
    cfs.liveRatio = newRatio;
} else {
    cfs.liveRatio = (cfs.liveRatio + newRatio) / 2.0;
}
{noformat}

 ;;;","18/Apr/12 23:14;jbellis;sounds reasonable to me.  committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decommission should take a token,CASSANDRA-4061,12546908,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,17/Mar/12 17:52,16/Apr/19 09:32,14/Jul/23 05:52,09/May/12 16:38,1.0.11,1.1.1,,,,,0,,,,,,,"Like removetoken, decom should take a token parameter.  This is a bit easier said than done because it changes gossip, but I've seen enough people burned by this (as I have myself.)  In the short term though *decommission still accepts a token parameter* which I thought we had fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/12 15:56;brandon.williams;4061.txt;https://issues.apache.org/jira/secure/attachment/12526175/4061.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232066,,,Wed May 09 16:38:13 UTC 2012,,,,,,,,,,"0|i0gri7:",95886,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"20/Mar/12 12:24;jonma;I think using ""./nodetool -h host -p port decommission "" is  more simple than using ""./nodetool -h host -p port decommision <the node's token>"" . If ""./nodetool -h host -p port decommision <the node's token>""  could be available ,it  can't be better.;;;","09/May/12 15:56;brandon.williams;To the dismay of my OCD, after looking at the options I think calling decommission on the host you want to decommission is the cleanest option we have.

However, as stated, nodetool should definitely not allow miscellaneous args to get passed, allow users to think it's doing something other than what it is.  Patch to fix this and clarify the help message, against 1.0 in the unlikely case we ever have reason to roll a 1.0.11.;;;","09/May/12 16:31;vijay2win@yahoo.com;+1;;;","09/May/12 16:38;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] guard against npe due to null sstable,CASSANDRA-4056,12546699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,16/Mar/12 00:48,16/Apr/19 09:32,14/Jul/23 05:52,16/Mar/12 10:28,1.1.1,,,,,,0,,,,,,,SSTableIdentityIterator ctor can be called from sibling ctor with a null sstable. So catch block's markSuspect should be npe guarded.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/12 00:49;dbrosius@apache.org;npe_guard.diff;https://issues.apache.org/jira/secure/attachment/12518587/npe_guard.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231857,,,Fri Mar 16 10:28:20 UTC 2012,,,,,,,,,,"0|i0grfr:",95875,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"16/Mar/12 00:49;dbrosius@apache.org;against trunk;;;","16/Mar/12 10:28;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hector RetryService drop host,CASSANDRA-4055,12546662,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,daningaddr,daningaddr,15/Mar/12 21:19,16/Apr/19 09:32,14/Jul/23 05:52,15/Mar/12 21:23,,,,,,,0,,,,,,,,"This bug is in Hector code.

If there is exception in addCassandraHost() before adding host to hostPools, since addCassandraHost does not throw exception, the host will be removed from downedHostQueue, and the host will be gone forever.

        if(downedHostQueue.contains(cassandraHost) && verifyConnection(cassandraHost)) {
          connectionManager.addCassandraHost(cassandraHost);
          downedHostQueue.remove(cassandraHost);
          return;
        }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231820,,,Thu Mar 15 21:23:10 UTC 2012,,,,,,,,,,"0|i0grfj:",95874,,,,,Critical,,,,,,,,,,,,,,,,,"15/Mar/12 21:23;jbellis;The Hector client is maintained at https://github.com/rantav/hector.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SStableImport and SStableExport does not serialize row level deletion,CASSANDRA-4054,12546566,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dr-alves,hanzhu,hanzhu,15/Mar/12 10:47,16/Apr/19 09:32,14/Jul/23 05:52,12/Jul/12 15:41,1.2.0 beta 1,,,Legacy/Tools,,,0,,,,,,,SSTableImport and SSTableExport does not serialize/de-serialize the row-level deletion info to/from the json file. This brings back the deleted data after restore from the json file.,,dr-alves,hudson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/12 16:59;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12536064/4054.patch","29/Jun/12 23:13;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12534069/4054.patch","21/Jun/12 04:31;dr-alves;4054.patch;https://issues.apache.org/jira/secure/attachment/12532821/4054.patch",,,,,,,,,,,,,,,3.0,dr-alves,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231724,,,Thu Jul 12 16:09:47 UTC 2012,,,,,,,,,,"0|i0grf3:",95872,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"15/Mar/12 21:17;jbellis;I think we could fix that by splitting the row into {'key': key, 'metadata': {...}, 'columns': [...]} instead of the current {key: [...]}.

Do you want to take a stab at that, Zhu?;;;","15/Mar/12 21:19;jbellis;Changing fix version to 1.1.0 since this would be backwards-incompatible.  If we miss 1.1.0 we can push to 1.2.;;;","21/Jun/12 04:36;dr-alves;Patch implements jbellis suggestion.
also:
- refactored SSTableImport's methods away from static (class had static internal state that was breaking tests in some configurations)
- added several tests to import that were missing (import sorted was only tested in the case we indicated we wanted to import sorted but there were unsorted keys).
- deletion info is serialized/deserialized with the same data as in DeletionInfo.Serializer.serializeToSSTable;;;","21/Jun/12 16:31;yukim;SSTableImport seems it does not handle SuperColumn deletion as well. (It has been commented out for long time.)
How about extending writeMeta/parseCFMetadata to accept SuperColumn (or more precisely, AbstractColumnContainer) and use them when handling both row and SuperColumn?
Also, it would be great if you remove whitespaces on empty line and always put braces on new line.;;;","29/Jun/12 23:13;dr-alves;additionally implements Yuki's suggestion. also deals with formatting and whitespace issues.;;;","29/Jun/12 23:20;jbellis;bq. I think we could fix that by splitting the row into {'key': key, 'metadata': {...}, 'columns': [...]} 

Thinking about it more ... do we really want a meta[data] object with just a single sub-object?  should we just pull deletion_info out and make it a top-level field and dispense with the meta container?

Nit: would prefer to spell out ""columns"" vs ""cols"";;;","29/Jun/12 23:33;dr-alves;the only point I can think of for the metadata container is to ease maintenance/compatibility down the line (i.e. writing/parsing metadata can change without having impact on the overall format), then again the less verbose the output the better (previous format had that in mind and that's why I used ""meta"" and ""cols"" instead of ""metadata"" and ""columns""). I'm happy either way. wdyt?
;;;","10/Jul/12 14:43;dr-alves;Yuki is the implementation on the latest patch what you had in mind?
wrt to pulling deletionInfo up and abandoning metadata +0, I'm happy either way.;;;","11/Jul/12 14:58;yukim;David, 
As you stated above, I think it is ok to leave metadata container.
So, I'm +1 here, only if it were ""metadata"" and ""columns"".
Could you make those changes and repost the patch?;;;","11/Jul/12 16:59;dr-alves;patch without abbreviated names;;;","12/Jul/12 15:40;yukim;+1 and committed. Thanks, David!;;;","12/Jul/12 16:09;hudson;Integrated in Cassandra #1691 (See [https://builds.apache.org/job/Cassandra/1691/])
    new json format with row level deletion; patch by David Alves, reviewed by yukim for CASSANDRA-4054 (Revision d569f873de40f0336a9a34e260c1942866e48950)

     Result = ABORTED
yukim : 
Files : 
* test/resources/SuperCF.json
* test/resources/UnsortedCF.json
* test/resources/SimpleCF.oldformat.json
* test/resources/CounterCF.json
* src/java/org/apache/cassandra/tools/SSTableExport.java
* CHANGES.txt
* test/unit/org/apache/cassandra/tools/SSTableImportTest.java
* test/resources/SimpleCFWithDeletionInfo.json
* test/resources/SimpleCF.json
* test/unit/org/apache/cassandra/tools/SSTableExportTest.java
* src/java/org/apache/cassandra/tools/SSTableImport.java
* test/resources/UnsortedSuperCF.json
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingTcpConnection can not be closed when the peer is brutaly terminated or switch is failed,CASSANDRA-4053,12546556,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,hanzhu,hanzhu,15/Mar/12 10:09,16/Apr/19 09:32,14/Jul/23 05:52,24/Sep/13 07:59,2.0.2,,,,,,0,,,,,,,"IncomingTcpConnection has no way to detect the peer is down when the peer meets power loss or the network infrastructure is failed, and the thread is leaked...

For safety, as least SO_KEEPALIVE should be set on those IncomingTcpConnections. The better way is to close the incoming connections when failure detector notifies the peer failure, but it requires some extra bookmarking.

Besides it, it would be better if IncomingTcpConnection and OutgoingTcpConnection is marked as daemon thread...

",,marcuse,stefan.fleiter@web.de,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 07:32;marcuse;0001-enable-keepalive-on-incoming-connections.patch;https://issues.apache.org/jira/secure/attachment/12604533/0001-enable-keepalive-on-incoming-connections.patch",,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231714,,,Tue Sep 24 07:59:01 UTC 2013,,,,,,,,,,"0|i0gren:",95870,,,,,Normal,,,,,,,,,,,,,,,,,"20/Sep/13 21:38;jbellis;Is this still relevant [~krummas]?;;;","23/Sep/13 07:32;marcuse;hmm it might actually be, attaching patch to enable tcp keepalive on incoming connections;;;","23/Sep/13 13:21;jbellis;+1;;;","24/Sep/13 07:59;marcuse;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stream sessions can only fail via the FailureDetector,CASSANDRA-4051,12546493,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,14/Mar/12 22:37,16/Apr/19 09:32,14/Jul/23 05:52,11/Apr/12 20:08,1.1.0,,,,,,0,streaming,,,,,,"If for some reason, FileStreamTask itself fails more than the number of retry attempts but gossip continues to work, the stream session will never be closed.  This is unlikely to happen in practice since it requires blocking the storage port from new connections but keeping the existing ones, however for the bulk loader this is especially problematic since it doesn't have access to a failure detector and thus no way of knowing if a session failed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4045,,,,,,,,,,,,,,,"27/Mar/12 18:57;yukim;4051-v2.txt;https://issues.apache.org/jira/secure/attachment/12520175/4051-v2.txt","30/Mar/12 17:54;yukim;4051-v3.txt;https://issues.apache.org/jira/secure/attachment/12520639/4051-v3.txt","16/Mar/12 18:06;brandon.williams;4051.txt;https://issues.apache.org/jira/secure/attachment/12518713/4051.txt",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231651,,,Wed Apr 11 20:08:22 UTC 2012,,,,,,,,,,"0|i0grdr:",95866,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"15/Mar/12 03:27;brandon.williams;It looks like we could extract/rebase the streaming changes from CASSANDRA-3112's first patch to solve this well enough for the bulk loader and BOF.;;;","16/Mar/12 18:06;brandon.williams;Updated patch extracted as mentioned, doesn't change any streaming behavior but does provide a way to detect errors that CASSANDRA-3112 and CASSANDRA-4045 can build on.;;;","27/Mar/12 16:42;yukim;Since CASSANDRA-3216 added IEndpointStateChangeSubscriber and IFailureDetectionEventListner to StreamOutSession, we need to keep that functionality. I proposed modified version of CASSANDRA-3112 except limiting retry part on CASSANDRA-3817, I would like to rebase that patch and add retry, so that I can post it here. (I will post it soon.);;;","27/Mar/12 18:57;yukim;Patch attached based on CASSANDRA-3817 with retry limit.
(I think it is nice to have retry limit per stream session, so that we can configure, say, no retry for bulk loading, which I think is enough. But that's beyond this issue.)

> Brandon

Can you test and see if BOF is OK with this patch?;;;","27/Mar/12 20:07;brandon.williams;BOF looks good, +1, committed.;;;","28/Mar/12 21:06;brandon.williams;Reopening because this only fixes the problem in one way, FileStreamTask can still fail all 8 times and never close the session.  In general, outbound streaming's ""fire and forget"" methodology is problematic for bulk loading.;;;","30/Mar/12 17:54;yukim;v3 attached for 1.1 branch.

It basically catches IOException on both sides and lets sessions closed.
I also implemented IStreamCallback#onFailure to make sure latches count down to avoid process hang.;;;","11/Apr/12 20:08;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rewrite RandomAccessReader to use FileChannel / nio to address Windows file access violations,CASSANDRA-4050,12546471,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,jn,jn,14/Mar/12 20:22,16/Apr/19 09:32,14/Jul/23 05:52,07/Apr/14 20:52,2.2.0 beta 1,,,Legacy/Local Write-Read Paths,,,1,Windows,,,,,,"On Windows w/older java I/O libraries the files are not opened with FILE_SHARE_DELETE.  This causes problems as hard-links cannot be deleted while the original file is opened - our snapshots are a big problem in particular.  The nio library and FileChannels open with FILE_SHARE_DELETE which should help remedy this problem.

Original text:
I'm using Cassandra 1.0.8, on Windows 7.  When I take a snapshot of the database, I find that I am unable to delete the snapshot directory (i.e., dir named ""{datadir}\{keyspacename}\snapshots\{snapshottag}"") while Cassandra is running:  ""The action can't be completed because the folder or a file in it is open in another program.  Close the folder or file and try again"" [in Windows Explorer].  If I terminate Cassandra, then I can delete the directory with no problem.

I expect to be able to move or delete the snapshotted files while Cassandra is running, as this should not affect the runtime operation of Cassandra.",Windows 7,akashirin,jn,JoshuaMcKenzie,maximp,prasanthnath,shalupov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8390,,,,,CASSANDRA-6283,,,,,,"26/Mar/14 19:36;JoshuaMcKenzie;CASSANDRA-4050_v1.patch;https://issues.apache.org/jira/secure/attachment/12636982/CASSANDRA-4050_v1.patch","07/Apr/14 17:51;JoshuaMcKenzie;CASSANDRA-4050_v2.patch;https://issues.apache.org/jira/secure/attachment/12639025/CASSANDRA-4050_v2.patch","07/Apr/14 20:28;JoshuaMcKenzie;CASSANDRA-4050_v3.patch;https://issues.apache.org/jira/secure/attachment/12639053/CASSANDRA-4050_v3.patch",,,,,,,,,,,,,,,3.0,JoshuaMcKenzie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231629,,,Mon Apr 07 20:52:03 UTC 2014,,,,,,,,,,"0|i05h8v:",29891,,benedict,,benedict,Low,,,,,,,,,,,,,,,,,"14/Mar/12 21:08;jbellis;Currently we take the snapshot using mklink /H, but I've experimented with the Java7 Files.createLink and see the same behavior.  It may simply be normal behavior for Windows that links are considered ""open"" until their creator is closed.

;;;","15/Mar/12 04:31;jn;Yeah I'm starting to think that might be true -- unfortunately.  I haven't found anything definitive, but the following postings imply a hardlink cannot be deleted while another hardlink to the same file is locked:

http://superuser.com/questions/387136/is-it-possible-to-delete-a-hardlink-to-a-locked-file
http://superuser.com/questions/301303/one-hardlink-is-locked-how-do-i-remove-the-other

Perhaps the data structure that records the lock is in the file object and not the hardlink object.;;;","05/Oct/12 23:00;akashirin;I have the same problem with Cassandra 1.1.5 on Windows 7 and Windows Server 2008 R2:

The nodetool stably creates snapshots that can't be deleted by the ""clearsnapshot"" command later (- IOException: Failed to delete ...). The bare facts are below.
 
For some (not for all) snapshot files (= hard links):
 
1) WinExplorer->Delete says ""The process cannot access the file because it is being used by another process"".
2) Command interpreter writes ""Access denied"" on ""del"" command.
3) Troubleshooting tools (- ""Process Explorer"" and ""Unlocker"") do not find open file handles. Moreover, Unlocker is able to delete these files without problems.
4) The files become deletable by the rest tools just after Cassandra-server has been stopped.
5) After restart, the same files become locked again.
6) Everything repeats after computer has been restarted.


;;;","06/Oct/12 04:06;jbellis;Right, so NTFS is locking the underlying data since the ""original"" sstable is still open in Cassandra, so the behavior described by Jim applies.  We'd have to add a workaround like the one given on superuser.com -- move the links to a ""garbage"" location to clean up on restart.

This is pretty low priority for me but I'd be glad to point someone interested in the right direction.;;;","25/Feb/14 21:51;JoshuaMcKenzie;There's been some discussion on CASSANDRA-6283 concerning this.  From the comments:

With our current io implementation on Windows, snapshots won't be deletable as long as the original sstable is locked. It's going to take a new FileDataInput based on FileChannel w/jdk7 using the FILE_SHARE_DELETE flag to allow deletion of hard links while the original file is open.

Bug with behavior: http://bugs.java.com/view_bug.do?bug_id=6607535
jdk7 support: http://www.docjar.com/html/api/sun/nio/fs/WindowsChannelFactory.java.html;;;","14/Mar/14 17:44;JoshuaMcKenzie;I did some testing to confirm - nio.2 resolves both deleting hard-links and deleting of files with other handles currently open on Windows.  It should be straightforward to convert to a FileChannel and ByteBuffer in RAR and wrap the FileDataInput interface over to the ByteBuffer's.;;;","17/Mar/14 21:57;JoshuaMcKenzie;After changing RAF over to nio.2-based I'm still seeing snapshot deletion errors.  A little digging turned up:
http://bugs.java.com/view_bug.do?bug_id=4715154

As this applies to deleting memory mapped files but not necessarily hard-links with memory mapped segments in the original I figured I'd test it.  I've confirmed that if the original file has any data that's in a MappedByteBuffer even if the original RAF it was associated with is closed, Windows refuses to delete the hard-link.

Some local logging indicates that we have MmappedSegmentedFile Segments open on the original keyspaces in question when deletion is attempted which isn't surprising.  The following implies that there *might* be a way around this but it involves turning off all page caching for these files which isn't what we want for SSTableReaders on Windows: http://www.osronline.com/showThread.cfm?link=64732;;;","18/Mar/14 15:53;JoshuaMcKenzie;Sanity checked - with nio.2 on RAR and bypassing MappedByteBuffers in MmappedSegmentedFile, snapshot-based repairs on Windows work and clean up without issue.  Not a solution long-term obviously but it helps support the hypothesis.

I think it makes sense to 1) convert RAR to nio.2 and 2) add a SnapshotDeletingTask similar to the SSTableDeletingTask to delete snapshot files once the original sstables are compacted and unmapped.  Alternatively we could allow snapshot files from repair to accrue and flag them for deletion on shutdown of the jvm and/or system reboot.;;;","18/Mar/14 16:40;jbellis;I've said elsewhere that I'm fine with dropping the mapped i/o path in 3.0.  It just isn't used often enough (and the performance difference isn't large enough) to justify the extra complexity.;;;","18/Mar/14 18:00;JoshuaMcKenzie;I'd like to drop the mapped i/o separately from converting the RAR over to nio.2 (assuming we want to go that route).  You want me to open a new ticket for that and hammer that out as a pre-req to this?

edit: also - do we have perf #'s from when we added the memory mapped i/o into the code-path?;;;","18/Mar/14 21:29;jbellis;bq. I'd like to drop the mapped i/o separately

WFM.

bq. do we have perf #'s from when we added the memory mapped i/o into the code-path

It was a long time ago (CASSANDRA-408, CASSANDRA-669).  It was a pretty big win at the time (10%?  20%?) because we were basically doing zero-copy reads from the mapped buffers.  The problem is that we had to give that up when we started manually unmapping obsolete sstables (CASSANDRA-2521) -- too hard to push refcounting all the way up the read path, so we gave up and just copy to a new buffer after all.;;;","19/Mar/14 20:25;JoshuaMcKenzie;Makes sense - it would require pushing some things pretty far back in the stack to hold a ref on a memory mapped segment on the read path.

If we're thinking 3.X release for removing mmap I can throw a workaround on this ticket to always return a BufferedPoolingSegmentedFile from SegmentedFile's getBuilder if the platform is Windows.  That + nio.2 should get us working snapshots and less weird file handle behaviors on Windows in 2.0.X without having to wait on clean-up of the old mmap code.;;;","19/Mar/14 21:01;jbellis;Rewriting to nio2 is the same scope as removing mmap.  Probably riskier actually.  So 3.0 for both.;;;","19/Mar/14 21:19;JoshuaMcKenzie;Fair enough.  RAR is only the root of all our file i/o, after all.  ;)  We should probably pursue either making snapshot deletion a quiet failure rather than deleteWithConfirm or disabling snapshot-based repair on Windows in 2.0.x.  I'm inclined to go with the latter since I'd rather not disrupt the *nix ecosystem based on Windows file-system eccentricities in our current stabilization-phase.;;;","22/Mar/14 01:03;jbellis;Created CASSANDRA-6907 for that.;;;","26/Mar/14 19:35;JoshuaMcKenzie;Attaching 1st run at converting to nio.2.  Test results on both Windows and linux at blends of 3/1, 50/1, 100/1 write/read ratios and the inverse look to be within margin of error, though we're not getting any huge gains out of this change.  3/1 sample:
{code:title=3/1 w/r test numbers}
         4050 mmap 3/1 r/w:
                        id, ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
             4 threadCount, 934400    ,   31029,   31030,   31029,     0.1,     0.1,     0.2,     0.2,     0.7,    48.3,   30.1,  0.01072
             8 threadCount, 1259400   ,   41576,   41607,   41576,     0.2,     0.2,     0.2,     0.4,     1.1,    37.9,   30.3,  0.01139
            16 threadCount, 1478350   ,   48565,   48592,   48565,     0.3,     0.3,     0.5,     1.0,     7.0,    73.6,   30.4,  0.01197
            24 threadCount, 1523350   ,   49177,      -0,   49177,     0.5,     0.4,     0.7,     1.5,    19.1,    71.8,   31.0,  0.01668
            36 threadCount, 1518900   ,   48679,   48718,   48679,     0.7,     0.6,     1.1,     2.3,    22.6,    92.7,   31.2,  0.01425
            54 threadCount, 1541050   ,   48020,   48113,   48020,     1.1,     0.9,     1.8,     4.1,    28.6,   212.6,   32.1,  0.03217
         trunk mmap 3/1 r/w:
                        id, ops       ,    op/s,adj op/s,   key/s,    mean,     med,     .95,     .99,    .999,     max,   time,   stderr
             4 threadCount, 926400    ,   30764,   30765,   30764,     0.1,     0.1,     0.2,     0.2,     0.7,    24.3,   30.1,  0.00997
             8 threadCount, 1283250   ,   42495,      -0,   42495,     0.2,     0.2,     0.2,     0.3,     0.9,    44.4,   30.2,  0.01254
            16 threadCount, 1478250   ,   48509,      -0,   48509,     0.3,     0.3,     0.5,     0.9,     4.1,    68.0,   30.5,  0.00912
            24 threadCount, 1507900   ,   48553,   48594,   48553,     0.5,     0.4,     0.8,     1.7,    21.2,   132.1,   31.1,  0.01290
            36 threadCount, 1515150   ,   48079,      -0,   48079,     0.7,     0.6,     1.2,     2.7,    23.3,   103.8,   31.5,  0.01531
            54 threadCount, 1517600   ,   47826,      -0,   47826,     1.1,     0.9,     1.6,     3.2,    25.0,   194.4,   31.7,  0.01819
{code}

I mention mmap in these results as using BufferedPoolingSegmentedFiles on both trunk and on this patch had a noticeable negative impact on throughput, more on nio.2 than on the byte[] raw usage.  On trunk with read-heavy workloads I'm seeing anywhere from a 30-40% hit in stress results on read performance.  1/50 r/w ratio stress w/BufferedPoolingSegmentedFiles was still 16% slower than my testing using MmappedSegmentedFiles.  I'll be attaching a sample of the perf #'s I've been getting to CASSANDRA-6890.

I put some yammer timers inside the RAR code on both trunk and on this branch and it looks like #'s are comparable up to about the 60th percentile or so across all major read or rebuffer operations - then they balloon.  In the territory of a max timestamp of 100+ms on a simple channel seek vs. .01 on mmap'ed.  GC count during stress is roughly double at a glance - I'll look into that further on 6890 but heap stress due to more activity on the heap is to be expected.

As noted earlier - in order to fully resolve this issue, either CASSANDRA-6890 will need to be resolved or some alternative solution for Windows if we keep mmap'ing in.;;;","27/Mar/14 16:24;jbellis;[~benedict] to review;;;","31/Mar/14 13:00;benedict;I've uploaded a tidied up version [here|https://github.com/belliottsmith/cassandra/tree/4050-nio2]

I've eliminated some unnecessary variables, simplified a couple of loops/conditions, and unified the AbstractDataInput/Small hierarchy. Also fixed a minor ""bug"" with getPosition() in RAR after close(), and CRAR now ensures that the current position is restored after rebuffer() - whilst currently this wouldn't cause any problems, it seems like an oversight.;;;","31/Mar/14 17:50;JoshuaMcKenzie;Good catch on getPosition - I accounted for that in current() but that hadn't triggered on any testing and was an oversight.

I kept AbstractDataInput and AbstractDataInputSmall separate in the type heirarchy because I didn't want to push the int -> long signature change down to all the classes that implemented the base.  I'm not sure if the added footprint justifies the added complexity or not - I was trying to minimize changes to unrelated classes due to the loss of RAF code.  I didn't like it, but I also don'e like the alternative that much.  It looks like we run the risk of Bad Things if someone does a MemoryInputStream.skipBytes that pushes the position past Max Int - this impl has us casting off the remainder on a seek call so you could end up in negative territory.

As for the tidying up - looks good to me.  Thanks for taking the time to do that - clean idiomatic usage of the nio API's clearly makes things easier to parse.

Tests on linux look good, snapshots on Windows behave w/benedict's revisions and no mmap, and read performance looks comparable so I +1 the changes with the above caveat.;;;","31/Mar/14 17:58;benedict;bq. It looks like we run the risk of Bad Things if someone does a MemoryInputStream.skipBytes that pushes the position past Max Int - this impl has us casting off the remainder on a seek call so you could end up in negative territory.

How so? The MemoryInputStream defines what its limit is, and the skipBytes method ensures it never goes above this. So seek() can never be called with a value that is out of range (since it is a protected method). We could put in an assert if we want to be doubly certain, however, and that's probably not a bad idea for simple declaration of intent.

I think the reduced code duplication (from readLine and skipBytes now being shared), and cleaner hierarchy is preferable, especially as ADISmall is not a very clear distinction from ADI. Think the overall footprint is reduced rather than increased...?

bq. Thanks for taking the time to do that - clean idiomatic usage of the nio API's clearly makes things easier to parse.

I find the NIO library tough to parse at the best of times, and wanted to be sure I was reading it right, so it was a freebie to change as I reviewed :);;;","31/Mar/14 18:16;JoshuaMcKenzie;{quote}
the skipBytes method ensures it never goes above this
{quote}

How is skipBytes protecting against blowing past our limit?  (note: me just being dense here is not out of the question)
{code:java, title=skipBytes}
 64     public int skipBytes(int n) throws IOException
 65     {
 66         if (n <= 0)
 67             return 0;
 68         seek(getPosition() + n);
 69         return position;
 70     }
{code}

It looks like this exposes seek() to the outside world with a protection against negative inputs but not much else.  That being said - the old code looks like it has the same potential problem:

{code:java, title=old code}
    public int skipBytes(int n) throws IOException
    {
        seekInternal(getPosition() + n);
        return position;
    }
{code};;;","31/Mar/14 18:19;benedict;Ah, this is my failure to delete the skipBytes method from MIS, as it now occurs in ADI (in a safe manner).;;;","31/Mar/14 18:20;benedict;In fact, it looks like that is simply a bug that has always been present - the new behaviour is no worse than the old, but deleting it is still the correct fix.

Good spot.;;;","31/Mar/14 18:23;JoshuaMcKenzie;Sure enough.  given the docs for skipBytes are 0-n bytes skipped I think the code in ADI looks good.  I'd much rather we not add more types to the hierarchy in this context.;;;","31/Mar/14 21:12;benedict;Sounds like we're in agreement then? Any further changes you want to make after my update?;;;","31/Mar/14 21:18;JoshuaMcKenzie;No other changes.  Depending on where we fall on CASSANDRA-6890 I may want to expand this ticket to cover using SSTableDeletingTasks on snapshot files to work around deleting memory mapped file segments on Windows in order to fix the issue that caused us to go down this path in the first place.  I don't like low-level I/O rewrites being under the guise of a Windows snapshot file ticket so I may do some housekeeping here.;;;","31/Mar/14 21:47;benedict;Rename/describe the ticket? :)

I had been thinking the same thing.;;;","04/Apr/14 21:53;JoshuaMcKenzie;I'll rebase your branch against trunk and post a revised patch early next week.  I know how much you love rebasing and I figure I owe you one for the house-cleaning on this patch.  ;);;;","07/Apr/14 17:52;JoshuaMcKenzie;New patch attached.  Passes the same tests trunk does and perf is in line.;;;","07/Apr/14 18:53;benedict;Might want to delete skipBytes from MIS as we discussed, otherwise LGTM and ready for commit.;;;","07/Apr/14 20:09;JoshuaMcKenzie;Odd.  I wonder if I didn't pull down that [commit|https://github.com/josh-mckenzie/cassandra/commit/51cf8e74db1d452d99ac554b6666c86829376d91] before I rebased locally.  Odd since I thought I saw that on the difftool run;  I'll fix it and repost.
;;;","07/Apr/14 20:28;JoshuaMcKenzie;v3 attached.  I removed the CommitLogTest changes that snuck on on the remove skipBytes from MIS commit from you as they looked unrelated to this ticket.;;;","07/Apr/14 20:30;benedict;+1;;;","07/Apr/14 20:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"when creating keyspace with simple strategy, it should only acception ""replication_factor"" as an option",CASSANDRA-4046,12546323,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,cywjackson,cywjackson,13/Mar/12 23:01,16/Apr/19 09:32,14/Jul/23 05:52,09/May/12 16:29,1.1.1,,,,,,0,,,,,,,"currently I could do this:

{panel}
[default@unknown] create keyspace test
...     with placement_strategy = 'SimpleStrategy'
...     and strategy_options = \{DC : testdc, replication_factor :1\};
ebc5f430-6d47-11e1-0000-edee3ea2cbff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] 
{panel}

while i don't think this creates any ""problem"" in terms of the actual replication being used for the CL , we probably should acknowledge to the user that ""DC : testdc"" is not an valid option for the SimpleStrategy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/12 23:00;dbrosius@apache.org;4046_warn_invalid_options.diff;https://issues.apache.org/jira/secure/attachment/12525528/4046_warn_invalid_options.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231481,,,Wed May 09 16:29:28 UTC 2012,,,,,,,,,,"0|i08pxz:",48802,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Apr/12 04:54;dbrosius@apache.org;Implementations of AbstractReplicationStrategy.validateOptions only does positive validations, negative validations for these would need to be implemented for this issue.;;;","03/May/12 22:55;dbrosius@apache.org;log at warning level options that are unrecognized for the specified strategy.

(but still succeed)

against trunk;;;","09/May/12 16:29;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BOF fails when some nodes are down,CASSANDRA-4045,12546321,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,13/Mar/12 22:39,16/Apr/19 09:32,14/Jul/23 05:52,16/Apr/12 18:16,1.1.1,,,,,,0,hadoop,,,,,,"As the summary says, we should allow jobs to complete when some targets are unavailable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4051,CASSANDRA-4146,,,,,,,,,,,,,,,,"13/Apr/12 21:43;brandon.williams;4045.txt;https://issues.apache.org/jira/secure/attachment/12522624/4045.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231479,,,Mon Apr 16 18:16:36 UTC 2012,,,,,,,,,,"0|i0grbz:",95858,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"13/Apr/12 21:43;brandon.williams;Patch requires CASSANDRA-4146.  It would be nice if we could support the notion of consistency level here, but it's not as easy as setting a ConsistencyLevel directive like other output formats, so instead we just allow defining a maximum number of hosts that are allowed to fail before the entire job fails, and default that to zero.;;;","16/Apr/12 18:13;yukim;+1;;;","16/Apr/12 18:16;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
examples/simple_authenticator README doesn't mention JVM args,CASSANDRA-4044,12546319,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,13/Mar/12 22:25,16/Apr/19 09:32,14/Jul/23 05:52,14/Mar/12 04:22,1.0.9,1.1.0,,Legacy/Documentation and Website,,,0,,,,,,,"To use SimpleAuthenticator/SimpleAuthority, you need to use the {{-Dpasswd.properties=conf/passwd.properties}} and {{-Daccess.properties=conf/access.properties}} java args.  The README doesn't mention this anywhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/12 22:41;thobbs;4044.txt;https://issues.apache.org/jira/secure/attachment/12518259/4044.txt",,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231477,,,Wed Mar 14 04:22:18 UTC 2012,,,,,,,,,,"0|i0grbj:",95856,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Mar/12 22:41;thobbs;Attached patch updates the README.;;;","14/Mar/12 04:22;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"add ""caching"" to CQL CF options",CASSANDRA-4042,12546270,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,xedin,xedin,13/Mar/12 18:09,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 15:10,1.1.0,,,,,,0,,,,,,,"""Caching"" option is missing from CQL ColumnFamily options.",,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/12 14:33;slebresne;4042_v2.txt;https://issues.apache.org/jira/secure/attachment/12520124/4042_v2.txt","13/Mar/12 19:58;xedin;CASSANDRA-4042.patch;https://issues.apache.org/jira/secure/attachment/12518231/CASSANDRA-4042.patch",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231428,,,Tue Mar 27 15:10:44 UTC 2012,,,,,,,,,,"0|i0gran:",95852,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"22/Mar/12 20:09;thepaul;partial dupe of CASSANDRA-3941. I could remove ""caching"" from that one, or we could add bloom_filter_fp_chance to this one. I like the second option, since this looks pretty easy to add the other option to this already-done work.;;;","22/Mar/12 20:18;xedin;I'm fine you just attach the ""bloom_filter_fp_change"" addition here as a separate patch and we will just close CASSANDRA-3941 as Duplicate or vice versa weather you prefer. :);;;","27/Mar/12 14:33;slebresne;Attaching v2 that handles the bloom filter fp change too.;;;","27/Mar/12 14:46;xedin;+1;;;","27/Mar/12 15:10;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: double wide unicode chars cause incorrect padding in select output,CASSANDRA-4033,12545883,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,tpatterson,tpatterson,09/Mar/12 17:59,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 18:53,1.0.9,1.1.0,,Legacy/Tools,,,0,cql,cqlsh,,,,,"{code}
CREATE COLUMNFAMILY cf3 (KEY text primary key);
INSERT INTO cf3 (KEY, col1, col2) VALUES ('a', '1234 1234 1234 1234', 'abcd');
INSERT INTO cf3 (KEY, col1, col2) VALUES ('b', '愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛', 'abcd');
SELECT * FROM cf3 WHERE key in ('a', 'b');
{code}
produces this output:
{code}
 KEY | col1                                                | col2
-----+-----------------------------------------------------+------
   a |                                 1234 1234 1234 1234 | abcd
   b |                       愛愛愛愛 愛愛愛愛 愛愛愛愛 愛愛愛愛 | abcd
{code}
note the extra spaces before the ""love"" glyphs.",Using the patch pending/4003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231066,,,Tue Mar 27 18:53:14 UTC 2012,,,,,,,,,,"0|i0gr6n:",95834,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"10/Mar/12 01:23;thepaul;good call. fix pushed to https://github.com/thepaul/cassandra/tree/pending/4033 (4033 branch and pending/4033 tag in my github repo);;;","27/Mar/12 18:53;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"memtable.updateLiveRatio() is blocking, causing insane latencies for writes",CASSANDRA-4032,12545857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,09/Mar/12 15:10,16/Apr/19 09:32,14/Jul/23 05:52,11/Apr/12 18:27,1.1.0,,,,,,0,,,,,,,"Reproduce by just starting a fresh cassandra with a heap large enough for live ratio calculation (which is {{O(n)}}) to be insanely slow, and then running {{./bin/stress -d host -n100000000 -t10}}. With a large enough heap and default flushing behavior this is bad enough that stress gets timeouts.

Example (""blocked for"" is my debug log added around submit()):

{code}
 INFO [MemoryMeter:1] 2012-03-09 15:07:30,857 Memtable.java (line 198) CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 8.89014894083727 (just-counted was 8.89014894083727).  calculation took 28273ms for 1320245 columns
 WARN [MutationStage:8] 2012-03-09 15:07:30,857 Memtable.java (line 209) submit() blocked for: 231135
{code}

The calling code was written assuming a RejectedExecutionException is thrown, but it's not because {{DebuggableThreadPoolExecutor}} installs a blocking rejection handler.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/12 22:29;jbellis;4032-v3.txt;https://issues.apache.org/jira/secure/attachment/12521777/4032-v3.txt","10/Apr/12 16:00;jbellis;4032-v4.txt;https://issues.apache.org/jira/secure/attachment/12522121/4032-v4.txt","09/Mar/12 15:31;scode;CASSANDRA-4032-1.1.0-v1.txt;https://issues.apache.org/jira/secure/attachment/12517729/CASSANDRA-4032-1.1.0-v1.txt","09/Mar/12 18:06;scode;CASSANDRA-4032-1.1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12517754/CASSANDRA-4032-1.1.0-v2.txt",,,,,,,,,,,,,,4.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231040,,,Wed Apr 11 18:27:14 UTC 2012,,,,,,,,,,"0|i0gr67:",95832,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"09/Mar/12 15:31;scode;Attaching patch that allows us to create both blocking and non-blocking {{DebuggableThreadPoolExecturor}}:s; use that for this particular case.;;;","09/Mar/12 15:36;jbellis;The DTPE change looks good, but don't you need a catch (Rejected) { } block in updateLiveRatio?;;;","09/Mar/12 15:44;scode;There already is one, it's just a NOOP because of the blocking nature of the DTPE.;;;","09/Mar/12 15:45;scode;This is the code:

{code}
        try
        {
            meterExecutor.submit(runnable);
        }
        catch (RejectedExecutionException e)
        {
            logger.debug(""Meter thread is busy; skipping liveRatio update for {}"", cfs);
        }
{code}
;;;","09/Mar/12 15:51;slebresne;Are we sure that what we want is a SynchronousQueue with task rejected? After all, there is only on global memoryMeter, so we could end up failing to updateLiveRatio just based on a race, even if calculations are fast. I'd suggest instead a bounded queue (but maybe not infinite and we could indeed just skip task if that queue gets full).;;;","09/Mar/12 15:57;jbellis;bq. This is the code:

Either I'm blind or that's not in the attached patch.;;;","09/Mar/12 16:01;scode;{quote}
Either I'm blind or that's not in the attached patch.
{quote}

It's not. It's *already there*, in the branch, committed. The code was written, I presume, without the author realizing that RejectedExecutionException would never be thrown.;;;","09/Mar/12 16:05;scode;{quote}
Are we sure that what we want is a SynchronousQueue with task rejected? After all, there is only on global memoryMeter, so we could end up failing to updateLiveRatio just based on a race, even if calculations are fast. I'd suggest instead a bounded queue (but maybe not infinite and we could indeed just skip task if that queue gets full).
{quote}

I agree it's fishy, though I'd suggest a separate ticket. This patch is intended to make the code behave the way the original commit intended.

This (from the code, not my patch) seems legit though:

{code}
    // we're careful to only allow one count to run at a time because counting is slow
    // (can be minutes, for a large memtable and a busy server), so we could keep memtables
    // alive after they're flushed and would otherwise be GC'd.
{code}

We could have one queue per unique CF and have a consumer that iterates over the set of queues, guaranteeing that each CF gets processed once per ""cycle"". A simpler solution is probably preferable though if we can think of one.
;;;","09/Mar/12 16:07;scode;How about just having an unbounded queue but having each CF just keep a flag that says whether or not there is a calculation currently pending or executing; it would be reset by the task when executed, and CAS:ed in the write path. This; single queue, same DTPE. We'd just submit a task with a reference to the AtomicBoolean which it will set to false on completion.;;;","09/Mar/12 16:09;scode;(That seems simple enough to just do right now and not bother with a separate ticket. If you agree I'll submit a patch.);;;","09/Mar/12 16:17;slebresne;Sounds fine to me.;;;","09/Mar/12 18:06;scode;Attaching {{v2}}. Keeps NBHM mapping CFS -> AtomicBoolean. Mappings are never removed (assuming reasonably bound number of unique CFS:s during a lifetime). Queue used for DTPE is unbounded.
;;;","09/Mar/12 18:07;scode;(v2 doesn't keep the changes to DTPE, which are no longer used.);;;","06/Apr/12 22:29;jbellis;v3 attached:

- uses putIfAbsent in the AtomicBoolean creation
- drops the catch block around executor submission;;;","10/Apr/12 15:34;slebresne;Wouldn't using a ConcurrentSkipListSet simply the code? Feels like a lot of boilerplate to record that there is already a metering running (it's not like this is a hot path enough to justify the NBHM). ;;;","10/Apr/12 16:00;jbellis;you're right; v4 attached w/ Set approach.

(used NBHS instead of CSLS since the latter requires defining a comparator.  I'm not sure the overhead is substantially different.);;;","11/Apr/12 14:27;slebresne;+1 with two nits:
* In the comment ""to a maximum of one per CFS using this map"" could have a s/map/set/
* Note sure if there was an intent in changing the maximumPoolSize of the meterExecutor to Integer.MAX_VALUE. As it stands, with an unbounded queue the maximumPoolSize is ignored so that doesn't really matter, but just wanted to mention it.;;;","11/Apr/12 18:27;jbellis;Committed, w/ nits amended;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during inserting emtpy string as column value on indexed column,CASSANDRA-4031,12545840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,mdymarek,mdymarek,09/Mar/12 13:09,16/Apr/19 09:32,14/Jul/23 05:52,20/Mar/12 23:05,1.1.0,,,,,,0,,,,,,,"Hi,
I`m running one node cluster(issue occurs also on other cluster(which has 2 nodes)) on snapshot from cassandra-1.1 branch(i used 449e037195c3c504d7aca5088e8bc7bd5a50e7d0 commit).
i have simple CF, definition of TestCF:
{noformat}
[default@test_keyspace] describe Test_CF;
    ColumnFamily: Test_CF
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: [Test_CF.Test_CF_test_index_idx]
      Column Metadata:
        Column Name: test_index
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: Test_CF_test_index_idx
          Index Type: KEYS
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor
{noformat}
I`m trying to add new row(log from cassandra-cli, note that there is index on test_index):
{noformat}
[default@test_keyspace] list Test_CF;              
Using default limit of 100

0 Row Returned.
Elapsed time: 31 msec(s).
[default@test_keyspace] set Test_CF[absdsad3][test_index]='';
null
TimedOutException()
	at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15906)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:788)
	at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:772)
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:894)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:211)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
[default@test_keyspace] list Test_CF;                        
Using default limit of 100
-------------------
RowKey: absdsad3
=> (column=test_index, value=, timestamp=1331298173009000)

1 Row Returned.
Elapsed time: 7 msec(s).
{noformat}
Exception from system.log:
{noformat}
 INFO [FlushWriter:56] 2012-03-09 13:42:02,500 Memtable.java (line 291) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-3251-Data.db (2077 bytes)
ERROR [MutationStage:2291] 2012-03-09 13:42:22,232 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:2291,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
        at org.apache.cassandra.db.index.SecondaryIndexManager.getIndexKeyFor(SecondaryIndexManager.java:294)
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:490)
        at org.apache.cassandra.db.Table.apply(Table.java:441)
        at org.apache.cassandra.db.Table.apply(Table.java:366)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:275)
        at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:446)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1228)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,forsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/12 20:21;yukim;4031.txt;https://issues.apache.org/jira/secure/attachment/12518933/4031.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,231023,,,Tue Mar 20 23:05:20 UTC 2012,,,,,,,,,,"0|i0gr5r:",95830,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Mar/12 15:43;slebresne;It's because indexed value are used as row key (in the index). Previously, empty row key was actually allowed by the code (but imo it was more of a bug we were not aware of).

The right fix may probably be to refuse empty value, though that could appear as a limitation. The other solution would be to create a special constructor for DK that allows an empty byte buffer. But I couldn't swear that other part of the code don't break subtly with empty row keys.;;;","12/Mar/12 22:45;yukim;Same error happens when performing searching for empty value(='') on indexed column.

I agree with Sylvain, empty row key should not be allowed. But for version 1.1, I think it is fine to use empty row key on secondary indices, otherwise we have to perform full data scan and filter out all that have empty value on indexed column(or refuse query which has ""=''"").

I will fix this by adding empty key DK only for secondary index.;;;","19/Mar/12 20:21;yukim;For v1.1, I propose not to check for empty key inside DK, removing assertion in its constructor.
I also added key validation check to CQL insert/update so that no empty key gets inserted(CLI/thrift already do validation).

At first I tried to allow empty key DK only in secondary index by creating static method that generates DK with empty key, but it turned out to be redundant because Memtable#put recreates DK when storing.;;;","20/Mar/12 23:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EC2 snitch incorrectly reports regions,CASSANDRA-4026,12545730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tnine,tnine,08/Mar/12 23:26,16/Apr/19 09:32,14/Jul/23 05:52,12/Mar/12 19:01,1.0.9,1.1.0,,,,,0,,,,,,,"Currently the org.apache.cassandra.locator.Ec2Snitch reports ""us-west"" in both the oregon and the california data centers.  This is incorrect, since they are different regions.

California => us-west-1
Oregon     => us-west-2

wget http://169.254.169.254/latest/meta-data/placement/availability-zone returns the value ""us-west-2a""


After parsing this returns

DC = us-west Rack = 2a


What it should return

DC = us-west-2 Rack = a


This makes it possible to use multi region when both regions are in the west coast.
",Ubuntu 10.10 64 bit Oracle Java 6,halorgium,ramsperger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7839,,,,,,,,"09/Mar/12 03:51;vijay2win@yahoo.com;0001-CASSANDRA-4026.patch;https://issues.apache.org/jira/secure/attachment/12517674/0001-CASSANDRA-4026.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230913,,,Wed Aug 27 16:05:19 UTC 2014,,,,,,,,,,"0|i0gr3j:",95820,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"09/Mar/12 00:00;vijay2win@yahoo.com;I ran into this a while ago. But the problem is that we cannot change the settings on the current live clusters which uses Ec2MultiregionSnitch or Ec2Snitch without taking a downtime. (If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the schema for the existing cluster).

Option 1: Leave the existing snitch as it is and add a new snitch.
Option 2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as us-west-2 is fairly new it wont affect a lot of us?

Brandon, Thoughts?
;;;","09/Mar/12 00:08;tnine;Could we create a new snitch that corrects the problem and deprecate the existing snitches?  This way people can migrate if they choose to, or keep the old snitches if it does not affect them.;;;","09/Mar/12 00:24;vijay2win@yahoo.com;I am ok with deprecating it, but the problem is unless the users have a way out of this we cannot remove it.;;;","09/Mar/12 00:26;brandon.williams;bq. If we change EC2Snitch/Ec2Multiregion snitch, we also need to change the schema for the existing cluster

Not the schema per se, but the datacenter name.  This is doable though if you're willing to repair afterwards.  Another option is to switch an entire DC's snitch at a time.

bq. Option 1: Leave the existing snitch as it is and add a new snitch.

Ugh, that will cause tremendous confusing for new users.  It would however be nice to get rid of this wart at some point.

bq. Option 2: Parse for us-west-1 as us-west and parse us-west-2 as us-west2, as us-west-2 is fairly new it wont affect a lot of us?

There aren't a lot good options here, I'm not sure how I feel about this one since it's definitely a hack, but only appending the number to the DC if > 1 might be the least painful for existing users.;;;","09/Mar/12 03:51;vijay2win@yahoo.com;>>> This is doable though if you're willing to repair afterwards.
The problem is that StorageProxy will start to write the data to the nodes which are not suppose to have the data (During upgrade, and restart takes a while)... hence after recovery they will not be able to be recovered via repair (Lets say Node A, B, C, D if B and C are upgraded A will start to write the data to D for thinking it as this datacenters replica).

>>> it's definitely a hack, but only appending the number to the DC if > 1 might be the least painful for existing users.
Agree, and attached patch does this.

BTW: The attached patch can break after we AWS has 24 AZ's which is highly unlikely but i will create a ticket requesting for API for Regions instead of AZ.;;;","12/Mar/12 18:16;brandon.williams;bq. hence after recovery they will not be able to be recovered via repair

One replica will always be in the right spot so you can repair.

bq. BTW: The attached patch can break after we AWS has 24 AZ's which is highly unlikely but i will create a ticket requesting for API for Regions instead of AZ.

That would be great.  Unfortunately when we have that, we'll still have to munge the name (and rack names) to be backwards compatible :(

This ticket makes me sad, but +1.;;;","12/Mar/12 19:01;vijay2win@yahoo.com;Committed to trunk and 1.0, Thanks!;;;","26/Aug/14 21:27;ramsperger;Not to reopen ancient tickets, but code was added in CASSANDRA-5897 to pull in SnitchProperties. Would it not be desirable to address this by looking at an additional property and choosing between legacy naming and a full, consistent with other EC2 APIs, naming scheme?

Something similar to this commit [https://github.com/ramsperger/cassandra/commit/bf1ee0251e5cf46b8af28282e22c4fbf29d85f33?w=1]?

;;;","26/Aug/14 21:58;brandon.williams;[~ramsperger] go ahead an open a new ticket for that.;;;","27/Aug/14 16:05;ramsperger;Thanks [~brandon.williams], opened as CASSANDRA-7839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.cassandra.io.util.FileUtils.delete(List<String>) only deletes every second file in the list,CASSANDRA-4025,12545704,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,mschuetz,mschuetz,08/Mar/12 19:48,16/Apr/19 09:32,14/Jul/23 05:52,08/Mar/12 20:16,,,,,,,0,,,,,,,The above mentioned function does not work as expected. I fixed it by iterating over the list in reverse.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230887,,,Thu Mar 08 20:16:40 UTC 2012,,,,,,,,,,"0|i0gr33:",95818,,,,,Low,,,,,,,,,,,,,,,,,"08/Mar/12 19:49;mschuetz;{code}
diff --git a/src/java/org/apache/cassandra/io/util/FileUtils.java b/src/java/org/apache/cassandra/io/util/FileUtils.java
index e264b8a..3d1b029 100644
--- a/src/java/org/apache/cassandra/io/util/FileUtils.java
+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java
@@ -151,7 +151,7 @@ public static boolean delete(String file)
     public static boolean delete(List<String> files)
     {
         boolean bVal = true;
-        for ( int i = 0; i < files.size(); ++i )
+        for ( int i = files.size() - 1; i >= 0; --i )
         {
             String file = files.get(i);
             bVal = delete(file);
{code};;;","08/Mar/12 20:16;slebresne;Actually that function wasn't used anywhere so instead I just removed it. Thanks for the report.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction of hints can get stuck in a loop,CASSANDRA-4022,12545687,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,brandon.williams,brandon.williams,08/Mar/12 17:46,16/Apr/19 09:32,14/Jul/23 05:52,31/Mar/12 04:45,1.2.0 beta 1,,,,,,0,,,,,,,"Not exactly sure how I caused this as I was working on something else in trunk, but:

{noformat}
 INFO 17:41:35,682 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-339-Data.db')]
 INFO 17:41:36,430 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.912220MB/s.  Time: 748ms.
 INFO 17:41:36,431 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-340-Data.db')]
 INFO 17:41:37,238 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 5.479976MB/s.  Time: 807ms.
 INFO 17:41:37,239 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-341-Data.db')]
 INFO 17:41:38,163 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.786083MB/s.  Time: 924ms.
 INFO 17:41:38,164 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-342-Data.db')]
 INFO 17:41:39,014 GC for ParNew: 274 ms for 1 collections, 541261288 used; max is 1024458752
 INFO 17:41:39,151 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.485132MB/s.  Time: 986ms.
 INFO 17:41:39,151 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-343-Data.db')]
 INFO 17:41:40,016 GC for ParNew: 308 ms for 1 collections, 585582200 used; max is 1024458752
 INFO 17:41:40,200 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.223821MB/s.  Time: 1,047ms.
 INFO 17:41:40,201 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-344-Data.db')]
 INFO 17:41:41,017 GC for ParNew: 252 ms for 1 collections, 617877904 used; max is 1024458752
 INFO 17:41:41,178 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 4.526449MB/s.  Time: 977ms.
 INFO 17:41:41,179 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-345-Data.db')]
 INFO 17:41:41,885 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes 
for 1 keys at 6.263938MB/s.  Time: 706ms.
 INFO 17:41:41,887 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-346-Data.db')]
 INFO 17:41:42,617 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 6.066311MB/s.  Time: 729ms.
 INFO 17:41:42,618 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-347-Data.db')]
 INFO 17:41:43,376 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 5.834222MB/s.  Time: 758ms.
 INFO 17:41:43,377 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-348-Data.db')]
 INFO 17:41:44,307 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.760323MB/s.  Time: 929ms.
 INFO 17:41:44,308 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-349-Data.db')]
 INFO 17:41:45,021 GC for ParNew: 245 ms for 1 collections, 731287832 used; max is 1024458752
 INFO 17:41:45,316 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db,].  4,637,160 to 4,637,160 (~100% of original) bytes for 1 keys at 4.395965MB/s.  Time: 1,006ms.
 INFO 17:41:45,316 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-350-Data.db')]
 INFO 17:41:46,022 GC for ParNew: 353 ms for 1 collections, 757476872 used; max is 1024458752
 INFO 17:41:46,451 Compacted to [/var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-351-Data.db,].  4,637,160 to 4,637
{noformat}

I suspect we broke something subtle in CASSANDRA-3955.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/12 19:19;yukim;4022-v2.txt;https://issues.apache.org/jira/secure/attachment/12518928/4022-v2.txt","09/Mar/12 21:39;yukim;4022.txt;https://issues.apache.org/jira/secure/attachment/12517793/4022.txt",,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230870,,,Sat Mar 31 04:45:37 UTC 2012,,,,,,,,,,"0|i0gr1r:",95812,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"08/Mar/12 18:01;brandon.williams;It seems part of the problem is it doesn't know about one of the sstables it needs to compact:

{noformat}

cassandra-1:/srv/cassandra# ls -l /var/lib/cassandra/data/system/HintsColumnFamily/
total 66804
-rw-r--r-- 1 root root 63642821 Mar  8 17:36 system-HintsColumnFamily-hd-13-Data.db
-rw-r--r-- 1 root root       80 Mar  8 17:36 system-HintsColumnFamily-hd-13-Digest.sha1
-rw-r--r-- 1 root root      976 Mar  8 17:36 system-HintsColumnFamily-hd-13-Filter.db
-rw-r--r-- 1 root root       26 Mar  8 17:36 system-HintsColumnFamily-hd-13-Index.db
-rw-r--r-- 1 root root     4344 Mar  8 17:36 system-HintsColumnFamily-hd-13-Statistics.db
-rw-r--r-- 1 root root  4637160 Mar  8 17:59 system-HintsColumnFamily-hd-639-Data.db
-rw-r--r-- 1 root root       81 Mar  8 17:59 system-HintsColumnFamily-hd-639-Digest.sha1
-rw-r--r-- 1 root root      496 Mar  8 17:59 system-HintsColumnFamily-hd-639-Filter.db
-rw-r--r-- 1 root root       26 Mar  8 17:59 system-HintsColumnFamily-hd-639-Index.db
-rw-r--r-- 1 root root     5944 Mar  8 17:59 system-HintsColumnFamily-hd-639-Statistics.db
-rw-r--r-- 1 root root        0 Mar  8 17:59 system-HintsColumnFamily-tmp-hd-640-Data.db
-rw-r--r-- 1 root root        0 Mar  8 17:59 system-HintsColumnFamily-tmp-hd-640-Index.db
{noformat};;;","08/Mar/12 18:11;brandon.williams;Yuki mentions that it may be caused by CASSANDRA-3442 too.;;;","08/Mar/12 18:12;brandon.williams;I should note that the machine does not hand anything off, so everything in these sstables must be tombstones.;;;","08/Mar/12 20:24;brandon.williams;What is happening very reproducibly now is that I started the node, and 5 minutes later the forced compaction check in ACS kicks off, and then I have looping compaction on the hints but it's only compacting the last sstable over and over.;;;","08/Mar/12 20:36;brandon.williams;Confirmed that this only happens with CASSANDRA-3442 applied.;;;","08/Mar/12 23:40;yukim;The basic idea behind CASSANDRA-3442 is to perform single sstable compaction if its droppable tombstone ratio is above threshold.
This works because single sstable compaction drops tombstones and lowers droppable tombstone ratio, which prevents recursive compaction on that sstable.

But there is a situation which compaction does not drop tombstones. That happens when the key in compacting sstable appears in another sstables.

Let me find the way to handle above case...;;;","08/Mar/12 23:58;jbellis;bq. But there is a situation which compaction does not drop tombstones. That happens when the key in compacting sstable appears in another sstables.

What if we checked the most-recent-timestamp of the other sstables, and avoid compaction only if there is potentially data old enough that we'd need the tombstones to suppress it?;;;","09/Mar/12 21:39;yukim;One possible solution is to add check for overwrap of key range stored in sstable.
Patch attached with minor fix for test.;;;","11/Mar/12 23:55;jbellis;I think we should check for overlaps *and* timestamp is old enough to have data we care about suppressing.  The former alone will be common in size-tiered compaction.;;;","14/Mar/12 19:41;yukim;I tried to use timestamp to determine whether sstable should be compacted, but it does not guarantee to suppress tombstones. Tombstones only get dropped when those keys don't appear in other sstables besides compacting ones. Currently I think the only way to stop compaction loop is to make sure interested sstable does not have overlap so its tombstones actually drop.;;;","14/Mar/12 19:57;jbellis;We don't need to suppress tombstones -- we just need to make sure that any data that the tombstones we're compacting, is new enough that we don't need the tombstones to suppress them.  In other words, that throwing away our tombstones won't make deleted data start showing up again.;;;","15/Mar/12 15:22;yukim;I understand the situation, but isn't it covered by just checking key overlap?
If there is no overlap, then tombstones in target sstable are guaranteed to be the only and the newest ones?;;;","15/Mar/12 16:38;jbellis;Right, if there's no overlap we're free to compact -- but I'm worried that with SizeTiered compaction we'll have overlap in a lot of cases where we could still compact if we looked closer.;;;","15/Mar/12 21:26;jbellis;Suppose for example we have two sstables:

SSTable A has a tombstone for row K, column foo, at time=100.

SSTable B has data for row K, column bar, at time=200.

We would like to allow A to be compacted by itself to get rid of tombstones, since even though B has overlapping data it is new enough that removing the TS is safe.;;;","19/Mar/12 19:19;yukim;Dropping tombstone is only done when the key tombstone belongs to does not appear in other sstables that are not compacting(ConpactionController#shouldPurge).
We may be able to tweak the above to look through timestamp of columns, but it will cost too much.

Instead, I come up with ""guessing"" how many tombstones there are outside of overlapping keys among sstables. When sstable has droppable tombstone ratio > threshold but overlaps keys with others, then calculate:

{code}
(# of keys outside of overlap) remainingKeys = sstable.estimatedKeys - sstable.estimatedKeysForRanges(overlapped range)
(# of columns outside of overlap) remainingColumns = sstable.estimatedColumnCount.percentile(remaingingKeys / total keys) * remainingKeys
{code}

and if (remainingColumns / total columns) * (droppable tombstone ratio) is greater than threshold, compact that sstable itself.

I think in this way, the chance of single sstable compaction increases, while avoiding recursive sstable compaction.;;;","23/Mar/12 17:41;jbellis;bq. Dropping tombstone is only done when the key tombstone belongs to does not appear in other sstables that are not compacting(ConpactionController#shouldPurge).

Right. I'm saying we can make that more sophisticated, e.g. by changing shouldPurge signature to {{(key, maxTombstoneTimestamp)}} which we could then compare to the min timestamp from the overlapping-but-not-compaction-participant sstables.

But, thinking about that more, it's unlikely to help much since both STCS and LCS mix data of different ages together routinely.  So I'll let that drop now. :);;;","23/Mar/12 18:25;jbellis;v2 lgtm but I'm going to rebase on top of CASSANDRA-4080 before committing.;;;","31/Mar/12 04:45;jbellis;rebased + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.scrubDataDirectories tries to delete nonexistent orphans,CASSANDRA-4021,12545682,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yulinyen,brandon.williams,brandon.williams,08/Mar/12 17:22,16/Apr/19 09:32,14/Jul/23 05:52,11/Mar/13 23:16,1.1.11,1.2.3,,,,,0,datastax_qa,,,,,,"The check only looks for a missing data file, then deletes all other components, however it's possible for the data file and another component to be missing, causing an error:

{noformat}

 WARN 17:19:28,765 Removing orphans for /var/lib/cassandra/data/system/HintsColumnFamily/system-HintsColumnFamily-hd-24492: [Index.db, Filter.db, Digest.sha1, Statistics.db, Data.db]
ERROR 17:19:28,766 Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
java.lang.AssertionError: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:357)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:352)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:105)
Exception encountered during startup: attempted to delete non-existing file system-HintsColumnFamily-hd-24492-Index.db
{noformat}",,colinkuo,jeromatron,rcoli,sgpope,xedin,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5071,,,,,,,,,,,"08/Mar/12 20:59;brandon.williams;4021.txt;https://issues.apache.org/jira/secure/attachment/12517607/4021.txt","06/Nov/12 14:19;brandon.williams;node1.log;https://issues.apache.org/jira/secure/attachment/12552274/node1.log","11/Mar/13 23:15;jbellis;patch_4021.txt;https://issues.apache.org/jira/secure/attachment/12573208/patch_4021.txt",,,,,,,,,,,,,,,3.0,yulinyen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230865,,,Mon Mar 11 23:15:33 UTC 2013,,,,,,,,,,"0|i0dvbb:",79002,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Mar/12 20:59;brandon.williams;Apparently this goes all the way back to CASSANDRA-1471 but occurs very rarely.  Patch to attempt deletion without caring about success.;;;","08/Mar/12 23:25;jbellis;The reasoning is that since the data component is written (renamed) last, the others should be there too if the data is.;;;","08/Mar/12 23:29;brandon.williams;Aren't we actually firing this when the data does not exist, though?

{code}
            if (components.contains(Component.DATA) && dataFile.length() > 0)
                // everything appears to be in order... moving on.
                continue;

            // missing the DATA file! all components are orphaned
{code};;;","11/Mar/12 23:59;jbellis;Not sure what you mean -- since data file is renamed last, it's normal (if the process was killed mid-flush) for components to exist without a data file.  But I can't think of a situation that would cause the inverse.  Are we perhaps deleting things asynchronously somewhere?;;;","12/Mar/12 00:11;brandon.williams;I'm not sure how this happened, I tried to repro artificially and wasn't able to.  Originally what happened is I was testing a patch that threw a TON of errors (all time was spent in logging), and after a ctrl-c and restart this happened.

Is it really important to confirm the deletion here?  Being unable to start rather sucks.;;;","30/Mar/12 09:08;slebresne;What's weird is that the INDEX component was clearly found at the beginning of scrubDataDirectories, but didn't existed anymore when we tried the deletion. However, this happens during AbstractDaemon.setup() so I don't think there can be any concurrent process deleting the file. Anyway, I don't thing the delete confirmation is very important but I'm not fond of changing code when we don't understand what's going on. At least as long as nobody have reproduced without using a buggy patch initially.;;;","30/Mar/12 16:00;brandon.williams;FWIW, there has been another case of this reported, but it was on Windows.;;;","02/Apr/12 21:49;jbellis;I don't suppose you have a log file for that case?;;;","30/Jul/12 21:46;rcoli;Trival repro case for something similar :

1) create file named something-that-looks-like-a-sstable-but-isn't, for example the tablesnap-created filename ""ObfuscatedCF-hd-8813-Statistics.db-listdir.json"", which users of tablesnap are relatively likely to accidentally have in their datadir.

2) stop node.

3) start.

This case seems like an issue with the sstable identifying code, real sstables names don't end with random strings like ""-listdir.json"". :);;;","30/Jul/12 21:49;jbellis;My mind boggles that you appear to expect Cassandra to cope with arbitrary user-created files in the directories it is supposed to have control over.;;;","30/Jul/12 22:48;rcoli;I don't especially ""expect"" Cassandra to cope with arbitrary user-created files in the directories it is supposed to have control over. The purpose of my comment was primarily to assist any other operator who might have accidentally created such a file, who would then google the exception and be confused because this ticket was marked no-repro.

However.. the comment for scrubDataDirectories says the following :
""
     * Removes unnecessary files from the cf directory at startup: these include temp files, orphans, zero-length files
     * and compacted sstables. Files that cannot be recognized will be ignored.
""

So it is a goal to ""recognize"" files properly, and to ""ignore"" files that are not ""recognized"" properly.

Further in the code we see..
""
if (!""snapshots"".equals(name) && !""backups"".equals(name) && !name.contains("".json""))
""

Which suggests that had my file not happened to have had suffix .json, it would have been ""recognized"" and at least logged an error about being an invalid file, even if it were not ""ignored,"" I would have had a chance of reading a relevant log message ...

I agree that the practice of creating arbitrary files named like sstables, but with an additional ""-"" in them should be considered hazardous!

But as we can easily ""recognize"" that any file with more ""-"" delimited elements in them than possible are not sstables, I continue to suggest that the user might prefer to discover this before Cassandra has tried and failed to treat such a file as a sstable, and refused to start as a result of trying to scrub the broken sstable. :D;;;","01/Nov/12 06:35;yulinyen;We also see similar error message, but it is not about the 'index'. We are using 1.0.10.

WARN [main] 2012-09-05 08:32:01,804 ColumnFamilyStore.java (line 383) Removing orphans for /opt/ruckuswireless/wsg/db/data/wsg/inventorySummary-hd-91: [Statistics.db, Filter.db, Digest.sha1, Data.db, Index.db]
ERROR [main] 2012-09-05 08:32:01,805 AbstractCassandraDaemon.java (line 373) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file inventorySummary-hd-91-Digest.sha1
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:388)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:193)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:356)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)

I was wondering if anyone could confirm that this is the same issue.;;;","01/Nov/12 21:51;brandon.williams;It looks similar, what kind of environment was this in?;;;","02/Nov/12 00:50;yulinyen;OS:
Linux TEST1 2.6.32.24 #1 SMP Wed Oct 3 14:51:26 CST 2012 x86_64 GNU/Linux

Hardware:
24 cores, 48G memory, 12G for heap. 

The first time we saw this was after the server was shutdown abruptly (due to the hardware issue). We have not seen the same issue for a while. However, it happened again just a few days ago. Unfortunately, we could not find a consistent way to reproduce it.;;;","06/Nov/12 14:19;brandon.williams;dtestbot managed to randomly reproduce this morning.  It looks like a race between compaction cleanup and forcible shutdown, then startup.  Log attached.;;;","07/Nov/12 01:12;yulinyen;I might be wrong. From the log, it looks like C* is started twice in a row. Is this a possible cause? Anyway to workaround this?;;;","12/Nov/12 21:49;jbellis;That's odd, because here's the code causing that assertion:

{code}
.           File dataFile = new File(desc.filenameFor(Component.DATA));
            if (components.contains(Component.DATA) && dataFile.length() > 0)
                // everything appears to be in order... moving on.
                continue;

            // missing the DATA file! all components are orphaned
            logger.warn(""Removing orphans for {}: {}"", desc, components);
            for (Component component : components)
            {
                FileUtils.deleteWithConfirm(desc.filenameFor(component));
            }
{code}

I must be missing something because these are the possibilities I see:

# .db exists and is non-empty.  we don't try to delete it.
# .db exists and is empty.  we delete it, and do not get a ""file does not exist"" failure
# .db does not exist (is not part of components), so we do not try to delete it
;;;","22/Jan/13 16:47;brandon.williams;Saw this in the wild again.  It was encountered multiple times on the same component set preventing startup, in the following order: CompressionInfo, CompressionInfo, Index, CompressionInfo, CompressionInfo.  After that, it was finally able to startup so we weren't able to see which components existed or did not, but it looks like it does make incremental progress on the deletions with each restart.  I don't want to paper over something without understanding what's going on here, but it's highly annoying and visible when this error is fatal.  We aren't making progress here too quickly, so maybe in the interim we should add more logging around this and log something at ERROR instead of dying.;;;","24/Jan/13 19:06;sgpope;Saw this today on one of our test machine. Windows box with Cassandra 1.1.5. It would be nice to have it log and continue instead of puking. :) On the bright side, it does make progress every time upon startup and eventually starts fine. Haven't been able to reproduce it since. Log:

WARN [main] 2013-01-24 13:01:50,480 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Statistics.db, Filter.db, Digest.sha1, Data.db, Index.db]
ERROR [main] 2013-01-24 13:01:50,480 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Filter.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
WARN [main] 2013-01-24 13:35:32,656 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Data.db, Digest.sha1, Filter.db, Index.db]
ERROR [main] 2013-01-24 13:35:32,656 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Data.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
WARN [main] 2013-01-24 13:35:44,559 ColumnFamilyStore.java (line 393) Removing orphans for C:\Program Files\Quest Software\MessageStats Business Insights\Storage\data\Doradus\MS_1_MessageParticipantPair_Terms\Doradus-MS_1_MessageParticipantPair_Terms-he-31: [Index.db, Digest.sha1, Data.db, Filter.db]
ERROR [main] 2013-01-24 13:35:44,559 AbstractCassandraDaemon.java (line 406) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file Doradus-MS_1_MessageParticipantPair_Terms-he-31-Data.db
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:68)
                at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:63)
                at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:398)
                at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:196)
                at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:389)
                at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
;;;","06/Mar/13 03:50;yulinyen;We saw this issue again on 1.0.10. It seems this happens when the file-name-change after compaction is not completed. The following are the sstables on our file system and the error we saw (the -ea option is removed):

-rw-r--r--   1 byan admin 66305 Mar  5 10:07 indexBytes-hd-111-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 10:07 indexBytes-hd-111-Digest.sha1
-rw-r--r--   1 byan admin  1936 Mar  5 10:07 indexBytes-hd-111-Filter.db
-rw-r--r--   1 byan admin   333 Mar  5 10:07 indexBytes-hd-111-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 10:07 indexBytes-hd-111-Statistics.db
-rw-r--r--   1 byan admin   240 Mar  5 12:50 indexBytes-hd-112-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 12:50 indexBytes-hd-112-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 12:50 indexBytes-hd-112-Filter.db
-rw-r--r--   1 byan admin    32 Mar  5 12:50 indexBytes-hd-112-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 12:50 indexBytes-hd-112-Statistics.db
-rw-r--r--   1 byan admin   240 Mar  5 15:18 indexBytes-hd-113-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:18 indexBytes-hd-113-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 15:18 indexBytes-hd-113-Filter.db
-rw-r--r--   1 byan admin    32 Mar  5 15:18 indexBytes-hd-113-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 15:18 indexBytes-hd-113-Statistics.db
-rw-r--r--   1 byan admin   120 Mar  5 15:26 indexBytes-hd-114-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:26 indexBytes-hd-114-Digest.sha1
-rw-r--r--   1 byan admin    16 Mar  5 15:26 indexBytes-hd-114-Filter.db
-rw-r--r--   1 byan admin    16 Mar  5 15:26 indexBytes-hd-114-Index.db
-rw-r--r--   1 byan admin  4336 Mar  5 15:26 indexBytes-hd-114-Statistics.db
-rw-r--r--   1 byan admin   333 Mar  5 15:26 indexBytes-hd-115-Index.db
-rw-r--r--   1 byan admin 66305 Mar  5 15:26 indexBytes-tmp-hd-115-Data.db
-rw-r--r--   1 byan admin    67 Mar  5 15:26 indexBytes-tmp-hd-115-Digest.sha1
-rw-r--r--   1 byan admin  1936 Mar  5 15:26 indexBytes-tmp-hd-115-Filter.db
 
WARN [main] 2013-03-05 10:43:22,494 ColumnFamilyStore.java (line 383) Removing orphans for /opt/test/tt/indexBytes-hd-115: [Digest.sha1, Index.db, Filter.db, Data.db]
ERROR [main] 2013-03-05 10:43:22,494 AbstractCassandraDaemon.java (line 373) Exception encountered during startup
java.io.IOError: java.io.IOException: Failed to delete /opt/test/tt/indexBytes-hd-115-Digest.sha1
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:392)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:193)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:356)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Caused by: java.io.IOException: Failed to delete /opt/ruckuswireless/wsg/db/data/wsg/indexBytes-hd-115-Digest.sha1
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:388)
	... 3 more


It seems the ""Descriptor"" does not take the ""temporary"" flag into account in the ""hash"" and ""equals"" function. I was wondering if anyone could confirm this.;;;","11/Mar/13 23:15;jbellis;Committed Boris's patch from the dev list (attached).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.util.ConcurrentModificationException in Gossiper,CASSANDRA-4019,12545635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,tbritz,tbritz,08/Mar/12 10:46,16/Apr/19 09:32,14/Jul/23 05:52,08/Mar/12 23:49,1.0.0,1.0.9,1.1.0,,,,0,,,,,,,"I have never seen this one before. Might be triggered by a race condition under heavy load. This error was triggered on 0.8.9


ERROR [GossipTasks:1] 2012-03-05 04:16:55,263 Gossiper.java (line 162) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.utils.AbstractStatsDeque.sum(AbstractStatsDeque.java:37)
        at org.apache.cassandra.utils.AbstractStatsDeque.mean(AbstractStatsDeque.java:60)
        at org.apache.cassandra.gms.ArrivalWindow.mean(FailureDetector.java:259)
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:282)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:155)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:538)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:157)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:1] 2012-03-05 04:16:55,263 Gossiper.java (line 737) Node /192.168.3.18 has restarted, now UP again
 INFO [GossipStage:1] 2012-03-05 04:16:55,264 Gossiper.java (line 705) InetAddress /192.168.3.18 is now UP
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/12 17:55;brandon.williams;4019-trunk.txt;https://issues.apache.org/jira/secure/attachment/12517580/4019-trunk.txt","08/Mar/12 16:19;brandon.williams;4019.txt;https://issues.apache.org/jira/secure/attachment/12517572/4019.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230818,,,Thu Mar 08 23:49:28 UTC 2012,,,,,,,,,,"0|i0gr0f:",95806,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Mar/12 15:19;brandon.williams;It seems like the Right Way to solve this is to refactor BoundedStatsDeque to operate like the dsnitch's AdapativeLatencyTracker, which *is* threadsafe, and then have ALT extend BSD.  BSD isn't used anywhere else except the FD.;;;","08/Mar/12 16:19;brandon.williams;For 0.8/1.0, do the simplest thing and switch BDS from ArrayDeque to LinkedBlockingDeque, imitating the ALT behavior (which is well tested.)  For trunk I'll do the aforementioned refator.;;;","08/Mar/12 17:55;brandon.williams;Patch for trunk which removes ASD since all we actually use is sum and mean and consolidates it into BDS.  Dsnitch's ALT is also removed since BDS itself contains all the functionality it had before.;;;","08/Mar/12 23:28;jbellis;+1 on both;;;","08/Mar/12 23:49;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WARN No appenders could be found for logger (org.apache.cassandra.confi,CASSANDRA-4013,12545613,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,leekh,leekh,08/Mar/12 06:05,16/Apr/19 09:32,14/Jul/23 05:52,08/Mar/12 06:16,,,,Legacy/Documentation and Website,,,0,,,,,,,"i'm installed apache-cassandra-1.1.0-beta1 version.
so, modify window7Config to Basic Linux Config 
and i play cmd mode....
write ""cassandra-cli"" and entered.
so i write ""create keyspace keyspace1;""
but not created. output is error....
what the... help...me...

and.. where is keyspace config?
different 0.6.8version to 1.1.0version
xml....to yaml?
 please send to mail ..
xyzxes@nate.com...
C:\apache-cassandra-1.1.0-beta1\bin>cassandra-cli
Starting Cassandra Client
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.1.0-beta1

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace keyspace1;
log4j:WARN No appenders could be found for logger (org.apache.cassandra.confi
atabaseDescriptor).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more
fo.
Cannot locate cassandra.yaml
Fatal configuration error; unable to start serv","win7 
apacheTomcat6.0
JDK1.6
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230799,,,Sun Mar 18 00:08:26 UTC 2012,,,,,,,,,,"0|i0gqyf:",95797,,leekh,,leekh,Normal,,,,,,,,,,,,,,,,,"08/Mar/12 06:16;jbellis;""Cannot locate cassandra.yaml"" was fixed in CASSANDRA-3986.  In the meantime if you run it from one directory up ({{bin\cassandra-cli}}) that should work.

The user mailing list is the right place to address questions like ""where is keyspace config."";;;","11/Mar/12 04:15;thegillis;Unfortunately the suggestion of running in the cassandra home folder did not work for me. It looks like the CLI bat file clears out the classpath and doesn't add the conf folder to it.

I just took the hint from the main cassandra.bat file and had to change the classpath blank init line to:

REM Ensure that any user defined CLASSPATH variables are not used on startup
set CLASSPATH=""%CASSANDRA_HOME%\conf""

1.0.8 on Windows

;;;","18/Mar/12 00:08;giorgiofran;Thanks Brian, 
  that solved my problem too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh still failing to handle decode errors in some column names,CASSANDRA-4003,12545279,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,06/Mar/12 04:55,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 19:31,1.0.9,1.1.0,,Legacy/Tools,,,0,cqlsh,,,,,,"Columns which are expected to be text, but which are not valid utf8, cause cqlsh to display an error and not show any output:

{noformat}
cqlsh:ks> CREATE COLUMNFAMILY test (a text PRIMARY KEY) WITH comparator = timestamp;
cqlsh:ks> INSERT INTO test (a, '2012-03-05') VALUES ('val1', 'val2');
cqlsh:ks> ASSUME test NAMES ARE text;
cqlsh:ks> select * from test;
'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}

the traceback with cqlsh --debug:

{noformat}
Traceback (most recent call last):
  File ""bin/cqlsh"", line 581, in onecmd
    self.handle_statement(st)
  File ""bin/cqlsh"", line 606, in handle_statement
    return custom_handler(parsed)
  File ""bin/cqlsh"", line 663, in do_select
    self.perform_statement_as_tokens(parsed.matched, decoder=decoder)
  File ""bin/cqlsh"", line 666, in perform_statement_as_tokens
    return self.perform_statement(cqlhandling.cql_detokenize(tokens), decoder=decoder)
  File ""bin/cqlsh"", line 693, in perform_statement
    self.print_result(self.cursor)
  File ""bin/cqlsh"", line 728, in print_result
    self.print_static_result(cursor)
  File ""bin/cqlsh"", line 742, in print_static_result
    formatted_names = map(self.myformat_colname, colnames)
  File ""bin/cqlsh"", line 413, in myformat_colname
    wcwidth.wcswidth(name.decode(self.output_codec.name)))
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xe1 in position 4: invalid continuation byte
{noformat}",,gadelkareem,jeromatron,lanzaa,psanford,thepaul,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7018,,,,,,"27/Mar/12 16:05;thepaul;4003-2.txt;https://issues.apache.org/jira/secure/attachment/12520140/4003-2.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230465,,,Tue Mar 31 21:47:14 UTC 2015,,,,,,,,,,"0|i0gqu7:",95778,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"07/Mar/12 00:45;thepaul;A fix is pushed to my 4003 github branch, also tagged at https://github.com/thepaul/cassandra/tree/pending/4003 .;;;","07/Mar/12 17:24;jbellis;It looks like the core of the fix is this:

{noformat}
+    def get_nametype(self, cursor, num):
+        """"""
+        Determine the Cassandra type of a column name from the current row of
+        query results on the given cursor. The column in question is given by
+        its zero-based ordinal number within the row.
+
+        Pretty big hack, but necessary to differentiate some things like ascii
+        vs. blob hex. Probably this should be available from the driver
+        somehow, instead.
+        """"""
+
+        row = cursor.result[cursor.rs_idx - 1]
+        col = row.columns[num]
+        schema = cursor.decoder.schema
+        return schema.name_types.get(col.name, schema.default_name_type)
{noformat}

Can you elaborate as to what's going on?;;;","07/Mar/12 17:51;thepaul;Sure. Since the CQL driver deserializes column names before the client software (cqlsh) can see them, and does not expose the Cassandra data type for the column names, it was not always possible to determine from returned column names how they were meant to be interpreted. For example, it was sometimes impossible to tell TimeUUIDType from UUIDType, or any of the various integer or counter types apart, or even BytesType from AsciiType.

Cqlsh makes an effort to display data in the most meaningful form, and secondarily to visually distinguish data that would otherwise be too ambiguous using colors. So it needs to know the original column name type.

The CQL driver does not expose that, so this code uses internals to get it. Clearly it would make more sense to expose the info from the driver side, and I plan to do that, but it takes some extra process and testing. This hack is backwards compatible with older CQL driver versions, but possibly not forwards-compat.

Maybe it would be best to do a runtime check against the driver to see if it supports exposing column types before making this call.;;;","27/Mar/12 16:05;thepaul;The attached patch (also present in my updated 4003 branch) will check for column name-type support in the CQL driver before using the direct-inspection approach. python-cql version 1.0.10 supports this, but we don't need to require support for 1.0.10 yet.;;;","27/Mar/12 16:06;thepaul;Since i linked the previous tag, the new one is at https://github.com/thepaul/cassandra/tree/pending/4003-2 .;;;","27/Mar/12 19:31;brandon.williams;Committed.;;;","02/Jul/12 10:56;jeromatron;this should be fixed in python-cql version 1.0.10 and cassandra 1.0.9?  I'm using dse 2.1 which is based on cassandra 1.0.10 and I did apt-show-versions of python cql and it says ""python-cql/stable uptodate 1.0.10-1"".  I'm still getting this error:
{quote}
'ascii' codec can't decode byte 0xc3 in position 9: ordinal not in range(128)
{quote}

Is there still the possibility for this to come up?  I'm utilizing a solr_query in the where clause btw, if that makes any difference.;;;","09/Jul/12 22:06;thepaul;That shouldn't make a difference. Can you tell me what data is in that cf/column and paste the full output of a minimal ""cqlsh --debug"" session with that query?;;;","31/Mar/15 13:51;gadelkareem;I experience this bug while importing a file using SOURCE file;;;","31/Mar/15 21:47;thobbs;[~gadelkareem] if you're seeing this problem with Cassandra 2.0 or 2.1, can you open a new ticket with more details and a new stacktrace?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Division by zero on get_slice,CASSANDRA-4000,12545192,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,vanger,vanger,05/Mar/12 15:45,16/Apr/19 09:32,14/Jul/23 05:52,07/Mar/12 10:14,1.0.9,,,,,,0,,,,,,,"We have a column family with String row keys and Long column keys.

Our WideEntityService is trying to get the first column in the range from 0 to Long.MAX. It's a batch operation performed for every row in the CF (rows count is approximately tens of thousands and each row contains from 0 to 1000 columns). 

After processing each row we are removing some of the columns we have queried. Also, at the same time we are writing in this CF in another threads but somewhat less intensive.

An error rises approximately for a one of 100 rows.


Exception itself:
[05-Mar-2012 18:47:25,247] ERROR [http-8095-1 WideEntityServiceImpl.java:142] - get: key1 - {type=RANGE, start=0, end=9223372036854775807, orderDesc=false, limit=1}
me.prettyprint.hector.api.exceptions.HCassandraInternalException: Cassandra encountered an internal error processing this request: TApplicationError type: 6 message:Internal error processing get_slice
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:31)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:285)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$7.execute(KeyspaceServiceImpl.java:268)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getSlice(KeyspaceServiceImpl.java:289)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:53)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery$1.doInKeyspace(ThriftSliceQuery.java:49)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftSliceQuery.execute(ThriftSliceQuery.java:48)","We start getting this exception after upgrading from 1.0.1 -> 1.0.8.

4 nodes cluster on Cassandra v1.0.8. RF = 3. 
Hector v0.8.0-3.",jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/12 10:32;slebresne;4000.txt;https://issues.apache.org/jira/secure/attachment/12517216/4000.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230378,,,Wed Mar 07 10:14:17 UTC 2012,,,,,,,,,,"0|i0gqsn:",95771,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Mar/12 17:39;amorton;What was the error in the server log ?;;;","06/Mar/12 06:41;vanger;ERROR [Thrift:104] 2012-03-01 20:19:57,332 Cassandra.java (line 3041) Internal error processing get_slice
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:87)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:724)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:283)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:365)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:326)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662);;;","06/Mar/12 10:32;slebresne;Slightly stupid bug introduced by CASSANDRA-3934. Fix attached;;;","06/Mar/12 16:18;jbellis;Can we add a test to catch this regression?;;;","06/Mar/12 16:37;slebresne;I've just pushed a dtest to reproduce (and checked the patch does fix). It's during short reads, so not sure there's an easy way to test that with a unit test.;;;","06/Mar/12 18:04;jbellis;+1, although we might want to query for 2*count instead of count+1 in the no-live-columns case, since if all count columns were dead the odds seem pretty good that just one more column won't help.;;;","06/Mar/12 18:40;slebresne;I hesitate on that one, but in fact I think that this case can realistically happen when:
# either the user requested 1 or 2 columns (like in this ticket). Asking for +1 or *2 doesn't change much.
# the row has been deleted but one node didn't got the tombstone yet (or at all). In that case, you mostly want for the RR to send the missed tombstone to the node missing it. However, we can't be sure the retry will arrive after the RR (i.e. we may need a third retry if we're unlucky), and so asking for twice more data from that one node may be less efficient.

I mean, clearly there is no need to over-think this and I'm good with either +1 and *2 but I'm not really convinced one will be better (nor really worse) than the other in general.;;;","06/Mar/12 19:00;jbellis;Either one works for me.;;;","07/Mar/12 10:14;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keys index skips results,CASSANDRA-3996,12545088,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,darkdimius,darkdimius,04/Mar/12 11:03,16/Apr/19 09:32,14/Jul/23 05:52,27/Mar/12 22:25,1.1.0,,,,,,0,,,,,,,"While scanning results page if range index meets result already seen in previous result set it decreases columnsRead that causes next iteration to treat columsRead<rowsPerQuery as if last page was not full and scan is done.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/12 22:06;xedin;CASSANDRA-3996-v2.patch;https://issues.apache.org/jira/secure/attachment/12520201/CASSANDRA-3996-v2.patch","27/Mar/12 18:06;xedin;CASSANDRA-3996.patch;https://issues.apache.org/jira/secure/attachment/12520163/CASSANDRA-3996.patch","12/Mar/12 07:25;darkdimius;KeysSearcher_fix_and_refactor-v2.patch;https://issues.apache.org/jira/secure/attachment/12517990/KeysSearcher_fix_and_refactor-v2.patch","04/Mar/12 11:20;darkdimius;KeysSearcher_fix_and_refactor.patch;https://issues.apache.org/jira/secure/attachment/12516991/KeysSearcher_fix_and_refactor.patch",,,,,,,,,,,,,,4.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230274,,,Tue Mar 27 22:25:37 UTC 2012,,,,,,,,,,"0|i0gqr3:",95764,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Mar/12 11:20;darkdimius;This patch also refactors keysSearcher for more generic way. It creates method nextIndexKey for generic index searchers that is suitable for indexes that return results from multiple indexCfs rows.

Created in Wikimart.ru;;;","12/Mar/12 07:25;darkdimius;git diff output;;;","13/Mar/12 11:15;xedin;Committed with CodeStyle refactoring, thanks!;;;","13/Mar/12 15:00;jbellis;So...  is this a real bug fix?  We should add a test case if so and commit to 1.1.0 if not 1.0.9.;;;","13/Mar/12 15:18;xedin;yeah, this is a real thing. Dmitry, can you work on the test-case? I have already committed it to 1.1.1, should I revert?;;;","13/Mar/12 15:58;jbellis;it may or may not be easier to merge forwards if we revert the current changes first.  hard to say with git. :);;;","16/Mar/12 10:23;slebresne;imho it's easier to revert and recommit to the right versions if only to ensure changelog entries ends up where they should.;;;","19/Mar/12 15:18;jbellis;Reverted from 1.1 and trunk;;;","27/Mar/12 18:06;xedin;This bug was introduced by (3297a96e1849f41d9c61f024282ed52d642e0794 - Merge get_indexed_slices with get_range_slices) in 1.1.0 and does not affect 1.0.x versions, attached patch adds test along side with updated code.;;;","27/Mar/12 22:06;xedin;v2 with actual change to the KeysSearcher.getIndexedIterator method (no refactoring) and test-case.;;;","27/Mar/12 22:12;jbellis;+1 on v2

I'm not necessarily against doing the rest of this refactor in another ticket, since -- and I think this is the goal -- it sets us up to do a merge of results from multiple indexes), but I'd like to see more comments describing the code organization for a relatively large change like that.  (From a quick look, it also looks like changing ""return endOfData"" to ""resetState"" is unnecessary -- I prefer the former since it makes it clear that once you get to this point in the code, the iterator is finished.);;;","27/Mar/12 22:25;xedin;Committed v2. Agree with Jonathan, such refactoring could be done in the separate task. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaDoc fix for org.apache.cassandra.db.filter.QueryFilter,CASSANDRA-3993,12545084,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,darkdimius,darkdimius,darkdimius,04/Mar/12 10:21,16/Apr/19 09:32,14/Jul/23 05:52,06/Mar/12 19:16,1.1.1,,,Legacy/Documentation and Website,,,0,,,,,,,@param should be on separate line,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,"04/Mar/12 10:23;darkdimius;JavaDoc_fix_in_QueryFilter.patch;https://issues.apache.org/jira/secure/attachment/12516985/JavaDoc_fix_in_QueryFilter.patch",,,,,,,,,,,,,,,,,1.0,darkdimius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230270,,,Tue Mar 06 19:16:59 UTC 2012,,,,,,,,,,"0|i0gqpr:",95758,,,,,Low,,,,,,,,,,,,,,,,,"04/Mar/12 10:23;darkdimius;Patch fixing the javaDoc

Created in wikimart.ru;;;","06/Mar/12 19:16;jbellis;committed, but for future reference it would be a good idea to address multiple trivial changes like this in a single ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cleanup/scrub/upgradesstables promotes all sstables to next level (LeveledCompaction),CASSANDRA-3989,12544881,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,makiw,makiw,02/Mar/12 03:21,16/Apr/19 09:32,14/Jul/23 05:52,07/Mar/12 17:39,1.0.9,1.1.0,,,,,0,lcs,,,,,,"1.0.7 + LeveledCompactionStrategy
If you run nodetool cleanup, scrub, or upgradesstables, Cassandra execute compaction for each sstable. During the compaction, it put the new sstable to next level of the original sstable. If you run cleanup many times, sstables will reached to the highest level, and CASSANDRA-3608 will happens at next cleanup.

Reproduce procedure:
# create column family CF1 with compaction_strategy=LeveledCompactionStrategy and compaction_strategy_options={sstable_size_in_mb: 5};
# Insert some data into CF1.
# nodetool flush
# Verify the sstable is created at L1 in CF1.json
# nodetool cleanup
# Verify sstable in L1 is removed and new sstable is created at L2 in CF1.json
# repeat nodetool cleanup some times",RHEL6,doubleday,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/12 13:38;slebresne;3989.txt;https://issues.apache.org/jira/secure/attachment/12517399/3989.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230067,,,Thu Mar 15 04:05:15 UTC 2012,,,,,,,,,,"0|i0gqnz:",95750,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Mar/12 03:30;makiw;Fix LeveledManifest.promote will not promote sstables to next level if the number of source sstables (removed) is one.;;;","02/Mar/12 18:36;jbellis;Nice bug hunting.  Committed a lightly edited version of your fix: https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=53fb52ac713e5471edd988b59cbd75f202a4f57b

Thanks!;;;","06/Mar/12 05:05;makiw;Johathan, 
This fix has a problem. If you have only one Level 0 sstable, and there are no Level 1 sstables involved in the compaction, promote method will chooses 0 as newLevel, and it causes assertion.
Should I create a new ticket?;;;","06/Mar/12 05:53;jbellis;No, let's reopen this one;;;","07/Mar/12 02:47;makiw;Another problem with the fix.
In following condition, the background compaction task start looping.
- No L0 sstable
- Enough number of L1 sstables, exceed L1 capacity (compaction score > 1.1)

It seems the background task try to promote sstables to reduce compaction score, but it can't by this fix, then it will compact each L1 sstables forever.
;;;","07/Mar/12 05:55;makiw;It is more complicated than I first thought.
getCandidatesFor(int level) returns next compaction candidates for the level by:
- sort the generations and find a sstable where we left off last time
- and then find overlappng sstables from next level

So if we add a new sstable into same level, getCompactionCandidates won't return empty, and then getNextBackgoundTask returns non-null task forever.
I think we should better to back out the fix and look for better strategy to resolve the issue.;;;","07/Mar/12 13:28;slebresne;I agree, the committed fix does more harm than it helps so I reverted it.
I'm trying some other approach so will hopefully attach another version soon.;;;","07/Mar/12 13:38;slebresne;Attaching another approach that makes the compaction type available to the leveled compaction that for cleanup, scrub and upgradeSSTables simply replace the old sstable by the new one (without changing level or anything else). The rational is those operation don't really ""change"" the sstable content and should simply ""replace"" sstables.;;;","07/Mar/12 17:16;jbellis;+1;;;","07/Mar/12 17:39;slebresne;Committed, thanks;;;","08/Mar/12 04:25;scode;Hmm, Does this address the case from CASSANDRA-3952?

{quote}
I think this might happen whenever there is exactly one sstable in L0 that is large enough for the score for L0 to be > 1, and if L1 is full (so that skipLevels doesn't promote).

It's possible the sstables I had lying around from sized-tiered compaction combined with my write load conspired to trigger this case.
{quote};;;","08/Mar/12 04:37;scode;Sorry, I think I just pulled before merge-to-trunk and was still looking at a trunk containing the original version of the patch.;;;","15/Mar/12 04:05;makiw;Removed first patch file I attached here because it is harmful.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in org.apache.cassandra.service.AntiEntropyService when repair finds a keyspace with no CFs,CASSANDRA-3988,12544851,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,wdhathaway,wdhathaway,01/Mar/12 22:32,16/Apr/19 09:32,14/Jul/23 05:52,04/Mar/12 10:17,1.0.9,,,,,,0,,,,,,,"2012-03-01 21:38:09,039 [RMI TCP Connection(142)-10.253.106.21] INFO  StorageService - Starting repair command #15, repairing 3 ranges.
2012-03-01 21:38:09,039 [AntiEntropySessions:14] INFO  AntiEntropyService - [repair #d68369f0-63e6-11e1-0000-8add8b9398fd] new session: will sync /10.253.106.21, /10.253.106.248, /10.253.106.247 on range (85070591730234615865843651857942052864,106338239662793269832304564822427566080] for PersonalizationDataService2.[]
2012-03-01 21:38:09,039 [AntiEntropySessions:14] ERROR AbstractCassandraDaemon - Fatal exception in thread Thread[AntiEntropySessions:14,5,RMI Runtime]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:691)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/12 17:59;slebresne;3988-v2.txt;https://issues.apache.org/jira/secure/attachment/12516864/3988-v2.txt","02/Mar/12 08:06;slebresne;3988.txt;https://issues.apache.org/jira/secure/attachment/12516800/3988.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,230037,,,Sun Mar 04 10:17:46 UTC 2012,,,,,,,,,,"0|i0gqnj:",95748,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Mar/12 08:06;slebresne;Attached patch to skip the repair when there is no CFs.;;;","02/Mar/12 17:20;jbellis;don't we use length == 0 CFs in forceTableRepair to mean ""load a list of all of them?""  iow i think the check needs to be on the result of getValidColumnFamilies.;;;","02/Mar/12 17:43;brandon.williams;We assert this can't happen:

{code}
java.lang.AssertionError: Repairing no column families seems pointless, doesn't it
{code}

So I'm not sure how we get into this situation, but apparently we can.;;;","02/Mar/12 17:59;slebresne;You are right, I was a bit too quick. v2 attached that do that after the call to getValidColumnFamilies.;;;","02/Mar/12 18:39;jbellis;bq. We assert this can't happen

That's a good point -- how is this getting by the RepairSession constructor?  Do we have two bugs?;;;","03/Mar/12 11:46;slebresne;@Bill do you run with assertion disabled?;;;","03/Mar/12 15:44;wdhathaway;@Sylvain - Correct, we are not enabling assertions (no -ea flag).;;;","03/Mar/12 16:29;jbellis;+1 on v2, then;;;","04/Mar/12 10:17;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli shouldn't call FBU.getBroadcastAddress needlessly,CASSANDRA-3986,12544801,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Mar/12 16:19,16/Apr/19 09:32,14/Jul/23 05:52,01/Mar/12 16:34,1.0.9,1.1.0,,,,,1,,,,,,,"The cli is calling this, which causes yaml to be loaded, but the broadcast address isn't needed.

{code}
            // adding default data center from SimpleSnitch
            if (currentStrategyOptions == null || currentStrategyOptions.isEmpty())
            {
                SimpleSnitch snitch = new SimpleSnitch();
                Map<String, String> options = new HashMap<String, String>();
                options.put(snitch.getDatacenter(FBUtilities.getBroadcastAddress()), ""1"");

                ksDef.setStrategy_options(options);
            }
{code}

because SimpleSnitch always returns 'datacenter1'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/12 16:20;brandon.williams;3986.txt;https://issues.apache.org/jira/secure/attachment/12516695/3986.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229987,,,Thu Mar 08 06:58:45 UTC 2012,,,,,,,,,,"0|i0gqmv:",95745,,dbrosius@apache.org,,dbrosius@apache.org,Normal,,,,,,,,,,,,,,,,,"01/Mar/12 16:22;dbrosius@apache.org;+1

Getting user reports that this is causing cli on windows to fail due to not being able to load the yaml for some reason (when creating keyspaces);;;","01/Mar/12 16:34;brandon.williams;Committed.  Probably only causes a problem on windows because the cli doesn't have the yaml in the classpath from the bat file.;;;","08/Mar/12 06:58;leekh;how to solve the proublem???

i don't know...
============================================================
           if (currentStrategyOptions == null || currentStrategyOptions.isEmpty())
            {
                SimpleSnitch snitch = new SimpleSnitch();
                Map<String, String> options = new HashMap<String, String>();
                options.put(snitch.getDatacenter(FBUtilities.getBroadcastAddress()), ""1"");

                ksDef.setStrategy_options(options);
            }
insert in cassandra-cli.bat??
============================================================
please help me..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure a directory is selected for Compaction,CASSANDRA-3985,12544758,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,amorton,amorton,01/Mar/12 10:38,16/Apr/19 09:32,14/Jul/23 05:52,03/May/12 23:02,1.0.10,1.1.1,,,,,0,,,,,,,"From http://www.mail-archive.com/user@cassandra.apache.org/msg20757.html

CompactionTask.execute() checks if there is a valid compactionFileLocation only if partialCompactionsAcceptable() . upgradesstables results in a CompactionTask with userdefined set, so the valid location check is not performed. 

The result is a NPE, partial stack 

{code:java}
$ nodetool -h localhost upgradesstables
Error occured while upgrading the sstables for keyspace MyKeySpace
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
        at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
        at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:995)
        at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1648)
<snip>
Caused by: java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:641)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:652)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1888)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
{code}

(night time here, will fix tomorrow, anyone else feel free to fix it.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/12 22:33;jbellis;3985-2.txt;https://issues.apache.org/jira/secure/attachment/12525520/3985-2.txt","05/Mar/12 19:23;amorton;cassandra-1.0-3985.txt;https://issues.apache.org/jira/secure/attachment/12517111/cassandra-1.0-3985.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229944,,,Thu May 03 23:02:50 UTC 2012,,,,,,,,,,"0|i0gqmf:",95743,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"05/Mar/12 19:23;amorton;Modified DatabaseDescriptor.getDataFileLocationForTable() to add ensureFreeSpace.

Reformatted getDataFileLocationForTable() as it was small. 
 
Removed unused DatabaseDescriptor.getNextAvailableDataLocation()

;;;","07/Mar/12 16:35;xedin;Question - what is the reason why we only return ""true"" from ""ensureFreeSpace"" if action is not user defined?

Few styling issues:

{code}
public synchronized static String getDataFileLocationForTable(String table, long expectedCompactedFileSize,
                                                                  boolean ensureFreeSpace )
{code}

should be changed to 

{code}
public synchronized static String getDataFileLocationForTable(String table, 
                                                              long expectedCompactedFileSize,
                                                              boolean ensureFreeSpace)
{code}

or all arguments written on the same line.

Also we don't use spaces to delimit operands e.g.

{code}
for ( int i = 0 ; i < dataDirectoryForTable.length ; i++ )
{code}

I can see those styling problems inside of getDataFileLocationForTable(...) method.
;;;","22/Mar/12 13:01;xedin;Figured out answer to my stupid question :)

Committed with code-style cleanup and removed ""currentIndex"" static variable from DatabaseDescriptor which is unused after getNextAvailableDataLocation() was dropped.;;;","01/Apr/12 20:03;amorton;Sorry for not getting back.
Thanks.;;;","03/May/12 22:30;jbellis;I don't understand this.  This will still return null for user defined compactions under the same conditions it would have before, with no log message, since ensureFreeSpace == !isUserDefined.;;;","03/May/12 22:33;jbellis;It seems to me that the real fix we need is to turn the assert in CompactionTask into an if statement so it can't be turned off.  Patch attached.;;;","03/May/12 22:35;jbellis;(All the other callers of {{getDataFileLocation}} and {{getDataFileLocationForTable}} already check for null correctly.);;;","03/May/12 22:51;xedin;+1, right now we won't try to return the directory with the biggest empty space and would abort compaction if there is no sufficient space left.;;;","03/May/12 23:02;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing documents for caching in 1.1,CASSANDRA-3984,12544699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,01/Mar/12 00:26,16/Apr/19 09:32,14/Jul/23 05:52,06/Mar/12 10:48,1.1.0,,,Legacy/Documentation and Website,,,0,,,,,,,add row cache and key cache setting documentation in CliHelp.yaml,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/12 00:46;vijay2win@yahoo.com;0001-docs-for-caching.patch;https://issues.apache.org/jira/secure/attachment/12516627/0001-docs-for-caching.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229885,,,Tue Mar 06 10:48:20 UTC 2012,,,,,,,,,,"0|i0gqlz:",95741,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"06/Mar/12 10:48;slebresne;Committed (I took the liberty to do some text modification). Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't include original exception class name in CQL message,CASSANDRA-3981,12544643,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Feb/12 16:33,16/Apr/19 09:32,14/Jul/23 05:52,01/Mar/12 08:16,1.1.0,,,,,,0,cql,,,,,,"In CreateColumnFamilyStatement, we do
{noformat}
catch (ConfigurationException e)
{
    throw new InvalidRequestException(e.toString());
}
{noformat}

This result in having the exception message looking like:
{noformat}
java.sql.SQLSyntaxErrorException: org.apache.cassandra.config.ConfigurationException: Cf1 already exists in keyspace Keyspace1
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/12 16:35;slebresne;3981.patch;https://issues.apache.org/jira/secure/attachment/12516579/3981.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229829,,,Thu Mar 01 08:17:14 UTC 2012,,,,,,,,,,"0|i0gqkv:",95736,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"29/Feb/12 16:35;slebresne;Attaching trivial but actually only against CQL3. It does affect CQL2 too, but in a way, changing the error message could break clients code (if they weren't very careful) so I figured, why not leaving it the way it is for CQL2.;;;","29/Feb/12 22:59;jbellis;+1;;;","01/Mar/12 08:17;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hints Should Be Dropped When Missing CFid Implies Deleted ColumnFamily,CASSANDRA-3975,12544447,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cherro,cherro,28/Feb/12 16:51,16/Apr/19 09:32,14/Jul/23 05:52,29/Feb/12 03:16,1.0.9,1.1.0,,,,,0,datastax_qa,,,,,,"If hints have accumulated for a CF that has been deleted, Hinted Handoff repeatedly fails until manual intervention removes those hints. For 1.0.7, UnserializableColumnFamilyException is thrown only when a CFid is unknown on the sending node. As discussed on #cassandra-dev, if the schema is in agreement, the affected hint(s) should be deleted to avoid indefinite repeat failures.",,cherro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229684,,,Wed Feb 29 03:16:10 UTC 2012,,,,,,,,,,"0|i0gqin:",95726,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Feb/12 16:51;cherro;Stack trace from 1.0.7:

 INFO [HintedHandoff:854] 2012-02-27 22:39:51,183 org.apache.cassandra.db.HintedHandOffManager Started hinted handoff for token: Token(bytes[7f]) with IP: /XX.XX.XX.XXX
ERROR [HintedHandoff:854] 2012-02-27 22:39:51,186 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[HintedHandoff:854,1,main]
java.lang.RuntimeException: org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=2391
        at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=2391
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:401)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:409)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:344)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:248)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:416)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30);;;","28/Feb/12 16:55;cherro;Suggest that for 1.0.x, UnserializableColumnFamilyException might be better named UnknownColumnFamilyException (since unknown cfId is the only time its ever thrown. It could then be caught within HHOM.deliverHintsToEndpointInternal and the affected hint deleted. Its unclear to me if this needs to be tackled differently for 1.1.;;;","28/Feb/12 23:28;jbellis;Patch posted to https://github.com/jbellis/cassandra/branches/3975;;;","29/Feb/12 00:33;brandon.williams;+1;;;","29/Feb/12 03:16;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig CounterColumnFamily support,CASSANDRA-3973,12544428,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jalkanen,jalkanen,jalkanen,28/Feb/12 15:08,16/Apr/19 09:32,14/Jul/23 05:52,28/Feb/12 15:48,1.0.9,1.1.0,,,,,0,counters,hadoop,pig,,,,"Included a patch to the current test script to demonstrate that CassandraStorage does not support counter columns, as well as a patch to fix the issue.

The core reason is that CassandraStorage assumes that counters can be unpacked using CounterColumnType, when in fact the column value is already a Long.","OSX 10.6.latest, Pig 0.9.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/12 15:21;brandon.williams;3973-v2.txt;https://issues.apache.org/jira/secure/attachment/12516338/3973-v2.txt","28/Feb/12 15:10;jalkanen;cassandrastorage.txt;https://issues.apache.org/jira/secure/attachment/12516337/cassandrastorage.txt","28/Feb/12 15:10;jalkanen;testpatch.txt;https://issues.apache.org/jira/secure/attachment/12516336/testpatch.txt","28/Feb/12 15:38;jalkanen;v3.txt;https://issues.apache.org/jira/secure/attachment/12516343/v3.txt",,,,,,,,,,,,,,4.0,jalkanen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229665,,,Tue Feb 28 15:48:30 UTC 2012,,,,,,,,,,"0|i0gqhz:",95723,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Feb/12 15:10;jalkanen;Patch for tests; another patch for CassandraStorage.;;;","28/Feb/12 15:21;brandon.williams;v2 to nip counters directly in parseType so we can catch them in super columns too.;;;","28/Feb/12 15:38;jalkanen;V2 patch wasn't working, getDefaultMarshallers() was invoking TypeParser.parse() directly.  Here's v3 which fixes that issue (and protects against null reference).;;;","28/Feb/12 15:48;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails to deliver any hints,CASSANDRA-3972,12544412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,28/Feb/12 12:35,16/Apr/19 09:32,14/Jul/23 05:52,28/Feb/12 16:35,1.1.0,,,,,,0,hintedhandoff,,,,,,"Summary says it all.  Whether in a memtable or sstable, no hints are delivered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4772,,,,,,,,"28/Feb/12 16:29;brandon.williams;3972.txt;https://issues.apache.org/jira/secure/attachment/12516349/3972.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229649,,,Tue Feb 28 16:35:36 UTC 2012,,,,,,,,,,"0|i05h0f:",29853,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"28/Feb/12 13:12;brandon.williams;At least one problem here is that CFS.removeDeleted is removing everything here (though listing the hints family in the cli works correctly):

{code}
ColumnFamily hintsPage = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(filter), Integer.MAX_VALUE);
{code}

At DEBUG you can see the columns collected, but then hintsPage is null.  If we simply change this to:

{code}
ColumnFamily hintsPage = hintStore.getColumnFamily(filter);
{code}

Then at least some of the hints get sent.;;;","28/Feb/12 14:35;jbellis;removeDeleted should just drop tombstones.;;;","28/Feb/12 14:58;brandon.williams;It's not too surprising then that a very long and arduous bisect points toward CASSANDRA-3716.;;;","28/Feb/12 15:45;slebresne;CASSANDRA-3716 made it so that purge don't use the current time internally and is only based on the column local deletion time and the value of gcBefore. So indeed, ColumnFamilyStore.removeDeleted(c, Integer.MAX_VALUE) will remove every expiring column since it basically means 'remove everything that will be expired at the end of time'. The current time should be used instead as gcBefore. Not sure what is the remaining problem though if it doesn't fix it fully.;;;","28/Feb/12 16:29;brandon.williams;That does indeed fix it.  Patch to use the current time when filtering tombstones.;;;","28/Feb/12 16:31;jbellis;+1;;;","28/Feb/12 16:35;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test fixes for Windows,CASSANDRA-3967,12544364,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,28/Feb/12 04:42,16/Apr/19 09:32,14/Jul/23 05:52,16/Mar/12 16:59,1.1.0,,,Legacy/Testing,,,0,,,,,,,"- SchemaLoader initializes CommitLog, creating empty segments; CleanupHelper then errors out trying to delete them (because they are still open)
- CFS.clearUnsafe resets the sstable generation but does not remove the sstables involved, so Windows will error out trying to rename over the existing ones",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/12 04:42;jbellis;3967.txt;https://issues.apache.org/jira/secure/attachment/12516274/3967.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229601,,,Fri Mar 16 16:59:43 UTC 2012,,,,,,,,,,"0|i0gqfj:",95712,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"28/Feb/12 09:20;slebresne;The problem is that most tests rely on the behavior of CFS.clearUnsafe to not remove sstables. Typically, in SSTableReaderTest, there is 'persistence' tests that use clearUnsafe followed by a loadNewSSTable to check we correctly reload sstables. CASSANDRA-3970 is another example where we rely on the sstable not being removed. Also ColumnFamilyStoreTest and RecoveryManagerTest.

The patch also seem to be breaking a test in DatabaseDescriptorTest (testTransKsMigration that doesn't fail without the patch) but that test doesn't use clearUnsafe so I'm not sure what's going on.;;;","08/Mar/12 22:41;jbellis;Posted a second attempt to https://github.com/jbellis/cassandra/branches/3967-2. This time I take the approach of changing load of new sstables to renumber when it loads to avoid conflicts.;;;","16/Mar/12 11:01;slebresne;There is a small bug in that after renaming we try to load the old (before renaming) descriptor. Added commit to https://github.com/pcmanus/cassandra/commits/3967-2 to fix that. With that, and while I don't have a windows and can't vouch that it fixes tests there, since that is anyway a more reasonable way to load new sstables, +1.;;;","16/Mar/12 16:59;jbellis;committed fixed version, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra code base has files with import statements having ""*"" causing compilation failure",CASSANDRA-3965,12544339,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,harishd,harishd,27/Feb/12 23:22,16/Apr/19 09:32,14/Jul/23 05:52,29/Feb/12 06:58,,,,,,,0,,,,,,,"I tried to download a jar as part of the new unit test I am writing. I ran my unit test successfully but later if I run ""ant"" without ant clean, I run into the following compilation issue

{code}
build-project:
     [echo] apache-cassandra: /Users/harish/workspace/cassandra/build.xml
    [javac] Compiling 40 source files to /Users/harish/workspace/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 568 source files to /Users/harish/workspace/cassandra/build/classes/main
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1607: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Iterable<DecoratedKey<?>> keySamples(Range<Token> range)
    [javac]                                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:196: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Collection<Range<Token>> getLocalRanges(String table)
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:201: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getLocalPrimaryRange()
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:912: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Map<Range<Token>, List<InetAddress>> getRangeToAddressMap(String keyspace)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1009: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Map<Range<Token>, List<InetAddress>> constructRangeToEndpointMap(String keyspace, List<Range<Token>> ranges)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1438: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1554: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private Multimap<Range<Token>, InetAddress> getChangedRangesForLeaving(String table, InetAddress endpoint)
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1975: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public AntiEntropyService.RepairFuture forceTableRepair(final Range<Token> range, final String tableName, final String... columnFamilies) throws IOException
    [javac]                                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2017: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Range<Token> getPrimaryRangeForEndpoint(InetAddress ep)
    [javac]            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2027: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     Collection<Range<Token>> getRangesForEndpoint(String table, InetAddress ep)
    [javac]                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2038: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Range<Token>> getAllRanges(List<Token> sortedTokens)
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2128: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public List<Token> getSplits(String table, String cfName, Range<Token> range, int keysPerSplit)
    [javac]                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2152: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private List<DecoratedKey> keySamples(Iterable<ColumnFamilyStore> cfses, Range<Token> range)
    [javac]                                                                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2762: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch streamRanges(final Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable)
    [javac]                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2816: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     private CountDownLatch requestRanges(final Map<String, Multimap<InetAddress, Range<Token>>> ranges)
    [javac]                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                                                                                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                     ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2866: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]     public Pair<Set<Range<Token>>, Set<Range<Token>>> calculateStreamAndFetchRanges(Collection<Range<Token>> current, Collection<Range<Token>> updated)
    [javac]                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:393: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Range range = new Range<Token>(token, token);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/db/ColumnFamilyStore.java:1328: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         assert !(range instanceof Range) || !((Range)range).isWrapAround() || range.right.isMinimum() : range;
    [javac]                                                ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:853: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>,List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:884: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:904: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, Collection<InetAddress>> entry : tokenMetadata_.getPendingRanges(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:919: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = getAllRanges(tokenMetadata_.sortedTokens());
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:959: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<Range<Token>, List<InetAddress>> entry : getRangeToAddressMap(keyspace).entrySet())
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:961: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range range = entry.getKey();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1011: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> rangeToEndpointMap = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1012: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1356: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> pendingRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1368: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> addressRanges = strategy.getAddressRanges();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1374: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> affectedRanges = new HashSet<Range<Token>>();
    [javac]                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1380: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : affectedRanges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1399: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1417: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1441: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> rangeAddresses = Table.open(table).getReplicationStrategy().getRangeAddresses(tokenMetadata_);
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1442: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<InetAddress, Range<Token>> sourceRanges = HashMultimap.create();
    [javac]                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1446: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<String, Map.Entry<InetAddress, Collection<Range<Token>>>> rangesToFetch = HashMultimap.create();
    [javac]                                                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1513: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1514: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Set<Range<Token>> myNewRanges = new HashSet<Range<Token>>();
    [javac]                                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1515: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<Range<Token>, InetAddress> entry : changedRanges.entries())
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1520: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> sourceRanges = getNewSourceRanges(table, myNewRanges);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1521: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : sourceRanges.asMap().entrySet())
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1530: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Map.Entry<InetAddress, Collection<Range<Token>>> entry : rangesToFetch.get(table))
    [javac]                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1533: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> ranges = entry.getValue();
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1557: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getRangesForEndpoint(table, endpoint);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1562: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<Range<Token>, List<InetAddress>> currentReplicaEndpoints = new HashMap<Range<Token>, List<InetAddress>>();
    [javac]                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1565: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1575: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Multimap<Range<Token>, InetAddress> changedRanges = HashMultimap.create();
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1582: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1916: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Collection<Range<Token>> ranges = getLocalRanges(tableName);
    [javac]                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:1921: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Range<Token> range : ranges)
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2045: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         List<Range<Token>> ranges = new ArrayList<Range<Token>>();
    [javac]                                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2049: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Range<Token> range = new Range<Token>(sortedTokens.get(i - 1), sortedTokens.get(i));
    [javac]                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2052: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = new Range<Token>(sortedTokens.get(size - 1), sortedTokens.get(0));
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2164: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Range<Token> range = getLocalPrimaryRange();
    [javac]         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2253: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStream = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2257: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesMM = getChangedRangesForLeaving(table, FBUtilities.getBroadcastAddress());
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                           ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2323: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<InetAddress, Range<Token>>> rangesToFetch = new HashMap<String, Multimap<InetAddress, Range<Token>>>();
    [javac]                                                                                                                    ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                              ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2324: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Map<String, Multimap<Range<Token>, InetAddress>> rangesToStreamByTable = new HashMap<String, Multimap<Range<Token>, InetAddress>>();
    [javac]                                                                                                               ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2336: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> currentRanges = getRangesForEndpoint(table, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2338: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Collection<Range<Token>> updatedRanges = strategy.getPendingAddressRanges(tokenMetadata_, newToken, localAddress);
    [javac]                        ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2342: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeAddresses = strategy.getRangeAddresses(tokenMetadata_);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2345: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Pair<Set<Range<Token>>, Set<Range<Token>>> rangesPerTable = calculateStreamAndFetchRanges(currentRanges, updatedRanges);
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2351: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesToFetchWithPreferredEndpoints = ArrayListMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2352: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toFetch : rangesPerTable.right)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2354: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 for (Range<Token> range : rangeAddresses.keySet())
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2367: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangeWithEndpoints = HashMultimap.create();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2369: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (Range<Token> toStream : rangesPerTable.left)
    [javac]                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2379: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> workMap = RangeStreamer.getWorkMap(rangesToFetchWithPreferredEndpoints);
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2507: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> changedRanges = getChangedRangesForLeaving(table, endpoint);
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2765: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<Range<Token>, InetAddress>> entry : rangesToStreamByTable.entrySet())
    [javac]                                         ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2767: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<Range<Token>, InetAddress> rangesWithEndpoints = entry.getValue();
    [javac]                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                 ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2777: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             final Set<Map.Entry<Range<Token>, InetAddress>> pending = new HashSet<Map.Entry<Range<Token>, InetAddress>>(rangesWithEndpoints.entries());
    [javac]                                                                                             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2779: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             for (final Map.Entry<Range<Token>, InetAddress> endPointEntry : rangesWithEndpoints.entries())
    [javac]                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2781: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 final Range<Token> range = endPointEntry.getKey();
    [javac]                       ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2819: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         for (Map.Entry<String, Multimap<InetAddress, Range<Token>>> entry : ranges.entrySet())
    [javac]                                                      ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2821: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]             Multimap<InetAddress, Range<Token>> endpointWithRanges = entry.getValue();
    [javac]                                   ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2835: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]                 Collection<Range<Token>> toFetch = endpointWithRanges.get(source);
    [javac]                            ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2868: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toStream = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]             ^
    [javac] /Users/harish/workspace/cassandra/src/java/org/apache/cassandra/service/StorageService.java:2869: reference to Range is ambiguous, both class org.apache.cassandra.dht.Range in org.apache.cassandra.dht and class com.google.common.collect.Range in com.google.common.collect match
    [javac]         Set<Range<Token>> toFetch  = new HashSet<Range<Token>>();
    [javac]                                                  ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 100 errors

BUILD FAILED
/Users/harish/workspace/cassandra/build.xml:704: Compile failed; see the compiler error output for details.


{code}

I think the jar that I imported as part of unit tests has further some dependencies. After looking into some of the sources files I see that some files like ColumnFamilyStore.java have statements like 

{code}
import com.google.common.collect.*;
import org.apache.cassandra.dht.*;
{code}

It seems like if instead of importing ""*"" if we import the exact class, we will not run into this compilation failure. I would be happy to patch this if somebody could approve this?",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229576,,,Wed Feb 29 04:29:58 UTC 2012,,,,,,,,,,"0|i0gqen:",95708,,,,,Normal,,,,,,,,,,,,,,,,,"28/Feb/12 01:45;dbrosius@apache.org;I think you are using a newer version of guava (> r08) that has conflicts with the class Range.


I believe this is already fixed in trunk.

;;;","28/Feb/12 17:45;harishd;Yes seems like it is fixed in trunk. Thanks I verified it. I could not find any ticket filed in the jira for this issue?;;;","28/Feb/12 20:26;dbrosius@apache.org;Yeah there wasn't one, it went in with https://issues.apache.org/jira/browse/CASSANDRA-3949;;;","29/Feb/12 04:29;harishd;Cool. Thanks that helps. I am resolving the issue for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception durint start up after updating cassandra,CASSANDRA-3963,12544234,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,mdymarek,mdymarek,27/Feb/12 09:42,16/Apr/19 09:32,14/Jul/23 05:52,27/Feb/12 16:59,1.1.0,,,,,,0,,,,,,,"Hi,
i`ve updated cassandra on two-nodes cluster and i`ve got this exception during start up:
{code}
INFO 09:06:06,520 Opening /cassandra/system/HintsColumnFamily/system-HintsColumnFamily-hc-1 (178828054 bytes)
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:384)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:332)
        at org.apache.cassandra.db.DefsTable.loadFromTable(DefsTable.java:156)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:514)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
{code}
we`re running 2 node cluster on snapshots from cassandra-1.1 branch:
* old snapshot: 5f5e00bc9a83bfd701723cbc7bf2307ea53da342 + patches from CASSANDRA-3740
* new snapshot: d65590ac8a5a3cfbe1529ef77346e84d6961db7d",,forsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/12 15:08;xedin;CASSANDRA-3963.patch;https://issues.apache.org/jira/secure/attachment/12516166/CASSANDRA-3963.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229471,,,Wed Feb 29 09:07:21 UTC 2012,,,,,,,,,,"0|i0gqdr:",95704,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Feb/12 15:08;xedin;patch for cassandra-1.1.0 branch (latest commit a15c35b0942f91d6f9e45139d99491e189989bde);;;","27/Feb/12 16:13;jbellis;+1;;;","27/Feb/12 16:59;xedin;Committed;;;","29/Feb/12 09:07;mdymarek;It works now, thanks for help..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage can't cast fields from a CF correctly,CASSANDRA-3962,12544197,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jalkanen,jalkanen,26/Feb/12 20:51,16/Apr/19 09:32,14/Jul/23 05:52,27/Feb/12 17:04,1.0.9,1.1.0,,,,,0,hadoop,pig,,,,,"Included scripts demonstrate the problem.  Regardless of whether the key is cast as a chararray or not, the Pig scripts fail with 

{code}
java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
	at org.apache.pig.backend.hadoop.HDataType.getWritableComparableTypes(HDataType.java:72)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map.collect(PigGenericMapReduce.java:117)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:269)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:262)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:64)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
{code}

","OSX 10.6.latest, Pig 0.9.2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/12 22:01;brandon.williams;0001-Add-LoadCaster-to-CassandraStorage.txt;https://issues.apache.org/jira/secure/attachment/12516124/0001-Add-LoadCaster-to-CassandraStorage.txt","27/Feb/12 15:46;brandon.williams;0002-Compose-key-from-marshaller.txt;https://issues.apache.org/jira/secure/attachment/12516167/0002-Compose-key-from-marshaller.txt","26/Feb/12 20:51;jalkanen;test.cli;https://issues.apache.org/jira/secure/attachment/12516115/test.cli","26/Feb/12 20:54;jalkanen;test.pig;https://issues.apache.org/jira/secure/attachment/12516116/test.pig",,,,,,,,,,,,,,4.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229434,,,Mon Feb 27 17:04:36 UTC 2012,,,,,,,,,,"0|i0gqcf:",95698,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"26/Feb/12 20:51;jalkanen;Generate the test CFs.;;;","26/Feb/12 20:54;jalkanen;The test pig script for both cases, you might want to comment out the ""dump a"" to let it continue.;;;","26/Feb/12 20:55;jalkanen;Relevant IRC log from #cassandra

{code}
[22:26] driftx: hmm, I think this is the udfcontext signature reuse problem
[22:26] driftx: jeromatron: what's the workaround for that again?
[22:26] Ecyrd: Is there an open bug?
[22:28] driftx: #2869, but we fixed it. hmm.
[22:28] CassBotJr: https://issues.apache.org/jira/browse/CASSANDRA-2869 : CassandraStorage does not function properly when used multiple times in a single pig script due to UDFContext sharing issues
[22:34] driftx:         case DataType.CHARARRAY:
[22:34] driftx:             return new NullableText((String)o);
[22:34] driftx: so it thinks it has a chararray, but it still has a bytearray
[22:42] driftx: I think we have to implement a LoadCaster to get around this
[22:43] Ecyrd: So I'm not insane, this is a real bug 
{code};;;","26/Feb/12 20:59;brandon.williams;I think implementing LoadCaster will fix this, but it's strange to me that pig doesn't allow going to the other way, casting a chararray to a bytearray since that's the only thing guaranteed to work here, in case the Bytes CF has keys that won't map to UTF8.;;;","26/Feb/12 22:01;brandon.williams;Patch to add a LoadCaster.  It does get used and converts the byte[] to String, but the join still fails with the same error :(;;;","27/Feb/12 12:09;jalkanen;Could the fact that these are row keys, not columns, have something to do with the issue? Looking at CassandraStorage.getNext(), there's a line

    // set the key
    tuple.append(new DataByteArray(ByteBufferUtil.getArray(key)));

So it looks to me like the key is *always* added as a DataByteArray, regardless of it's actual type? getSchema() does seem to read the value from CfDef correctly tho'.;;;","27/Feb/12 15:46;brandon.williams;You're totally right.  It wasn't a problem casting the key from Bytes, but trying to use the one from U8.  Patch to compose the key from the marshaller.;;;","27/Feb/12 16:46;xedin;+1;;;","27/Feb/12 17:04;brandon.williams;Committed 0002, and also incorporated Janne's test into the existing tests.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Supercolumn serialization assertion failure,CASSANDRA-3957,12544061,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,24/Feb/12 21:51,16/Apr/19 09:32,14/Jul/23 05:52,07/Mar/12 17:39,1.0.9,1.1.0,,,,,1,datastax_qa,,,,,,"As reported at http://mail-archives.apache.org/mod_mbox/cassandra-user/201202.mbox/%3CCADJL=w5kH5TEQXOwhTn5Jm3cmR4Rj=NfjcqLryXV7pLyASi95A@mail.gmail.com%3E,

{noformat}
ERROR 10:51:44,282 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 4690 to accomodate data size
of 2347 (predicted 2344) for RowMutation(keyspace='Player',
key='36336138643338652d366162302d343334392d383466302d356166643863353133356465',
modifications=[ColumnFamily(PlayerCity [SuperColumn(owneditem_1019
[]),SuperColumn(owneditem_1024 []),SuperColumn(owneditem_1026
[]),SuperColumn(owneditem_1074 []),SuperColumn(owneditem_1077
[]),SuperColumn(owneditem_1084 []),SuperColumn(owneditem_1094
[]),SuperColumn(owneditem_1130 []),SuperColumn(owneditem_1136
[]),SuperColumn(owneditem_1141 []),SuperColumn(owneditem_1142
[]),SuperColumn(owneditem_1145 []),SuperColumn(owneditem_1218
[636f6e6e6563746564:false:5@1329648704269002
,63757272656e744865616c7468:false:3@1329648704269006
,656e64436f6e737472756374696f6e54696d65:false:13@1329648704269007
,6964:false:4@1329648704269000,6974656d4964:false:15@1329648704269001
,6c61737444657374726f79656454696d65:false:1@1329648704269008
,6c61737454696d65436f6c6c6563746564:false:13@1329648704269005
,736b696e4964:false:7@1329648704269009,78:false:4@1329648704269003
,79:false:3@1329648704269004,]),SuperColumn(owneditem_133
[]),SuperColumn(owneditem_134 []),SuperColumn(owneditem_135
[]),SuperColumn(owneditem_141 []),SuperColumn(owneditem_147
[]),SuperColumn(owneditem_154 []),SuperColumn(owneditem_159
[]),SuperColumn(owneditem_171 []),SuperColumn(owneditem_253
[]),SuperColumn(owneditem_422 []),SuperColumn(owneditem_438
[]),SuperColumn(owneditem_515 []),SuperColumn(owneditem_521
[]),SuperColumn(owneditem_523 []),SuperColumn(owneditem_525
[]),SuperColumn(owneditem_562 []),SuperColumn(owneditem_61
[]),SuperColumn(owneditem_634 []),SuperColumn(owneditem_636
[]),SuperColumn(owneditem_71 []),SuperColumn(owneditem_712
[]),SuperColumn(owneditem_720 []),SuperColumn(owneditem_728
[]),SuperColumn(owneditem_787 []),SuperColumn(owneditem_797
[]),SuperColumn(owneditem_798 []),SuperColumn(owneditem_838
[]),SuperColumn(owneditem_842 []),SuperColumn(owneditem_847
[]),SuperColumn(owneditem_849 []),SuperColumn(owneditem_851
[]),SuperColumn(owneditem_852 []),SuperColumn(owneditem_853
[]),SuperColumn(owneditem_854 []),SuperColumn(owneditem_857
[]),SuperColumn(owneditem_858 []),SuperColumn(owneditem_874
[]),SuperColumn(owneditem_884 []),SuperColumn(owneditem_886
[]),SuperColumn(owneditem_908 []),SuperColumn(owneditem_91
[]),SuperColumn(owneditem_911 []),SuperColumn(owneditem_930
[]),SuperColumn(owneditem_934 []),SuperColumn(owneditem_937
[]),SuperColumn(owneditem_944 []),SuperColumn(owneditem_945
[]),SuperColumn(owneditem_962 []),SuperColumn(owneditem_963
[]),SuperColumn(owneditem_964 []),])])
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
",,cherro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/12 16:19;slebresne;3957.txt;https://issues.apache.org/jira/secure/attachment/12517414/3957.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229299,,,Wed Mar 07 18:22:52 UTC 2012,,,,,,,,,,"0|i0gq9z:",95687,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Feb/12 22:04;jbellis;I note that this is the Periodic commitlog executor, which does NOT block for CommitLog.add.  So, if the CF is serialized (later) while another thread modifies it, we could hit this error.

We don't modify the mutation CF directly in CFS.apply, since we clone into the arena allocator.

The only place I see where we modify the mutation CF is in ignoreObsoleteMutations...  but that only happens for indexed columns, so that can't be the cause here since it involves SuperColumns.

(Note that the mutation CF won't be changed by updateRowCache, since the mutation CF is never inserted into the cache; it's only used to add to an existing cache row, if one exists.)

Am I missing something?

Perhaps it would be best to start by creating a test to serialize the above RowMutation and see if it reproduces in the absence of concurrent activity, to rule out the possibility of serializedSize simply having a bug w/ SuperColumns.;;;","02/Mar/12 20:42;cywjackson;below is another similar stacktrace where only standard column (no super, no composite) was used.

{panel}
ERROR 08:50:12,479 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 2090383 to accomodate data size of 736236 (predicted 2090383) for RowMutation(keyspace='cfs', key='6337343566396330363438373131653130303030653562646562313236356262', modifications=[ColumnFamily(sblocks [6337343839316430363438373131653130303030653562646562313236356262:false:736121@1330707012466,])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:680)
{panel};;;","06/Mar/12 18:57;jbellis;Jackson's stacktrace involving sblocks was caused by a bug in DSE's CFS code re-using a ByteBuffer it had handed off to Thrift.

Could be that the original report had a similar problem.;;;","06/Mar/12 22:39;cywjackson;this comes from a difference use case. the client code is in hector, maybe we need to look into how hector is handling the bytebuffer too?

ERROR [COMMIT-LOG-WRITER] 2012-02-20 06:16:56,621 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 275 to accomodate data size of 271 (predicted 275) for RowMutation(keyspace='foo', key='984fb8106d3ff5043365736457c45085', modifications=[ColumnFa
mily(Dora_la [SuperColumn(1329718356206 [5f656e64:false:1@1329718616576004,5f6c6173744576656e7454696d65:false:8@1329718616576003,5f6c61737450617468:false:8@1329718616576005,5f76697369744576
656e74496e646578:false:4@1329718616576002,5f76697369744964:false:16@1329718616576000,5f7669736974496e646578:false:4@1329718616576001,]),])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)

ERROR [COMMIT-LOG-WRITER] 2012-02-07 22:33:09,691 org.apache.cassandra.service.AbstractCassandraDaemon Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError: Final buffer length 550 to accomodate data size of 275 (predicted 271) for RowMutation(keyspace='dyn', key='7836ab62d10ddb48d00668639ebdf6ce', modifications=[ColumnFa
mily(Dora_la [SuperColumn(1328653749409 [5f656e64:false:1@1328653989655004,5f6c6173744576656e7454696d65:false:8@1328653989655003,5f6c61737450617468:false:12@1328653989655005,5f7669736974457
6656e74496e646578:false:4@1328653989655002,5f76697369744964:false:16@1328653989655000,5f7669736974496e646578:false:4@1328653989655001,]),])])
at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:279)
at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:122)
at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:599)
at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662);;;","07/Mar/12 16:19;slebresne;Here's my theory.

The stacktrace concerning the sblocks is due to a bug in DSE's CFS. This happened because CFS sometimes communicate with C* in the same JVM without going trough the wire, so the re-use of ByteBuffer was problematic.

The other stacktrace however can't be the same problem. Whatever Hector does, it goes over the wire and thus can't change the size of a ByteBuffer in C*.

However, if we eliminate the sblocks trace, all the other trace uses SuperColumn. And as it turns out, I think we have a race with SC. As Jonathan pointed out, with the period commit log, if we happen to modify a mutation that was sent to the commit log at any time, that would be a bug. This means not modifying the CF object in the RowMutation, but for SuperColumn, this also mean we shouldn't modify those super columns. However, when we update the row cache (and if it's not a SerializingCache), we do potentially store references to the original SCs in the cached row, which could then lead to the stack trace on this issue. I'm attaching a patch that fixes this.

To make sure this could be the issue here, it would help to know if the stacktrace comes from column families where row cache was used and was not set to the serializing cache.

I'll note however that the very small difference in the stacktraces between the predicted size and the actual serialized side support the theory of having say just one column of a super column updated, while in the sblocks case, the difference was much bigger, supporting the theory of the ByteBuffer content being changed to something completely different.
;;;","07/Mar/12 17:26;jbellis;+1;;;","07/Mar/12 17:39;slebresne;Committed, thanks;;;","07/Mar/12 18:22;jbellis;Thomas (the reporter of the original stack trace in the description) just confirmed on the mailing list that he was using row cache / CLHC.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HintedHandoff won't compact a single sstable, resulting in repeated log messages",CASSANDRA-3955,12544002,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,24/Feb/12 15:33,16/Apr/19 09:32,14/Jul/23 05:52,24/Feb/12 16:47,1.0.9,1.1.0,,,,,0,,,,,,,"First introduced by CASSANDRA-3554, and then mostly solved in CASSANDRA-3733, there is still one special case where the HH log message will repeat every 10 mins for 0 rows: when there have previously been hints delivered to the node, but now only a single sstable exists.  Because the we refused to compact a single sstable, and it contains tombstones for the hints, the message repeats.",,christianmovi,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/12 16:26;jbellis;3955-v2.txt;https://issues.apache.org/jira/secure/attachment/12515937/3955-v2.txt","24/Feb/12 16:17;brandon.williams;3955.txt;https://issues.apache.org/jira/secure/attachment/12515936/3955.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229240,,,Fri Feb 24 16:47:07 UTC 2012,,,,,,,,,,"0|i0gq8v:",95682,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Feb/12 15:40;jbellis;CASSANDRA-3442 seems like the ""right"" way to fix this.  Is it an easy special case for 1.0.9?;;;","24/Feb/12 16:17;brandon.williams;No need to special case, instead we'll just always force a user defined compaction.;;;","24/Feb/12 16:26;jbellis;v2 attached to cover both places HHOM compacts;;;","24/Feb/12 16:29;brandon.williams;+1;;;","24/Feb/12 16:47;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions during start up after schema disagreement,CASSANDRA-3954,12543997,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,mdymarek,mdymarek,24/Feb/12 14:49,16/Apr/19 09:32,14/Jul/23 05:52,16/Mar/12 11:04,1.1.0,,,,,,0,,,,,,,"Hi,
i`ve got schema disaggreement after dropping down keyspace,
i`ve switched off one nodes in cluster, after starting i`ve got bunch of these exceptions:
{code}
ERROR [SSTableBatchOpen:1] 2012-02-24 14:21:00,759 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer
        at org.apache.cassandra.db.marshal.UTF8Type.compare(UTF8Type.java:27)
        at org.apache.cassandra.dht.LocalToken.compareTo(LocalToken.java:45)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:89)
        at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
        at java.util.TreeMap.getEntry(TreeMap.java:328)
        at java.util.TreeMap.containsKey(TreeMap.java:209)
        at java.util.TreeSet.contains(TreeSet.java:217)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:393)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:189)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:227)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
and this one on the end of start up:
{code}
ERROR [MigrationStage:1] 2012-02-24 14:37:22,750 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:282)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:216)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:330)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:240)
        at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:124)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
Any ideas why they`ve appeared?",,forsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/12 22:04;xedin;CASSANDRA-3954-v2.patch;https://issues.apache.org/jira/secure/attachment/12518555/CASSANDRA-3954-v2.patch","14/Mar/12 20:02;xedin;CASSANDRA-3954.patch;https://issues.apache.org/jira/secure/attachment/12518364/CASSANDRA-3954.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229235,,,Fri Mar 16 11:04:09 UTC 2012,,,,,,,,,,"0|i0gq87:",95679,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Feb/12 14:59;jbellis;Sounds like you changed your cluster's partitioner.  That's not supported.;;;","24/Feb/12 16:04;forsberg;To change partitioner you would have to edit cassandra.yaml, right? There was no such change before this restart.;;;","24/Feb/12 16:21;jbellis;Can you elaborate then on what exactly changed?

Is this a 1.1 cluster or 1.0?;;;","24/Feb/12 17:05;forsberg;We had a schema disagreement in this two-node cluster. Tried restarting one of the nodes, and these exceptions occured in log.

bq. Is this a 1.1 cluster or 1.0?

You're not going to like this: It's a snapshot from cassandra-1.1 branch from about a week ago. I can find out the exact commit, but I don't have it readily available right now.;;;","24/Feb/12 22:53;jbellis;Pavel fixed CASSANDRA-3884 yesterday -- might be the same thing?;;;","29/Feb/12 09:12;mdymarek;it might be,
I`ve installed new snapshot(made from commit 449e037195c3c504d7aca5088e8bc7bd5a50e7d0)
and second exception is not thrown any more,
First type of exception( aka java.lang.ClassCastException: java.math.BigInteger cannot be cast to java.nio.ByteBuffer)
is still thrown during startup, in fact i`ve got several of them in startup logs...
Note that node is starting up even that these exceptions are thrown...;;;","14/Mar/12 04:23;jbellis;Been thinking about this for a while, and I now have a hypothesis: I bet it's some kind of confusion w/ the new unified key cache. Can you have a look, Pavel?;;;","14/Mar/12 11:16;xedin;Mariusz, can you please provide additional information - do you use secondary indexes and what is the SSTable name it fails to open (it should be on top of the exception ""Opening <sstable-name> (<n> bytes)"")?  My current guess is that it could be related to the secondary indexes.;;;","14/Mar/12 12:45;mdymarek;
yep, we`re using secondary index a lot,
{noformat}
 INFO [SSTableBatchOpen:2] 2012-02-24 14:21:00,752 SSTableReader.java (line 156) Opening /cassandra/test_keyspace/test_cf/test_keyspace-test_cf.test_cf_test_column_with_index_idx-hc-2 (727 bytes)
{noformat}
we have lost some secondary index information after restarting of cluster, we had to rebuild indexes after that...;;;","14/Mar/12 20:02;xedin;We don't support secondary index caching with global caches so I have added condition to skip reading key cache for secondary index CFs.;;;","15/Mar/12 21:28;jbellis;What if we just created the index CFMetadata with caching=NONE instead of adding a special flag?;;;","15/Mar/12 21:35;xedin;That is a good idea, v2 implements that.;;;","15/Mar/12 21:53;jbellis;better, but we should actually load key cache only for KEYS_ONLY or ALL, rather than !NONE (which would include ROWS ONLY).;;;","15/Mar/12 22:04;xedin;updated v2 which skips key cache read if caching is NONE or ROWS_ONLY.;;;","16/Mar/12 03:41;jbellis;+1

nit: it's preferred to use == with enums;;;","16/Mar/12 11:04;xedin;Committed with nit fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rename RandomAccessReader.MAX_BYTES_IN_PAGE_CACHE,CASSANDRA-3948,12543773,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,scode,scode,23/Feb/12 00:35,16/Apr/19 09:32,14/Jul/23 05:52,29/Feb/12 10:10,1.1.1,,,,,,0,,,,,,,"This should make the fadvising useless (mostly). See CASSANDRA-1470 for why, including links to kernel source. I have not investigated the history of when this broke or whether it was like from the beginning.

For the record I have not confirmed this by testing, only by code inspection. I happened to notice it working on other things, so there is some chance that I'm just mis-reading the code.
",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/12 09:58;scode;CASSANDRA-3948-trunk.txt;https://issues.apache.org/jira/secure/attachment/12516540/CASSANDRA-3948-trunk.txt",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,229012,,,Wed Feb 29 10:10:48 UTC 2012,,,,,,,,,,"0|i0gq5j:",95667,,scode,,scode,Normal,,,,,,,,,,,,,,,,,"23/Feb/12 13:15;xedin;We algorithm behind it - we do fadvice(fd, <start_position>, 0) after each 128MB of data written, flush is done in the process of each re-buffer (which is each 64KB by default) so we can skip doing sync when we do fadvice() and just use 0 which would hint kernel so skip everything starting from <start_position>. ;;;","26/Feb/12 00:38;scode;So as long as we're only writing few number of very large files, it should in practice work fairly well. Default settings are 30 seconds expiry of dirty buffers, up to 5% of page cache dirty, on Linux.

That said, MAX_BYTES_IN_PAGE_CACHE is thus not really max bytes in page cache, but rather just fadvise interval in bytes.
;;;","26/Feb/12 00:43;xedin;Exactly, we can't really control (measure) the contents of the page cache so instead we just define intervals for our files when to call fadvice.;;;","29/Feb/12 09:58;scode;Suggesting attached patch to rename the variable to reflect this (trunk only, since no functional change).;;;","29/Feb/12 10:00;xedin;+1;;;","29/Feb/12 10:10;scode;Committed (1.1 + trunk after all).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter shouldn't stream any empty data/index files that might be created at end of flush,CASSANDRA-3946,12543638,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,lenn0x,lenn0x,22/Feb/12 08:15,16/Apr/19 09:32,14/Jul/23 05:52,17/Apr/12 21:27,1.0.10,1.1.0,,,,,0,,,,,,,"If by chance, we flush sstables during BulkRecordWriter (we have seen it happen), I want to make sure we don't try to stream them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/12 19:45;yukim;0001-Abort-SSTableWriter-when-exception-occured.patch;https://issues.apache.org/jira/secure/attachment/12523009/0001-Abort-SSTableWriter-when-exception-occured.patch","22/Feb/12 08:24;lenn0x;0001-CASSANDRA-3946-BulkRecordWriter-shouldn-t-stream-any.patch;https://issues.apache.org/jira/secure/attachment/12515552/0001-CASSANDRA-3946-BulkRecordWriter-shouldn-t-stream-any.patch","17/Apr/12 20:24;yukim;3946-1.0.txt;https://issues.apache.org/jira/secure/attachment/12523016/3946-1.0.txt",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228877,,,Tue Apr 17 21:27:20 UTC 2012,,,,,,,,,,"0|i0gq4n:",95663,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Feb/12 14:49;jbellis;I'm not sure I understand -- is it trying to stream a sstable that's in the process of being written?  Or did you actually get zero-length files created?  The latter is Not Supposed To Happen.;;;","04/Apr/12 18:19;jbellis;Sounds like the right fix here is to make BRW not output zero-length sstables.;;;","04/Apr/12 18:22;brandon.williams;I'm not convinced that having the loader skip empty files is right yet.  Why are empty files being created?;;;","13/Apr/12 21:08;jbellis;Yuki, does anything look suspicious in BulkRecordWriter for zero-length file creation?;;;","13/Apr/12 21:20;brandon.williams;I suspect if there is a problem here it's actually in UnsortedSimpleSSTableWriter.;;;","17/Apr/12 19:45;yukim;The only way bulk writer writes 0 size sstable is when bad thing happens during writing.
Attached patch makes sure writer get aborted so that incomplete files are removed when exception happend during writes. In order to do this, I have to let SSTableWriter create ""temp"" file, but it should be OK since closeAndOpenReader renames file.;;;","17/Apr/12 20:01;jbellis;LGTM.  Could you also post a version against 1.0, Yuki?;;;","17/Apr/12 20:24;yukim;Patch for 1.0 branch attached.;;;","17/Apr/12 21:27;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter will throw NullExceptions if no data is sent with the reducer,CASSANDRA-3944,12543634,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,22/Feb/12 07:55,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 08:03,1.1.0,,,,,,0,,,,,,,"In the BulkRecordWriter, in the close() method if no actual output is sent to the reducer, which can be caused by having too many reducers and not enough map data, we throw null exceptions and the job can fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/12 08:01;lenn0x;0001-Fix-BulkRecordWriter-to-not-throw-NPE-if-reducer-get.patch;https://issues.apache.org/jira/secure/attachment/12515549/0001-Fix-BulkRecordWriter-to-not-throw-NPE-if-reducer-get.patch",,,,,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228873,,,Wed Feb 22 08:03:01 UTC 2012,,,,,,,,,,"0|i0gq3r:",95659,,,,,Normal,,,,,,,,,,,,,,,,,"22/Feb/12 08:03;lenn0x;Commited;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordReader can report progress > 100%,CASSANDRA-3942,12543611,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,tjake,tjake,22/Feb/12 02:53,16/Apr/19 09:32,14/Jul/23 05:52,21/Jun/12 19:47,1.1.2,,,,,,0,,,,,,,CFRR.getProgress() can return a value > 1.0 since the totalRowCount is a estimate.,,patrik.modesto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/12 17:24;brandon.williams;3942.txt;https://issues.apache.org/jira/secure/attachment/12532908/3942.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228850,,,Thu Jun 21 19:47:37 UTC 2012,,,,,,,,,,"0|i0gq2v:",95655,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jun/12 16:11;brandon.williams;So, is the idea simply to clamp it at 1.0?  Since all we have is an estimate we can't really get any more accurate.;;;","21/Jun/12 17:24;brandon.williams;Patch to clamp getProgress to 1.0 or less.;;;","21/Jun/12 18:58;jbellis;+1;;;","21/Jun/12 19:47;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"mergeShardsChance deprecated; remove from thrift?",CASSANDRA-3940,12543576,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thepaul,thepaul,21/Feb/12 22:00,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 15:32,1.1.0,,,Legacy/CQL,,,0,thrift_protocol,,,,,,Or at least it should be marked deprecated somehow.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/12 10:03;slebresne;0001-Remove-merge_shard_chance.txt;https://issues.apache.org/jira/secure/attachment/12515569/0001-Remove-merge_shard_chance.txt","22/Feb/12 10:03;slebresne;0002-Thrift-generated-files-updates.txt;https://issues.apache.org/jira/secure/attachment/12515570/0002-Thrift-generated-files-updates.txt",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228815,,,Wed Feb 22 15:32:28 UTC 2012,,,,,,,,,,"0|i0gq1z:",95651,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Feb/12 10:03;slebresne;It is indeed now useless. Attached patches to remove (against 1.1.0).;;;","22/Feb/12 14:50;jbellis;+1;;;","22/Feb/12 15:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
occasional failure of CliTest,CASSANDRA-3939,12543558,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,21/Feb/12 20:25,16/Apr/19 09:32,14/Jul/23 05:52,21/Feb/12 22:48,1.0.8,,,Legacy/Testing,,,0,,,,,,,"{{CliTest}} will occasionally fail with an NPE.

{noformat}
[junit] Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
[junit] java.lang.NullPointerException
[junit] java.lang.RuntimeException: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1039)
[junit] 	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:228)
[junit] 	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:213)
[junit] 	at org.apache.cassandra.cli.CliTest.testCli(CliTest.java:241)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.validateSchemaIsSettled(CliClient.java:2855)
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1030)
{noformat}

This occurs because no default for {{schema_mwt}} is applied unless {{main()}} is invoked.

(Trivial )patch to follow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/12 20:29;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3939-properly-initialize-CliSessionState.sch.txt;https://issues.apache.org/jira/secure/attachment/12515483/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3939-properly-initialize-CliSessionState.sch.txt",,,,,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228797,,,Tue Feb 21 22:48:48 UTC 2012,,,,,,,,,,"0|i0gq1j:",95649,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"21/Feb/12 20:51;brandon.williams;+1;;;","21/Feb/12 22:48;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection is broken,CASSANDRA-3934,12543390,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Feb/12 18:44,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 13:25,1.0.8,1.1.0,,,,,0,,,,,,,"When a read needs to do more than one retry (due to short reads), the originalCount is not preserved by the retry leading to returning more than the requested number of columns.
Moreover, when a retried read checks whether more retry is needed, it doesn't compare the number of live column retrieved against the original number of columns requested by the user, but against the number of columns requested during the retry, making it much more likely to actually do one more retry.

This catch by the two tests 'short_read_test' and 'short_read_reversed_test' at https://github.com/riptano/cassandra-dtest/blob/master/consistency_test.py that are failing intermittently.",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/12 18:50;slebresne;3934.txt;https://issues.apache.org/jira/secure/attachment/12515269/3934.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228636,,,Wed Feb 22 13:25:05 UTC 2012,,,,,,,,,,"0|i0gpzj:",95640,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"20/Feb/12 18:50;slebresne;Patch is against 1.0. The patch:
* don't retry if we have enough columns to satisfy the initial query
* keep the original count intact even if we do multiple retries.
* optimize the number of columns asked by a retry based on the ratio of number of column got/total asked on the initial query.;;;","22/Feb/12 11:32;brandon.williams;+1;;;","22/Feb/12 13:25;slebresne;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./bin/cqlsh `describe keyspace <keyspace>` command doesn't work when ColumnFamily row_cache_provider wasn't specified.,CASSANDRA-3933,12543356,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,xedin,xedin,20/Feb/12 15:08,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 10:34,1.0.8,,,,,,0,cqlsh,,,,,,"I have created Keyspace and ColumnFamily using CLI

{noformat}
/bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to Cassandra CLI version 1.0.7-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace ks;
f89384f0-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use ks;
Authenticated to keyspace: ks
[default@ks] create column family cf;
fc807690-5bd3-11e1-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] quit;
{noformat}

and then I have tried to describe keyspace using CQLsh

{noformat}
./bin/cqlsh                     
Connected to Test Cluster at localhost:9160.
[cqlsh 2.0.0 | Cassandra 1.0.7-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.20.0]
Use HELP for help.
cqlsh> describe keyspace ks;

CREATE KEYSPACE ks WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:datacenter1 = '1';

USE ks;

CREATE COLUMNFAMILY cf (
CfDef instance has no attribute 'row_cache_provider'
  KEY blob PRIMARY KEYcqlsh> 
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228602,,,Wed Feb 22 10:34:59 UTC 2012,,,,,,,,,,"0|i0gpz3:",95638,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"21/Feb/12 22:23;thepaul;Ignore expected-but-missing CF options. Change is in my 3933-1.0 branch at https://github.com/thepaul/cassandra/commit/12bcc56422 .

Patch is pretty trivial, but since it needs a tiny extra change in 1.1, I have it pre-merged in my 3933-1.1 branch at https://github.com/thepaul/cassandra/commit/a401034f56 .;;;","22/Feb/12 10:34;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossipers notion of schema differs from reality as reported by the nodes in question,CASSANDRA-3931,12543184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,19/Feb/12 04:09,16/Apr/19 09:32,14/Jul/23 05:52,21/Feb/12 19:28,1.1.0,,,,,,0,,,,,,,"On a 1.1 cluster we happened to notice that {{nodetool gossipinfo | grep SCHEMA}} reported disagreement:

{code}
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:bcdbd318-82df-3518-89e3-6b72227b3f66
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
  SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f
{code}

However, the result of a thrift {{describe_ring}} on the cluster claims they all agree and that {{b0d7bab7-c13c-37d9-9adb-8ab8a5b7215d}} is the schema they have.

The schemas seem to ""actually"" propagate; e.g. dropping a keyspace actually drops the keyspace.",,brandon.williams,cherro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/12 19:14;brandon.williams;3931-v2.txt;https://issues.apache.org/jira/secure/attachment/12515469/3931-v2.txt","21/Feb/12 18:40;brandon.williams;3931.txt;https://issues.apache.org/jira/secure/attachment/12515461/3931.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228470,,,Wed Dec 05 17:00:10 UTC 2012,,,,,,,,,,"0|i0gpy7:",95634,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"19/Feb/12 04:11;scode;I forgot to mention that {{describe_ring}} is using a piece of code that actually sends messages to other nodes asking for their schema, while {{gossipinfo}} is giving the information contained in the Gossiper's endpoint state map.

Given that the schema seems to *actually* be propagated, I suspect this is only a gossiping propagation bug but that is not confirmed.;;;","21/Feb/12 14:49;brandon.williams;Hmm, does hinted handoff work in this state?  I ask because we've had this problem before and addressed it there:

{code}
        waited = 0;
        // then wait for the correct schema version.
        // usually we use DD.getDefsVersion, which checks the local schema uuid as stored in the system table.
        // here we check the one in gossip instead; this serves as a canary to warn us if we introduce a bug that
        // causes the two to diverge (see CASSANDRA-2946)
{code};;;","21/Feb/12 18:40;brandon.williams;Patch to update gossip when merging remote versions.;;;","21/Feb/12 19:14;brandon.williams;v2 replaces all the one-off calls of passiveAnnounce by introducing updateVersionAndAnnounce in Schema.;;;","21/Feb/12 19:25;xedin;+1 with nit: change in Migration is unnecessary because passiveAnnounce get's called as part of Migration.announce() routine so we don't need to change apply() behavior.;;;","21/Feb/12 19:28;brandon.williams;Committed w/Migration.apply change removed.;;;","22/Feb/12 05:31;scode;@Brandon For the record, we had HH turned off so I don't know.;;;","05/Dec/12 17:00;cherro;FYI the fixes for this issue introduced issue CASSANDRA-5025.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
all column validator options are not represented in cli help,CASSANDRA-3926,12542950,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kirktrue,jeromatron,jeromatron,16/Feb/12 22:10,16/Apr/19 09:32,14/Jul/23 05:52,13/Mar/12 14:59,1.1.0,,,Legacy/Documentation and Website,,,0,cli,lhf,,,,,"The options added to column validators from CASSANDRA-2530 are not shown as options in the CLI help.  I was going to create a column family with a float validator and double checked the help and it wasn't shown.  So I just had to double check that I could.  Would be nice to have those added to those docs, even though CQL is the way forward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/12 00:40;kirktrue;trunk-2530.txt;https://issues.apache.org/jira/secure/attachment/12517343/trunk-2530.txt",,,,,,,,,,,,,,,,,1.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228236,,,Tue Mar 13 14:59:17 UTC 2012,,,,,,,,,,"0|i0gpw7:",95625,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"16/Feb/12 22:11;jeromatron;fwiw, the docs are found in src/resources/org/apache/cassandra/cli/CliHelp.yaml;;;","07/Mar/12 00:40;kirktrue;Patch to add appropriate lines to CliHelp.yaml.;;;","07/Mar/12 00:50;kirktrue;I noticed some other places in that file where types are mentioned, outside the context of column creation/modification. 

For example, in the {{NODE_THRIFT_GET}} section, the {{function}} can be specified to parse arguments. There are no entries for dates, et al. there. Should there be?;;;","13/Mar/12 14:59;xedin;Functions are different thing so we don't need to update doc for them. Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming from all (not one) neighbors during rebuild/bootstrap,CASSANDRA-3922,12542825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,scode,scode,scode,16/Feb/12 06:17,16/Apr/19 09:32,14/Jul/23 05:52,16/Feb/12 08:40,1.1.0,,,,,,0,,,,,,,"The last round of changes that happened in CASSANDRA-3483 before it went in actually changed behavior - we now stream from *ALL* neighbors that have a range, rather than just one. This leads to data size explosion.

Attaching patch to revert to intended behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/12 06:18;scode;CASSANDRA-3922-1.1.txt;https://issues.apache.org/jira/secure/attachment/12514762/CASSANDRA-3922-1.1.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228111,,,Thu Feb 16 08:40:57 UTC 2012,,,,,,,,,,"0|i0gpuf:",95617,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"16/Feb/12 08:40;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction doesn't clear out expired tombstones from SerializingCache,CASSANDRA-3921,12542796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Feb/12 23:56,16/Apr/19 09:32,14/Jul/23 05:52,16/Feb/12 15:28,1.1.0,,,,,,0,,,,,,,"Compaction calls removeDeletedInCache, which looks like this:

{code}
.   public void removeDeletedInCache(DecoratedKey key)
    {
        ColumnFamily cachedRow = cfs.getRawCachedRow(key);
        if (cachedRow != null)
            ColumnFamilyStore.removeDeleted(cachedRow, gcBefore);
    }
{code}

For the SerializingCache, this means it calls removeDeleted on a temporary, deserialized copy, which leaves the cache contents unaffected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,228082,,,Thu Feb 16 15:28:14 UTC 2012,,,,,,,,,,"0|i0gptz:",95615,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"15/Feb/12 23:57;jbellis;I'm not sure what the right fix is.  We could invalidate the row, but that means a big compaction could wipe out the row cache entirely, which is worse.  Should we just wontfix this?;;;","16/Feb/12 01:30;scode;With a typical workload, not evicting it would just fail to ""optimize"" - you have to wait for the data to get dropped out of cache.

But if you do have a use-case where you have a lot of requests to non-existent data, this could ""silently"" render the row caches almost useless in a way that is very hard to detect. Suppose you have rows with a hot set where you want caching, but the data is made up of ""churning"" columns (insertions/deletions; the set of columns constantly  But, the ""damaged"" is limited to the ratio of requests for empty rows vs. other requests, and is less prominent the bigger the average requested row in relation to the size of a tombstone.

The above should be true if the rows in cache only contain the tombstones.

But consider a use-case where you have rows (hot) that gets constantly updated with insertion/deletion (let's say it has 1-5 columns at any point in time, with columns constantly ""churned""). How you have something which is hot in cache due to reads (for the ones in hot set), data being over-written, and the per-row average size increasing over time. Cache locality becomes worse and worse and no one can see why, and a restart magically fixes it. This should hold true as long as row cache entries aren't entirely re-read on writes, which would in part defeat the purpose of caching (but I realize now I'm not sure what we do here).
;;;","16/Feb/12 07:59;slebresne;Yeah, it's the serializing cache, we invalidate on each put, so the chance of tons of tombstone building up overtime is anecdotal imo. We could probably fix this by cloning the cachedRow after the get, apply removeDeleted on the clone and do a conditional replace (if it's still the value we got). This involve a bunch of serialization/deserialization so it's unclear that doing that is a better optimization than leaving the tombstone. So I'm good leaving it the way it is. Except that we may want to make removeDeletedInCache be a noop for copying caches just to avoid the useless deserialization. ;;;","16/Feb/12 08:10;scode;If we invalidate on every put, I'm +1 on just ignoring the problem. Sure, it's possible to have a constant subset of a hotset repeatedly read, and someone making an attempt to make it take less space in cache by deleting data and waiting for tombstones... but that's so obscure/extreme that we can probably fix 500 other JIRA:s before this is a priority :) Definitely +1 on NOOP:ing the method though; or more importantly, documenting why it's a NOOP.

(Btw the incoherence of my previous comment is what happens when you split the posting of a comment in two pieces with a meeting in between...);;;","16/Feb/12 15:28;jbellis;bq. we may want to make removeDeletedInCache be a noop for copying caches just to avoid the useless deserialization

done in 94860c6c3713cda4f17dabb2ac2ce30cfe92f6e2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System test failures in 1.1,CASSANDRA-3917,12542706,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,15/Feb/12 16:14,16/Apr/19 09:32,14/Jul/23 05:52,15/Feb/12 17:40,1.1.0,,,,,,0,,,,,,,"On branch 1.1, I currently see two system test failures:
{noformat}
======================================================================
FAIL: system.test_thrift_server.TestMutations.test_get_range_slice_after_deletion
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1937, in test_get_range_slice_after_deletion
    assert len(result[0].columns) == 1
AssertionError
{noformat}
and
{noformat}
======================================================================
FAIL: Test that column ttled expires from KEYS index
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/mcmanus/Git/cassandra/test/system/test_thrift_server.py"", line 1908, in test_index_scan_expiring
    assert len(result) == 1, result
AssertionError: []

----------------------------------------------------------------------
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/12 17:11;slebresne;3917.txt;https://issues.apache.org/jira/secure/attachment/12514662/3917.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227992,,,Wed Feb 15 17:40:23 UTC 2012,,,,,,,,,,"0|i0gps7:",95607,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"15/Feb/12 17:11;slebresne;The test_get_range_slice_after_deletion failure is actually due to a typo during a merge (my fault). Attached trivial patch to fix.

Somehow I'm not able to reproduce the second failure so far, I'm not sure why.;;;","15/Feb/12 17:37;brandon.williams;+1;;;","15/Feb/12 17:40;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix LazilyCompactedRowTest,CASSANDRA-3915,12542697,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,15/Feb/12 15:19,16/Apr/19 09:32,14/Jul/23 05:52,15/Feb/12 15:43,1.0.8,1.1.0,,Legacy/Testing,,,0,,,,,,,"LazilyCompactedRowTest.testTwoRowSuperColumn has never really worked. It uses LazilyCompactedRowTest.assertBytes() that assumes standard columns, even though that test is for super columns. For some reason, the deserialization of the super columns as columns was not breaking stuff and so the test was ""working"", but CASSANDRA-3872 changed that and LazilyCompactedRowTest.testTwoRowSuperColumn fails on current cassandra-1.1 branch (but it's not CASSANDRA-3872 fault, just the test that is buggy).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/12 15:21;slebresne;3915.patch;https://issues.apache.org/jira/secure/attachment/12514649/3915.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227983,,,Wed Feb 15 15:43:00 UTC 2012,,,,,,,,,,"0|i0gprb:",95603,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Feb/12 15:21;slebresne;Attached patch to fix the tests. I believe the patch apply cleanly to both 1.0 (for which the test don't fail but is still broken) and 1.1 (where it does fix the test failure).;;;","15/Feb/12 15:33;brandon.williams;+1;;;","15/Feb/12 15:43;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect InetAddress equality test,CASSANDRA-3913,12542670,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,15/Feb/12 12:19,16/Apr/19 09:32,14/Jul/23 05:52,15/Feb/12 15:17,1.0.0,1.0.8,,,,,0,,,,,,,CASSANDRA-3485 introduced some InetAddress checks using == instead of .equals.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/12 12:20;brandon.williams;3913.txt;https://issues.apache.org/jira/secure/attachment/12514633/3913.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227956,,,Wed Feb 15 15:17:43 UTC 2012,,,,,,,,,,"0|i0gpq7:",95598,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Feb/12 13:47;jbellis;+1

I'd suggest committing to 0.8 as well just in case we do another release there;;;","15/Feb/12 15:17;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should handle wide rows,CASSANDRA-3909,12542535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Feb/12 14:29,16/Apr/19 09:32,14/Jul/23 05:52,16/Apr/12 22:42,1.1.1,,,,,,0,,,,,,,Pig should be able to use the wide row support in CFIF.,,jeromatron,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3883,,,,,,,,,,,,,,,,,"16/Apr/12 14:14;brandon.williams;3909.txt;https://issues.apache.org/jira/secure/attachment/12522774/3909.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227821,,,Thu Apr 19 15:48:48 UTC 2012,,,,,,,,,,"0|i0gpon:",95591,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"16/Apr/12 14:14;brandon.williams;CASSANDRA-3264 (and subsequently CASSANDRA-3883) added wide row support to hadoop, by returning one column in the row in every call.  Pig, however, is fancy enough that it could handle a wide row in a bag, since bags spill to disk; it just needs the pagination for transport since thrift doesn't stream.  Also, if we returned what CFIF gave us, a user wanting to work within the row would need another costly M/R job to join the row back to its original state, so we essentially need to 'undo' the pagination and rebuild the row as a bag.   This patch does that, with the caveat that you cannot access any indexes (and frankly if you have indexes on a wide row you're probably doing something wrong) since it's impossible for us to order the indexes correctly ahead of time in a wide row.;;;","16/Apr/12 22:36;xedin;+1;;;","16/Apr/12 22:42;brandon.williams;Committed.;;;","17/Apr/12 21:20;brandon.williams;Sylvain, any reason we can't put this in 1.1.0?  It has to be explicitly enabled so it can't break anything existing, and it goes well with the hadoop wide row support we already put in 1.1.0.;;;","17/Apr/12 21:21;mdennis;+1 on inclusion in 1.1.0 (and if not, ASAP after 1.1.0);;;","19/Apr/12 12:54;slebresne;Is that a big deal if it's only in 1.1.1? I mean, personally I do trust you on that ""this can't break anything"" and I don't object on putting it in 1.1.0. I do however think that in general there would be some merit to stick to more strict rules. But that's not a debate related to this issue in particular so let's leave that discussing to some other venue.
;;;","19/Apr/12 15:48;brandon.williams;bq. personally I do trust you on that ""this can't break anything""

<3

bq. I do however think that in general there would be some merit to stick to more strict rules.

I agree, however my reasoning is thus: if we support wide rows in 1.1.0 (and we do) then why not pig?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support compression using BulkWriter,CASSANDRA-3907,12542482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,14/Feb/12 05:18,16/Apr/19 09:32,14/Jul/23 05:52,14/Feb/12 17:59,1.1.0,,,,,,0,,,,,,,Currently there is no way to enable compression using BulkWriter. ,,forsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3740,,,,,,,,,,,,,,,,,"14/Feb/12 16:59;lenn0x;0001-Add-compression-support-to-BulkWriter.patch;https://issues.apache.org/jira/secure/attachment/12514505/0001-Add-compression-support-to-BulkWriter.patch",,,,,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227768,,,Wed Feb 15 12:33:49 UTC 2012,,,,,,,,,,"0|i0gpnz:",95588,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"14/Feb/12 17:43;brandon.williams;LGTM, +1;;;","15/Feb/12 08:26;forsberg;So, with this patch, if I enable compression with BulkWriter, sstables will be compressed on the hadoop side, streamed in compressed form and the cassandra daemon does not have to compress them before writing to disk?

Or am I misunderstanding how this works?;;;","15/Feb/12 12:33;brandon.williams;Your understanding is correct.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkRecordWriter throws NPE for counter columns,CASSANDRA-3906,12542474,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,14/Feb/12 02:48,16/Apr/19 09:32,14/Jul/23 05:52,14/Feb/12 17:13,1.1.0,,,,,,0,,,,,,,"Using BulkRecordWriter, fails with counters due to an NPE (we used column instead of counter_column). I also noticed this broke for super columns too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/12 02:50;lenn0x;0001-Fixed-BulkRecordWriter-to-not-throw-an-NPE-for-count.patch;https://issues.apache.org/jira/secure/attachment/12514436/0001-Fixed-BulkRecordWriter-to-not-throw-an-NPE-for-count.patch","14/Feb/12 14:13;brandon.williams;3906-v2.txt;https://issues.apache.org/jira/secure/attachment/12514491/3906-v2.txt",,,,,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227760,,,Tue Feb 14 17:05:45 UTC 2012,,,,,,,,,,"0|i0gpnr:",95587,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"14/Feb/12 14:13;brandon.williams;This patch doesn't look quite right, at the least the removal of the newSuperColumn call can't be right.  Attaching v2.;;;","14/Feb/12 17:05;lenn0x;Good call on seeing that missing writer.newSuperColumn line. Oops. Looks good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix typo in nodetool help for repair,CASSANDRA-3905,12542446,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,13/Feb/12 21:44,16/Apr/19 09:32,14/Jul/23 05:52,14/Feb/12 10:46,1.0.8,,,,,,0,,,,,,,It says to use {{-rp}} instead of {{-pr}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/12 21:45;scode;CASSANDRA-3905.txt;https://issues.apache.org/jira/secure/attachment/12514401/CASSANDRA-3905.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227732,,,Tue Feb 14 10:46:24 UTC 2012,,,,,,,,,,"0|i0gpnb:",95585,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Feb/12 22:08;jbellis;+1;;;","14/Feb/12 10:46;slebresne;Committed (to 1.0 and upwards);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent unexpected errors: possibly race condition around CQL parser?,CASSANDRA-3903,12542433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,13/Feb/12 19:29,16/Apr/19 09:32,14/Jul/23 05:52,17/Feb/12 08:11,1.1.0,,,,,,0,,,,,,,"When running multiple simultaneous instances of the test_cql.py piece of the python-cql test suite, I can reliably reproduce intermittent and unpredictable errors in the tests.

The failures often occur at the point of keyspace creation during test setup, with a CQL statement of the form:

{code}
        CREATE KEYSPACE 'asnvzpot' WITH strategy_class = SimpleStrategy
            AND strategy_options:replication_factor = 1
    
{code}

An InvalidRequestException is returned to the cql driver, which re-raises it as a cql.ProgrammingError. The message:

{code}
ProgrammingError: Bad Request: line 2:24 no viable alternative at input 'asnvzpot'
{code}

In a few cases, Cassandra threw an ArrayIndexOutOfBoundsException and this traceback, closing the thrift connection:

{code}
ERROR [Thrift:244] 2012-02-10 15:51:46,815 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
        at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:744)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:898)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1245)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3458)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3446)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{code}

Sometimes I see an ArrayOutOfBoundsError with no traceback:

{code}
ERROR [Thrift:858] 2012-02-13 12:04:01,537 CustomTThreadPoolServer.java (line 205) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException
{code}

Sometimes I get this:

{code}
ERROR [MigrationStage:1] 2012-02-13 12:04:46,077 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:284)
        at org.apache.cassandra.db.migration.MigrationHelper.addColumnFamily(MigrationHelper.java:209)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyImpl(AddColumnFamily.java:49)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:66)
        at org.apache.cassandra.cql.QueryProcessor$1.call(QueryProcessor.java:334)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

Again, around 99% of the instances of this {{CREATE KEYSPACE}} statement work fine, so it's a little hard to git bisect out, but I guess I'll see what I can do.","Mac OS X 10.7 with Sun/Oracle Java 1.6.0_29
Debian GNU/Linux 6.0.3 (squeeze) with Sun/Oracle Java 1.6.0_26

several recent commits on cassandra-1.1 branch. at least:

0183dc0b36e684082832de43a21b3dc0a9716d48, 3eefbac133c838db46faa6a91ba1f114192557ae, 9a842c7b317e6f1e6e156ccb531e34bb769c979f

Running cassandra under ccm with one node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/12 11:45;slebresne;0001-Fix-CFS.all-thread-safety.patch;https://issues.apache.org/jira/secure/attachment/12514483/0001-Fix-CFS.all-thread-safety.patch","14/Feb/12 11:45;slebresne;0002-Fix-fixCFMaxId.patch;https://issues.apache.org/jira/secure/attachment/12514484/0002-Fix-fixCFMaxId.patch",,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227719,,,Fri Feb 17 08:11:31 UTC 2012,,,,,,,,,,"0|i0gpmf:",95581,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"13/Feb/12 19:36;thepaul;I should mention that I adjusted the python-cql tests to be able to run cleanly in parallel, in the parallel-tests branch.;;;","14/Feb/12 11:45;slebresne;For
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 7
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1520)
        at org.apache.cassandra.thrift.ThriftValidation.validateCfDef(ThriftValidation.java:634)
{noformat}
that's because CFS.all() is not threadSafe as a new keyspace can be added between the allocation of the array and the addition of the column family stores. Attaching patch that use an ArrayList instead of a plain array so that it can grow when that happens (It still use the same ""estimate"" for the initial size of the ArrayList as 99% of the time this will be the right size, but the point is that it doesn't crash if there is a concurrent modification).

For
{noformat}
java.lang.IllegalArgumentException: value already present: 1558
        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
        at com.google.common.collect.AbstractBiMap.putInBothMaps(AbstractBiMap.java:111)
        at com.google.common.collect.AbstractBiMap.put(AbstractBiMap.java:96)
        at com.google.common.collect.HashBiMap.put(HashBiMap.java:84)
        at org.apache.cassandra.config.Schema.load(Schema.java:392)
{noformat}
I believe that's because Schema.fixCFMaxId() may reset cfIdGen to a smaller value since it doesn't check the current value of cfIdGen. Patch attached for that too.

Not sure what's wrong with the ArrayOutOfBoundsError without stacktrace though.

I'm also not sure at all that this will fix the 'no viable alternative at input' error, as I don't think any of those error should trigger that (if only because both happens after the parsing). Not sure how we could have a race in the parser actually, since in theory the parsing of each request should be fully mono-threaded.
;;;","15/Feb/12 22:32;thepaul;I can't reproduce any of the failure modes above with these patches applied-- not even the ""no viable alternative"" one, so they must help.

In reviewing the patches, though, I don't think I understand how 0001-Fix-CFS.all-thread-safety.patch helps anything. Isn't that array wholly made and manipulated on the stack?;;;","16/Feb/12 07:39;slebresne;bq. Isn't that array wholly made and manipulated on the stack?

It is, the ""thread safety"" I'm talking about about is that a new keyspace can be added while CFS.all() is running. If so, there can be say 6 keyspace when the array is created, but on the next line, when Table.all() is called, it could return 7 entries.;;;","16/Feb/12 18:31;thepaul;Ohhhh, I see. Derp. Thanks for the explanation.

I +1 these changes.;;;","17/Feb/12 08:11;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove no-longer-valid values from ColumnFamilyArgument enum,CASSANDRA-3888,12542173,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,10/Feb/12 18:52,16/Apr/19 09:32,14/Jul/23 05:52,10/Feb/12 23:43,1.1.0,,,Legacy/Tools,,,0,,,,,,,"{code}
[default@churnkeyspace] update column family churncf with keys_cached=100;
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.cli.CliClient.updateCfDefAttributes(CliClient.java:1244)
	at org.apache.cassandra.cli.CliClient.executeUpdateColumnFamily(CliClient.java:1091)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/12 19:16;scode;CASSANDRA-3888-1.1.txt;https://issues.apache.org/jira/secure/attachment/12514144/CASSANDRA-3888-1.1.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227460,,,Fri Feb 10 23:43:26 UTC 2012,,,,,,,,,,"0|i0gpg7:",95553,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Feb/12 18:57;scode;Due to CASSANDRA-3143. Trying to figure out whether the intent is to not allow keys_cached, or to allow it but silently ignore it. The patch in 3143 certainly removes the switch case.;;;","10/Feb/12 19:11;jbellis;I'm not 100% sure what the intent is either, but if it's the former, it should be IRE instead of AssertionError.;;;","10/Feb/12 19:16;scode;The precedent is IAE due to enum valueOf() failure:

{code}
[default@churnkeyspace] update column family churncf with asdfasfadsfdsf=13;
java.lang.IllegalArgumentException: No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.ASDFASFADSFDSF
{code};;;","10/Feb/12 19:18;scode;(IRE would be cleaner, but applies generally and is not specific to this particular bug.);;;","10/Feb/12 19:23;jbellis;Oh, this is a CliClient error, not a server one...;;;","10/Feb/12 19:25;scode;Yes. Sorry if I wasn't clear on that.;;;","10/Feb/12 23:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig can't store some types after loading them,CASSANDRA-3886,12542147,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,10/Feb/12 15:59,16/Apr/19 09:32,14/Jul/23 05:52,13/Feb/12 20:55,1.0.8,,,,,,0,,,,,,,"In CASSANDRA-2810, we removed the decompose methods in putNext instead relying on objToBB, however it cannot sufficiently handle all types.  For instance, if longs are loaded and then an attempt to store them is made, this causes a cast exception: java.io.IOException: java.io.IOException: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.pig.data.DataByteArray Output must be (key, {(column,value)...}) for ColumnFamily or (key, {supercolumn:{(column,value)...}...}) for SuperColumnFamily
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/12 16:35;brandon.williams;3886.txt;https://issues.apache.org/jira/secure/attachment/12514114/3886.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227434,,,Mon Feb 13 20:55:49 UTC 2012,,,,,,,,,,"0|i0gpfb:",95549,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"10/Feb/12 16:17;brandon.williams;Patch to convert all possible types.  Includes UUID, even though setTupleValue should make it impossible for a UUID to get here, just in case.;;;","10/Feb/12 17:47;xedin;+1, with following nit: I think we should make objToBB method uniform and return in each case e.g.

{noformat}
private ByteBuffer objToBB(Object o)
{
    if (o == null)
        return (ByteBuffer)o;
    if (o instanceof java.lang.String)
        return new DataByteArray((String)o);
    if (o instanceof Integer)
        return IntegerType.instance.decompose((BigInteger)o);
    if (o instanceof Long)
        return LongType.instance.decompose((Long)o);
    if (o instanceof Float)
        return FloatType.instance.decompose((Float)o);
    if (o instanceof Double)
        return DoubleType.instance.decompose((Double)o);
    if (o instanceof UUID)
        return ByteBuffer.wrap(UUIDGen.decompose((UUID) o));
    
    return null;
}
{noformat};;;","10/Feb/12 18:10;brandon.williams;Committed w/nit fixed.;;;","13/Feb/12 20:51;brandon.williams;We actually do need the catch-all:

{noformat}
return ByteBuffer.wrap(((DataByteArray) o).get());
{noformat}

To cast all the pig-native types like CharArray, but these are all guaranteed to be castable to DataByteArray.;;;","13/Feb/12 20:52;xedin;+1;;;","13/Feb/12 20:55;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent SchemaDisagreementException,CASSANDRA-3884,12542015,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,09/Feb/12 19:51,16/Apr/19 09:32,14/Jul/23 05:52,23/Feb/12 15:34,1.1.0,,,,,,0,,,,,,,"Set up a cluster of two nodes (on cassandra-1.1), create some keyspaces and column families, and then make several schema changes. Everything is being done through only one of the nodes.  About once every 10 times (on my setup) I get a SchemaDisagreementException when creating and dropping keyspaces. 

There is a dtest for this: schema_changes_test.py. If your environment behaves like mine, you might need to run it 10 times to get the error.",using ccm on ubuntu. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/12 13:46;xedin;CASSANDRA-3884-v2.patch;https://issues.apache.org/jira/secure/attachment/12515750/CASSANDRA-3884-v2.patch","22/Feb/12 17:19;xedin;CASSANDRA-3884.patch;https://issues.apache.org/jira/secure/attachment/12515617/CASSANDRA-3884.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227302,,,Thu Feb 23 15:34:09 UTC 2012,,,,,,,,,,"0|i0gpen:",95546,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Feb/12 17:40;tpatterson;I found another problem that might be related. When I drop and re-create a column family while that column family is being compacted, I sometimes see the following error in the logs: 
{code}
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,773 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-73-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO [CompactionExecutor:4] 2012-02-10 00:32:27,774 CompactionTask.java (line 226) Compacted to [/tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db,].  94,406,208 to 11,542,817 (~12% of original) bytes for 302,584 keys at 1.436150MB/s.  Time: 7,665ms.
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,774 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-69-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,778 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-80-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2012-02-10 00:32:27,779 SSTableDeletingTask.java (line 76) Unable to delete /tmp/dtest-l4IQpd/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-79-Data.db (it will be removed on server restart; we'll also retry after GC)
{code}
The dtest failing_tests/change_schema_under_load.py illustrates the problem.;;;","10/Feb/12 18:19;tpatterson;Here is another error that happens intermittently when running failing_tests/change_schema_under_load.py:
{code}
Error occured during compaction
java.util.concurrent.ExecutionException: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:236)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1511)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1708)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOError: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:60)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:821)
	at org.apache.cassandra.db.compaction.CompactionIterable.getScanners(CompactionIterable.java:73)
	at org.apache.cassandra.db.compaction.CompactionIterable.<init>(CompactionIterable.java:56)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:126)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:261)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.io.FileNotFoundException: /tmp/dtest-cf_hbP/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1-hc-82-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:102)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:965)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:56)
	... 12 more
{code};;;","11/Feb/12 12:31;xedin;The first rule of the schema migration (which you have violated) is to make sure that cluster is reasonably quiet for KS/CF you do updates upon because different bad things can happen if you migrate under heavy load. Can you please attach to the ticket system.log from both nodes in the situation when you get SchemaDisagreementException?;;;","11/Feb/12 15:46;jbellis;bq. he first rule of the schema migration (which you have violated) is to make sure that cluster is reasonably quiet for KS/CF you do updates upon because different bad things can happen if you migrate under heavy load

That sounds buggy to me, the goal of CASSANDRA-1391 was to make it so you don't have to care about that kind of thing anymore.;;;","11/Feb/12 17:58;xedin;bq. That sounds buggy to me, the goal of CASSANDRA-1391 was to make it so you don't have to care about that kind of thing anymore.

There is still no way to make migrations atomic so the same rule applies, we made possible to concurrent schema propagation with CASSANDRA-1391 so we don't need to worry about ordering (or time) of the changes but what happens when you update keyspace/column_family simultaneously doing heavy write/read is still unpredictable because that would require some sort of global lock while KSMetaData/CFMetaData are mutated.;;;","11/Feb/12 20:29;jbellis;If you mean you can race inserts into the new CF, with the CF creation, then sure, there's no way we can make that work.  But all the other CFs should be fine.;;;","11/Feb/12 20:37;xedin;Yes, this is what I mean and all existing CFs not involved in schema mutation are not touched by migration process.;;;","20/Feb/12 16:53;slebresne;The test Tyler is talking about is there: https://github.com/riptano/cassandra-dtest/blob/master/schema_changes_test.py.

On my machine, the test fails every time on 1.1 with a schema version disagreement during the column family drop. The test can be simplified a bit (and still fail) so that all it does is creating a keyspace, creating a column family and then dropping that column family. There is no insertion going on at all. There is also more than a 1 second wait between the initial creation and the drop. Last but not least, that same test is working perfectly fine on 1.0. It *is* a regression of 1.1.;;;","22/Feb/12 17:19;xedin;I have localized and fixed the problem which was that coordinator wasn't sending the mutations used to change schema but a snapshot of the schema itself after each of the migrations.;;;","23/Feb/12 13:24;slebresne;It seems to perfectly make sense to send only the new mutation rather the entire schema so I'm good with that and it does fix the distributed test. But I'm not sure I understand why. Since Migration.announce() was call after applying the mutation, the new changes just applied should be part of the entire schema read from SystemTable, shouldn't they? So why did we get a SchemaDisagreementException before?

Also a few other remarks/nits/questions:
* In MigrationHelper if withSchemaRecord is false the mutations will be null, and most function will return a list containing null. We should return an empty list instead or null (but in that last case, Migration.apply() should deal with null). Also MigrationHelper.dropColumnFamily() directly return null, so we should make it match whatever we do for the other method.
* It's slightly more efficient to use Collections.singleton() than Arrays.asList with one element.
* Why does the tests now need to start gossip?
;;;","23/Feb/12 13:35;xedin;The answer to your main question is - compaction, everything works well when you add new or modify columns but when you e.g. delete cf columns from keyspace and compaction kicks in before you grabbed the whole schema that schema will be missing updates for that columns so they won't be pushed to the remote nodes leaving cf attributes in their schema_columnfamilies.

bq. In MigrationHelper if withSchemaRecord is false the mutations will be null, and most function will return a list containing null. We should return an empty list instead or null (but in that last case, Migration.apply() should deal with null). Also MigrationHelper.dropColumnFamily() directly return null, so we should make it match whatever we do for the other method

Sure, I will make it return Collections.singleton()

bq. It's slightly more efficient to use Collections.singleton() than Arrays.asList with one element.

Sure, will change it in updated v2.

bq. Why does the tests now need to start gossip?

I have experienced gossip related NPE exceptions (in isEnabled() method for example) in the MM.passiveAnnounce method when Gossiper wasn't started.;;;","23/Feb/12 13:46;xedin;v2 with modifications mentioned by Sylvain.;;;","23/Feb/12 13:58;slebresne;bq. I have experienced gossip related NPE exceptions (in isEnabled() method for example) in the MM.passiveAnnounce method when Gossiper wasn't started.

Is that related to this patch? Otherwise I'd prefer leaving that to another ticket (at least a different commit).;;;","23/Feb/12 14:11;slebresne;+1 with one nit: if I understand correctly Migration.applyImpl() should never return an empty list of RowMutation, so it would be nice to assert that fact in Migration.apply().

And as said above, I'd prefer separating the test changes to another commit (or open a ticket for them, but I can live with those change being committed directly).;;;","23/Feb/12 15:34;xedin;Committed with fixed nit and changes to tests added to the separate commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFIF WideRowIterator only returns batch size columns,CASSANDRA-3883,12541989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,09/Feb/12 16:15,16/Apr/19 09:32,14/Jul/23 05:52,11/Apr/12 18:28,1.1.0,,,,,,0,,,,,,,"Most evident with the word count, where there are 1250 'word1' items in two rows (1000 in one, 250 in another) and it counts 198 with the batch size set to 99.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3909,,,,,,,,,,,,,,,"13/Feb/12 16:02;brandon.williams;3883-v1.txt;https://issues.apache.org/jira/secure/attachment/12514368/3883-v1.txt","09/Apr/12 17:41;jbellis;3883-v2.txt;https://issues.apache.org/jira/secure/attachment/12521974/3883-v2.txt","09/Apr/12 21:59;jbellis;3883-v3.txt;https://issues.apache.org/jira/secure/attachment/12522025/3883-v3.txt",,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227276,,,Wed Apr 11 18:28:36 UTC 2012,,,,,,,,,,"0|i0gpe7:",95544,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"12/Feb/12 20:02;brandon.williams;At least one problem here is that after retrieving the first page of a row, we set the startToken to the token of the last row:
{noformat}
startToken = partitioner.getTokenFactory().toString(partitioner.getToken(lastRow.key));
{noformat}

which is exclusive and thus advances to the next row, without completely paging through the first.;;;","13/Feb/12 14:06;brandon.williams;My original description here is incorrect; I can't repro the 198 count (not sure what happened there) but now the wide row tests counts 1033 'word1' items.  As far as I can tell, WordCountSetup actually inserts a total of 2002 'word1' matches, one in each of text1 and text2, and a thousand in each of text3 and text4.  I'm not sure what is causing the count discrepancy, but in any case 1033 is far above the batch size of 99, and and the 4th word count test using a secondary index is counting 197 items, so I think something may be fundamentally wrong with word count.

That said, I've been adding wide row support to pig and testing with that, and the problem of not being able to completely paginate wide rows is a definite problem.;;;","13/Feb/12 16:02;brandon.williams;v1 isn't perfect but it's a start; if the batch starts on a wide row, we reuse the token and iterate until we're done.  Unfortunately if we don't start on one, I'm not sure if there's a way to detect that we're in a wide row without making an extra rpc against the last row seen every time.;;;","13/Feb/12 18:41;jbellis;bq. Unfortunately if we don't start on one, I'm not sure if there's a way to detect that we're in a wide row without making an extra rpc against the last row seen every time.

If we can easily address this w/ some extra logic in get_paged_slice then great, otherwise doing one extra rpc call out of (split size * pages per row in split) doesn't seem like a big deal to me.;;;","14/Feb/12 17:24;brandon.williams;There's no sane way to do this with get_paged_slice as it currently is.  We can do the extra rpc to determine if we're at the end of a row, but then we can end up in an ugly situation where there's only one or two more columns outside of the batch size, but when we slice those on the next iteration those are the only columns we can return because our slice predicate is invalid for anything else; even if we happen to get a full batch back.;;;","07/Mar/12 16:35;slebresne;How would get_paged_slice need to be to make that sane, and is there any short term solution to get this fixed for 1.1.0?;;;","07/Mar/12 19:25;brandon.williams;Optimally, we'd have a way to express ""I'm at this column offset in this row, give me the next X number of columns, even if it requires going to the next row.""  But I'm not sure how to do that sanely, either.  I know Jake is using a special CFIF for hive to handle wide rows that basically just grabs one row at a time and paginates it, which is fine if all the rows are wide, but will take a performance hit if they are not.  Still, that might be the best thing to do since using get_page_slices is currently so hairy.;;;","06/Apr/12 22:40;jbellis;bq. Optimally, we'd have a way to express ""I'm at this column offset in this row, give me the next X number of columns, even if it requires going to the next row."" But I'm not sure how to do that sanely, either

What if we allowed mixing (start key, end token) in KeyRange?  Wouldn't that fix it?

- 1st get_paged_slice call: ((start token, end token), empty start column) from slice
- subsequent get_paged_slice calls: ((last row key, end token), last column name)
;;;","06/Apr/12 22:47;brandon.williams;That sounds like it could work.;;;","09/Apr/12 17:41;jbellis;v2 attached w/ that approach;;;","09/Apr/12 21:59;jbellis;v3 to avoid double-counting the startColumn.  also cleans up lastRow cruftiness a bit.;;;","10/Apr/12 21:16;jbellis;Latest is at https://github.com/jbellis/cassandra/branches/3383-5. I think it's working now, except for being blocked by CASSANDRA-4136.;;;","11/Apr/12 14:29;jbellis;https://github.com/jbellis/cassandra/branches/3883-6 is up, with CASSANDRA-4136 incorporated.  the results look good for the word_count test, as posted on 4136.

(the other minor change with -6 is adding conf/ to the classpath for log4j.);;;","11/Apr/12 16:21;brandon.williams;LGTM, +1;;;","11/Apr/12 18:28;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
avoid distributed deadlock in migration stage,CASSANDRA-3882,12541956,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,richardlow,scode,scode,09/Feb/12 10:36,16/Apr/19 09:32,14/Jul/23 05:52,01/Jun/12 10:56,1.1.1,,,,,,0,,,,,,,"This is follow-up work for the remainders of CASSANDRA-3832 which was only a partial fix. The deadlock in the migration stage needs to be fixed, as it can cause bootstrap (at least) to take potentially a very very long time to complete, and might also cause a lack of schema propagation until otherwise ""poked"".",,richardlow,scode,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3832,,,,,,"21/May/12 12:00;richardlow;CASSANDRA-3882-async.patch;https://issues.apache.org/jira/secure/attachment/12528412/CASSANDRA-3882-async.patch","10/May/12 03:33;scode;CASSANDRA-3882-hack.txt;https://issues.apache.org/jira/secure/attachment/12526291/CASSANDRA-3882-hack.txt",,,,,,,,,,,,,,,,2.0,richardlow,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227243,,,Fri Jun 01 10:56:42 UTC 2012,,,,,,,,,,"0|i0gpdr:",95542,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"10/May/12 03:33;scode;Attaching a pure hack that works around this on large clusters (schema migrations are essentially impossible/useless on large clusters without it).

It is not a clean solution, but it was very fast to hack together, should be very safe, and solves our burning problem. It simply spreads out migration tasks slowly over time so that the probability of triggering deadlocks becomes vastly smaller. (We're also running with a hack to make the timeout on migration messages be 500 ms.)

You'll tend to see schema converging very quickly, and then see a flurry of secondly memtable flushes as it processes rectification in the background, even though they don't actually ""do"" anything to the schema.
;;;","10/May/12 03:36;scode;Let me clarify: I'm not suggesting this be committed. I am merely posting it so that others in this position may be helped. Unfortunately I do not have the time to work on a clean solution, but I wanted to at least share the hack instead of sitting on it.;;;","10/May/12 04:31;jbellis;Remind me what problem we're solving post-3832?;;;","10/May/12 05:33;scode;CASSANDRA-3832 avoided the *gossip* stage being backed up as a result of distributed migration manager deadlock, by having the schema rectification be executed asynchronously with respect to the gossip stage. This allowed bootstrap to complete, among other things (""other things"" being anything blocking on gossip stage getting to execute incoming tasks).

As indicated in CASSANDRA-3832 (https://issues.apache.org/jira/browse/CASSANDRA-3832?focusedCommentId=13201012&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13201012) there was still the underlying distributed deadlock (or almost deadlock, since timeouts allow it to eventually complete) in the migration stage itself.

The posted hack is band-aid for that problem. We're still doing useless work performing migration:s beyond what is required, and it doesn't *eliminate* the problem - just makes it vastly less likely.;;;","10/May/12 14:17;jbellis;Quoting from 3832: ""[the deadlock] is because all nodes, when a node is marked alive, just know that it has a different schema - not who has the ""newer"" schema. So when a node joins it gets migration messages from others while it also tries to send migration messages to others and waiting on the response. Whenever it sends a migration message to someone whose migration stage is busy waiting on a response from the node in question - deadlock (until timeout)."";;;","10/May/12 15:55;scode;I'm not sure what your intention is with the quote. If you mean that it implies it's only during bootstrap, it's because I was only considering bootstrap at the time and didn't think far enough to realize this would be a problem in general for schema changes.

The empirical observation is that all nodes are stuck with lots of migrationstage pending, schema requests time out, schema updates at CLI sit there forever, and it very very very slowly recovers since each guy is waiting on RPC timeout (depending on said timeout).

jstacking shows waiting in the migration stage on that future, for a response.
;;;","10/May/12 16:17;jbellis;My intention was only to clarify what problem we're trying to solve here, so everyone doesn't have to xref w/ 3832.;;;","11/May/12 22:36;jbellis;Why do we have block-for-response at all?  Would anything break if we just made the response processing asynchronous?;;;","21/May/12 11:59;richardlow;I have observed this problem too, and have implemented and tested Jonathan's suggestion of processing the response asynchronously.  I have a simple reproduction that works about 50% of the time:

1. Create a two node cluster (nodes A and B)
2. Create a keyspace
3. Take down A and wipe its data and commit log directories
4. Bring up A
5. A never learns about the keyspace

The issue is that when A restarts, it sends a MIGRATION_REQUEST message to B.  But B also sends the message to A at about the same time.  Since the processing of this is done synchronously in MigrationTask, the single thread on the MigrationStage is blocked on both A and B.  The messages timeout and retry, and after 30 seconds it gives up.

The simple solution is to process the response asynchronously - the patch CASSANDRA-3882-async.patch makes this change.  With this, node A always learns about the new keyspace very quickly after restarting.

As a general rule, only async processing should be done on stages, otherwise distributed deadlocks are very likely.

Peter, is this the same issue you were seeing?

I can't think of anything that should break doing this asynchronously - DefsTable.mergeSchema is static synchronized so it looks safe.;;;","23/May/12 02:48;scode;Yes, that is the issue I'm seeing. And I completely agree about synchronous (with respect to other nodes) operations on stages.

I also agree that I can't think of why specifically this would be unsafe to do asynchronously; I was just taking the ultra-conservative approach.

Looked through the patch very briefly and it seems reasonable, but I haven't looked at it carefully or tested it. One thing I'm concerned with for large clusters is the total number of messages back and fourth, since I believe it ends up being quadratic in cluster size. That said, maybe that's okay up to at least several hundred nodes.;;;","23/May/12 02:49;scode;I will try to look at it more closely/test it, but if I can't make it by thursday or so it's likely to not happen until a week or two later.;;;","01/Jun/12 10:56;slebresne;Looks good to me, +1. Committed, thanks.

bq. One thing I'm concerned with for large clusters is the total number of messages back and fourth

That's a separate problem so let's leave that for some other ticket.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reduce computational complexity of processing topology changes,CASSANDRA-3881,12541955,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,soverton,scode,scode,09/Feb/12 10:31,16/Apr/19 09:32,14/Jul/23 05:52,03/Jul/12 18:24,1.2.0 beta 1,,,,,,0,vnodes,,,,,,"This constitutes follow-up work from CASSANDRA-3831 where a partial improvement was committed, but the fundamental issue was not fixed. The maximum ""practical"" cluster size was significantly improved, but further work is expected to be necessary as cluster sizes grow.

_Edit0: Appended patch information._

h3. Patches
||Compare||Raw diff||Description||
|[00_snitch_topology|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology]|[00_snitch_topology.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology.diff]|Adds some functionality to TokenMetadata to track which endpoints and racks exist in a DC.|
|[01_calc_natural_endpoints|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints]|[01_calc_natural_endpoints.patch|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints.diff]|Rewritten O(logN) implementation of calculateNaturalEndpoints using the topology information from the tokenMetadata.|

----

_Note: These are branches managed with TopGit. If you are applying the patch output manually, you will either need to filter the TopGit metadata files (i.e. {{wget -O - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1}}), or remove them afterward ({{rm .topmsg .topdeps}})._",,brandon.williams,dr-alves,jbellis,jeromatron,scode,soverton,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4119,,,,,,,,,CASSANDRA-3831,,,,,,,,,,,,,,,,,,,,,,,0.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227242,,,Tue Jul 03 18:24:25 UTC 2012,,,,,,,,,,"0|i0gpdb:",95540,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Feb/12 01:26;scode;Even with post-CASSANDRA-3831 code, the need here is great. When bootstrapping a new node into a pre-existing cluster of 200+ nodes, the time it takes being CPU bound in gossip stage on the node being bootstrapped, is so large that we're close to seeing it initiate the bootstrap streaming process before being done CPU spinning i gossip stage.

We're increasing the bootstrap wait delay for now to work around this.;;;","26/Feb/12 01:26;scode;Another work-around that is still hacky is to wait for the gossip stage pending to also be 0, after waiting for the ring delay, before proceeding with bootstrap.;;;","26/Feb/12 01:32;scode;Apologies for spamming, but I want to make clear: This applies when bootstrapping nodes into a ring with at least one other node already bootstrapping (otherwise calculatePendingRanges won't get called repeatedly as the ring is populated in the storage service). The greater the number of bootstrapping nodes, the greater the probability of pending range calculations having to be done more often (it is determined by how many nodes get populated into the storage service' notion of the ring before the first bootstrapping node is seen), and the greater the probability of initiating bootstrap based on an incomplete ring view.;;;","15/May/12 15:44;soverton;First the important bit: With these patches, StorageService.calculatePendingRanges is almost three orders of magnitude faster when calculating two nodes bootstrapping into a cluster with 2048 nodes (22ms vs 14.6sec). See the [graph here|https://github.com/acunu/cassandra/wiki/images/calculate_pending_ranges.png]. This was tested with 1 DC and 1 rack with RF=2.

The problem lies in NetworkTopologyStrategy.calculateNaturalEndpoints. The main problems with the existing implementation are:

1. for each datacentre:
2. it iterates through all the tokens in the ring at least once
3. then does an NlogN sort of those tokens
4. then if number of racks < RF it will iterate through all tokens again because it can't tell if it has exhausted the racks in that DC
5. then if number of hosts in that DC < RF it will iterate through all tokens again, otherwise it will iterate through until it has RF hosts in that DC

so it's doing O(DC * (N + NlogN + N + N)) operations just to work out the endpoints for a single token. StorageService.calculatePendingRanges then puts this inside other loops (such as AbstractReplicationStrategy.getAddressRanges) which makes it at least O(N^2*logN).

These patches fix (1) by iterating through the tokens only once, and processing all DCs simultaneously.

(2,3&5) relate to knowing which endpoints exist in a given DC, (4) relates to knowing which racks appear in a DC, so the first patch adds this knowledge to the snitch. The second patch makes use of this knowledge to simplify calculateNaturalEndpoints.

||branch|| || ||description
|p/3881/00_snitch_topology|[compare|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology]|[raw diff|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/00_snitch_topology...p/3881/00_snitch_topology.diff]|Adds some functionality to AbstractEndpointSnitch to track which endpoints and racks exist in a DC (allows for fixing of 2-5).
|p/3881/01_calc_natural_endpoints|[compare|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints]|[raw diff|https://github.com/acunu/cassandra/compare/refs/top-bases/p/3881/01_calc_natural_endpoints...p/3881/01_calc_natural_endpoints.diff]|Rewritten O(logN) implementation of calculateNaturalEndpoints using the topology information from the snitch.


Note: These branches are managed with [TopGit|http://repo.or.cz/w/topgit.git].  If you are applying the patch output manually, you will either need to filter the TopGit metadata files ( {{wget -O - <url> | filterdiff -x*.topdeps -x*.topmsg | patch -p1}} ), or remove them afterward ( {{rm .topmsg .topdeps}} ).;;;","15/May/12 16:59;jbellis;Peter, are you available to review?;;;","15/May/12 17:19;soverton;Just added an additional test to NetworkTopologyStrategyTest. 

The above patches will be up to date, or if you already downloaded the diff then here is the [incremental patch|https://github.com/acunu/cassandra/commit/b199d14f1a33115c66552b3c821af7dc1f0ceedb] ([raw diff|https://github.com/acunu/cassandra/commit/b199d14f1a33115c66552b3c821af7dc1f0ceedb.diff]);;;","16/May/12 10:58;soverton;AbstractEndpointSnitch should handle every state change, not just onChange and onRemove

[incremental patch|https://github.com/acunu/cassandra/commit/b462500e7b8edad0581c60b8e2011cf76e1c7a92] ([raw diff|https://github.com/acunu/cassandra/commit/b462500e7b8edad0581c60b8e2011cf76e1c7a92.diff]);;;","16/May/12 11:51;soverton;Some remaining issues:
1. Handling changes to dc/rack assignment. Currently the topology maps would get out of sync if any endpoint changes dc/rack. This will require Ec2Snitch and PropertyFileSnitch to notify the superclass when they detect a topology change.
2. Delegate getTopology in DynamicSnitch to the subsnitch.

;;;","18/May/12 08:20;scode;Jonathan, I'll try to do so within the next few days.;;;","25/May/12 10:26;soverton;The original approach was not quite there. The snitch was tracking the topology of nodes in NORMAL state for the benefit of NTS.calculateNaturalEndpoints, but calculateNaturalEndpoints is called with modified TokenMetadata (eg, with leaving nodes removed, or a bootstrapped node added or some other modification) to calculate ranges for some future state of the ring, not the current state as tracked by the snitch.

The correct solution is to have TokenMetadata track the topology of the nodes which it considers to be part of the ring, so that when a tokenMetadata is cloned and modified it also updates its view of the topology. This is also much simpler and cleaner.

Patches above are updated.;;;","25/Jun/12 17:03;jbellis;This looks reasonable.  I have two concerns:

- Topology syncronization: a mix between ""Topology synchronizes internally"" and ""caller must synchronize externally"" is a recipe for trouble.  Maybe just synchronizing getDatacenterEndpoints/getDatacenterRacks and returning copies, would be enough.  Alternatively, we could just say ""you must clone TMD before calling calculateNaturalEndpoints"" and possibly get rid of all the Topology synchronization (relying on TMD's on the update path)
- I think there is a hole in the rack-handling logic in cNE: we only check skippedDcEndpoints when a new rack is found.  So if there is (for instance) a single rack in a DC w/ RF=3, we'll add the first endpoint in that rack, then the rest will get added to the skipped list, but never added to replicas.

00 nits:
- would like to see javadoc for Topology
- type specification is not necessary for HMM.create calls (this is why guava prefers factories to constructors)
- I recognize that you were following precedent here, but I would prefer the param-less TMD constructor to call new TMD(HashBimap.create(), new Topology()) instead of (null, null) + special casing in the 2-param version

01 nits:
- @Override is redundant for calculateNaturalEndpoints since parent declares it abstract
- some ""} else"" formatting issues in cNE
- ""!skippedDcEndpoints.get(dc).isEmpty()"" is redundant since the empty iterator case will just be a no-op in the following loop
- perhaps dcReplicas would be a better name than replicaEndpoints, for symmetry w/ ""replicas""
- would be cleaner to move the ""replicaEndpoints.get(dc).size() >= Math.min(allEndpoints.get(dc).size(), getReplicationFactor)"" check into ""have we already found all replicas for this dc"", instead of playing games w/ mutating replicaEndpoints as an (unimportant) optimization.  (Note that ""datacenters.containsKey(dc)"" remains a sufficient check for ""is this a dc we care about at all."");;;","26/Jun/12 15:58;soverton;Thanks Jonathan. I've addressed the above in the following commits:
7eab101 [incremental patch|https://github.com/acunu/cassandra/commit/7eab101fd1649737682d4ce30004a15d0c6c2343] ([raw diff|https://github.com/acunu/cassandra/commit/7eab101fd1649737682d4ce30004a15d0c6c2343.diff])
1474cf0 [incremental patch|https://github.com/acunu/cassandra/commit/1474cf090f5b2c80bbe573d16041458f1782ecbb] ([raw diff|https://github.com/acunu/cassandra/commit/1474cf090f5b2c80bbe573d16041458f1782ecbb.diff])

(patch links above updated)

except for the following:

{quote}
* Topology syncronization: a mix between ""Topology synchronizes internally"" and ""caller must synchronize externally"" is a recipe for trouble. Maybe just synchronizing getDatacenterEndpoints/getDatacenterRacks and returning copies, would be enough. Alternatively, we could just say ""you must clone TMD before calling calculateNaturalEndpoints"" and possibly get rid of all the Topology synchronization (relying on TMD's on the update path)
{quote}

I was trying to avoid any copying, as calculateNaturalEndpoints will be called thousands of times with vnodes in some code paths. I prefer the latter solution of cloning TMD before using it in any method which will use the Topology. The only places where cloning will be necessary to avoid concurrent updates are those where StorageService.instance.tokenMetadata is used directly. I'll update the patches shortly.

{quote}
* I think there is a hole in the rack-handling logic in cNE: we only check skippedDcEndpoints when a new rack is found. So if there is (for instance) a single rack in a DC w/ RF=3, we'll add the first endpoint in that rack, then the rest will get added to the skipped list, but never added to replicas.
{quote}
I think this case is already handled: the subsequent endpoints for that duplicate rack will hit this line first:
{noformat}
    // can we skip checking the rack?
    if (seenRacks.get(dc).size() == racks.get(dc).keySet().size())
{noformat}
and they get added as a replica immediately because we know we have exhausted the racks for that DC. Did I miss something?

;;;","26/Jun/12 18:06;jbellis;bq. The only places where cloning will be necessary to avoid concurrent updates are those where StorageService.instance.tokenMetadata is used directly. I'll update the patches shortly.

Sounds good, I'll have a look when that hits.

bq. I think this case is already handled: the subsequent endpoints for that duplicate rack will hit this line first

Ah, right.  LGTM.;;;","27/Jun/12 11:15;soverton;I tried out the different approaches for removing this sychronization requirement on Topology:

* [clone TokenMetadata before passing to calculateNaturalEndpoints|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-clone-tmd] ([raw diff|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-clone-tmd.diff])

This looks much more hairy than having synchronization - the places where tokenMetadata needs to be cloned look arbitrary and it's much less obvious what the convention is for other people looking at this (eg. why do you need to clone before passing into AbstractReplicationStrategy.getAddressRanges() but not AbstractReplicationStrategy.getPendingAddressRanges() ? A: it's because getPendingAddressRanges makes its own clone to update a token). 

* [return copies from Topology.getDatacenterEndpoints() and Topology.getDatacenterRacks()|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-copy-topo] ([raw diff|https://github.com/acunu/cassandra/compare/bb6f4b40c8a8276971e9e4c50e92e6801dba08bf...3881-review-copy-topo.diff])

This is less error-prone because Topology now handles all its own synchonization. Unfortunately copying those maps adds a large overhead which removes most of the benefit of this ticket. See the [updated graph of calculatePendingRanges vs. cluster size|https://github.com/acunu/cassandra/wiki/images/calculate_pending_ranges_topo_copy.png]. This is not surprising because these copies require O(N) time. 

* [document the synchronization requirements of Topology so that it's clear what is necessary|https://github.com/acunu/cassandra/commit/6d25673c372587beea3971d8ce31b06372525861] ([raw diff|https://github.com/acunu/cassandra/commit/6d25673c372587beea3971d8ce31b06372525861.diff])

This is my preferred solution. Currently only calculateNaturalEndpoints uses the TMD.Topology, so requiring to synchronize around it seems like a reasonable solution to me. The javadoc should make it obvious to any new uses of it that they need to synchronize.;;;","27/Jun/12 17:43;dr-alves;Hi

small nit:
I was using the ctor {code}public TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap){code} in non-commited code. That ctor has now become {code}public TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap, Topology topology){code} since the ctor in Topology is protected this ctor in TokenMetadata can't be called outside of protected scope from now on. I suggest either broadening the scope of the ctor in Topology or restricting the scope in TokenMetadata.;;;","28/Jun/12 09:45;soverton;Hi David, is this non-committed code that's part of another ticket?;;;","28/Jun/12 09:54;dr-alves;hi sam

yes I was using that ctor to test StorageService.effectiveOwnership, included in the CASSANDRA-3047 patch. 

I worked around it since it makes sense that it makes sense that TokenMetadata receives token->endpoint mappings through {code}updateNormalTokens{code}, in order to build topology. The thing was that while previously the ctor {code}TokenMetadata(BiMap<Token, InetAddress> tokenToEndpointMap){code} was usable, now it is not, at least not without getting topology from another TokenMetadata instance.;;;","28/Jun/12 10:29;soverton;Ok, I was going to suggest using updateNormalTokens, or you could change ctor visibility in your patch if required.;;;","28/Jun/12 21:59;jbellis;bq. the places where tokenMetadata needs to be cloned look arbitrary and it's much less obvious what the convention is for other people looking at this

What if we just added an assert ({{assert tokenMetadata != StorageService.tokenMetadata}}) to enforce this requirement?

Granted that we are choosing lesser evils here, but I like that better than trying to reason about synchronized(Topology) mixed with locks mixed with synchronized(bootstrapTokens).;;;","28/Jun/12 22:48;jbellis;Pushed this to https://github.com/jbellis/cassandra/tree/3881-clone-tmd with some extra synchronization cleanup;;;","28/Jun/12 22:57;jbellis;More general assert instead (against TM.getTopology) pushed to https://github.com/jbellis/cassandra/tree/3881-clone-tmd-2;;;","02/Jul/12 17:38;soverton;Agreed that having a single lock is easier to reason about its correctness.

There was one more extra synchronization to cleanup - pushed to [https://github.com/acunu/cassandra/tree/3881-clone-tmd-3].

I'll get these changes merged into the TopGit branches. They'll possibly need separating out for the two separate patches.;;;","02/Jul/12 17:57;jbellis;I can merge/squash/commit from here.;;;","02/Jul/12 18:12;jbellis;I take it back, history is a mess w/o topgit. :)  Will wait for your update.;;;","03/Jul/12 13:31;soverton;It's all merged in now, so the patch links in the ticket description are up to date.

There was one more place in some tests that TMD needed to be cloned: https://github.com/acunu/cassandra/commit/08620c55a77ba1dc257b853610386297ab0c379b
;;;","03/Jul/12 18:24;jbellis;thanks, committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool removetoken force causes an inconsistent state,CASSANDRA-3876,12541794,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,soverton,soverton,soverton,08/Feb/12 12:41,16/Apr/19 09:32,14/Jul/23 05:52,08/Feb/12 14:29,1.0.8,,,,,,0,force,nodetool,removetoken,,,,"Steps to reproduce (tested on 1.0.7 and trunk):
* Create a cluster of 3 nodes
* Insert some data
* stop one of the nodes
* Call removetoken on the token of the stopped node
* Immediately after, do removetoken force 
  - this will cause the original removetoken to fail with an error after 30s since the generation changed for the leaving node, but this is a convenient way of simulating the case where a removetoken hangs at streaming since the cleanup logic at the end of StorageService.removeToken is never executed.
  - if you want a more realistic reproduction then get a removetoken to hang in streaming, then do removetoken force

Effects:
* ""removetoken status"" now throws an exception because StorageService.removingNode is not cleared, but the endpoint is no longer a member of the ring:

$ nodetool -h localhost removetoken status
{noformat}
Exception in thread ""main"" java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:304)
	at org.apache.cassandra.service.StorageService.getRemovalStatus(StorageService.java:2369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:683)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:672)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:619)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}

* truncate no longer works in the cli because the removed endpoint is not removed from Gossiper.unreachableEndpoints. 
The cli errors immediately with:
{noformat}
[default@ks1] truncate cf1;
null
UnavailableException()
	at org.apache.cassandra.thrift.Cassandra$truncate_result.read(Cassandra.java:20978)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_truncate(Cassandra.java:942)
	at org.apache.cassandra.thrift.Cassandra$Client.truncate(Cassandra.java:929)
	at org.apache.cassandra.cli.CliClient.executeTruncate(CliClient.java:1417)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:270)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)
{noformat}

The logs show:
{noformat}
INFO [Thrift:11] 2012-02-08 11:55:50,135 StorageProxy.java (line 1172) Cannot perform truncate, some hosts are down
{noformat}

* there are probably other schema related things that fail for the same reason although this wasn't tested

Workaround:
* Restart the affected node.

Fix:
It looks like StorageService.forceRemoveCompletion is missing some cleanup logic which is present at the end of StorageService.removeToken. Adding this cleanup logic to forceRemoveCompletion fixes the above issues (see attached).",,richardlow,soverton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/12 12:42;soverton;3876.patch;https://issues.apache.org/jira/secure/attachment/12513805/3876.patch",,,,,,,,,,,,,,,,,1.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227081,,,Wed Feb 08 14:29:04 UTC 2012,,,,,,,,,,"0|i0gpav:",95529,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"08/Feb/12 12:42;soverton;Attached patch against trunk;;;","08/Feb/12 14:29;brandon.williams;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SuperColumn may ignore relevant tombstones,CASSANDRA-3875,12541791,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,08/Feb/12 11:39,16/Apr/19 09:32,14/Jul/23 05:52,08/Feb/12 15:07,0.8.10,,,,,,0,,,,,,,"QueryFilter.isRelevant() consider that a super column that is gc-able but contains only non-gcable tombstone is irrelevant (if the tombstone timestmap is greater than the super column timestamp, which is almost implied by the fact that the tombstone are non-gcable while the SC is).",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/12 11:41;slebresne;3875.patch;https://issues.apache.org/jira/secure/attachment/12513800/3875.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227078,,,Wed Feb 08 15:07:11 UTC 2012,,,,,,,,,,"0|i0gpaf:",95527,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Feb/12 11:41;slebresne;Attaching patch with a unit test.

Some more details about this issue can be found in the comments of CASSANDRA-3872.;;;","08/Feb/12 13:56;jbellis;+1;;;","08/Feb/12 15:07;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: handle situation where data can't be deserialized as expected,CASSANDRA-3874,12541721,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,08/Feb/12 00:36,16/Apr/19 09:32,14/Jul/23 05:52,16/Feb/12 15:43,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,,,"When cqlsh tries to deserialize data which doesn't match the expected type (either because the validation type for the column/key alias was changed, or ASSUME has been used), it just fails completely and in most cases won't show any results at all. When there is only one misbehaving value out of a large number, this can be frustrating.

cqlsh should either show some failure marker in place of the bad value, or simply show the bytes along with some indicator of a failed deserialization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/12 20:59;thepaul;3874-1.0.patch.txt;https://issues.apache.org/jira/secure/attachment/12514706/3874-1.0.patch.txt","15/Feb/12 20:59;thepaul;3874-1.1.patch.txt;https://issues.apache.org/jira/secure/attachment/12514707/3874-1.1.patch.txt",,,,,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227008,,,Thu Feb 16 15:43:06 UTC 2012,,,,,,,,,,"0|i0gp9z:",95525,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Feb/12 20:58;thepaul;Changes made in my 3874-1.0 branch at github: https://github.com/thepaul/cassandra/tree/3874-1.0

Since the merge forward is not completely trivial, my 3874-1.1 branch has those commits already merged to 1.1: https://github.com/thepaul/cassandra/tree/3874-1.1 , and it will be an easy merge from that to your updated cassandra-1.1, whatever it is.

I'll attach patch versions too, in case you still prefer those.;;;","15/Feb/12 21:02;jbellis;Is this a big enough change that we should keep it to 1.1 only?;;;","15/Feb/12 21:09;thepaul;I think it's worth having in 1.0, especially if it's going to live for a few more releases. It's technically only a bugfix. But I don't mind much, whichever.;;;","15/Feb/12 23:56;thepaul;updated the 3874-1.1 branch just now to add the 1.0.9 version of the python-cql library, since that is required for this fix to work. 1.0 doesn't have an embedded python-cql lib, so it's fine.;;;","16/Feb/12 15:43;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sub-columns removal is broken in 1.1,CASSANDRA-3872,12541652,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,07/Feb/12 16:56,16/Apr/19 09:32,14/Jul/23 05:52,14/Feb/12 11:01,1.1.0,,,,,,0,,,,,,,"CASSANDRA-3716 actually broke sub-columns deletion. The reason is that in QueryFilter.isRelevant, we've switched in checking getLocalDeletionTime() only (without looking for isMarkedForDelete). But for columns containers (in this case SuperColumn), the default local deletion time when not deleted is Integer.MIN_VALUE. In other words, a SC with only non-gcable tombstones will be considered as not relevant (while it should).

This is caught by two unit tests (RemoveSuperColumnTest and RemoveSubColumnTest) that are failing currently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/12 17:06;slebresne;3872.patch;https://issues.apache.org/jira/secure/attachment/12513634/3872.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226939,,,Tue Feb 14 11:01:22 UTC 2012,,,,,,,,,,"0|i0gp93:",95521,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Feb/12 17:06;slebresne;Attaching patch for this. The idea is to make containers use MAX_VALUE instead of MIN_VALUE  for localDeleteionTime. There is however two things to consider:
# for containers, we do serialize the localDeletionTime, so we have to be careful for upgrade (mixed version cluster). What the patch does is to recognize when someone provided MIN_VALUE and transforming to MAX_VALUE. This only work when old node send use MIN_VALUE, however *I think* it is actually OK to send MAX_VALUE to old node (pre-1.1 code uses markedForDeleteAt to decide if a container is deleted, not localDeletionTime, so we shouldn't break anything).
# CFS.removeDeleted has to be able to distinguish between an empty container that is marked for deletion (but not gcable) and one that is not marked for deletion. In the former, we should keep the container, not in the latter. This means that this patch actually reintroduce the use of isMarkedForDelete() for containers in CFS.removeDeleted (for containers, isMarkedForDelete is not timing dependent).

Note that there would likely be other ways to fix this issue (reverting CASSANDRA-3716 would be one).

In any case, the patch fixes the two unit tests.;;;","07/Feb/12 23:54;jbellis;Could we do a simpler fix by turning column.mostRecentLiveChangeAt into column.mostRecentChangeAt? I don't think restricting to live columns is actually useful here (although it is in SQF, so we'd need a new method instead of replacing mRLCA).;;;","08/Feb/12 11:11;slebresne;Apologies, I didn't realized that there was actually kind of 2 problems and I mixed both, so let's be more precise.

The regression that CASSANDRA-3716 introduced and that is making the tests fail is that a *live* super column with only non-gcable tombstones is now considered irrelevant, while it is relevant in say 1.0.

That led me to realize that containers are using MIN_VALUE for gLDT when not deleted. Now, leaving aside the tests failure, I do think we should change that. Post-CASSANDRA-3716, we've made sure that for normal columns, testing {{getLocalDeletionTime() < gcbefore}} allows us solely to decide whether the column is gcable (and thus by extension tombstoned) or not. I think it's a good thing, the local deletion time is here for the tombstone gc and so that makes sense. But currently, this is not true for SuperColumns nor ColumnFamily (but it's worst for SC since they actually are IColumn), where {{getLocalDeletionTime() < gcbefore}} means 'either gcable or not deleted at all'. Leaving things that way would be very error-prone imho and defeat the purpose of CASSANDRA-3716 since we're reintroducing subtle differences that are hard to work with.

Now, as it happens, I think we have an old bug in QF.isRelevant (i.e. including in 0.8 and probably before that). Namely that a *live* SC with only non-gcable tombstones is considered relevant, but a *gcable* SC with only non-gcable is considered irrelevant but I don't think it should. To fix that, we can indeed do the mostRecentChangeAt change (and that would fix the unit tests in particular but as said above, I think we should still do the MIN_VALUE->MAX_VALUE change for other reasons). However, that would imply that *gcable* SC with only *gcable* tombstones would be considered relevant (if the max tombstone timestamp is greater than the SC timestamp), so maybe we want to switch to a column.mostRecentNonGcableChangeAt() instead.
I'll open a separate ticket for that change, as this affect previous version too. I'm leaving this one open to focus on the MIN_VALUE->MAX_VALUE change.;;;","10/Feb/12 23:46;jbellis;Isn't this patch kind of a wash?  The MIN_VALUE checks go away but in return we need to add extra isMarkedForDelete checks.
;;;","13/Feb/12 11:15;slebresne;I do not pretend this reduce line of codes, but I do think that it makes it easier to not make subtle mistakes.

Currently, there is a mismatch between how Column (the class) and the two IColumnContainer classes (CF and SC) handles getLocalDeletionTime() for non-deleted. The former uses MAX_VALUE, the latter uses MIN_VALUE. The lack of consistency alone is annoying but as long as SC lives it is made much worst by the fact that SC is both a IColumn and a IColumnContainer.

The attached patch tries to make things more consistent. The localDeletionTime is here for the purpose of tombstone garbage collection, so it seems to me that it is cleaner to use it for that purpose and that purpose only. In other words, with this patch, {{(getLocalDeletionTime() < gcbefore)}} tells you without ambiguity if you're dealing with a gcable tombstone or not.

Now there is the fact that live but empty containers are not returned to the user. I believe that was one of the reason of using MIN_VALUE for live containers. But imho this is a hack and it's much more clear in removeDeleted to read:
{noformat}
if (cf.getColumnCount() == 0 && (!cf.isMarkedForDelete() || cf.getLocalDeletionTime() < gcBefore))
{noformat}
which directly translate into: if the cf is empty and it's either a gcable tombstone or a live cf, we can skip it, rather that having to check the code of ColumnFamily to understand why that does skip live empty CF *and* to have to remember each time you use CF.localDeletionTime that it may be MIN_VALUE for non-deleted CF and assert if it matters or not.;;;","13/Feb/12 23:59;jbellis;+1

Can you add a reference to this ticket to the ""new default is MAX_VALUE"" comment on commit?;;;","14/Feb/12 11:01;slebresne;Committed (with added link to the issue), thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing batch_mutate: java.util.ConcurrentModificationException on CounterColumn,CASSANDRA-3870,12541595,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,amorton,vjevdokimov,vjevdokimov,07/Feb/12 10:30,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 08:32,1.0.8,,,,,,0,counters,,,,,,"Cassandra throws an exception below while performing batch_mutate with counter column insertion mutation to increment column with 1:

ERROR [Thrift:134] 2012-02-03 15:51:02,800 Cassandra.java (line 3462) Internal error processing batch_mutate
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:532)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.waitForHints(AbstractWriteResponseHandler.java:89)
        at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:58)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:201)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639)
        at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590)
        at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598)
        at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Column family definition:

create column family CountersColumnFamily1
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 1000000.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 0.0
  and key_cache_save_period = 14400
  and read_repair_chance = 0.1
  and gc_grace = 43200
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'SerializingCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';","Debian 6.0 x64
x64 Sun Java 6u26
Cassandra 1.0.7
JNA
2 DCs, 1 ring/DC, 8 nodes/ring, RF=3/DC, Random partitioner.
Disk access auto (mmap)",maheeg,slebresne,sumit.thakur@rancoretech.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/12 18:43;slebresne;3870.txt;https://issues.apache.org/jira/secure/attachment/12515463/3870.txt","21/Feb/12 10:50;amorton;cassandra-1.0-3870-V2.txt;https://issues.apache.org/jira/secure/attachment/12515342/cassandra-1.0-3870-V2.txt","21/Feb/12 19:46;amorton;cassandra-1.0-3870-V3.txt;https://issues.apache.org/jira/secure/attachment/12515477/cassandra-1.0-3870-V3.txt","16/Feb/12 17:41;amorton;cassandra-1.0-3870.txt;https://issues.apache.org/jira/secure/attachment/12514823/cassandra-1.0-3870.txt",,,,,,,,,,,,,,4.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226882,,,Tue Apr 23 12:37:03 UTC 2013,,,,,,,,,,"0|i0gp87:",95517,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"08/Feb/12 09:44;amorton;I've done some digging and this is what I think is happening...

StorageProxy.counterWriteOnCoordinatorPerformer() creates a counterWriteTask and put it in the local MUTATION TP. This will run the mutation and then put a task in REPLICATE_ON_WRITE to send if the counter write to other nodes, if a node is down hints a scheduled and AbstractWriteResponseHandler.addFutureForHint() is called with the hint mutation future. 

However the response handler was originally created in the request thread, which may now have called get() to wait on it. This may miss hints that have not already been added or may get the CME in the error here. 

It looks like this is only a problem for counter writes where the coordinator is a replica, and then only in certain circumstances. Counters where another replica is the leader look ok and so do regular writes.

I was thinking:
* Change the AWRH.hintFutures to a CopyOnWriteArrayList (to be cautious) and
* Change AWRH.get() to wait on hints after the normal request has complet instead of before.
 
I'll try to pick it up again tomorrow.;;;","08/Feb/12 10:08;vjevdokimov;In our case coordinator almost always a replica, since our Cassandra client is aware about token range and selects endpoints calculating token for a key.;;;","08/Feb/12 16:41;amorton;Realized when I went to bed I will also need to change StorageProxy.counterWriteTask to call responeHandler.response() after the hints have been sent, which happening on the REPLICATE_ON_WRITE TP. So needs additional coordination. 

Should be able to find some time today to keep working on this.  ;;;","16/Feb/12 17:41;amorton;Patch to add IWriteResponseHandler.finalizeHints() - called so the handler knows that all hints have been sent and it is safe to begin waiting on them. ;;;","17/Feb/12 09:11;slebresne;I think you're right on the reasons that causes the exception, and that our fix would avoid that exception. There is however two problems/annoying things (not really related to the patch per se):
# this break the current property that counter writes at CL.ONE don't have to wait on the read. In other words, that would increase the latency of CL.ONE counter writes.
# this completely break counters when replicate_on_write == false. This may not be a problem anymore on 1.1 if we remove that option (CASSANDRA-3868) but breaking it in 1.0 (on a minor release) is more problematic.

A ""solution"" to prevent those two problems would be to add a way to specify that we don't want to wait on hints being locally written and use that for counter at CL.ONE, but that means we would skip the current property of 'when a write returns, we guarantee that all hint for nodes that are *known* dead have been written locally'. Which leads me to a question: what does that latter property really give us?;;;","19/Feb/12 20:15;amorton;Sorry got some sick kids at home. Will try to find some time this week. 
;;;","21/Feb/12 10:50;amorton;added -V2 

Fixed problem where StorageProxy.mutateCounter() would not call handler.finalizeHints() if 
it was not a replica for the row.

Improved timeout when waiting for hints.

bq. this break the current property that counter writes at CL.ONE don't have to wait on the read. In other words, that would increase the latency of CL.ONE counter writes.

Added IWriteResponseHanlder.dontBlockOnHints() called from StorageProxy.counterWriteTask() if CL is ONE. 

bq. this completely break counters when replicate_on_write == false. This may not be a problem anymore on 1.1 if we remove that option (CASSANDRA-3868) but breaking it in 1.0 (on a minor release) is more problematic.

Not sure what you mean. In StoragrProxy.counterWriteTask() if not cm.shouldReplicateOnWrite() i finalize the hints with the handler. So that get() will not wait for more futures. 

bq. guarantee that all hint for nodes that are known dead have been written locally'. Which leads me to a question: what does that latter property really give us?

It slows down the client and reduces the chance that a node can get overwhelmed if it has to store a lot of hints. But at CL ONE the client is asking the server to do the minimal amount of work before returning so it makes sense.  ;;;","21/Feb/12 14:12;slebresne;bq. if not cm.shouldReplicateOnWrite() i finalize the hints with the handler

Sorry I missed that.

bq. it slows down the client and reduces the chance that a node can get overwhelmed if it has to store a lot of hints

I don't think that is the goal of that code. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints.

The only reasonable intent of that code (the code that makes writes wait that hint are locally written) that I can see would be to ensure that when we acknowledge the client we can guarantee that hints for dead nodes are stored (and thus will eventually get delivered). However, this only works for known dead nodes, so we cannot really make that guarantee (unfortunately).

I'm not saying the attached patch won't work, but it does help making the write path more complicated and 'messy' that I'd like it to be. Typically having to not forget to call finalizeHints() is a bit error-prone, etc...  So I'm wondering, do we really have a strong reason for waiting for hints during writes in the first place.


On the patch though:
* We should raise a timeout exception if waiting for the hints to be finalized timeouts inserting of throwing an assertion error. It is possible (at least in theory) for that to timeout.
* Not sure I understand dontBlockOnHints(). It seems to only skip the fact that we signal hintsFinalized. I think we should just call finalizeHints() when we don't want to block on hints.
;;;","21/Feb/12 16:00;jbellis;bq. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints

I think you're right. Originally we had ""wait for hint delivery before ack"" as an attempt to prevent overload and OOM, but then we added the more fine-grained checks in sTHE instead.;;;","21/Feb/12 18:43;slebresne;Attaching a version that take the approch of removing the 'wait on hints future' from the write path. Again the rational is that:
# Since that code was added, we've introduced a better way to deal with node being overwhelm by hints in sTHE.
# Since we'll have hint future only for node that are known dead, waiting on only those doesn't really give any special new guarantee to the user.;;;","21/Feb/12 19:41;amorton;bq. I don't think that is the goal of that code. We already have code for that (make sure a node don't get overwhelm writing hints locally) in sendToHintedEndpoints.

Missed the totalHintsInProgress check.

bq. So I'm wondering, do we really have a strong reason for waiting for hints during writes in the first place.

IMHO no, other than CL ANY. I know it's different in 1.0 but HH provides weak guarantees. Recording a hint locally does not guarantee that it will be applied to the endpoint before the next read for the row. 

bq. I'm not saying the attached patch won't work, but it does help making the write path more complicated and 'messy' that I'd like it to be

Agree. 

If we stick with waiting for hints I could change it to *not* wait for acknowledgement that all hint futures collected by default. Counter mutations on the coordinator are the exception, they can tell the handler to wait.

bq. We should raise a timeout exception if waiting for the hints to be finalized timeouts inserting of throwing an assertion error. It is possible (at least in theory) for that to timeout.

Changed.

bq. Not sure I understand dontBlockOnHints(). It seems to only skip the fact that we signal hintsFinalized. I think we should just call finalizeHints() when we don't want to block on hints.

finalizeHints() stops modifications to the hints collection, so calling addFutureForHint() after it will raise an exception. dontBlockOnHints() tells get() to ignore the hints collection and prevents finalizeHints() from stopping modification to the hints collection. 

This is to support sendToHintedEndpoints passing hint futures to the handler when using CL ONE. I wanted calls to addFutureForHint() to always work unless finalizeHints() had been called.    


;;;","21/Feb/12 19:46;amorton;version 3

raise TimeoutException if finaliseHints() not called.;;;","21/Feb/12 19:53;slebresne;Just to be clear, I'm *really* leaning towards the path of removing the 'wait for hint future' from the writeHandler (as of 3870.txt) unless someone remember a reason why that code is still useful that I've missed.;;;","21/Feb/12 20:40;amorton;Agree.

+1 for your patch. 

Not in your patch but if sendToHintedEndpoints() throws a TimeoutException because too many inflight hints when performing a counter write on a leader that is not the coordinator. CounterMutationVerbHandler will swallow the exception and wait for the coordinator to time out.  Would it be better to use a different exception and return to the coordinator ? ;;;","22/Feb/12 08:32;slebresne;Committed, thanks


bq. Would it be better to use a different exception and return to the coordinator ?

The thing is that the leader is using a WriteHandler to wait on the response of the coordinator, which in its current form doesn't have any kind of failure mode (A WriteResponse does have a boolean to indicate success but no code use it ever). So basically it would be possible but would add much complication when just waiting on the coordinator to timeout is probably good enough.
;;;","23/Apr/13 12:29;sumit.thakur@rancoretech.com;Hello All,

I got same exception in Apache Cassandra 1.1.5  

ERROR [Thrift:32] 2013-02-23 19:07:53,581 Cassandra.java (line 3462) Internal error processing batch_mutate
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:60)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
	at org.apache.cassandra.service.StorageProxy.insertLocal(StorageProxy.java:419)
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:308)
	at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:120)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:255)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:194)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639)
	at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


SUMIT THAKUR;;;","23/Apr/13 12:37;slebresne;This is not at all the same exception. Your exception mean that some write has been attempted while the node was shutting down. Though noisy, it's harmless.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disablethrift and Enablethrift can leaves behind zombie connections on THSHA server,CASSANDRA-3867,12541499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,06/Feb/12 20:43,16/Apr/19 09:32,14/Jul/23 05:52,13/Feb/12 20:43,1.0.8,1.1.1,,,,,0,,,,,,,"While doing nodetool disable thrift we disable selecting threads and close them... but the connections are still active...
Enable thrift creates a new Selector threads because we create new ThriftServer() which will cause the old connections to be zombies.

I think the right fix will be to call server.interrupt(); and then close the connections when they are done selecting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/12 02:53;vijay2win@yahoo.com;0001-CASSANDRA-3867.patch;https://issues.apache.org/jira/secure/attachment/12514196/0001-CASSANDRA-3867.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226786,,,Mon Feb 13 20:43:13 UTC 2012,,,,,,,,,,"0|i0gp6v:",95511,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"11/Feb/12 02:53;vijay2win@yahoo.com;Simple patch to close the selector when disablethrift.;;;","13/Feb/12 16:36;brandon.williams;+1;;;","13/Feb/12 20:43;vijay2win@yahoo.com;Committed to trunk and 1.0 trunk (as it is a trivial bug fix)... Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli returns 'command not found' instead of syntax error,CASSANDRA-3865,12541467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,elubow,elubow,06/Feb/12 19:32,16/Apr/19 09:32,14/Jul/23 05:52,26/May/12 19:54,1.1.2,,,,,,0,cassandra-cli,,,,,,"When creating a column family from the output of 'show schema' with an index, there is a trailing comma after ""index_type: 0,""  The return from this is a 'command not found'  This is misleading because the command is found, there is just a syntax error.

'Command not found: `create column family $cfname ...`

",DSE 1.0.5,dbrosius@apache.org,elubow,hudson,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/12 01:12;dbrosius@apache.org;3865_better_cli_ex_handling.txt;https://issues.apache.org/jira/secure/attachment/12529462/3865_better_cli_ex_handling.txt","24/May/12 02:09;dbrosius@apache.org;parse_doubles_better.txt;https://issues.apache.org/jira/secure/attachment/12528837/parse_doubles_better.txt",,,,,,,,,,,,,,,,2.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226754,,,Sat May 26 22:37:50 UTC 2012,,,,,,,,,,"0|i0gp67:",95508,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"07/Feb/12 14:33;yukim;This is fixed in 1.0.7 (CASSANDRA-3714).;;;","23/May/12 15:56;elubow;This doesn't appear to be fixed.

Welcome to Cassandra CLI version 1.0.8

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@linkcurrent] create column family social_poll_deltas
...	  with column_type = 'Standard'
...	  and comparator = 'CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.LongType)'
...	  and default_validation_class = 'UUIDType'
...	  and key_validation_class = 'UUIDType'
...	  and rows_cached = 0.0
...	  and row_cache_save_period = 0
...	  and row_cache_keys_to_save = 2147483647
...	  and keys_cached = 200000.0
...	  and key_cache_save_period = 14400
...	  and read_repair_chance = .25
...	  and gc_grace = 864000
...	  and min_compaction_threshold = 4
...	  and max_compaction_threshold = 32
...	  and replicate_on_write = true
...	  and row_cache_provider = 'SerializingCacheProvider'
...	  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
...	  and comment = 'Social poll totals and deltas ';
Command not found: `create column family social_poll_deltas with column_type = 'Standard' and comparator = 'CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.LongType)' and default_validation_class = 'UUIDType' and key_validation_class = 'UUIDType' and rows_cached = 0.0 and row_cache_save_period = 0 and row_cache_keys_to_save = 2147483647 and keys_cached = 200000.0 and key_cache_save_period = 14400 and read_repair_chance = .25 and gc_grace = 864000 and min_compaction_threshold = 4 and max_compaction_threshold = 32 and replicate_on_write = true and row_cache_provider = 'SerializingCacheProvider' and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' and comment = 'Social poll totals and deltas ';`. Type 'help;' or '?' for help.;;;","23/May/12 16:53;elubow;As an update, the problem with this (after MUCH trial and error), was that the read_repair_chance was .25 instead of 0.25.;;;","24/May/12 02:09;dbrosius@apache.org;support decimal parsing as 0.5 and .5

against trunk;;;","24/May/12 02:36;yukim;That't one way to fix parsing problem, but in my understanding, what Eric is saying is that it's hard to find the error from the message above.
If we can display more user friendly message like ""Parse error: read_repair_chance = .25"", instead of saying ""Command not found"", user can easily fix what's wrong.

I'm fine with current Double definition if we can fix the error message. What do you think, Pavel?;;;","24/May/12 03:12;elubow;As Yuki said, I'd be more interested in the fix being a complaint of a syntax error (or parse error) when attempting to create a column family as opposed to ""command not found"" (which is clearly ambiguous).  This way I (as a user) know that the CLI is aware that I am attempting to create a column family and it's failing.  Now not only do I know that I am on the right track with the command I'm trying to execute, but then I know roughly where the problem is. Fixing the parse error is just a band-aid.;;;","24/May/12 13:29;xedin;I think that Dave's patch is one part of it, another would be to change ""Command not found"" to ""Error in the command"" and add information from RecognitionException (which NoViableAltException extends) to were recognition error have actually happend.

Edit: Also, Dave, please don't forget that this is intended for inclusion into 1.0 so patches should be against cassandra-1.0 branch instead.;;;","24/May/12 20:38;jbellis;My fault, I added 1.0 as a fix target after Dave's patch since I thought it was going to be a quick fix.  Let's target 1.1 instead.;;;","25/May/12 01:12;dbrosius@apache.org;3865_better_cli_ex_handling.txt

has better decimal parsing as before.

Also has better error messaging when the exception is NoViableAltException.

For some reason the code special cased that exception and returned the non-useful ""Command not found"" message. I just removed the special casing and now the message is useful.

If there is some case that someone knows about that the message ""Command not found"" is useful, i can put it back in for that specific case, but i couldn't see it.

against cassandra-1.1;;;","26/May/12 12:53;xedin;+1;;;","26/May/12 15:59;dbrosius@apache.org;committed to branch cassandra-1.1 as commit 2d72056c14f9b97e67dd94e48691f3ec1a88d9d6;;;","26/May/12 16:08;xedin;You also need to add line about that into CHANGES.txt and up-merge it into trunk.;;;","26/May/12 19:54;xedin;ok, I did CHANGES.txt update and merge for you this time :);;;","26/May/12 20:37;hudson;Integrated in Cassandra #1453 (See [https://builds.apache.org/job/Cassandra/1453/])
    add missing CHANGES.txt entry for CASSANDRA-3865 (Revision d16c4468e86fbdb82ed981ef6c6ced6f275e4f21)

     Result = FAILURE
xedin : 
Files : 
* CHANGES.txt
;;;","26/May/12 20:59;xedin;I have fixed small bug and committed to 1.1 and up-merged into trunk.;;;","26/May/12 22:37;hudson;Integrated in Cassandra #1455 (See [https://builds.apache.org/job/Cassandra/1455/])
    fixes small CLI bug introduced by CASSANDRA-3865 (Revision 46722cc69e47e9b4bdcc5c28f426cf7f0a6a3a7d)

     Result = FAILURE
xedin : 
Files : 
* src/java/org/apache/cassandra/cli/Cli.g
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests failures in 1.1,CASSANDRA-3864,12541465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slebresne,slebresne,06/Feb/12 19:09,16/Apr/19 09:32,14/Jul/23 05:52,15/Feb/12 15:14,1.1.0,,,,,,0,,,,,,,"On the current 1.1 branch I get the following errors:
# SSTableImportTest:
{noformat}
[junit] Testcase: testImportSimpleCf(org.apache.cassandra.tools.SSTableImportTest):	Caused an ERROR
[junit] java.lang.Integer cannot be cast to java.lang.Long
[junit] java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
[junit] 	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:132)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:191)
[junit] 	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:174)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:290)
[junit] 	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:255)
[junit] 	at org.apache.cassandra.tools.SSTableImportTest.testImportSimpleCf(SSTableImportTest.java:60)
{noformat}
# CompositeTypeTest:
{noformat}
[junit] Testcase: testCompatibility(org.apache.cassandra.db.marshal.CompositeTypeTest):	Caused an ERROR
[junit] Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] org.apache.cassandra.config.ConfigurationException: Invalid comparator class org.apache.cassandra.db.marshal.CompositeType: must define a public static instance field or a public static method getInstance(TypeParser).
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:294)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:268)
[junit] 	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:81)
[junit] 	at org.apache.cassandra.db.marshal.CompositeTypeTest.testCompatibility(CompositeTypeTest.java:216)
{noformat}
# DefsTest:
{noformat}
[junit] Testcase: testUpdateColumnFamilyNoIndexes(org.apache.cassandra.db.DefsTest):	FAILED
[junit] Should have blown up when you used a different comparator.
[junit] junit.framework.AssertionFailedError: Should have blown up when you used a different comparator.
[junit] 	at org.apache.cassandra.db.DefsTest.testUpdateColumnFamilyNoIndexes(DefsTest.java:539)
{noformat}
# CompactSerializerTest:
{noformat}
[junit] null
[junit] java.lang.ExceptionInInitializerError
[junit] 	at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:437)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:195)
[junit] 	at org.apache.cassandra.utils.NodeId$LocalIds.<clinit>(NodeId.java:43)
[junit] 	at java.lang.Class.forName0(Native Method)
[junit] 	at java.lang.Class.forName(Class.java:169)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:96)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest$1DirScanner.scan(CompactSerializerTest.java:87)
[junit] 	at org.apache.cassandra.io.CompactSerializerTest.scanClasspath(CompactSerializerTest.java:129)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:574)
[junit] 	at org.apache.cassandra.db.Table.<clinit>(Table.java:82)
{noformat}

There is also some error RemoveSubColumnTest and RemoveSubColumnTest but I'll open a separate ticket for those as they may require a bit more discussion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/12 19:10;slebresne;0001-Fix-DefsTest.patch;https://issues.apache.org/jira/secure/attachment/12513469/0001-Fix-DefsTest.patch","06/Feb/12 19:10;slebresne;0002-Fix-SSTableImportTest.patch;https://issues.apache.org/jira/secure/attachment/12513470/0002-Fix-SSTableImportTest.patch","06/Feb/12 19:10;slebresne;0003-Fix-CompositeTypeTest.patch;https://issues.apache.org/jira/secure/attachment/12513471/0003-Fix-CompositeTypeTest.patch",,,,,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226752,,,Wed Feb 15 15:14:44 UTC 2012,,,,,,,,,,"0|i0gp5r:",95506,,,,,Normal,,,,,,,,,,,,,,,,,"06/Feb/12 19:18;slebresne;Attached patches are fixing failures 1, 2 and 3 above. More precisely:
# SSTableImportTest: the patch speaks for itself. It's likely due a change of the jackson lib version or something like that.
# CompositeTypeTest: this is a regression of commit 96ecaff5 that removed a perfectly fine (and useful) method
# DefsTest: CASSANDRA-3657 mad possible the kind of comparator change the test was testing (UTF8Type -> BytesType). The patch replaces that by a non-compatible change.

For the CompactSerializerTest, I'm a little bit less sure what the right fix is. As weird as that sound, the problem comes from BulkRecordWriter. Somehow the class is loaded by the test, which makes it replace the config field of DatabaseDescriptor by a Config object where data_file_directories is null (more precisely DatabaseDescriptor.initDefaultsOnly() is called). I'm pretty sure there is something a little bit ugly in there, but since I'm not sure how BulkRecordWriter is supposed to be used, I leave that to someone else.;;;","07/Feb/12 14:08;jbellis;+1;;;","07/Feb/12 16:08;slebresne;Committed, thanks.

I'm keeping this open so that we deal with the CompactSerializerTest/BulkRecordWriter failure, but I'll assign someone that actually know something about BulkRecordWriter.;;;","07/Feb/12 18:32;brandon.williams;Hmm, I had to add SnapshotCommandSerializer to expectedClassNames in CompactSerializerTest, but after that it passes for me.;;;","08/Feb/12 14:56;slebresne;That's weird, as I still get it every time.
But actually I think that is a bit dependent on the class loader, so this may be platform dependent (I'm using sun java 1.6.0_26).

In any case, I think there do is something fishy in there. Namely, BulkRecordWriter override the global configuration in it's static initializer. And the config it override with has data_file_directories == null. Which means that if the class Table is loaded after that, it will try to create default directories as part of the class initialization (unless StorageService.isClientMode == true) and will throw a NPE because data_file_directories == null. I'm not too sure in which context exactly BulkRecordWriter is supposed to be used, but outside of making the unit test fail at least on my machine, it does sound fairly fragile.
Of course, that's another way to say that all our static initialization business is fragile, and we should fix that someday. Nevertheless, would it be reasonable for instance in BulkLoaderWriter to call something to set StorageService.isClientMode to true before changing the configuration (so that if Table is loaded later, we don't try to create the default directories)? ;;;","08/Feb/12 17:41;brandon.williams;bq. That's weird, as I still get it every time.

I think that's because I was running this test by itself, so BRW was never loaded (though I'm pretty sure we need to add SnapshotCommandSerializer if we do get past this.)

bq. BulkRecordWriter override the global configuration in it's static initializer

That was a failed attempt to get around DD's static initialization (and yes, all our static initialization is a mess), but I've reworked the approach in CASSANDRA-3740 and that should pass since there is no static initialization.
;;;","08/Feb/12 17:48;slebresne;bq. I think that's because I was running this test by itself

I get it even if I run the test by itself. That's because CompactSerializerTest actually load pretty much every class during it's scanning. But I suppose the order in which it load stuffs is very platform dependent.

bq. I'm pretty sure we need to add SnapshotCommandSerializer if we do get past this

That may very well be.

bq. I've reworked the approach in CASSANDRA-3740 and that should pass since there is no static initialization

Alright, let's wait for it. I'll retest and close this if that fixes it.;;;","15/Feb/12 15:14;slebresne;Ok, I think the problems of that ticket are now all solved, closing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool ring output not sorted by token order,CASSANDRA-3863,12541457,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,mallen,mallen,06/Feb/12 18:38,16/Apr/19 09:32,14/Jul/23 05:52,07/Feb/12 17:32,1.1.0,,,Tool/nodetool,,,0,,,,,,,"Prior to 1.1 the output of nodetool ring was sorted in token order.  It looks like StorageService.getTokenToEndpointMap has been changed to return Map<String,String> in place of the previously used Map<Token, String>.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/12 17:15;yukim;cassandra-1.1-3863.txt;https://issues.apache.org/jira/secure/attachment/12513638/cassandra-1.1-3863.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226744,,,Tue Feb 07 17:32:42 UTC 2012,,,,,,,,,,"0|i0gp53:",95503,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"07/Feb/12 17:15;yukim;Patch attached to let {{getTokenToEndpointMap }} return Map with tokens in ascending order.;;;","07/Feb/12 17:32;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowCache misses Updates,CASSANDRA-3862,12541448,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,doubleday,doubleday,06/Feb/12 17:17,16/Apr/19 09:32,14/Jul/23 05:52,28/Feb/12 18:03,1.1.0,,,,,,0,,,,,,,"While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.

During my stress test I hava lots of threads running with some of them only reading other writing and re-reading the value.

This seems to happen:

- Reader tries to read row A for the first time doing a getTopLevelColumns
- Row A which is not in the cache yet is updated by Writer. The row is not eagerly read during write (because we want fast writes) so the writer cannot perform a cache update
- Reader puts the row in the cache which is now missing the update

I already asked this some time ago on the mailing list but unfortunately didn't dig after I got no answer since I assumed that I just missed something. In a way I still do but haven't found any locking mechanism that makes sure that this should not happen.

The problem can be reproduced with every run of my stress test. When I restart the server the expected column is there. It's just missing from the cache.

To test I have created a patch that merges memtables with the row cache. With the patch the problem is gone.

I can also reproduce in 0.8. Haven't checked 1.1 but I haven't found any relevant change their either so I assume the same aplies there.",,doubleday,hudson,mauzhang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/12 23:19;jbellis;3862-7.txt;https://issues.apache.org/jira/secure/attachment/12515996/3862-7.txt","13/Feb/12 18:26;jbellis;3862-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12514380/3862-cleanup.txt","08/Feb/12 13:58;slebresne;3862-v2.patch;https://issues.apache.org/jira/secure/attachment/12513810/3862-v2.patch","15/Feb/12 09:36;slebresne;3862-v4.patch;https://issues.apache.org/jira/secure/attachment/12514613/3862-v4.patch","22/Feb/12 22:19;jbellis;3862-v5.txt;https://issues.apache.org/jira/secure/attachment/12515652/3862-v5.txt","23/Feb/12 16:01;jbellis;3862-v6.txt;https://issues.apache.org/jira/secure/attachment/12515760/3862-v6.txt","27/Feb/12 15:03;jbellis;3862-v8.txt;https://issues.apache.org/jira/secure/attachment/12516165/3862-v8.txt","07/Feb/12 16:04;slebresne;3862.patch;https://issues.apache.org/jira/secure/attachment/12513625/3862.patch","13/Feb/12 13:30;slebresne;3862_v3.patch;https://issues.apache.org/jira/secure/attachment/12514350/3862_v3.patch","27/Feb/12 16:22;slebresne;3862_v8_addon.txt;https://issues.apache.org/jira/secure/attachment/12516179/3862_v8_addon.txt","06/Feb/12 17:19;doubleday;include_memtables_in_rowcache_read.patch;https://issues.apache.org/jira/secure/attachment/12513452/include_memtables_in_rowcache_read.patch",,,,,,,11.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226735,,,Wed Jul 04 23:16:47 UTC 2012,,,,,,,,,,"0|i0gp4n:",95501,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Feb/12 17:19;doubleday;Dunno if there's a better way to do it...;;;","06/Feb/12 18:04;slebresne;I believe you are absolutely right that this is a bug.

Unfortunately I don't think including the memtables during cache reads really solves it. If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.

One partial solution I see could be that when a read 'reads for caching', it starts adding some sentinel object in the cache for the given row key. That sentinel would need to be an actual (empty) row but marked with the fact it's only a sentinel. When a write look if the row is cache, if it's a sentinel we would add the write to the sentinel. Once the read returns and we actually put the row in cache, we would it (atomically) with the content of the sentinel. A read that check the cache and see a sentinel would just skip the cache (and would not put it's result into the cache). Adapting that to the serializingCache is trivial.

Unfortunately, this is not perfect because this would screw counters. Though I guess for counters we could do the same thing as we would do for the serializingCache, i.e, if a read that 'reads for caching' see that the sentinel is not empty, we would just not cache the result (i.e, a row would be cache only if we are sure no write were done concurrently to the read).;;;","06/Feb/12 19:32;doubleday;bq.  If you miss an update, it won't ever get added to the cached row, but the update itself will be flushed at some point and thus not be in any memtable anymore.

Very true ...

How about adopting the strategy we apply with CASSANDRA-2864:

- Writers dont update the cache at all
- Readers merge cache with memtables
- Upon flush merge memtables with cache;;;","06/Feb/12 19:58;slebresne;{quote}
How about adopting the strategy we apply with CASSANDRA-2864:

* Writers dont update the cache at all
* Readers merge cache with memtables
* Upon flush merge memtables with cache
{quote}
The problem with that is that I don't see how we can make that work for counters at all. I also think it would be nice not having to merge on reads if we can avoid it (even if it's in-memory, it still uses CPU).

As a side note, I also suspect it's not bulletproof in theory, as a memtable could be fully flushed while a 'read to be cached' happens and with a bad timing during that, we could still miss an update. Of course, that kind of timing have almost no chance to happen. But in the case where a user triggers a flush manually, a memtable with only a handful of columns could be flushed very quickly, and I suspect the behavior could be observed. However unlikely that is, it'd be better if we can fix this problem once and for all.

I'll probably give a shot to my 'sentinel' proposal described above, I don't think it's too much code.;;;","06/Feb/12 21:25;doubleday;Hmokay ... don't want to abuse Jira as an educational forum but maybe as a reward for the bugreport :-) ... are you saying that a reader could see a memtable view where flushing memtables are gone (flushed) and sstables don't contain the flushed memtables?

If that's the case than yes the cache would lose an update. But that what also imply that a read could miss an update without caching being in place at all no?

Otherwise (and that's how I read the code) given that the memtable switch will only happen after the merge the reader will read all updates because they are either in (flushing) memtables or in sstables and the cache will be in fact valid.

;;;","07/Feb/12 07:38;slebresne;To be precise, what I'm saying is that (at least in theory) the following scenario would be possible:
* A read-for-cache read the memtables grabing updates
* then it start reading the sstables
* while the previous happens, a new update arrives. The memtable is then flushed and happens to be fully flushed *before* our read-for-cache completes.

In that case, the new update won't be part of the cached row (ever) because during the flush (when we would merge the memtable to the cache) the row was not in the cache yet. That may seem far fetched but consider a simple implementation of you proposition, where the 'upon flush merge memtables with cache' phase happens in the same loop over rows that is used for flushing. It is actually possible for a new write to be ""flushed"" within a few milliseconds of being received by the node: if the update triggers the memtable threshold *and* sorts at the very beginning of the memtable. But don't get me wrong, it would probably be possible to deal with that problem, but it feels a bit complicated and error prone.

;;;","07/Feb/12 16:04;slebresne;Patch attached with my ""sentinel"" idea (The patch is against 1.1 currently). I *think* this fixes the problem, and this deal with counters.;;;","07/Feb/12 21:48;doubleday;Just had a look at it and maybe I got it wrong but:

CFS.getRawCachedRow returns null for a sentinel and CFS.updateRowCache calls this.
Isn't updateRowCache supposed to add changes to the sentinel so that cacheRow can detect the race?;;;","08/Feb/12 13:58;slebresne;You're right, thanks for catching that. Attached v2 fixed this (I realized that when we hit the cache during range_slice queries we don't update the statistics, which I'm not sure is what we want, but it's unrelated to that issue so haven't changed it).;;;","11/Feb/12 00:01;jbellis;Looks to me like we might be able to simplify things by splitting the ""initialize row cache"" code (which can assume the cache is empty, and does not need a filter) out from the ""look up a cached row and cache it if it is not present"" method.

Nit: although not perfect, IMO ""getRawCachedRow"" is a better method name than ""getCachedRowNoStats"" -- the important thing to convey is that we're only inspecting the cache's contents, not changing them.;;;","13/Feb/12 13:33;slebresne;Attaching v3. This mostly fix a but of the previous version where sentinels were not handled correctly in cacheRow(). I've also switch back to getRawCachedRow.
I'm not fully sure what you proposed to split exactly, but v3 does split cacheRow() in the hope of increasing clarity. ;;;","13/Feb/12 18:23;jbellis;Attached cleanup patch that applies on top of v3.  Most of the changes are adding docstrings/comments and cleaning up typos.

A minor change to the code was to make cacheRow take just cfId and filter, removing the redundant filter.key as a parameter.

I also renamed cacheRow to getThroughCache.  Still not 100% happy with that, but my goal is to make the distinction between readAndCache more obvious.

Finally, I've modified the logic in invalidateCachedRow according to the reasoning in this comment:

{noformat}
.       // This method is used to (1) drop obsolete entries from a copying cache after the row in question was updated
        // and to (2) make sure we're not wasting cache space on rows that don't exist anymore post-compaction.
        // Sentinels complicate this because it means we've caught a read thread in the process of loading
        // the cache, and we don't know (in case 2) if it will do so with rows from before the compaction or after,
        // so we need to loop until the load completes.
{noformat}

(I also negated the loop condition, which looked like an oversight.);;;","14/Feb/12 11:16;slebresne;The cleanup lgtm.

For the change to invalidateCacheRow however, I wonder if it's worth it. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike. On the other side, if we just leave the sentinel in that case, the only risk we take is that a read may end up putting tombstone in the cache that are already expired. But it doesn't seem like a big deal, especially given that it will very rarely happen.

But in any case, you're right about negating the loop condition. ;;;","14/Feb/12 13:52;jbellis;bq. By waiting when we found a sentinel, we may have writes waiting on a read to complete, which could involve a non negligible latency spike

You're right, that's a worse negative than leaving tombstones in the cache.  I'm fine with changing it back if you update the comments accordingly. :);;;","15/Feb/12 09:36;slebresne;Actually then handling of the copying patch by the preceding patches is wrong.  When a put arrives and there is a sentinel, the patch does not add the put to the sentinel correctly. But thinking about it, for the copying cache, we should avoid having writes check the current value in the cache, because that have a non-negligible performance impact. What we should do is let invalidate actually invalidate sentinels. The only problem we're faced with if we do that, is that when a read-for-caching returns, it must make sure his own sentinel hasn't been invalidated. And in particular it must be careful of the case where the sentinel has been invalidated and another read has set another sentinel.

Anyway, attaching a v4 (that include the comments cleanups) that choose that strategy instead (and thus is (hopefully) not buggy even in the copying cache case). Note that it means that reads must be able to identify sentinels uniquely (not based on the content), so the code assign a unique ID to sentinel and use that for comparison.
;;;","22/Feb/12 22:19;jbellis;Attached v5 with a simpler approach: for serializing cache, getThroughCache does a classic CAS loop with a sentinel vs the write's invalidate.

v5 also adds a containsCachedRow method to CFS so that callers that don't care about the value don't force a deserialize in the serializing cache case.;;;","23/Feb/12 09:11;slebresne;I don't think v5 works. All sentinels are empty CF, so all sentinels will be equal (in SerializingCache.contentsEqual()). Which means we can have the following sequence of actions:
* a read r1 comes, the cache is empty, it sets sentinel s1 and start reading from disk
* a write w comes and invalidate s1.
* a read r2 comes, the cache is (now) empty, it sets sentinel s2 and start reading from disk
* r1 finish reading from disk having missed w. It'll do the replace, but since all sentinel are equals this will succeed (even though the current sentinel is the one of the second read) and we'll end up having missed w.

That's the reason of the sentinel IDs of v4.;;;","23/Feb/12 15:56;jbellis;v6 pulls RCS out to a separate file and adds a uuid version and equals/hashcode methods.  SerializingCacheProvider uses a custom CF serializer that is RCS-aware.  SerializingCache.replace is simplified to use RCS.equals.  CAS loop is extended to non-serializing cache: since cache/write race is extremely rare, I'd rather take the occasional re-read penalty, than increase the overhead of every row in the cache by making them RCS objects permanently.;;;","24/Feb/12 09:53;slebresne;Remarks on v6:
* Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily. We should probably create a CachedRow class extended by both Sentinel (that would really just be an identifier, no metadata needed) and ColumnFamily and use that as cache values. It'll be cleaner and more importantly more type safe (a cache lookup won't be able to ignore by mistake that it could get a sentinel).
* Not adding stuffs to the sentinel also mean that in getThroughCache the counter special case is not needed anymore.
* In getThroughCache, if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading. Better let the next client read cache the data than getting a crappy latency on the current read.
* Is it really an improvement to use UUIDs (over an AtomicLong)? I have nothing against UUID per se but it takes twice the space (and we serialize them) and without having benchmarked it, I'm willing to bet are much faster to generate. And let's be honest, the risk of overflow with an AtomicLong is science-fiction (or to be precise, at 1 millions sentinels created per seconds (which is *way* more than we'll ever see), you'd need more than 100,000 year of uptime to overflow).
;;;","24/Feb/12 23:19;jbellis;v7 attached.

bq. Since we don't add stuffs to the sentinel, it has no reason to be a subclass of ColumnFamily

True, but when I tried this I ended up with a LOT of casting cache values to CF.  I think it might be the lesser of evils the way it is.

bq. the counter special case is not needed anymore

Updated.

bq. if we fail to replace the sentinel, I think we should still better return the data rather than looping and re-reading

Makes sense, updated.

bq. Is it really an improvement to use UUIDs (over an AtomicLong)? 

I'd rather have the reduced contention on instantiation than the 8 bytes of space (during the sentinel lifetime -- this goes away once the sentinel is replaced by the data CF).;;;","27/Feb/12 09:28;slebresne;* In SerializingCache, remove misses a ""not"" in the while condition (this date back from one of my earlier patch). We don't need that new remove method anymore though so it's probably as simple to just remove it from the patch.
* In CFS.getThroughCache, the following line
{noformat}
boolean sentinelSuccess = !CacheService.instance.rowCache.putIfAbsent(key, sentinel);
{noformat}
should not be negated.
* Also in CFS.getThroughCache, we won't remove the sentinel if there is an exception during the read. It's not a big deal but it doesn't cost much to prevent it from happening.

bq. True, but when I tried this I ended up with a LOT of casting cache values to CF. I think it might be the lesser of evils the way it is.

In that case, I think I wouldn't mind too much casts and I would prefer getting the type safety of knowing that a method that take a ColumnFamily can't ever get a sentinel (and to make it explicit when you need to care about sentinel or not), but that's a bit subjective. There would also be some small wins like the fact we wouldn't need to save the cfId when serializing a sentinel.

bq. I'd rather have the reduced contention on instantiation than the 8 bytes of space

My point was that the UUID don't reduce contention. UUID.randomUUID() uses SecureRandom.nextBytes() that is synchronized (and thus likely entail a much bigger degradation in face of contention than an AtomicLong) and probably a bit CPU intensive. For reference, I did a quick micro-benchmark having 50 threads generating 10,000 ids simultaneously using both methods, using an AtomicLong is two orders of magnitude faster.
;;;","27/Feb/12 15:03;jbellis;v8 attached w/ long sentinel and IRowCacheEntry.;;;","27/Feb/12 15:49;slebresne;v8 lgtm mostly except for the 3 remarks at the beginning of my previous comment. Added simple patch on top of v8 that does the proposed modifications.;;;","27/Feb/12 16:17;jbellis;shouldn't {{if (data == null)}} in the finally block be {{if (sentinelSuccess && data == null)}} ?;;;","27/Feb/12 16:22;slebresne;Oups, you're right. Patch updated.;;;","27/Feb/12 16:24;jbellis;+1;;;","28/Feb/12 18:03;slebresne;Committed, thanks;;;","04/Jul/12 23:16;hudson;Integrated in Cassandra #1646 (See [https://builds.apache.org/job/Cassandra/1646/])
    restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision fbb5ec0374e1a5f1b24680f1604b6e9201fb535f)
fix build - re-add CompactionController.removeDeletedInCache for commit fbb5ec0374e1a5f1b24680f1604b6e9201fb535f restore pre-CASSANDRA-3862 approach to removing expired tombstones during compaction (Revision 086c06ad7fb211de6be877c3c1ea2ee4f86c6d7e)

     Result = ABORTED
jbellis : 
Files : 
* src/java/org/apache/cassandra/db/compaction/CompactionIterable.java
* CHANGES.txt

dbrosius : 
Files : 
* src/java/org/apache/cassandra/db/compaction/CompactionController.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RemoveDeleted dominates compaction time for large sstable counts,CASSANDRA-3855,12541341,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,stuhood,stuhood,05/Feb/12 21:52,16/Apr/19 09:32,14/Jul/23 05:52,20/Jul/12 22:35,1.2.0 beta 1,,,,,,0,compaction,deletes,leveled,,,,"With very large numbers of sstables (2000+ generated by a `bin/stress -n 100,000,000` run with LeveledCompactionStrategy), PrecompactedRow.removeDeletedAndOldShards dominates compaction runtime, such that commenting it out takes compaction throughput from 200KB/s to 12MB/s.

Stack attached.",,hudson,slebresne,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/12 15:25;yukim;3855.txt;https://issues.apache.org/jira/secure/attachment/12537361/3855.txt","05/Feb/12 21:52;stuhood;with-cleaning-java.hprof.txt;https://issues.apache.org/jira/secure/attachment/12513372/with-cleaning-java.hprof.txt",,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226629,,,Fri Jul 20 23:19:03 UTC 2012,,,,,,,,,,"0|i0gp1b:",95486,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"14/Jun/12 15:38;jbellis;Yuki, can you benchmark trunk and see if this is still a bottleneck?  I think we want to bench both LCS and STCS.;;;","15/Jun/12 18:16;slebresne;I'll precise that I try to do a quick test to see if I could reproduce ""back in the days"" but wasn't really able to reproduce something similar to the attached hprof log. I didn't wait up to 100,000,000 keys though.;;;","19/Jul/12 22:59;yukim;I ran cpu profile on trunk and 1.1 with LCS and about 1000 sstables. On 1.1 branch, there is no indication of dominating removeDeletedAndOldShards. But for trunk, I noticed that it seemed unnecessary CompactionController#shouldPurge is called inside removeDeletedAndOldShards, where shouldPurge is supposed to be called only when CF has tombstones. So I looked up the code and I'm not sure if this line(https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java#L201) is correct. If CF is live, returning false for hasIrrelevantData seems right. Sylvain, what do you think?;;;","19/Jul/12 23:13;jbellis;That's definitely wrong...  I think it should be {{if (info != LIVE) return false}};;;","20/Jul/12 08:31;slebresne;Agreed that it is wrong, but I think that it's more than the first line that is wrong. I think that method should be:
{noformat}
public boolean hasIrrelevantData(int gcBefore)
{
    if (deletionInfo().isLive())
        return false;

    // Do we have gcable deletion infos?
    if (!deletionInfo().purge(gcbefore).equals(deletionInfo()))
        return true;

    // Do we have colums that are either deleted by the container or gcable tombstone?
    for (IColumn column : columns)
        if (deletionInfo().isDeleteted(column) || column.hasIrrelevantData(gcBefore))
            return true;

    return false;
}
{noformat};;;","20/Jul/12 14:27;jbellis;We definitely don't want ""if row is live, nothing to do here"" behavior, otherwise we'll never purge column-level tombstones without a full row deletion.;;;","20/Jul/12 14:37;slebresne;Your right, that's a broken optimization, the 2 first lines should be removed.;;;","20/Jul/12 14:44;jbellis;+1 for proposed method w/ first 2 lines removed;;;","20/Jul/12 15:25;yukim;So I summarized and attached patch. Tested on trunk and confirmed it fixed.;;;","20/Jul/12 17:06;jbellis;+1;;;","20/Jul/12 22:35;yukim;Committed to trunk.;;;","20/Jul/12 23:19;hudson;Integrated in Cassandra #1734 (See [https://builds.apache.org/job/Cassandra/1734/])
    fix incorrect hasIrrelevantData result for live CF; patch by yukim, reviewed by jbellis/slebresne for CASSANDRA-3855 (Revision d74103735126658d64cb92a16f4bb40f63d5e2e8)

     Result = ABORTED
yukim : 
Files : 
* src/java/org/apache/cassandra/db/AbstractColumnContainer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_indexed_slices losts index expressions,CASSANDRA-3850,12541180,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,philip.andronov,philip.andronov,04/Feb/12 10:11,16/Apr/19 09:32,14/Jul/23 05:52,04/Feb/12 14:29,1.1.0,,,,,,0,get,indexing,search,,,,"in trunk 
CassandraServer.get_indexed_slices(ColumnParent , IndexClause , SlicePredicate , ConsistencyLevel)
 looses  index_clause.expressions when calling  constructing RangeSliceCommand by using wrong constructor.

This makes examples on http://wiki.apache.org/cassandra/CassandraCli produce wrong output as well as any get involving ""where"" check.
Patch to fix this issue http://pastebin.com/QQT0Tfpc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226532,,,Sat Feb 04 14:29:44 UTC 2012,,,,,,,,,,"0|i0goz3:",95476,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"04/Feb/12 14:29;slebresne;Committed, thanks.

(don't hesitate to attach the patch to the issue next time, it's slightly more convenient :));;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved CF row cache breaks when upgrading to 1.1,CASSANDRA-3849,12541166,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,04/Feb/12 02:03,16/Apr/19 09:32,14/Jul/23 05:52,06/Feb/12 14:18,1.1.0,,,,,,0,,,,,,,"Enabled row and key caching. Used stress to insert some data. ran nodetool flush, then nodetool compact. Then read the data back to populate the cache. Turned row_cache_save_period and key_cache_save_period really low to force saving the cache data. I verified that the row and key cache files existed in /var/lib/cassandra/saved_caches/.

I then killed cassandra, checked out branch cassandra-1.1, compiled and tried to start the node. The node failed to start, and I got this error:
{code}
 INFO 01:33:30,893 reading saved cache /var/lib/cassandra/saved_caches/Keyspace1-Standard1-RowCache
ERROR 01:33:31,009 Exception encountered during startup
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: Row cache is not enabled on column family [Standard1]
{code}",1 node cluster running on branch cassandra-1.0. Ubuntu. both key and row caching were enabled.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/12 19:39;xedin;CASSANDRA-3849.patch;https://issues.apache.org/jira/secure/attachment/12513259/CASSANDRA-3849.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226518,,,Mon Feb 06 14:18:34 UTC 2012,,,,,,,,,,"0|i0goyn:",95474,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"06/Feb/12 14:18;slebresne;+1, committed, thks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig should throw a useful error when the destination CF doesn't exist,CASSANDRA-3847,12541137,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,03/Feb/12 21:30,16/Apr/19 09:32,14/Jul/23 05:52,06/Feb/12 21:11,1.0.8,,,,,,0,,,,,,,"When trying to store data to nonexistent CF, no good error is returned.

Instead you get a message like:

{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
{noformat}

Which, if you follow its advice, will eventually lead you to an NPE in initSchema.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/12 20:49;brandon.williams;3847.txt;https://issues.apache.org/jira/secure/attachment/12513497/3847.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226489,,,Mon Feb 06 21:11:37 UTC 2012,,,,,,,,,,"0|i0goxr:",95470,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"06/Feb/12 20:49;brandon.williams;Unfortunately, there's nothing we can do about pig catching errors and rethrowing the ""org.apache.pig.impl.logicalLayer.FrontendException: ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false."" message, but we can at least throw a helpful error into the log instead of an NPE.;;;","06/Feb/12 21:00;xedin;+1;;;","06/Feb/12 21:11;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh can't show data under python2.5, python2.6",CASSANDRA-3846,12541134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,03/Feb/12 20:52,16/Apr/19 09:32,14/Jul/23 05:52,06/Feb/12 19:42,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,,,"Kris Hahn discovered a python2.6-ism in recent cqlsh changes:

{code}
        bval = escapedval.encode(output_encoding, errors='backslashreplace')
{code}

before python2.7, str.encode() didn't accept a keyword argument for the second parameter. the semantics are the same without naming the parameter, though, so removing the ""errors="" bit should suffice to make it run right.

does not affect any released version, but does affect HEAD of cassandra-1.0, cassandra-1.1, and trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/12 20:56;thepaul;3846.patch.txt;https://issues.apache.org/jira/secure/attachment/12513171/3846.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226486,,,Mon Feb 06 19:42:17 UTC 2012,,,,,,,,,,"0|i0goxb:",95468,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"03/Feb/12 20:56;thepaul;..or https://github.com/thepaul/cassandra/commit/24cfab5ddb4dd1c619c9a14878dd31fd18a3cbe9 , found on my 3846 branch;;;","03/Feb/12 21:13;thepaul;edit: this is also a problem on python2.6, not just python2.5. resolution is still the same.;;;","06/Feb/12 19:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate leaves behind non-CFS backed secondary indexes,CASSANDRA-3844,12541078,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,tjake,tjake,03/Feb/12 14:28,16/Apr/19 09:32,14/Jul/23 05:52,07/Feb/12 15:25,1.0.8,,,Feature/2i Index,,,0,,,,,,,"If you setup a CF with a non-cfs backed secondary index then trucate it, nothing happens to the secondary index. we need a hook for CFStore to clean these up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/12 14:48;xedin;CASSANDRA-3844.patch;https://issues.apache.org/jira/secure/attachment/12513618/CASSANDRA-3844.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226430,,,Tue Feb 07 15:25:25 UTC 2012,,,,,,,,,,"0|i0gowf:",95464,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"07/Feb/12 14:08;xedin;This patch encapsulates truncate logic into SecondaryIndex.truncate(long) abstract method so every type of indices would be able to implement desired logic (currently implemented for KeysIndex), which also removes a good bit of code from CompactionManager.submitTruncate(...).;;;","07/Feb/12 14:13;xedin;renamed CFS.truncateSSTables(long) to CFS.discardSSTables(long).;;;","07/Feb/12 14:41;tjake;SIM.getIndexes() should use a IdenenityHashMap since PerRowSecondaryIndexes share the same instance across rows.;;;","07/Feb/12 14:48;xedin;uses IdentityHashMap in SecondaryIndexManager.getIndexes();;;","07/Feb/12 14:52;tjake;+1;;;","07/Feb/12 15:25;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary  ReadRepair request during RangeScan,CASSANDRA-3843,12541058,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,philip.andronov,philip.andronov,03/Feb/12 11:40,16/Apr/19 09:32,14/Jul/23 05:52,09/Feb/12 21:43,1.0.8,,,,,,0,,,,,,,"During reading with Quorum level and replication factor greater then 2, Cassandra sends at least one ReadRepair, even if there is no need to do that. 

With the fact that read requests await until ReadRepair will finish it slows down requsts a lot, up to the Timeout :(

It seems that the problem has been introduced by the CASSANDRA-2494, unfortunately I have no enought knowledge of Cassandra internals to fix the problem and do not broke CASSANDRA-2494 functionality, so my report without a patch.

Code explanations:
{code:title=RangeSliceResponseResolver.java|borderStyle=solid}
class RangeSliceResponseResolver {
    // ....
    private class Reducer extends MergeIterator.Reducer<Pair<Row,InetAddress>, Row>
    {
    // ....

        protected Row getReduced()
        {
            ColumnFamily resolved = versions.size() > 1
                                  ? RowRepairResolver.resolveSuperset(versions)
                                  : versions.get(0);
            if (versions.size() < sources.size())
            {
                for (InetAddress source : sources)
                {
                    if (!versionSources.contains(source))
                    {
                          
                        // [PA] Here we are adding null ColumnFamily.
                        // later it will be compared with the ""desired""
                        // version and will give us ""fake"" difference which
                        // forces Cassandra to send ReadRepair to a given source
                        versions.add(null);
                        versionSources.add(source);
                    }
                }
            }
            // ....
            if (resolved != null)
                repairResults.addAll(RowRepairResolver.scheduleRepairs(resolved, table, key, versions, versionSources));
            // ....
        }
    }
}
{code}


{code:title=RowRepairResolver.java|borderStyle=solid}
public class RowRepairResolver extends AbstractRowResolver {
    // ....
    public static List<IAsyncResult> scheduleRepairs(ColumnFamily resolved, String table, DecoratedKey<?> key, List<ColumnFamily> versions, List<InetAddress> endpoints)
    {
        List<IAsyncResult> results = new ArrayList<IAsyncResult>(versions.size());

        for (int i = 0; i < versions.size(); i++)
        {
            // On some iteration we have to compare null and resolved which are obviously
            // not equals, so it will fire a ReadRequest, however it is not needed here
            ColumnFamily diffCf = ColumnFamily.diff(versions.get(i), resolved);
            if (diffCf == null)
                continue;
        // .... 
{code}

Imagine the following situation:
NodeA has X.1 // row X with the version 1
NodeB has X.2 
NodeC has X.? // Unknown version, but because write was with Quorum it is 1 or 2

During the Quorum read from nodes A and B, Cassandra creates version 12 and send ReadRepair, so now nodes has the following content:
NodeA has X.12
NodeB has X.12

which is correct, however Cassandra also will fire ReadRepair to NodeC. There is no need to do that, the next consistent read have a chance to be served by nodes {A, B} (no ReadRepair) or by pair {?, C} and in that case ReadRepair will be fired and brings nodeC to the consistent state

Right now we are reading from the Index a lot and starting from some point in time we are getting TimeOutException because cluster is overloaded by the ReadRepairRequests *even* if all nodes has the same data :(",,arthurm,jeromatron,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/12 21:43;jbellis;3843-v2.txt;https://issues.apache.org/jira/secure/attachment/12514012/3843-v2.txt","09/Feb/12 04:29;jbellis;3843.txt;https://issues.apache.org/jira/secure/attachment/12513904/3843.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226410,,,Tue Feb 21 16:48:32 UTC 2012,,,,,,,,,,"0|i0govz:",95462,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"09/Feb/12 04:29;jbellis;The null version was added for CASSANDRA-2680.  I think the core problem here is that the RSRR is being created with *all* the replica endpoints ({{liveEndpoints}}), not just the ones being asked to respond to the query ({{handler.endpoints}}).  This is a bit tricky since the handler wants to know its resolver at creation time, and unlike RowDigestResolver, RSRR wants to initialize its endpoints at creation time too.  Kind of hackish patch attached.
;;;","09/Feb/12 05:19;vijay2win@yahoo.com;+1;;;","09/Feb/12 15:57;philip.andronov;> The null version was added for CASSANDRA-2680.
Oh, good point. Sorry, I've should pay more attention on git history, not only on annotations :)

Anyway, thanks for the patch, now we could apply correct patch on our servers.;;;","09/Feb/12 21:43;jbellis;committed, with the same fix for the 2nd occurrence of the RSRR in 1.0 StorageProxy. v2 attached in case that makes it easyer for anyone to test.;;;","13/Feb/12 13:58;jeromatron;We'll be upgrading to 1.0.8 as soon as we can, but this seems like a significant issue for anyone doing range scans - does it make sense to backport to 0.8.x?;;;","13/Feb/12 18:33;jbellis;It's a relatively small patch, but StorageProxy and its callbacks can be fragile...  I almost didn't commit it to 1.0 either.  Tell you what though, I'll post a backported patch here and if you want you can run with it. :);;;","13/Feb/12 18:36;jbellis;Looks to me like the 1.0 code changes from v2 apply cleanly to 0.8.  (CHANGES diff does not apply but can be ignored.);;;","14/Feb/12 08:16;jeromatron;I patched the version of 0.8.4 that we use with the change.  I applied it to all of our staging nodes.  However, the problem with writes on the column family it was simply doing range scans of still persists.  I had major compacted a column family on all of the nodes, then did a simple pig job to read the contents of that CF, then I got a lot of minor compactions for that column family.;;;","14/Feb/12 16:58;jbellis;I suggest testing with a single range scan at debug level.  Too much hay to see the needle when you're doing 100s or 1000s of scans.;;;","14/Feb/12 16:58;jbellis;... You did patch with v2, right?;;;","14/Feb/12 17:07;jeromatron;I did patch with v2.  Doing more testing today and it appears that there are writes occurring but it looks like a definite reduction.  It could be a valid repair thing.  I'll do some more testing and hopefully repair every node and compact every node and then do a scan across a large column family and see what happens.;;;","21/Feb/12 14:29;jeromatron;I did repairs on all the nodes and then compacts on all the nodes. Then I did a pig job to simply count the number of rows in the column family. Again I think the overall writes were reduced but there are writes going on. I need to turn debug on and do the same test again. I did the compactions at 6:42 and the range scans at 14:16:

{code}
-rw-r--r-- 1 root root 40106228511 Feb 21 06:42 account_snapshot-g-792-Data.db
-rw-r--r-- 1 root root   206884816 Feb 21 06:42 account_snapshot-g-792-Filter.db
-rw-r--r-- 1 root root  2913796038 Feb 21 06:42 account_snapshot-g-792-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 06:42 account_snapshot-g-792-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-793-Compacted
-rw-r--r-- 1 root root      287286 Feb 21 14:16 account_snapshot-g-793-Data.db
-rw-r--r-- 1 root root         976 Feb 21 14:16 account_snapshot-g-793-Filter.db
-rw-r--r-- 1 root root       20857 Feb 21 14:16 account_snapshot-g-793-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:16 account_snapshot-g-793-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-794-Compacted
-rw-r--r-- 1 root root    87770771 Feb 21 14:17 account_snapshot-g-794-Data.db
-rw-r--r-- 1 root root      293944 Feb 21 14:17 account_snapshot-g-794-Filter.db
-rw-r--r-- 1 root root     6377968 Feb 21 14:17 account_snapshot-g-794-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-794-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-795-Compacted
-rw-r--r-- 1 root root    78459166 Feb 21 14:17 account_snapshot-g-795-Data.db
-rw-r--r-- 1 root root      262600 Feb 21 14:17 account_snapshot-g-795-Filter.db
-rw-r--r-- 1 root root     5698156 Feb 21 14:17 account_snapshot-g-795-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-795-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-796-Compacted
-rw-r--r-- 1 root root    69838937 Feb 21 14:17 account_snapshot-g-796-Data.db
-rw-r--r-- 1 root root      234000 Feb 21 14:17 account_snapshot-g-796-Filter.db
-rw-r--r-- 1 root root     5077447 Feb 21 14:17 account_snapshot-g-796-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-796-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-797-Compacted
-rw-r--r-- 1 root root    68094433 Feb 21 14:17 account_snapshot-g-797-Data.db
-rw-r--r-- 1 root root      227808 Feb 21 14:17 account_snapshot-g-797-Filter.db
-rw-r--r-- 1 root root     4943098 Feb 21 14:17 account_snapshot-g-797-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:17 account_snapshot-g-797-Statistics.db
-rw-r--r-- 1 root root   304163307 Feb 21 14:20 account_snapshot-g-798-Data.db
-rw-r--r-- 1 root root     1019776 Feb 21 14:20 account_snapshot-g-798-Filter.db
-rw-r--r-- 1 root root    22096669 Feb 21 14:20 account_snapshot-g-798-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-798-Statistics.db
-rw-r--r-- 1 root root    65874829 Feb 21 14:18 account_snapshot-g-799-Data.db
-rw-r--r-- 1 root root      220192 Feb 21 14:18 account_snapshot-g-799-Filter.db
-rw-r--r-- 1 root root     4777809 Feb 21 14:18 account_snapshot-g-799-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-799-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-800-Compacted
-rw-r--r-- 1 root root    50067413 Feb 21 14:18 account_snapshot-g-800-Data.db
-rw-r--r-- 1 root root      167416 Feb 21 14:18 account_snapshot-g-800-Filter.db
-rw-r--r-- 1 root root     3632313 Feb 21 14:18 account_snapshot-g-800-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-800-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-801-Compacted
-rw-r--r-- 1 root root    50575719 Feb 21 14:18 account_snapshot-g-801-Data.db
-rw-r--r-- 1 root root      169160 Feb 21 14:18 account_snapshot-g-801-Filter.db
-rw-r--r-- 1 root root     3669880 Feb 21 14:18 account_snapshot-g-801-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:18 account_snapshot-g-801-Statistics.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-g-802-Compacted
-rw-r--r-- 1 root root    41788766 Feb 21 14:19 account_snapshot-g-802-Data.db
-rw-r--r-- 1 root root      139776 Feb 21 14:19 account_snapshot-g-802-Filter.db
-rw-r--r-- 1 root root     3033069 Feb 21 14:19 account_snapshot-g-802-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-802-Statistics.db
-rw-r--r-- 1 root root    46547146 Feb 21 14:19 account_snapshot-g-803-Data.db
-rw-r--r-- 1 root root      155720 Feb 21 14:19 account_snapshot-g-803-Filter.db
-rw-r--r-- 1 root root     3378457 Feb 21 14:19 account_snapshot-g-803-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-803-Statistics.db
-rw-r--r-- 1 root root   142719184 Feb 21 14:20 account_snapshot-g-804-Data.db
-rw-r--r-- 1 root root      478576 Feb 21 14:19 account_snapshot-g-804-Filter.db
-rw-r--r-- 1 root root    10356119 Feb 21 14:20 account_snapshot-g-804-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-804-Statistics.db
-rw-r--r-- 1 root root    55373874 Feb 21 14:19 account_snapshot-g-805-Data.db
-rw-r--r-- 1 root root      185160 Feb 21 14:19 account_snapshot-g-805-Filter.db
-rw-r--r-- 1 root root     4017391 Feb 21 14:19 account_snapshot-g-805-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-805-Statistics.db
-rw-r--r-- 1 root root    46399227 Feb 21 14:19 account_snapshot-g-806-Data.db
-rw-r--r-- 1 root root      155120 Feb 21 14:19 account_snapshot-g-806-Filter.db
-rw-r--r-- 1 root root     3365947 Feb 21 14:19 account_snapshot-g-806-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-806-Statistics.db
-rw-r--r-- 1 root root    58491393 Feb 21 14:19 account_snapshot-g-807-Data.db
-rw-r--r-- 1 root root      196048 Feb 21 14:19 account_snapshot-g-807-Filter.db
-rw-r--r-- 1 root root     4253922 Feb 21 14:19 account_snapshot-g-807-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:19 account_snapshot-g-807-Statistics.db
-rw-r--r-- 1 root root    47609635 Feb 21 14:20 account_snapshot-g-808-Data.db
-rw-r--r-- 1 root root      159320 Feb 21 14:20 account_snapshot-g-808-Filter.db
-rw-r--r-- 1 root root     3456985 Feb 21 14:20 account_snapshot-g-808-Index.db
-rw-r--r-- 1 root root        4276 Feb 21 14:20 account_snapshot-g-808-Statistics.db
-rw-r--r-- 1 root root    46923060 Feb 21 14:20 account_snapshot-tmp-g-809-Data.db
-rw-r--r-- 1 root root           0 Feb 21 14:20 account_snapshot-tmp-g-809-Index.db
-rw-r--r-- 1 root root    49693602 Feb 21 14:20 account_snapshot-tmp-g-810-Data.db
-rw-r--r-- 1 root root      166600 Feb 21 14:20 account_snapshot-tmp-g-810-Filter.db
-rw-r--r-- 1 root root     3614750 Feb 21 14:20 account_snapshot-tmp-g-810-Index.db
{code};;;","21/Feb/12 15:51;brandon.williams;I'm unable to repro against 1.0 HEAD.;;;","21/Feb/12 16:48;jeromatron;Thanks - good to know - we'll upgrade to 1.0.8 as soon as we can then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test timing out,CASSANDRA-3841,12540963,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,mallen,mallen,02/Feb/12 18:05,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/12 14:58,1.1.6,,,Legacy/Testing,,,0,,,,,,,"    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.compaction.LongCompactionSpeedTest:BeforeFirstTest:	Caused an ERROR
    [junit] Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LongCompactionSpeedTest FAILED (timeout)
    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 64.536 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 41.104 sec
    [junit] 

BUILD FAILED
/Users/mallen/dstax/repos/git/cassandra/build.xml:1113: The following error occurred while executing this line:
/Users/mallen/dstax/repos/git/cassandra/build.xml:1036: Some long test(s) failed.

Total time: 63 minutes 9 seconds
",,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/12 05:16;jasobrown;0001-CASSANDRA-3841-long-test-timing-out.patch;https://issues.apache.org/jira/secure/attachment/12545524/0001-CASSANDRA-3841-long-test-timing-out.patch",,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226316,,,Tue Sep 18 14:58:25 UTC 2012,,,,,,,,,,"0|i0gov3:",95458,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Feb/12 00:12;jbellis;I see LCST fail as well when run as part of the full long-test suite.  When I run just LCST I get this:

{noformat}
$ ant clean long-test -Dtest.name=LongCompactionSpeedTest
...
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 20.981 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 992 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 4383 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 807 ms
    [junit] ------------- ---------------- ---------------

BUILD FAILED
/home/notroot/cassandra/build.xml:1075: Problem: failed to create task or type jvmarg
Cause: The name is undefined.
Action: Check the spelling.
Action: Check that any custom tasks/types have been declared.
Action: Check that any <presetdef>/<macrodef> declarations have taken place.
{noformat}

I think this means that
* LCST passes in isolation, but something isn't getting cleaned up from the prior test (MeteredFlusherTest) that causes it to timeout in the full suite
* There is some ant problem with -Dtest.name in conjunction with long-test;;;","18/Sep/12 05:16;jasobrown;I was able to reproduce the originally reported error - it was just the junit test timing out. The previous time limit was 300000 ms (5 minutes), and I increased it to 600000 ms (10 minutes).

However, I was not able to reproduce Jonathan's reported problems, even after executing a run of all the individual tests:

{code}for i in LongTableTest MeteredFlusherTest LongCompactionSpeedTest LongBloomFilterTest LongLegacyBloomFilterTest; \
do ant clean long-test -Dtest.name=$i; done{code};;;","18/Sep/12 05:19;jasobrown;Patch is trivial - just a property value modification. Should be able to apply to both 1.1 and 1.2 branches.;;;","18/Sep/12 14:58;jbellis;I'm not seeing the ""failed to create task"" problem anymore, either.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkOutputFormat binds to wrong client address when client is Dual-stack and server is IPv6,CASSANDRA-3839,12540923,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,forsberg,forsberg,02/Feb/12 13:46,16/Apr/19 09:32,14/Jul/23 05:52,02/Feb/12 23:19,1.1.0,,,,,,0,bulkloader,,,,,,"Trying to run a map/reduce job with BulkOutputFormat, in an environment where the Hadoop nodes have Dual-stack (IPv4+IPv6) and the Cassandra servers are IPv6-only, it seems like the TCP connection setup for streaming is explicitly setting the source address to the IPv4 address of the Hadoop node, even though the destination address is IPv6. 

I'm seeing connection attempts where source address is an IPv4-represented-in-IPv6 address and destination is IPv6 of cassandra node. 

In the log output from the Hadoop M/R job, I see:

{noformat}
2012-02-01 16:49:19,909 WARN org.apache.cassandra.streaming.FileStreamTask: Failed attempt 1 to connect to /2001:4c28:a030:30:72f3:95ff:fe02:2936 to stream /var/lib/hadoop/mapred/local/taskTracker/forsberg/jobcache/job_201201120812_0204/attempt_201201120812_0204_m_000000_0/test/Histograms/test-Histograms-hc-1-Data.db sections=1 progress=0/749048 - 0%. Retrying in 4000 ms. (java.net.ConnectException: Connection timed out)
{noformat}

So, digging a bit down the code, I see that org.apache.cassandra.hadoop.BulkRecordWriter successfully creates a Thrift connection to my Cassandra cluster, over IPv6. It successfully retrieves tokenrange information.

Later on, in org.apache.cassandra.streaming.FileStreamTask, it fails to connect to the destination cassandra node. It seems to me that the problem is that org.apache.cassandra.net.OutboundTcpConnectionPool is asking FBUtilities.getLocalAddress for the address to bind to, and getLocalAddress is returning an IPv4 address when DatabaseDescriptor has not been initialized. And DatabaseDescriptor has not been initialized, becase in BulkOutputFormat we're not reading cassandra.yaml. 

I actually have a workaround for this which involves not applying patch that removes need to read cassandra.yaml, then point to a cassandra.yaml generated specifically for the purpose on each hadoop node, with listen_address set to the IPv6 address of the node. 

This is with net.ipv6.bindv6only=0 in Linux sysctl - something you must have for Hadoop to run. 

Also tried -D mapred.child.java.opts=""-Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true"", i.e. setting properties to prefer IPv6 stack to M/R job, but didn't help.

In this case, we would probably be better of not explicitly binding to any address - the OS would do that for us. I understand binding explicitly makes sense when this code is running inside Cassandra server.","Linux 2.6.32-5-amd64, Java 1.6.0_26-b03",vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3740,,,,,CASSANDRA-3740,,,"02/Feb/12 21:37;brandon.williams;0005-Allow-using-any-interface-for-outgoing-connections.txt;https://issues.apache.org/jira/secure/attachment/12513038/0005-Allow-using-any-interface-for-outgoing-connections.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226276,,,Thu Feb 02 23:19:09 UTC 2012,,,,,,,,,,"0|i0gou7:",95454,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"02/Feb/12 14:34;brandon.williams;bq. It seems to me that the problem is that org.apache.cassandra.net.OutboundTcpConnectionPool is asking FBUtilities.getLocalAddress for the address to bind to, and getLocalAddress is returning an IPv4 address when DatabaseDescriptor has not been initialized. And DatabaseDescriptor has not been initialized, becase in BulkOutputFormat we're not reading cassandra.yaml.

bq. I actually have a workaround for this which involves not applying patch that removes need to read cassandra.yaml, then point to a cassandra.yaml generated specifically for the purpose on each hadoop node, with listen_address set to the IPv6 address of the node.

What's actually happening here is it's treating listen_address as being blank, which means we fall back to hostname resolution to determine the interface, so alternative you could modify your /etc/hosts and /etc/hostname to point to the ipv6 address.  However, I don't think we need to explicitly bind an outgoing address in OTCP either.  If we do, we can add some logic there to allow overriding it though.  Vijay, what do you think?;;;","02/Feb/12 18:21;vijay2win@yahoo.com;Hi Brandon, allowing override might be better in this case. Because if we change this, cassandra's stream might get confused in some cases... in the constructor of IncomingStreamReader, as we do socket.getRemoteSocketAddress() to see where the stream comes from. if we dont bind it to the right address there may be some edge cases where it might not work. ;;;","02/Feb/12 19:08;brandon.williams;Patch 0005 on CASSANDRA-3740 includes these changes, but I attached it there because it depends on it.;;;","02/Feb/12 21:37;brandon.williams;Attaching here too for review.;;;","02/Feb/12 23:06;vijay2win@yahoo.com;+1;;;","02/Feb/12 23:19;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair Streaming hangs between multiple regions,CASSANDRA-3838,12540877,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Feb/12 06:10,16/Apr/19 09:32,14/Jul/23 05:52,05/Feb/12 21:40,1.0.8,,,,,,0,,,,,,,"Streaming hangs between datacenters, though there might be multiple reasons for this, a simple fix will be to add the Socket timeout so the session can retry.

The following is the netstat of the affected node (the below output remains this way for a very long period).
[test_abrepairtest@test_abrepair--euwest1c-i-1adfb753 ~]$ nt netstats
Mode: NORMAL
Streaming to: /50.17.92.159
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1523325354/2475291786 - 61%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%



""Streaming:1"" daemon prio=10 tid=0x00002aaac2060800 nid=0x1676 runnable [0x000000006be85000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:297)
        at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:286)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:743)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:731)
        at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:59)
        - locked <0x00000006afea1bd8> (a com.sun.net.ssl.internal.ssl.AppOutputStream)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:117)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:152)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Streaming from: /46.51.141.51
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2241-Data.db sections=7231 progress=0/1548922508 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2231-Data.db sections=4730 progress=0/296474156 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2244-Data.db sections=7650 progress=0/1580417610 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2217-Data.db sections=7682 progress=0/196689250 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2220-Data.db sections=7149 progress=0/478695185 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2171-Data.db sections=443 progress=0/78417320 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2222-Data.db sections=4590 progress=0/1310718798 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2213-Data.db sections=7876 progress=0/3308781588 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2216-Data.db sections=7386 progress=0/2868167170 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2254-Data.db sections=4618 progress=0/215989758 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1542191546/2475291786 - 62%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2210-Data.db sections=6698 progress=0/2304563183 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2229-Data.db sections=7386 progress=0/1324787539 - 0%


""Thread-198896"" prio=10 tid=0x00002aaac0e00800 nid=0x4710 runnable [0x000000004251b000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
        at com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)
        at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:798)
        - locked <0x00000005e220a170> (a java.lang.Object)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
        at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
        - locked <0x00000005e220a1b8> (a com.sun.net.ssl.internal.ssl.AppInputStream)
        at com.ning.compress.lzf.LZFDecoder.readFully(LZFDecoder.java:392)
        at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:190)
        at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)
        at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:115)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:119)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:244)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:148)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:90)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:185)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3569,,,,,,"02/Feb/12 06:13;vijay2win@yahoo.com;0001-Add-streaming-socket-timeouts.patch;https://issues.apache.org/jira/secure/attachment/12512915/0001-Add-streaming-socket-timeouts.patch","05/Feb/12 18:30;vijay2win@yahoo.com;0001-CASSANDRA-3838-v2.patch;https://issues.apache.org/jira/secure/attachment/12513313/0001-CASSANDRA-3838-v2.patch","05/Feb/12 06:52;vijay2win@yahoo.com;0001-CASSANDRA-3838.patch;https://issues.apache.org/jira/secure/attachment/12513289/0001-CASSANDRA-3838.patch",,,,,,,,,,,,,,,3.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226231,,,Sun Feb 05 21:40:51 UTC 2012,,,,,,,,,,"0|i0gotr:",95452,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"04/Feb/12 17:23;slebresne;Is there any usefulness to set the SO_TIMEOUT on the socket that is writing?

I also wonder if we really should reuse the rpc timeout for this (and my initial intuition is that we probably shouldn't). As far as I'm concerned, I'm fine adding a new streaming_socket_timeout option for this (we don't even have to document it in the yaml if we consider it's an advanced thing).;;;","04/Feb/12 21:51;vijay2win@yahoo.com;Hi Sylvain,
My observation on this is that... when there is network congestion the Routers will start to drop the packets and which will cause the write on the socket to hang.... Until we write again to the socket we will not know if the socket is closed or not... hence it will be better to have it in both the sides... 

I will add streaming_socket_timeout and add documentation in the next patch... if you are ok with the above Thanks!;;;","05/Feb/12 00:57;scode;Note that simply adding a socket timeout is not a good idea unless both sides are truly expected to never starve (this is why I didn't suggest it for CASSANDRA-3569, and why TCP keep-alive is the ""correct"" solution because it does not generate spurious timeouts by lack of in-band data on the channel - but as noted in that ticket, the practical reality is that we don't control keep alive parameters on a per-socket basis).

For example if one of the ends is waiting for a few seconds for a particularly expensive fsync(), or waiting for some kind of lock, you'd have spurious failures (whereas this is not the case for keep-alive, because the transport is alive and kicking at the kernel level). Depending on surrounding logic, it could be dangerous if it causes the receiver to believe it received the file while the sender believes it doesn't (e.g. multiple streaming -> disk space explosion).

I would suggest TCP keep-alive for the reasons mentioned here and discussed in CASSANDRA-3569, and suggest that the TCP keep-alive settings be tweaked to fail quicker if that is desired.

If adding a socket timeout, thought needs to go into what kind of false failure cases will be created. If both ends are truly expected not to block on anything like compaction locks or whatever else there might be, it might be okay.

In either case, definitely *don't* use rpc timeout IMO; the concerns are completely different. A low-timeout cluster with an rpc timeout of 0.5 seconds for example would be extremely sensitive to even the slightest hiccup (such as waitnig 1 second for an fsync(), or a GC pause, etc) and it would truly be useless and extremely damaging to kill streams for that.

In general, as with CASSANDRA-3569, I strongly argue that streaming should not be caused to spuriously fail because the impact of that can be huge, particularly on clusters with large nodes.

As for reads vs. writes: You definitely want timeouts on both sides in order to guarantee that you never hang under any circumstance regardless of the nature of the TCP connection loss, unless you have some other method to accomplish the same thing.

If this (socket timeouts) does go in, I argue even more strongly than before that the tear-down of streams due to failure detector as in CASSANDRA-3569 is truly just negative rather than positive (but as noted in that ticket, not hanging forever on repairs and such remains a concern).
;;;","05/Feb/12 00:58;scode;Vijay, I do believe though that if you don't care about having to wait for a few hours for streams to abort, simply setting keep alive is the easiest and least-likely-to-have-negative-side-effects fix to your problem of inter-dc streams.
;;;","05/Feb/12 01:02;scode;Turns out we can do per-socket keep-alive on Linux if we're willing to be platform specific, see https://issues.apache.org/jira/browse/CASSANDRA-3569?focusedCommentId=13200613&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13200613;;;","05/Feb/12 01:24;scode;Let me be more clear about why keep-alive is better.

TCP keep-alive is at the transport level, and thus independent of in-band data (or lack thereof). Imagine that you're implementing a remote procedure call protocol where the client sends:

{code}
INVOKE name-of-process arg1 arg2
{code}

The server invokes the method, and responds:

{code}
RET success|failure exit-value|exception
{code}

The first thing you need if you are using this in some kind of production scenario, is to ensure that requests can time out. But there is a problem. Suppose you're making the assumption that this software is running on well-connected networks and a high number of requests per second; there is no reason to not quickly time out requests if the remote host is unreachable. So you set a socket timeout to 1 second. The only problem is that it will also time out on all requests that take longer than 1 second because the method call legitimately took longer.

The conflict happens because the selection of timeout was made based on the transport level circumstances (fast local network, high throughput, no need to wait if a host is down) while the effect of the timeout is at the in-band data level and is thus triggered by a slow request.

One way to fix this is to extend the protocol between client and server such that they can constantly be exchanging PING/PONG type messages (witness IRC for an example of this). This allows you to utilize socket (or read/write op) timeouts to detect a broken transport, under the assumption/premise that both sides have dedicated code for the ping/pong stuff which is independent of any delay in processing the otherwise in-band data.

Disadvantages of this approach can include the need to actually change the protocol, and (depending on implementation) additional implementation complexity as you suddenly need to actively model the transport as such.

TCP keep-alive is a way to let the kernel/tcp, which is already supposed to support this, deal with this without adding complexity to the application. It allows what effectively boils down to a ""timeout"" at the transport level which can be selected based on use-case and expected networking characteristics, and is independent of the nature of the in-band data sent over that transport.

In the Cassandra case, the equivalent of the slow RPC call might be that a write() during streaming blocks for 5 seconds because socket buffers on both ends are full, and the other end is going a GC or waiting on an fsync().

By using keep-alives we get more ""correct"" behavior in that such blocks won't cause connection tear-downs, while at the same time not having to change the protocol and/or add complexity to the code base to implement a protocol-within-tcp in which to mux the actual payload for streaming.
;;;","05/Feb/12 01:27;vijay2win@yahoo.com;>>>> In either case, definitely don't use rpc timeout IMO; the concerns are completely different. A low-timeout cluster with an rpc timeout of 0.5 seconds 
We will add a configuration  streaming_socket_timeout  which will be different than rpc_timeout...  

>>> If this (socket timeouts) does go in, I argue even more strongly than before that the tear-down of streams due to failure detector as in CASSANDRA-3569
I dont have any option on that ticket, but it looks reasonable. I would say so_timeout will be a better solution for streaming as it is not a long lived connections... but i also think Keep alive should be set for the Messaging connection as you mentioned in the other ticket.

>>> I do believe though that if you don't care about having to wait for a few hours for streams to abort
We definitely dont want to wait for hours.... And i dont think we have to wait for hours when we have a better option, even if we set streaming_socket_timeout to 30 Seconds or even a minute.

>>> As for reads vs. writes: You definitely want timeouts on both sides in order to guarantee that you never hang under any circumstance 
Agree, i will get the patch done in few min.;;;","05/Feb/12 16:32;slebresne;The default for this should be to not timeout at all, we should be conservative.

I'm good with keep alive on principle, but I think the actual hitch Vijay is trying to hitch is to not wait hours to retry the repair. So given that for keep alive that would involve potential non-portable code and such, let's keep that for later unless someone is willing to actually write that keep-alive patch in a timely fashion. ;;;","05/Feb/12 18:30;vijay2win@yahoo.com;Hi Sylvain the default is set to no timeout in the new patch. Thanks!;;;","05/Feb/12 20:03;scode;{quote}
So given that for keep alive that would involve potential non-portable code and such, let's keep that for later unless someone is willing to actually write that keep-alive patch in a timely fashion.
{quote}

I have nothing against that at all, to be clear. I didn't mean to sound like I was arguing against this going in. I am very much for it, +1. In fact I'd rather have this just be turned on by default (with a reasonably high default timeout) than having FD convictions kill streams. So no argument here.

Timeouts like these can be exchanged for keep-alive:s in the future without really affecting the surrounding logic, if that feature becomes available.;;;","05/Feb/12 21:40;slebresne;Commmitted, thanks.

I've slightly updated the option name to streaming_socket_timeout_in_ms (I added the _in_ms) to mirror the rpc_timeout_in_ms option and to avoid having people wonder which unit that was.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FB.broadcastAddress fixes and Soft reset on Ec2MultiRegionSnitch.reconnect,CASSANDRA-3835,12540857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,02/Feb/12 00:41,16/Apr/19 09:32,14/Jul/23 05:52,02/Feb/12 03:59,1.0.8,1.1.0,1.2.0 beta 1,,,,0,,,,,,,"looks like OutboundTcpConnectionPool.reset will clear the queue which might not be ideal for Ec2Multiregion snitch.
there is additional cleanup needed for FB.broadCastAddress.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/12 00:42;vijay2win@yahoo.com;0001-fix-fb-broadcastAddress.patch;https://issues.apache.org/jira/secure/attachment/12512881/0001-fix-fb-broadcastAddress.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226211,,,Thu Feb 02 04:08:49 UTC 2012,,,,,,,,,,"0|i0gosf:",95446,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"02/Feb/12 01:24;brandon.williams;+1, looks like most of the BCA cleanup came from merges from 0.8 fixes.;;;","02/Feb/12 03:59;vijay2win@yahoo.com;Fixed on 1.0.8, 1.1 and trunk, Thanks!;;;","02/Feb/12 04:08;brandon.williams;Looks like this broke something in the cli, dtests are failing the chunk length test.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip stage backed up due to migration manager future de-ref,CASSANDRA-3832,12540842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,scode,scode,scode,01/Feb/12 23:45,16/Apr/19 09:32,14/Jul/23 05:52,09/Feb/12 10:14,1.1.0,,,,,,0,,,,,,,"This is just bootstrapping a ~ 180 trunk cluster. After a while, a
node I was on was stuck with thinking all nodes are down, because
gossip stage was backed up, because it was spending a long time
(multiple seconds or more, I suppose RPC timeout maybe) doing the
following. Cluster-wide restart -> back to normal. I have not
investigated further.

{code}
""GossipStage:1"" daemon prio=10 tid=0x00007f9d5847a800 nid=0xa6fc waiting on condition [0x000000004345f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005029ad1c0> (a java.util.concurrent.FutureTask$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:364)
	at org.apache.cassandra.service.MigrationManager.rectifySchema(MigrationManager.java:132)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:75)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:802)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:918)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
{code}
",,lenn0x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3882,,,,,,,,"05/Feb/12 23:14;scode;CASSANDRA-3832-trunk-dontwaitonfuture.txt;https://issues.apache.org/jira/secure/attachment/12513377/CASSANDRA-3832-trunk-dontwaitonfuture.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226196,,,Thu Feb 09 10:37:50 UTC 2012,,,,,,,,,,"0|i0gor3:",95440,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"05/Feb/12 22:46;scode;This happens w/o bootstrap too. I just took the same ~ 180 node cluster on 0.8 and deployed trunk on it. All nodes restarted almost at the same time, but no one is in any state but Normal. It's sitting there not really coming up (probably is, but slowly) despite this just being a full cluster restart effectively.;;;","05/Feb/12 23:00;scode;Meanwhile, MigrationStage is stuck like this:

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007fb5b450e800 nid=0x3395 waiting on condition [0x0000000043479000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000005032ed688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:119)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{code}

The GossipStage submits the job on the migration state on the local node and waits for the result. The migration stage in turn sends a message and waits for the response synchronously.

The migration request runs on the migration stage on the remote node, which is presumably stuck with it's own task on the migration stage.

In effect, we are causing a distributed deadlock (or almost deadlock, I'm not sure - I suppose we might get unstuck eventually since things do time out after rpc timeout).
;;;","05/Feb/12 23:02;scode;Though in this case so far as I was looking at code, the cluster is not showing signs of recovering. Essentially, the cluster is in a ""fallen and can't get up"" state.;;;","05/Feb/12 23:12;jbellis;It looks like the intent of the wait is to avoid sending more than one request to the same target for the same version.  One possible fix would be to replace the wait with a Set of in-progress requests a la HHOM.;;;","05/Feb/12 23:14;scode;Attaching simple patch to just not wait on the future. Given that we have no special code path to handle timeouts anyway, this does not introduce any actual lack of failure handling beyond what is already there, so as far as I can tell it should not cause any failure to reach schema agreement that we would not already be vulnerable to.

Also upping priority since this bug causes clusters to refuse to start up even with full cluster re-starts by the operator.;;;","05/Feb/12 23:15;scode;(Sorry about the e-mail spam I just generated, there are race conditions in the JIRA UI and the 'submit patch' button exhcnages places with start/stop progress...);;;","05/Feb/12 23:17;scode;jbellis: Ok, that seems plausible (about the intent). I wonder if it is worth it though? What are the negative consequences of multiple requests beyond the CPU usage?;;;","05/Feb/12 23:18;scode;I will submit a patch that does it and you can decide.;;;","05/Feb/12 23:22;jbellis;committed;;;","05/Feb/12 23:27;scode;A concern here though is that in order to make this scale on very large clusters, you probably want to limit the amount of schema migration attempts that are in progress for a given schema version, and not just limit the amount of outstanding for a single node.

But doing that requires complicating the code so that we don't fail to migrate to a new schema just because one node happened to go down just after we were notified of 500 nodes being e.g. alive and having a schema we don't recognize.

For now, I will proceed with avoiding duplicate (endpoint, schema) pairs rather than global throttling.
;;;","05/Feb/12 23:28;scode;My last comment was made before reading ""committed"", sorry.;;;","05/Feb/12 23:28;scode;If you still want the augmented patch, let me know.;;;","05/Feb/12 23:58;jbellis;I'm happy to wait and see if it's actually a problem, before fixing it. :);;;","06/Feb/12 03:35;scode;Something is still fishy. A node that I did a decommission+bootstrap on is not finishing bootstrap and perpetually claiming to wait for schema, with migration stage backed up like below. Investigating.

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007f63ac0bb000 nid=0xc8a waiting on condition [0x000000004355b000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000508776fa8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:122)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}
;;;","06/Feb/12 03:37;scode;And the test is exactly to see whether there is someone active in migration stage:

{code}
    public static boolean isReadyForBootstrap()
    {
        return StageManager.getStage(Stage.MIGRATION).getActiveCount() == 0;
    }
{code}

;;;","06/Feb/12 03:38;scode;It eventually got unstuck, but it took quite some time. I'm not sure why it was having to wait for those responses.;;;","06/Feb/12 05:23;scode;I added logging to see which node it's waiting on a response from, and quickly logged into that node to catch it red handed - it was sitting in the exact same place in the migration manager on the migration stage:

{code}
""MigrationStage:1"" daemon prio=10 tid=0x00007f18ec4dc800 nid=0x1d64 waiting on condition [0x0000000043391000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000050157fdd0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2116)
	at org.apache.cassandra.net.AsyncResult.get(AsyncResult.java:61)
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:124)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

I guess we're triggering distributed deadlock ""internally"" within the migration stage even though we fixed it so that the gossip stage wouldn't be backed up. If my understanding is correct, this is because all nodes, when a node is marked alive, just know that it has a different schema - not who has the ""newer"" schema. So when a node joins it gets migration messages from others while it also tries to send migration messages to others and waiting on the response. Whenever it sends a migration message to someone whose migration stage is busy waiting on a response from the node in question - deadlock (until timeout).
;;;","06/Feb/12 05:32;scode;What I'd like in this case is to send a request and register a callback with the response that just puts a task on the migration stage that does the schema merge. That, and keep track of outstanding requests and alter the logic for ""is ready to bootstrap"".

I'm still fuzzy on the overall design of messaging; is it me or is there no way to send a request and wait for a response without hogging a thread for it? I want an on-delivery callback, not be waiting for a synchronous get(). A hacky work-around would be to just keep track of all outstanding and get() them in order - it would work, but would be a bit unclean to have one slow request block others from being de-ref:ed.
;;;","09/Feb/12 10:14;slebresne;So if I read this correctly, the initial did fix the initial issue of bootstrap being stuck. I understand it's still not perfect but for the sake of keeping track of changes (in particular what went into 1.1.0), I'm closing this one. Let's fix remaining problems in a separate ticket.;;;","09/Feb/12 10:37;scode;The remaining issue is still causing problems for bootstrap, though not quite as sever as the original problem. Follow-up work filed in CASSANDRA-3882.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scaling to large clusters in GossipStage impossible due to calculatePendingRanges ,CASSANDRA-3831,12540841,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,01/Feb/12 23:44,16/Apr/19 09:32,14/Jul/23 05:52,09/Feb/12 10:31,1.1.0,,,,,,0,,,,,,,"(most observations below are from 0.8, but I just now tested on
trunk and I can trigger this problem *just* by bootstrapping a ~180
nod cluster concurrently, presumably due to the number of nodes that
are simultaneously in bootstrap state)

It turns out that:

* (1) calculatePendingRanges is not just expensive, it's computationally complex - cubic or worse
* (2) it gets called *NOT* just once per node being bootstrapped/leaving etc, but is called repeatedly *while* nodes are in these states

As a result, clusters start exploding when you start reading 100-300
nodes. The GossipStage will get backed up because a single
calculdatePenginRanges takes seconds, and depending on what the
average heartbeat interval is in relation to this, this can lead to
*massive* cluster-wide flapping.

This all started because we hit this in production; several nodes
would start flapping several other nodes as down, with many nodes
seeing the entire cluster, or a large portion of it, as down. Logging
in to some of these nodes you would see that they would be constantly
flapping up/down for minutes at a time until one became lucky and it
stabilized.

In the end we had to perform an emergency full-cluster restart with
gossip patched to force-forget certain nodes in bootstrapping state.

I can't go into all details here from the post-mortem (just the
write-up would take a day), but in short:

* We graphed the number of hosts in the cluster that had more than 5
  Down (in a cluster that should have 0 down) on a minutely timeline.
* We also graphed the number of hosts in the cluster that had GossipStage backed up.
* The two graphs correlated *extremely* well
* jstack sampling showed it being CPU bound doing mostly sorting under calculatePendingRanges
* We were never able to exactly reproduce it with normal RING_DELAY and gossip intervals, even on a 184 node cluster (the production cluster is around 180).
* Dropping RING_DELAY and in particular dropping gossip interval to 10 ms instead of 1000 ms, we were able to observe all of the behavior we saw in production.

So our steps to reproduce are:

* Launch 184 node cluster w/ gossip interval at 10ms and RING_DELAY at 1 second.
* Do something like: {{while [ 1 ] ; do date ; echo decom ; nodetool decommission ; date ; echo done leaving decommed for a while ; sleep 3 ; date ; echo done restarting; sudo rm -rf /data/disk1/commitlog/* ; sudo rm -rf /data/diskarray/tables/* ; sudo monit restart cassandra ;date ; echo restarted waiting for a while ; sleep 40; done}} (or just do a manual decom/bootstrap once, it triggers every time)
* Watch all nodes flap massively and not recover at all, or maybe after a *long* time.

I observed the flapping using a python script that every 5 second
(randomly spread out) asked for unreachable nodes from *all* nodes in
the cluster, and printed any nodes and their counts when they had
unreachables > 5. The cluster can be observed instantly going into
massive flapping when leaving/bootstrap is initiated. Script needs
Cassandra running with Jolokia enabled for http/json access to
JMX. Can provide scrit if needed after cleanup.

The phi conviction, based on logging I added, was legitimate. Using
the 10 ms interval the average heartbeat interval ends up being like 25
ms or something like that. As a result, a single ~ 2 second delay in
gossip stage is huge in comparison to those 25 ms, and so we go past
the phi conviction threshold. This is much more sensitive than in
production, but it's the *same* effect, even if it triggers less
easily for real.

The best work around currently internally is to memoize
calculatePendingRanges so that we don't re-calculate if token meta
data, list of moving, list of bootstrapping and list of leaving are
all the same as on prior calculation. It's not entirely clear at this
point whether there is a clean fix to avoid executing
calculatePendingRanges more than once per unique node in this state.

It should be noted though that even if that is fixed, it is not
acceptable to spend several seconds doing these calculations on a ~
200 node cluster and it needs to be made fundamentally more efficient.

Here is a dump of thoughts by me in an internal JIRA ticket (not
exhaustive, I just went as far as to show that there is an issue;
there might be worse things I missed, but worse than cubic is bad
enough that I stopped):

(Comment uses 0.8 source.)

{quote}
Okay, so let's break down the computational complexity here.

Suppose ring size is {{n}} and number of bootstrapping/leaving tokens is {{m}}.  One of two places that take time (by measurement) is this part of calculatePendingRanges():

{code}
       // At this stage pendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add and remove them one by one to
        // allLeftMetadata and check in between what their ranges would be.
        for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
        {
            InetAddress endpoint = entry.getValue();

            allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
            for (Range range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
                pendingRanges.put(range, endpoint);
            allLeftMetadata.removeEndpoint(endpoint);
        }
{code}

I'll ignore stuff that's log(n) or better.

The outer loops is {{O(m)}}. The inner loop is {{O(n)}}, making aggregate so far {{O(nm)}}.

We have a call in there to updateNormalTokens() which implies a sorting, which his {{O(n log(n))}}. So now we're at {{O(n log(n) m)}}.

Next up we call {{getAddressRanges()}} which immediately does another {{O(n log(n)}} sort. we're still at {{O(n log(n) m}}. It then iterates (linear) and:

* calls {{getPrimaryRangeFor()}} for each.
* calls {{calculateNaturalEndpoints}} for each.

The former ends up sorting again, so now we're at {{O(n log(n) n log(n) m}} (worse than quadratic).

{{NTS.calculateNaturalEndpoints}} starts by collecting token meta data for nodes in the DC, by using {{updateNormalToken}}, which *implies sorting*. Woha woha. Now we're at {{O(n log(n) n log (n) n log(n) m)}}.

I might have missed things that are even worse, but this is bad enough to warrant this ticket. To put into perspective, 168 ^ 3 is 4.7 million.
{quote}
",,kohlisankalp,lenn0x,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3881,,CASSANDRA-3856,,,,,,"04/Feb/12 01:24;scode;CASSANDRA-3831-memoization-not-for-inclusion.txt;https://issues.apache.org/jira/secure/attachment/12513221/CASSANDRA-3831-memoization-not-for-inclusion.txt","06/Feb/12 00:36;scode;CASSANDRA-3831-trunk-group-add-dc-tokens.txt;https://issues.apache.org/jira/secure/attachment/12513381/CASSANDRA-3831-trunk-group-add-dc-tokens.txt",,,,,,,,,,,,,,,,2.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226195,,,Thu Feb 09 10:31:55 UTC 2012,,,,,,,,,,"0|i0goqf:",95437,,,,,Normal,,,,,,,,,,,,,,,,,"02/Feb/12 02:08;scode;Correction: All above applies, except on 'trunk': I have not confirmed that calculatePendingTokens() indeed gets called repeatedly (not just once per node that starts a bootstrap/leave/etc) on trunk. I have only confirmed it being CPU spinning there, and gossip stage being backed up as a result.

On 0.8, it is specifically confirmed that it does get called repeatedly both in my test case and when we saw this happen in production.;;;","04/Feb/12 01:24;scode;I am attaching {{CASSANDRA\-3831\-memoization\-not\-for\-inclusion.txt}} as an ""FYI"" and in case it helps others. It's against 0.8, and implements memoization of calculate pending ranges.

The correct/clean fix is probably to change behavior so that it doesn't get called unnecessarily to begin with (and to make sure the computational complexity is reasonable when it does get called). This patch was made specifically to address the production issue we are having in a minimally dangerous fashion, and is not to be taken as a suggested fix.;;;","04/Feb/12 15:12;jbellis;calculatePendingRanges is only supposed to be called when the ring changes.  So I'd say the right fix would be to eliminate whatever is breaking that design, rather than adding a memoization bandaid.

(I eyeballed 1.1 and didn't see anything obvious, so either it's subtle or it got fixed post-0.8.)

I don't suppose your CPU spinning test got any more of a call tree to go on?;;;","04/Feb/12 19:56;scode;I agree (I did say that myself already ;)). The memoization (+ being ready to change the cluster-wide phi convict threshold through JMX) was just the safest way to fix the situation on our production cluster so that we could continue to add capacity. It was never intended as a suggested fix. But I still wanted to upload it instead of keeping the patch private, in case someone's helped by it.

But the larger issue is that calculatePendingRanges must be faster to begin with. Even if only called once, if it takes 1-4 seconds on a ~ 180 node cluster and it's worse than {{O(n^3)}} it's *way* too slow and won't scale. First due to the failure detector, and of course at some point it's just too slow to even wait for the calculation to complete at all (from a RING_DELAY standpoint for example).

I'll see later this weekend about doing more tests on trunk confirm/deny whether it is getting called multiple times. As I indicated I never confirmed that particular bit on trunk and it's very possible it doesn't happen there.

I haven't had time to seriously look at suggesting changes to fix the computational complexity. Might be very easy for all I know; I just haven't looked at it yet.
;;;","05/Feb/12 23:45;scode;With respect to trunk and whether it gets called repeatedly, I gave that a try now. I picked a random node to tail (and running a version that logs calculate pending ranges and its time), and did a decommission of another node:

{code}
 INFO [GossipStage:1] 2012-02-05 23:33:32,675 StorageService.java (line 1275) calculate pending ranges called, took 885 ms
 INFO [OptionalTasks:1] 2012-02-05 23:34:00,828 HintedHandOffManager.java (line 180) Deleting any stored hints for /XXX.XXX.XXX.XXX
 INFO [GossipStage:1] 2012-02-05 23:34:00,828 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1217) Removing token 23117008622346363007022731483136427400 for /XXX.XXX.XXX.XXX
{code}

At least two of those are expected - once when it goes into leaving, and once when it drops out of the cluster. Not sure at this point why we see a third call.

For bootstrapping the guy back into the cluster I see:

{code}
 INFO [GossipStage:1] 2012-02-05 23:38:15,832 StorageService.java (line 1275) calculate pending ranges called, took 1413 ms
 INFO [GossipStage:1] 2012-02-05 23:38:45,229 ColumnFamilyStore.java (line 590) Enqueuing flush of Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,229 Memtable.java (line 252) Writing Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,236 Memtable.java (line 293) Completed flushing /data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db (89 bytes)
 INFO [GossipStage:1] 2012-02-05 23:38:45,236 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,237 CompactionTask.java (line 115) Compacting [SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-19-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-18-Data.db'), SSTableReader(path='/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-17-Data.db')]
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,248 CompactionTask.java (line 226) Compacted to [/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-21-Data.db,].  7,388 to 7,047 (~95% of original) bytes for 4 keys at 0.610958MB/s.  Time: 11ms.
{code}

So it got called twice - which is expected. Once when it entered in joining state, and once when it flipped into Normal.

Relatedly:

On the other hand, on the node that is *bootstrapping*, I (and this is expected) have calculate pending ranges calls lots of times - presumably (not confirmed) at least once for every node in the cluster, as the gossiper emits events to inform it of each. If there is a single node bootstrapping this is kind of okay because the lack of ""another"" guy bootstrapping means the calculations are quick. But if other nodes are bootstrapping too (highly likely if you're doing capacity adds on large clusters) that would be expected to take a long time to process. This could throw off the node bootstrapping which tries to wait for RING_DELAY on start-up, but is spending a lot of that time doing these calculations rather than staying up-to-date with ring information (for the record though I have not specifically timed/tested this particular case).
;;;","06/Feb/12 00:36;scode;Attaching {{CASSANDRA\-3831\-trunk\-group\-add\-dc\-tokens.txt}} which adds a ""group-update"" interface to TokenMetadata that allows NTS to use it when constructing it's local per-dc meta datas.

This is not even close to a complete fix for this issue, but I do think it is a ""clean"" change because it makes sense in terms of TokenMetadata API to provide a group-update method given the expense involved. And given it's existence, it makes sense for NTS to use it.

This change mitigates the problem significantly on the ~ 180 node test cluster since it takes a way an {{n}} from the complexity, and should significantly raise the bar of how many nodes in a cluster is realistic without other changes.

I think this might be a fix worthwhile committing because it feels safe and is maybe a candidate for the 1.1 release, assuming review doesn't yield anything obvious. But, leaving the JIRA open for a more overarching fix (I'm not sure what that is at the moment; I'm mulling it over).

;;;","06/Feb/12 00:56;scode;I filed CASSANDRA-3856 which might related to a proper fix to this.
;;;","07/Feb/12 19:08;jbellis;committed the group add patch with a minor tweak to move the isempty check to the top so we can skip lock/unlock too, and a more important one from {{sortedTokens = sortedTokens()}} (which is a no-op) to {{sortedTokens = sortTokens()}}.;;;","08/Feb/12 03:29;scode;Wow, that's embarrassing. Thanks for catching that!;;;","09/Feb/12 10:11;slebresne;Since code has been committed so can we close this one and open a separate ticket for the remaining work for the sake of keeping track of what went into 1.1.0 and was doesn't?;;;","09/Feb/12 10:31;scode;CASSANDRA-3881 filed for further work.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkOutputFormat shouldn't need flags to specify the output type,CASSANDRA-3828,12540780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Feb/12 21:13,16/Apr/19 09:32,14/Jul/23 05:52,03/Feb/12 00:09,1.1.0,,,,,,0,,,,,,,"BOF currently requires the IS_SUPER boolean to be set to determine if the output CF is going to be a super or not, and would similarly use a flag to indicate counters (if there was support for that yet.)  Instead, it should be able to introspect the mutations to determine what kind of columns to write.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/12 23:45;brandon.williams;3828-v2.txt;https://issues.apache.org/jira/secure/attachment/12513068/3828-v2.txt","02/Feb/12 22:28;brandon.williams;3828.txt;https://issues.apache.org/jira/secure/attachment/12513049/3828.txt",,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226175,,,Fri Feb 03 00:09:30 UTC 2012,,,,,,,,,,"0|i0gopb:",95432,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Feb/12 22:28;brandon.williams;Patch to determine the CF type once by the very first mutation.;;;","02/Feb/12 22:56;jbellis;It looks like it would be cleaner to split it into container type (super/normal) and data type (counter/normal).;;;","02/Feb/12 23:45;brandon.williams;v2 splits by super/normal and counter/normal.;;;","03/Feb/12 00:05;jbellis;+1;;;","03/Feb/12 00:09;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nosetests / system tests fail,CASSANDRA-3827,12540778,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,mallen,mallen,01/Feb/12 21:06,16/Apr/19 09:32,14/Jul/23 05:52,07/Feb/12 15:34,1.1.0,,,Legacy/Testing,,,0,,,,,,,"CQL Driver version used: 1.0.8.

{code}
EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_bad_batch_calls
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/nose/case.py"", line 381, in setUp
    try_run(self.inst, ('setup', 'setUp'))
  File ""/usr/local/lib/python2.7/site-packages/nose/util.py"", line 478, in try_run
    return func()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 113, in setUp
    self.define_schema()
  File ""/var/lib/jenkins/jobs/Cassandra/workspace/test/system/__init__.py"", line 158, in define_schema
    Cassandra.CfDef('Keyspace1', 'Super1', column_type='Super', subcomparator_type='LongType', row_cache_size=1000, key_cache_size=0),
TypeError: __init__() got an unexpected keyword argument 'key_cache_size'
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/12 23:03;xedin;CASSANDRA-3827.patch;https://issues.apache.org/jira/secure/attachment/12512864/CASSANDRA-3827.patch",,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226173,,,Tue Feb 07 15:34:01 UTC 2012,,,,,,,,,,"0|i0goov:",95430,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"01/Feb/12 21:19;xedin;you can just remove key_cache_size and row_cache_size from ThriftTester.define_schema(self), we don't support them anymore.;;;","01/Feb/12 23:03;xedin;removed {row,key}_cache_size as deprecated from ThriftTester.define_schema(self) and fixed removed FilterClause to KeyRange for index tests. Now all tests pass for me:

{noformat}
PYTHONPATH=test nosetests --tests=system.test_thrift_server
............................................................................................
----------------------------------------------------------------------
Ran 92 tests in 626.972s

OK
{noformat};;;","02/Feb/12 00:38;mallen;It still fails for me with the patch applied, though I'm getting a different error now:

{code}
PYTHONPATH=/Users/mallen/cql-1.0.8:/Users/mallen/git-repos/cassandra/interface/thrift/gen-py/cassandra nosetests -x test/system
.E
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_bad_calls
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 710, in test_bad_calls
    _expect_exception(lambda: get_range_slice(client, ColumnParent('S'), SlicePredicate(column_names=['', '']), '', '', 5, ConsistencyLevel.ONE), InvalidRequestException)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 207, in _expect_exception
    r = fn()
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 710, in <lambda>
    _expect_exception(lambda: get_range_slice(client, ColumnParent('S'), SlicePredicate(column_names=['', '']), '', '', 5, ConsistencyLevel.ONE), InvalidRequestException)
  File ""/Users/mallen/dstax/repos/git/cassandra/test/system/test_thrift_server.py"", line 217, in get_range_slice
    kr = KeyRange(start, end, count=count, row_filter=row_filter)
TypeError: __init__() got an unexpected keyword argument 'row_filter'

----------------------------------------------------------------------
Ran 2 tests in 7.296s

FAILED (errors=1)
{code}
;;;","02/Feb/12 00:43;xedin;I forgot to mention that I re-generated Python Thrift bindings using `ant gen-thrift-py` before running tests. This is probably what you need because as I can see KeyRange does have a `row_filter` parameter.;;;","02/Feb/12 01:29;mallen;Oh right. Thanks, Pavel.  All tests are passing for me now too.;;;","02/Feb/12 01:31;xedin;Great! Let's wait up what Jonathan has to say about code.;;;","07/Feb/12 15:26;brandon.williams;LGTM, +1;;;","07/Feb/12 15:34;xedin;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig cannot use output formats other than CFOF,CASSANDRA-3826,12540773,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,01/Feb/12 20:18,16/Apr/19 09:32,14/Jul/23 05:52,02/Feb/12 19:14,1.1.0,,,,,,0,,,,,,,Pig has ColumnFamilyOutputFormat hard coded.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/12 18:27;brandon.williams;3826.txt;https://issues.apache.org/jira/secure/attachment/12513013/3826.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226168,,,Thu Feb 02 19:14:58 UTC 2012,,,,,,,,,,"0|i0goof:",95428,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"01/Feb/12 21:09;brandon.williams;Patch to keep CFIF/OF as defaults, but allow overriding both with environment variables.;;;","01/Feb/12 21:44;xedin;I guess we better do `if (format.contains("".""))` at the time when {input,output}format is set instead of getter methods? I also can suggest to make ""org.apache.cassandra.hadoop.ColumnFamilyInputFormat"" and ""org.apache.cassandra.hadoop.ColumnFamilyOutputFormat"" as DEFAULT_{INPUT, OUTPUT}_FORMAT and just set them to {input,output}format variables when user didn't give any by System.env(...), what do you think?;;;","02/Feb/12 18:27;brandon.williams;Updated patch.  I wanted to do some of this in ConfigHelper, but it doesn't really make sense because we can't cleanly return instance like getOutputPartitioner does since we have both the old and new hadoop interfaces to comply with, but also because Configuration is not how hadoop determines the input/output formats, those are set on the Job directly.  So it's probably best to keep this pig-specific, since other M/R jobs can already control these classes that way.;;;","02/Feb/12 18:54;xedin;+1 with following nit:

{code}
private String getFullyQualifiedClassName(String classname)
{
    String fqcn = classname.contains(""."") ? classname : ""org.apache.cassandra.hadoop."" + classname;
    return fqcn;
}
{code}

can be changed to 

{code}
private String getFullyQualifiedClassName(String classname)
{
    return classname.contains(""."") ? classname : ""org.apache.cassandra.hadoop."" + classname;
}
{code};;;","02/Feb/12 19:14;brandon.williams;Committed w/ternary change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] add missing break in nodecmd's command dispatching for SETSTREAMTHROUGHPUT,CASSANDRA-3824,12540681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,01/Feb/12 06:05,16/Apr/19 09:32,14/Jul/23 05:52,01/Feb/12 07:47,1.1.0,,,Legacy/Tools,,,0,,,,,,,code falls thru SETSTREAMTHROUGHPUT into REBUILD case.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/12 06:06;dbrosius@apache.org;add_missing_break.diff;https://issues.apache.org/jira/secure/attachment/12512721/add_missing_break.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226076,,,Wed Feb 01 07:47:31 UTC 2012,,,,,,,,,,"0|i0gonj:",95424,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"01/Feb/12 07:47;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcDate.getString(ByteBuffer) appears to not be multithreaded safe,CASSANDRA-3822,12540678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,01/Feb/12 05:38,16/Apr/19 09:32,14/Jul/23 05:52,01/Feb/12 23:05,,,,,,,0,,,,,,,"JdbcDate.getString(ByteBuffer) makes use of a static SimpleDateFormat

static final SimpleDateFormat FORMATTER = new SimpleDateFormat(DEFAULT_FORMAT);

SimpleDateFormat is not thread safe, as it uses a field from parent class DateFormat

    protected Calendar calendar;

to convert dates to calendars.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/12 21:57;dbrosius@apache.org;use_thread_local_sdfs.diff;https://issues.apache.org/jira/secure/attachment/12512813/use_thread_local_sdfs.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226073,,,Wed Feb 01 23:05:41 UTC 2012,,,,,,,,,,"0|i0gomv:",95421,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"01/Feb/12 18:36;jbellis;I poked at this a bit and decided that replacing SDF w/ Calendar isn't quite as simple as a text substitution. :);;;","01/Feb/12 21:57;dbrosius@apache.org;use thread local for holding SimpleDateFormat;;;","01/Feb/12 23:05;urandom;committed w/ minor changes (style and convention); thanks Dave!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters in super columns don't preserve correct values after cluster restart,CASSANDRA-3821,12540677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,tpatterson,tpatterson,01/Feb/12 05:37,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 08:42,1.1.0,,,,,,0,,,,,,,"Set up a 3-node cluster with rf=3. Create a counter super column family and increment a bunch of subcolumns 100 times each, with cf=QUORUM. Then wait a few second, restart the cluster, and read the values back. They almost all come back different (and higher) then they are supposed to be.

Here are some extra things I've noticed:
 - Reading back the values before the restart always produces correct results.
 - Doing a nodetool flush before killing the cluster greatly improves the results, though sometimes a value will still be incorrect. You might have to run the test several times to see an incorrect value after a flush.
 - This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

A dtest has been added to demonstrate this issue. It is called ""super_counter_test.py"".","ubuntu, 'trunk' branch, used ccm to create a 3 node cluster with rf=3. A dtest was created to demonstrate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/12 19:09;slebresne;3821.txt;https://issues.apache.org/jira/secure/attachment/12515467/3821.txt",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226072,,,Wed Feb 22 08:42:34 UTC 2012,,,,,,,,,,"0|i0gomf:",95419,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"06/Feb/12 23:23;yukim;Here is my initial look at the issue (might be wrong):

Concurrent counter mutation replay from commitlog and AtomicSortedColumns inside Memtable seem to be the cause of over count.
There is a race condition when adding column to memtable, and when it happens AtomicSortedColumns calls {{{IColumn#reconcile}}} multiple times until column is stored. It causes over count since counter column's {{reconcile}} is not idempotent operation.;;;","07/Feb/12 08:36;slebresne;Damn you super columns, there is nothing super about you!

Yuki is right that super columns are a problem for AtomicSortedColumns. Columns are immutable, so for them the multiple reconcile of ASC is not a problem since it's done on a cloned underlying map. But SuperColumns are not immutable. So even though the ASC (potential) mutliple attempts are made on a cloned map, the super columns structure themselves are not cloned, and so all these attempts modify the original SC. This does mean that SC are not isolated (I did knew about that, but somehow forget about it and certainly didn't realize the consequence on counters).

Anyway, I don't see an easy one line fix, so I would suggest to rather add back the ThreadSafeSortedColumns backing map and to use that for super column family, since super column family are not really isolated anyway. Then I guess, I'll make CASSANDRA-3237 a higher priority on my todo list.

bq. This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

The explanation above doesn't explain that. It would be worth investigating that separately (maybe a separate ticket).;;;","21/Feb/12 19:09;slebresne;Attached patch that as said in previous comment reintroduce ConcurrentSkipListMap backed CF and use them for super column family. It fixes the distributed test.;;;","21/Feb/12 22:31;yukim;I also ran dtest and confirmed the issue was fixed. so +1.;;;","22/Feb/12 08:42;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Columns missing after upgrade from 0.8.5 to 1.0.7.,CASSANDRA-3820,12540649,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,alienth,alienth,31/Jan/12 23:25,16/Apr/19 09:32,14/Jul/23 05:52,25/Jan/13 23:16,,,,,,,0,,,,,,,"After an upgrade, one of our CFs had a lot of rows with missing columns. I've been able to reproduce in test conditions. Working on getting the tables to DataStax(data is private).


0.8 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=date, value=313332333932323930392e3531, timestamp=1323922909506508)
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
=> (column=REDACTED, value=30, timestamp=1323922909506432)
=> (column=thing1_id, value=REDACTED, timestamp=1323922909506475)
=> (column=thing2_id, value=REDACTED, timestamp=1323922909506486)
=> (column=REDACTED, value=31, timestamp=1323922909506518)
=> (column=REDACTED, value=30, timestamp=1323922909506497)
{code}


1.0 results:

{code}
[default@reddit] get CommentVote[36353467625f63333837336f32];
=> (column=ip, value=REDACTED, timestamp=1327048432717348, ttl=2592000)
=> (column=name, value=31, timestamp=1327048433000740)
{code}



A few notes:

* The rows with missing data were fully restored after scrubbing the sstables.
* The row which I reproduced on happened to be split across multiple sstables.
* When I copied the first sstable I found the row on, I was able to 'list' rows from the sstable, but any and all 'get' calls failed.
* These SStables were natively created on 0.8.5; they did not come from any previous upgrade.",,christianmovi,edevil,jjordan,marcuse,psanford,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,226044,,,Fri Jan 25 23:16:35 UTC 2013,,,,,,,,,,"0|i0a4yv:",57078,,,,,Normal,,,,,,,,,,,,,,,,,"01/Feb/12 01:53;scode;Check whether the .bf files contain all zeroes above roughly 235 mb or so. If you have lots of rows, your BF will be that large.

We encountered a bug internally whereby all bloom filters larger than 2^31 bits were large on disk, but everything after the first 2^31 bits were all zeroes.

Unfortunately I don't know whether this is specific to patches made to our branch, and I have been so busy I haven't been able to follow up to figure out whether it affects the upstream version.

But - just ""tail -c 1000 | hexdump"". If you only have zeroes, this is the bug. Make sure to tail on a large .bf file (take the largest, easiest).

;;;","01/Feb/12 01:55;scode;And to be clear, I also discovered this when loading sstables from our 0.8 onto 1.0, and certain counter shards not being read from some sstables for a given row key as a result. As you indicate, I also were able to fix it by scrubbing on 1.0.

Presumably (unverified) the meta data is treated correctly on 1.0 but in the 0.8 version there is still some kind of 2^31 overflow, such that 0.8 can successfully reads its own (but will suffer elevated bloom filter false positives) eve though it's buggy (because it's buggy consistently), while 1.0 reads them correctly - but then suffers from the buggy behavior. This paragraph is speculation, and *not* confirmed.;;;","01/Feb/12 01:56;scode;To be clear, if this is indeed the bug: You are only potentially vulnerable if you have > 143 million row keys. If you have > 143, you still may or may not be vulnerable depending on whether you have individual sstables with > 143 row keys. Anyone < 143 million row keys should be fine (again, *if* it's the same bug).
;;;","01/Feb/12 02:45;brandon.williams;ISTM a bloom filter problem would suppress the entire row, not a portion of columns.  But since this is spread across multiple sstables, I guess that could make sense and is worth checking.;;;","01/Feb/12 04:36;alienth;Peter: You're right! All zeroes on the end of my largest bloomfilter file.

The affected CF has billions of keys, and the sstable in question is 35GB, so I'm guessing it is likely over 143 million.;;;","01/Feb/12 07:26;scode;@Brandon - Yes, the implicit assumption was that the row was spread over multiple sstables.

@Jason - Great, so to speak. Now all that's needed is a fix ;) Good to know what the problem is.
;;;","05/Mar/12 16:13;jjordan;Will running scrub fix this problem?  Will data be lost in the scrub?  Does it have to be scrub or will upgradesstables fix the issue as well?;;;","05/Mar/12 16:34;jbellis;Yes, scrub and upgradesstables both fix the problem w/o data loss.;;;","25/Jan/13 23:16;jbellis;Closing as Fixed, but it's more of a ""workaround exists and is understood."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disabling m-a-t for fun and profit (and other ant stuff),CASSANDRA-3818,12540484,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,urandom,urandom,31/Jan/12 02:32,16/Apr/19 09:32,14/Jul/23 05:52,01/Jan/13 19:00,2.0 beta 1,,,Packaging,,,0,build,,,,,,"It should be possible to disable maven-ant-tasks for environments with more rigid dependency control, or where network access isn't available.

Patches to follow.",,dbrosius,dbrosius@apache.org,richardlow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/12 18:31;jbellis;3818-v2.txt;https://issues.apache.org/jira/secure/attachment/12562686/3818-v2.txt","31/Dec/12 06:27;dbrosius@apache.org;3818-v3.txt;https://issues.apache.org/jira/secure/attachment/12562761/3818-v3.txt","21/Nov/12 20:48;dbrosius;3818.txt;https://issues.apache.org/jira/secure/attachment/12554565/3818.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3818-keep-init-in-init-target.txt;https://issues.apache.org/jira/secure/attachment/12512516/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3818-keep-init-in-init-target.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-clean-up-avro-generation-dependencies-and-dependants.txt;https://issues.apache.org/jira/secure/attachment/12512517/ASF.LICENSE.NOT.GRANTED--v1-0002-clean-up-avro-generation-dependencies-and-dependants.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0003-remove-useless-build-subprojects-target.txt;https://issues.apache.org/jira/secure/attachment/12512518/ASF.LICENSE.NOT.GRANTED--v1-0003-remove-useless-build-subprojects-target.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0004-group-test-targets-under-test-all.txt;https://issues.apache.org/jira/secure/attachment/12512519/ASF.LICENSE.NOT.GRANTED--v1-0004-group-test-targets-under-test-all.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0005-add-property-to-disable-maven-junk.txt;https://issues.apache.org/jira/secure/attachment/12512520/ASF.LICENSE.NOT.GRANTED--v1-0005-add-property-to-disable-maven-junk.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0006-add-property-to-disable-rat-license-header-writing.txt;https://issues.apache.org/jira/secure/attachment/12512521/ASF.LICENSE.NOT.GRANTED--v1-0006-add-property-to-disable-rat-license-header-writing.txt","31/Jan/12 02:34;urandom;ASF.LICENSE.NOT.GRANTED--v1-0007-don-t-needlessly-regenerate-thrift-code.txt;https://issues.apache.org/jira/secure/attachment/12512522/ASF.LICENSE.NOT.GRANTED--v1-0007-don-t-needlessly-regenerate-thrift-code.txt",,,,,,,,10.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225897,,,Tue Jan 01 19:00:13 UTC 2013,,,,,,,,,,"0|i07hsn:",41650,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"31/Jan/12 02:43;urandom;Changes attached as patches, and pushed to Github as: https://github.com/eevans/cassandra/tree/3818

0001 / 8ad3ad
0002 / 9e26a1
0003 / 294bd1

The first 3 changesets are basically cleanups.

0004 / fc8d65

Changeset #4 creates a new target called _test-all_, which runs test, test-compression, long-test, and test-clientutil.jar per the discussion on dev@

0005 / d110ae

Adds a property (-Dwithout.maven) that disables maven-base dependency resolution, (at least for the build and test targets).

0006 / e093d8

Adds a property (-Dwithout.rat) that disables the automatic prepending of license headers to the Thrift generated classes.

0007 / 1037a9

Adds a check that makes gen-thrift-java a noop if {{interface/cassandra.thrift}} has not been updated (useful in environments where the Thrift code is regenerated as part of the build).;;;","04/Apr/12 22:05;stephenc;I will review in the next couple of days as soon as I get the bandwidth;;;","15/Nov/12 00:21;jbellis;Are you still planning to review, [~stephenc]?;;;","21/Nov/12 11:07;jbellis;Dave, what can you make of this?;;;","21/Nov/12 20:48;dbrosius;rebased the patch for trunk (1.3) as one file->3818.txt

I inserted bogus proxy server, cassandra still builds fine (and faster).

perhaps new targets should get description attributes so they show up in ant -p, but otherwise lgtm.;;;","29/Dec/12 18:31;jbellis;Added descriptions to test-all and write-java-license-headers as v2.

But I'm getting the following error:

{noformat}
BUILD FAILED
/Users/johnathanellis/projects/cassandra/git/build.xml:1113: The following error occurred while executing this line:
/Users/johnathanellis/projects/cassandra/git/build.xml:1055: Reference cobertura.classpath not found.
{noformat};;;","30/Dec/12 20:30;dbrosius@apache.org;what target are you running?;;;","31/Dec/12 00:23;jbellis;ant clean test;;;","31/Dec/12 06:27;dbrosius@apache.org;fix cobertura.classpath issue in v3;;;","01/Jan/13 14:43;jbellis;WFM.  Ship it!;;;","01/Jan/13 19:00;dbrosius;pushed to trunk as commit 565f675b16747e7bfa827cb220cfce28f33c5b81;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't start a node with row_cache_size_in_mb=1,CASSANDRA-3812,12540413,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,tpatterson,tpatterson,30/Jan/12 19:25,16/Apr/19 09:32,14/Jul/23 05:52,30/Jan/12 23:44,1.1.0,,,,,,0,,,,,,,"I consistently get the following error when trying to run 'bin/cassandra':

{code}
ERROR 12:20:28,144 Fatal exception during initialization
org.apache.cassandra.config.ConfigurationException: Found system table files, but they couldn't be loaded!
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:279)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:174)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:367)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
{code}","'trunk' in the git repo as of 1/20/2012. Use the standard cassandra.yaml config file, except change row_cache_size_in_mb to 1. Happened on both ubuntu and osx.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/12 23:19;yukim;3812.txt;https://issues.apache.org/jira/secure/attachment/12512494/3812.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225826,,,Mon Jan 30 23:44:44 UTC 2012,,,,,,,,,,"0|i0goj3:",95404,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"30/Jan/12 23:19;yukim;Setting row_cache_size_in_mb > 0 causes particular reads from column family whose row cache is disabled to force cache row, and results in reading null.

Attached patch makes sure value is read from column family, not from row cache when row caching is disabled.;;;","30/Jan/12 23:44;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
merge from 1.0 (aa20c7206cdc1efc1983466de05c224eccac1084) breaks build,CASSANDRA-3806,12540249,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,28/Jan/12 06:05,16/Apr/19 09:32,14/Jul/23 05:52,28/Jan/12 14:22,1.1.0,,,,,,0,,,,,,,"{code}
build-project:
     [echo] apache-cassandra: /tmp/cas/cassandra/build.xml
    [javac] Compiling 40 source files to /tmp/cas/cassandra/build/classes/thrift
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 296 source files to /tmp/cas/cassandra/build/classes/main
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]     ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                   ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                     ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                    ^
    [javac] StorageService.java:1343: illegal start of expression
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                          ^
    [javac] StorageService.java:1343: ';' expected
    [javac]     private Multimap<InetAddress, Range<Token>> getNewSourceRanges(String table, Set<Range<Token>> ranges)
    [javac]                                                                                                           ^
    [javac] 6 errors
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/12 06:08;scode;CASSANDRA-3806.txt;https://issues.apache.org/jira/secure/attachment/12512293/CASSANDRA-3806.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225663,,,Sat Jan 28 14:22:26 UTC 2012,,,,,,,,,,"0|i0gogf:",95392,,,,,Normal,,,,,,,,,,,,,,,,,"28/Jan/12 06:08;scode;Trivial patch attached.;;;","28/Jan/12 14:22;jbellis;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade problems from 1.0 to trunk,CASSANDRA-3804,12540242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,tpatterson,tpatterson,28/Jan/12 04:01,16/Apr/19 09:32,14/Jul/23 05:52,23/Feb/12 17:10,1.1.0,,,,,,0,,,,,,,"A 3-node cluster is on version 0.8.9, 1.0.6, or 1.0.7 and then one and only one node is taken down, upgraded to trunk, and started again. An rpc timeout exception happens if counter-add operations are done. It usually takes between 1 and 500 add operations before the failure occurs. The failure seems to happen sooner if the coordinator node is NOT the one that was upgraded. Here is the error: 

{code}

======================================================================
ERROR: counter_upgrade_test.TestCounterUpgrade.counter_upgrade_test
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib/pymodules/python2.7/nose/case.py"", line 187, in runTest
    self.test(*self.arg)
  File ""/home/tahooie/cassandra-dtest/counter_upgrade_test.py"", line 50, in counter_upgrade_test
    cursor.execute(""UPDATE counters SET row = row+1 where key='a'"")
  File ""/usr/local/lib/python2.7/dist-packages/cql/cursor.py"", line 96, in execute
    raise cql.OperationalError(""Request did not complete within rpc_timeout."")
OperationalError: Request did not complete within rpc_timeout.

{code}

","ubuntu, cluster set up with ccm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/12 10:26;xedin;CASSANDRA-3804-1.1-v2.patch;https://issues.apache.org/jira/secure/attachment/12515740/CASSANDRA-3804-1.1-v2.patch","15/Feb/12 07:07;xedin;CASSANDRA-3804-1.1.patch;https://issues.apache.org/jira/secure/attachment/12514605/CASSANDRA-3804-1.1.patch","11/Feb/12 19:17;xedin;CASSANDRA-3804.patch;https://issues.apache.org/jira/secure/attachment/12514228/CASSANDRA-3804.patch","16/Feb/12 16:21;slebresne;node1.log;https://issues.apache.org/jira/secure/attachment/12514813/node1.log","16/Feb/12 16:21;slebresne;node2.log;https://issues.apache.org/jira/secure/attachment/12514814/node2.log",,,,,,,,,,,,,5.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225656,,,Tue Apr 24 17:01:37 UTC 2012,,,,,,,,,,"0|i0gofr:",95389,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"30/Jan/12 14:01;slebresne;This is not counter related (but you have to use CL.ALL to reproduce without counters as otherwise it's hidden by the fact that only the non-upgraded coordinator acknowledges writes) and it is related to CASSANDRA-1391.

This is due to the inability of doing schema changes in a mixed pre/post-1.1 cluster, if I trust the following log (from the upgraded node):
{noformat}
java.lang.RuntimeException: java.io.IOException: Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.
        at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:544)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Can't accept schema migrations from Cassandra versions previous to 1.1, please update first.
        at org.apache.cassandra.service.MigrationManager.deserializeMigrationMessage(MigrationManager.java:233)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:231)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
{noformat}

There is however two problems imho:
# Not supporting migrations during the upgrade process is one thing, but it should put the cluster in a broken state, which I'm not sure it doesn't do. Ideally, new nodes would still accept old migrations from old nodes, but would refuse to schema changes themselves until they know all nodes are upgraded. We could then throw an UnavailableException with a message.
# On top of the exception above, the logs during that test are filled with errors that don't sound too reassuring. On every node (upgraded or not), there is a handful of:
{noformat}
ERROR [MutationStage:34] 2012-01-30 14:35:39,041 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[MutationStage:34,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:66)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
        at java.io.DataInputStream.readUTF(DataInputStream.java:572)
        at java.io.DataInputStream.readUTF(DataInputStream.java:547)
        at org.apache.cassandra.db.TruncationSerializer.deserialize(Truncation.java:80)
        at org.apache.cassandra.db.TruncationSerializer.deserialize(Truncation.java:70)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:44)
        ... 4 more
{noformat}
On the upgraded node, there is a few:
{noformat}
ERROR [MutationStage:38] 2012-01-30 14:35:50,772 RowMutationVerbHandler.java (line 61) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:401)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:409)
        at org.apache.cassandra.db.RowMutation.fromBytes(RowMutation.java:357)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:42)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
And on the non-upgraded ones, there is a few:
{noformat}
ERROR [GossipStage:1] 2012-01-30 14:35:13,363 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID
        at java.util.UUID.timestamp(UUID.java:308)
        at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
        at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
        at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat};;;","30/Jan/12 14:39;xedin;This was discussed with Jonathan on the process of CASSANDRA-1391, users should make sure that all of the nodes are updated to 1.1 before running any schema changes because it's impossible to apply old migrations even if we accept them and users will be getting exceptions from your #2 anyway.;;;","30/Jan/12 14:52;jbellis;We can't support mixed-version schema changes, but we should make sure that we don't leave the cluster broken if a user does that anyway.;;;","30/Jan/12 15:31;xedin;This exception (taken from Sylvain's #2) explains what will happen when you only partially migrate:

{noformat}
ERROR [GossipStage:1] 2012-01-30 14:35:13,363 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID
        at java.util.UUID.timestamp(UUID.java:308)
        at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
        at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
        at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat} 

As we switched from Time-based UUID for schema versions MigrationManager on the old nodes will fail all the time when nodes with new schema start-up or when they will request migrations from it (because they see that their schema version is different from others). Even if we make a fix in MigrationManager.rectify(...) method for 1.0.x, nodes with new/old schema will never come to agreement because of different types of the UUID and because they unable to run schema mutations anymore.;;;","30/Jan/12 15:43;jbellis;""Never come to agreement"" is fine as long as normal reads/writes (against existing CFs) continue to work.;;;","30/Jan/12 15:59;xedin;bq. ""Never come to agreement"" is fine as long as normal reads/writes (against existing CFs) continue to work.

reads/writes should work against existing CFs. failure from description and first comment are related to the way how cassandra-dtest works because it tries to re-create schema for every test-case which won't work for in the mixed version cluster, if, for example, it was to create a ColumnFamily before updating one of the nodes to trunk, reads/writes to that ColumnFamily would still work after update even tho nodes will be in schema disagreement.;;;","10/Feb/12 05:01;jbellis;Can we log something telling the user ""you can't change schema until you finish upgrade everything"" instead of a scary-looking ""not a time-based UUID"" uncaught exception?;;;","11/Feb/12 19:17;xedin;patch for cassandra-1.0 branch, which does a UUID version check in MigrationManager.rectify(UUID, InetAddress) to determine if version is time-based UUID if not it would log an error and return.;;;","13/Feb/12 23:34;jbellis;It's the 1.0 side that has the problem?  I missed that...  I'd rather not require people upgrade to 1.0.8+, before upgrading to 1.1.

Can we just have 1.1 use MS version info to not send schema to nodes that can't understand it?;;;","13/Feb/12 23:37;xedin;We can do that. ""java.lang.UnsupportedOperationException: Not a time-based UUID"" happens on 1.0 nodes so I thought that it would be appropriate to fix it there also.;;;","15/Feb/12 07:07;xedin;changes the MigrationManager.announce to skip the nodes with versions older than 1.1;;;","16/Feb/12 14:38;slebresne;Tried the two attached patches. It does remove a bunch of the exceptions. For some reason there is still quite a few EOFExceptions related to truncation, but the test doesn't truncate at all so I'm not sure were that coming from.  I'm attaching the logs from the nodes for reference (that's the log after the two patches are applied). node1.log is the 1.1 node and node2.log is a 1.0 node. The thing that triggers those exception is the creation of a CF on node2 (the old one).

So it'd be nice to figure out what triggers those exception, but if we're going to patch both 1.1 and 1.0, why not just (or rather in addition) have schema changes check the (known) version of all other nodes before doing anything and just throw an InvalidRequestException if we know the schema change will fail?
;;;","16/Feb/12 15:19;xedin;I will investigate how truncate is related to the schema modifications. I don't think that we have any intention to patch both 1.0 and 1.1 because we don't want to require people to update to 1.0.8+ before upgrading to 1.1. ;;;","16/Feb/12 15:37;slebresne;I'm personally fine saying that it is recommended to upgrade to 1.0.8+ before 1.1 if that allow smoother/safer upgrade. Besides, I don't think we really have a choice if we want to avoid scary looking logs issue. And I don't really understand your remark anyway since the patches *you* attached to this issue targets *both* 1.0 and 1.1.;;;","16/Feb/12 16:01;jbellis;If 1.1 doesn't send new-style schema changes to 1.0, that should be sufficient to avoid scary-looking logs.;;;","16/Feb/12 16:02;xedin;if you take a look at the comment from Jonathan at ""13/Feb/12 23:34"" it will give you a better understanding why there are both 1.0 and 1.1 patches.;;;","16/Feb/12 16:40;slebresne;Reattaching the two logs with only the 1.1 patch applied. Putting asides all the truncation related EOFException for now:
* on node1 (the 1.1 node), we do see a ""Can't accept schema migrations from Cassandra versions previous to 1.1, please update first"", though I suppose it would be nice to  avoid the 'Fatal exception' and stacktrace that tends to scare people.
* on node2, we still have ""UnsupportedOperationException: Not a time-based UUID"" exceptions.

That being said, even if we fix all those exceptions (which we should, there no question on that), it is still the case that the schema change is applied to the 1.0 nodes and no error is returned to the user (that is, outside of the log file), but the added column family is not really usable (the user will get timeouts) at least until the cluster upgrade is complete. Even if the user do watch the log and see the error (and will probably assume the creation failed), the column family is still created. That's not really user friendly. So I still think it would be a good idea to add code to 1.0 and 1.1 to refuse upfront schema changes when the cluster is known to have mixed pre-1.1/post-1.1 versions. That don't necessarily mean we would *require* an upgrade to 1.0.8+ before upgrading to 1.1, but it would mean that for all those that does upgrade from 1.0.8 (possibly a majority of users), we're being more user friendly.;;;","19/Feb/12 20:12;xedin;I have found the problem with EOFException in Truncate - it was caused by MIGRATION_REQUEST  misinterpreted by 1.0 as TRUNCATE message, I have added a check into MM.rectifySchema(UUID, InetAddress) to skip that request if node with changed schema is older than 1.1. Two patches in combination now return no exceptions but node 1.0 without patch would still throw UnsupportedOperationException because Gossiper always propagates passive schema announce.

Edit: I have also changed ""Can't accept schema migrations from Cassandra versions previous to 1.1, please update first."" in 1.1 to be log error instead of IOException.;;;","23/Feb/12 10:13;slebresne;bq. it was caused by MIGRATION_REQUEST misinterpreted by 1.0 as TRUNCATE message

That shouldn't be. In the StorageService.Verb enumeration, MIGRATION_REQUEST needs to be moved to the end of the enum. Otherwise lots of stuff will be misinterpreted.;;;","23/Feb/12 10:23;xedin;yeah, it seems like I have missed the comment about that, but fact is fact it was misinterpreted because of that. Have placed MIGRATION_REQUEST at the end of the Verb enum in updated v2.;;;","23/Feb/12 10:26;xedin;fixed the comment about migration request.;;;","23/Feb/12 17:10;slebresne;Ok, it does leave some InsupportedOperationException on the 1.0 but it clearly improve things and the misplaced MIGRATION_REQUEST is a real bug so I've committed v2 (putting MIGRATION_REQUEST before the UNUSED ones as that's where it should be added). I'll open another issue to see if we can make smooth even more the case where people (mistakenly) do schema update in a mixed cluster.;;;","24/Apr/12 17:01;tpatterson;I removed the dtest because it always fails, and will always fail in a properly running cluster.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snapshot-before-compaction snapshots entire keyspace,CASSANDRA-3803,12540203,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Jan/12 22:04,16/Apr/19 09:32,14/Jul/23 05:52,09/Feb/12 03:56,1.0.8,1.1.0,,,,,0,compaction,,,,,,Should only snapshot the CF being compacted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/12 20:24;jbellis;3803.txt;https://issues.apache.org/jira/secure/attachment/12513362/3803.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225618,,,Thu Feb 09 03:56:46 UTC 2012,,,,,,,,,,"0|i0gofb:",95387,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/Jan/12 22:18;jbellis;I can't think of a good reason to snapshot the entire keyspace, and doing so can dramatically increase the space needed to enable snapshot_before_compaction.;;;","04/Feb/12 17:27;slebresne;I think the attached patch is the wrong one.;;;","05/Feb/12 20:24;jbellis;2nd try. Also switches to snapshot-without-flush.;;;","05/Feb/12 21:50;slebresne;+1, but I would actually suggest pushing this in 1.0.8. The fact we were flushing all CFs on each compaction is pretty bad (not sure anyone actually uses snapshot-before-compaction but still).;;;","05/Feb/12 21:51;slebresne;As a side not, a nice alternative would be to snapshot only the files we're going to compact. But again, we probably don't care about that feature that much.;;;","09/Feb/12 03:56;jbellis;committed to 1.0.8 + 1.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy static initialization not triggered until thrift requests come in,CASSANDRA-3797,12540003,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,27/Jan/12 06:37,16/Apr/19 09:32,14/Jul/23 05:52,29/Feb/12 04:36,1.1.0,,,,,,0,,,,,,,"While plugging in the metrics library for CASSANDRA-3671 I realized (because the metrics library was trying to add a shutdown hook on metric creation) that starting cassandra and simply shutting it down, causes StorageProxy to not be initialized until the drain shutdown hook.

Effects:

* StorageProxy mbean missing in visualvm/jconsole after initial startup (seriously, I thought I was going nuts ;))
* And in general anything that makes assumptions about running early, or at least not during JVM shutdown, such as the metrics library, will be problematic
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/12 21:42;jbellis;3797-forname-this-time-for-sure.txt;https://issues.apache.org/jira/secure/attachment/12516218/3797-forname-this-time-for-sure.txt","27/Jan/12 17:22;jbellis;3797-forname.txt;https://issues.apache.org/jira/secure/attachment/12512192/3797-forname.txt","27/Jan/12 07:32;scode;CASSANDRA-3797-trunk-v1.txt;https://issues.apache.org/jira/secure/attachment/12512088/CASSANDRA-3797-trunk-v1.txt",,,,,,,,,,,,,,,3.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225506,,,Wed Feb 29 04:36:00 UTC 2012,,,,,,,,,,"0|i0gocn:",95375,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Jan/12 07:32;scode;Attaching patch that has StorageService call a NOOP method on StorageProxy during start-up if not in client mode.

This feels unclean to me, but barring a ""bigger"" change to avoid the subtle static initialization order problem properly it was the easiest/cleanest fix I could think of.;;;","27/Jan/12 08:04;scode;Maybe a Class.forName() is cleaner (a Class klass = StorageProxy.class does not (necessarily) cause it to be loaded).
;;;","27/Jan/12 17:22;jbellis;Hmm.  Neither this nor the forName approach (attached) is sufficient to o.a.c.service.StorageProxy show up in jconsole for me.;;;","27/Jan/12 18:54;scode;Really? That's strange. I just re-tried again to double-check (ant clean and everything) and the patch seems to make a difference here.

In the case of Class.forName() I cannot believe that is not supposed to be guaranteed. Are you *sure* it's not working? I would have to assume static initialization must by definition happen for Class.forName() to fullfil its contract (even if I could possibly buy slightly more easily that the staticallyInitialize() NOOP didn't, but even that seems like a stretch).

I am just building and {{./bin/cassandra -f}} and immediately attaching with visualvm.;;;","27/Feb/12 20:09;scode;Looks like {{3797-forname.txt}} is the same file as the original patch. In any case, suppose we just go for Class.forName() to avoid introducing that annoying method, and assuming it makes the metrics from CASSANDRA-3671 work, can I get a +1?;;;","27/Feb/12 21:30;brandon.williams;I tested this with CASSANDRA-3671 and everything worked.;;;","27/Feb/12 21:42;jbellis;correct forname patch attached.

figured out my problem from last time: I was looking for StorageProxy under the old location in o.a.c.service, instead of o.a.c.db.  It appears correctly on startup in o.a.c.db with this patch.;;;","29/Feb/12 04:36;scode;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
post-2392 trunk does not build with java 7,CASSANDRA-3796,12540000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,27/Jan/12 05:31,16/Apr/19 09:32,14/Jul/23 05:52,28/Jan/12 05:17,,,,,,,0,,,,,,,"See below, on a fresh clone. Builds w/ java 6.

{code}
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:419: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]         int index = Collections.binarySearch(indexSummary.getKeys(), key);
    [javac]                                ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:509: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]             int left = Collections.binarySearch(samples, leftPosition);
    [javac]                                   ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
    [javac] /tmp/c2/cassandra/src/java/org/apache/cassandra/io/sstable/SSTableReader.java:521: error: no suitable method found for binarySearch(List<DecoratedKey>,RowPosition)
    [javac]                       : Collections.binarySearch(samples, rightPosition);
    [javac]                                    ^
    [javac]     method Collections.<T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>) is not applicable
    [javac]       (cannot instantiate from arguments because actual and formal argument lists differ in length)
    [javac]     method Collections.<T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2) is not applicable
    [javac]       (no instance(s) of type variable(s) T#2 exist so that argument type List<DecoratedKey> conforms to formal parameter type List<? extends Comparable<? super T#2>>)
    [javac]   where T#1,T#2 are type-variables:
    [javac]     T#1 extends Object declared in method <T#1>binarySearch(List<? extends T#1>,T#1,Comparator<? super T#1>)
    [javac]     T#2 extends Object declared in method <T#2>binarySearch(List<? extends Comparable<? super T#2>>,T#2)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/12 06:58;scode;CASSANDRA-3796-trunk-v1.txt;https://issues.apache.org/jira/secure/attachment/12512084/CASSANDRA-3796-trunk-v1.txt",,,,,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225503,,,Sat Jan 28 05:17:32 UTC 2012,,,,,,,,,,"0|i0goc7:",95373,,,,,Low,,,,,,,,,,,,,,,,,"27/Jan/12 06:58;scode;Attaching patch. It would be great if someone from CASSANDRA-2392 could review and make sure I am not introducing a subtle bug by implementing the Comparator at the RowPosition level.;;;","27/Jan/12 10:04;slebresne;This is just a generics problem (for some reason Java 7 became more picky about this, one could almost call it a bug but anyway) so I'd prefer not adding real code to fix (though I don't think there was anything wrong with the patch per se). Committed http://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=00613729 that adds enough generic info for the compiler to stop complaining. I'll let you check this fixes it for you too.

As a side note, the generics missing info was for DecoratedKey. DK has been generic for as long as I can remember but I don't really think this is of any use in that case, so it could be worth removing the type parameter from DK altogether, which would avoid adding a <?> every time. This would affect a high number of lines though (all those where we have DK<?>) so I've sticked to just adding the generic info for now.;;;","28/Jan/12 05:17;scode;+1, works for me (and sounds good).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad validator lookup (bad key type),CASSANDRA-3789,12539858,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:59,16/Apr/19 09:32,14/Jul/23 05:52,26/Jan/12 07:26,1.0.8,,,,,,0,,,,,,,"code looks up an entry in a map by a byte[] even tho the map is keyed by ByteBuffer, add a ByteBuffer.wrap call to the key.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/12 02:59;dbrosius@apache.org;bad_contains_check_for_cdname.diff;https://issues.apache.org/jira/secure/attachment/12511938/bad_contains_check_for_cdname.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225361,,,Thu Jan 26 07:26:33 UTC 2012,,,,,,,,,,"0|i0go93:",95359,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Jan/12 07:26;jbellis;committed (this one to 1.0.8 too);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison in hadoop cf recorder reader,CASSANDRA-3788,12539857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:53,16/Apr/19 09:32,14/Jul/23 05:52,26/Jan/12 07:22,1.1.0,,,,,,0,hadoop,,,,,,"code does

rows.get(0).columns.get(0).column.equals(startColumn)

which is a Column against a ByteBuffer

changed to 

rows.get(0).columns.get(0).column.name.equals(startColumn)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/12 02:53;dbrosius@apache.org;bad_column_name_compare.diff;https://issues.apache.org/jira/secure/attachment/12511937/bad_column_name_compare.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225360,,,Thu Jan 26 07:22:49 UTC 2012,,,,,,,,,,"0|i0go8f:",95356,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Jan/12 07:22;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison of column name against * or 1,CASSANDRA-3787,12539856,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:46,16/Apr/19 09:32,14/Jul/23 05:52,28/Jan/12 03:46,1.1.0,,,,,,0,cql,,,,,,"code does 

(!selectClause.get(0).equals(""*"") && !selectClause.get(0).equals(""1"")))

which is a ColumnDefinition against a string 

changed to

String columnName = selectClause.get(0).toString();
if (!columnName.equals(""*"") && !columnName.equals(""1""))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/12 02:47;dbrosius@apache.org;bad_column_compare.diff;https://issues.apache.org/jira/secure/attachment/12511936/bad_column_compare.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225359,,,Thu Jan 26 13:13:58 UTC 2012,,,,,,,,,,"0|i0go7z:",95354,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Jan/12 07:16;jbellis;This doesn't quite preserve the intent which is to throw on *either* more than 1 clause, or the count body mismatch.  Updated to this on commit:

{code}
.               if (selectClause.size() != 1)
                    throw new InvalidRequestException(""Only COUNT(*) and COUNT(1) operations are currently supported."");
                String columnName = selectClause.get(0).toString();
                if (!columnName.equals(""*"") && !columnName.equals(""1""))
                    throw new InvalidRequestException(""Only COUNT(*) and COUNT(1) operations are currently supported."");
{code}
;;;","26/Jan/12 13:13;dbrosius@apache.org;dang it.. thanks, good catch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix bad comparison of IColumn to ByteBuffer,CASSANDRA-3786,12539854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,26/Jan/12 02:36,16/Apr/19 09:32,14/Jul/23 05:52,26/Jan/12 07:11,1.1.0,,,,,,0,,,,,,,"Code does

firstColumn.equals(startKey)

changed to 

firstColumn.name().equals(startKey)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/12 02:37;dbrosius@apache.org;bad_compare.diff;https://issues.apache.org/jira/secure/attachment/12511934/bad_compare.diff",,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225357,,,Thu Jan 26 07:11:27 UTC 2012,,,,,,,,,,"0|i0go7j:",95352,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Jan/12 07:09;jbellis;This is tagged affects 1.0.7 but patch only applies to trunk for me.  Do we need a different patch for 1.0 branch, or should this be affects-1.1?;;;","26/Jan/12 07:11;jbellis;looks to me like the latter -- will commit on that basis, please reopen if I'm wrong;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RangeTest.java compilation error on Range.rangeSet in Eclipse (not Ant or IntelliJ),CASSANDRA-3784,12539806,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kirktrue,kirktrue,kirktrue,25/Jan/12 18:50,16/Apr/19 09:32,14/Jul/23 05:52,25/Jan/12 21:27,1.1.0,,,Legacy/Testing,,,0,,,,,,,"When building from trunk:

{noformat}
The method rangeSet(Range<T>...) in the type Range is not applicable for the arguments (Range[])
RangeTest.java	/cassandra/test/unit/org/apache/cassandra/dht	line 184
{noformat}
",,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/12 19:26;kirktrue;CASSANDRA-3784.patch;https://issues.apache.org/jira/secure/attachment/12511870/CASSANDRA-3784.patch",,,,,,,,,,,,,,,,,1.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,225309,,,Wed Jan 25 21:27:29 UTC 2012,,,,,,,,,,"0|i0go6n:",95348,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"25/Jan/12 19:26;kirktrue;Patch that adds a type for T such that we can pass the compiler's generics checks.;;;","25/Jan/12 21:27;tjake;committed thx!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh is missing cqlshlib (ImportError: No module named cqlshlib),CASSANDRA-3767,12539326,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thepaul,mattman,mattman,22/Jan/12 01:21,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 10:41,1.0.8,,,Legacy/Tools,,,0,cqlsh,,,,,,"After a clean install of Cassandra, when running cqlsh I get the following error:

{code}>cqlsh
Traceback (most recent call last):
  File ""/usr/local/bin/cqlsh"", line 54, in <module>
    from cqlshlib import cqlhandling, pylexotron
ImportError: No module named cqlshlib
{code}
","Mac OS X 10.7.2 Snow Leopard
Cassandra 1.0.7
Installed using Homebrew",mattman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224828,,,Wed Feb 22 10:41:18 UTC 2012,,,,,,,,,,"0|i0gnzb:",95315,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"22/Jan/12 02:22;mattman;FYI, I managed to get cqlsh working by downloading http://svn.apache.org/repos/asf/cassandra/trunk/pylib/setup.py, and and cqlshlib from http://svn.apache.org/repos/asf/cassandra/trunk/pylib/cqlshlib/, and then running python install setup.py.

Just not sure why it's not installed by default and why I had to install the cqlshlib manually.

Matt;;;","20/Feb/12 15:53;thepaul;Looks like the homebrew packaging neglects to ship all the necessary parts of cqlsh.;;;","20/Feb/12 22:11;thepaul;Submitted https://github.com/mxcl/homebrew/pull/10353 ; we'll see what they think about that.

Merge my cqlsh-in-cassandra branch at git@github.com:thepaul/homebrew.git into your Homebrew repo to get this fix earlier.;;;","21/Feb/12 20:12;thepaul;Actually, the change seems like a useful one to merge back upstream here, although it's unnecessary for the other types of deployment we expect to see.

One-liner change is in my 3767 branch on github: https://github.com/thepaul/cassandra/commit/12cbb648f;;;","22/Feb/12 10:41;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeysIndex is broken by CASSANDRA-1600,CASSANDRA-3766,12539280,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,philip.andronov,philip.andronov,philip.andronov,21/Jan/12 12:41,16/Apr/19 09:32,14/Jul/23 05:52,22/Jan/12 15:50,1.1.0,,,,,,0,,,,,,,"CASSANDRA-1600 introduces bug which cause KeySearch throws an Exception during search

KeySearcher:163
logger.debug(""Skipping {}"", baseCfs.getComparator().getString(firstColumn.name()));

Here firstColumn is a column from *index* row, so column name is a *key* in the baseCf. 

So the correct version would be:
logger.debug(""Skipping {}"", baseCfs.metadata.getKeyValidator().getString(firstColumn.name()));

Right now it is not possible to query KeysIndex. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,philip.andronov,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224782,,,Sun Jan 22 15:50:48 UTC 2012,,,,,,,,,,"0|i0gnyv:",95313,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Jan/12 14:42;jbellis;committed, thanks!;;;","22/Jan/12 15:50;philip.andronov;no problems :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hadoop word count example is unable to output to cassandra with default settings,CASSANDRA-3765,12539274,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Jan/12 06:05,16/Apr/19 09:32,14/Jul/23 05:52,23/Jan/12 22:05,1.1.0,,,,,,0,hadoop,,,,,,"{noformat}
12/01/21 06:03:16 WARN mapred.LocalJobRunner: job_local_0001
java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.newPartitioner(FBUtilities.java:407)
        at org.apache.cassandra.hadoop.ConfigHelper.getOutputPartitioner(ConfigHelper.java:384)
        at org.apache.cassandra.client.RingCache.<init>(RingCache.java:58)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:99)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter.<init>(ColumnFamilyRecordWriter.java:93)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:132)
        at org.apache.cassandra.hadoop.ColumnFamilyOutputFormat.getRecordWriter(ColumnFamilyOutputFormat.java:62)
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:553)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)
{noformat}

(Output to filesystem still works.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/12 22:11;jbellis;3765.txt;https://issues.apache.org/jira/secure/attachment/12511410/3765.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224776,,,Mon Jan 23 22:05:00 UTC 2012,,,,,,,,,,"0|i0gnyf:",95311,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"21/Jan/12 22:11;jbellis;Patch to fix defaults post-CASSANDRA-3197.

Still seeing (different) errors in record writer in the word count example; will address after CASSANDRA-2878 since that touches the same code.;;;","23/Jan/12 21:24;brandon.williams;+1 on e66fc06 at https://github.com/jbellis/cassandra/tree/2878-rebased;;;","23/Jan/12 22:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't error out immediately for use of invalid keyspace,CASSANDRA-3764,12539273,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Jan/12 05:54,16/Apr/19 09:32,14/Jul/23 05:52,26/Jan/12 01:30,1.0.8,,,Legacy/Tools,,,0,,,,,,,"{noformat}
cqlsh> use wordcoun;
cqlsh:wordcoun> select * from input_words;
Bad Request: Keyspace wordcoun does not exist
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/12 06:02;jbellis;3764.txt;https://issues.apache.org/jira/secure/attachment/12511364/3764.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224775,,,Thu Jan 26 01:30:33 UTC 2012,,,,,,,,,,"0|i0gnxz:",95309,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"21/Jan/12 05:57;jbellis;Looks like this is a server bug, not cqlsh's fault:

{code}
.   public void setKeyspace(String ks)
    {
        keyspace = ks;
    }
{code}
;;;","21/Jan/12 06:02;jbellis;quick fix attached.;;;","23/Jan/12 20:12;thepaul;I always thought this was expected behavior. Is there any chance this might break some existing code? I guess probably not.;;;","24/Jan/12 18:51;thepaul;+1;;;","26/Jan/12 01:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on invalid CQL DELETE command,CASSANDRA-3755,12539021,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,thepaul,thepaul,19/Jan/12 19:31,16/Apr/19 09:32,14/Jul/23 05:52,29/Mar/12 14:33,1.0.9,,,,,,0,cql,,,,,,"The CQL command {{delete from k where key='bar';}} causes Cassandra to hit a NullPointerException when the ""k"" column family does not exist, and it subsequently closes the Thrift connection instead of reporting an IRE or whatever. This is probably wrong.",,dbrosius@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/12 06:48;dbrosius@apache.org;unknown_cf.diff;https://issues.apache.org/jira/secure/attachment/12511230/unknown_cf.diff","27/Mar/12 23:58;dbrosius@apache.org;unknown_cf_2.diff;https://issues.apache.org/jira/secure/attachment/12520215/unknown_cf_2.diff",,,,,,,,,,,,,,,,2.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224523,,,Thu Mar 29 14:33:22 UTC 2012,,,,,,,,,,"0|i0gnu7:",95292,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"20/Jan/12 06:48;dbrosius@apache.org;throw a IRE exception with understandable error message, rather than NPE.;;;","24/Jan/12 07:22;dbrosius@apache.org;ah, that's much better, thanks...don't allow write actions from the client on system keyspace  -> attached.;;;","24/Jan/12 16:12;jbellis;The second patch is for CASSANDRA-3759. I'll apply it there.  Did you mean to revise this one as well?;;;","24/Jan/12 22:33;dbrosius@apache.org;Arg, my apologies, posted to wrong bug.;;;","25/Jan/12 00:25;jbellis;So the patch here is ready for review for fixing the NPE problem?;;;","25/Jan/12 01:48;dbrosius@apache.org;yes;;;","08/Feb/12 17:12;slebresne;I believe the right fix would be to use ThriftValidation.validateColumnFamily that catches that kind of problems. Looking at DeleteStatement, it seems that it re-validate the column family for each key in mutationForKey(), but that's done later and thus the NPE is thrown first. We should probably move the validateColumnFamily up in prepareRowMutations() and then pass the resulting metadata as an argument of mutationForKey to avoid the multiple validation.;;;","27/Mar/12 14:29;jbellis;Dave, did you want to take a stab at that approach?;;;","27/Mar/12 23:35;dbrosius@apache.org;sure;;;","27/Mar/12 23:58;dbrosius@apache.org;pushed up Thrift.validateColumnFamily as Sylvain suggests.

(against trunk)

unknown_cf_2.diff;;;","29/Mar/12 14:33;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bulk loader no longer finds sstables,CASSANDRA-3752,12538787,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,18/Jan/12 13:06,16/Apr/19 09:32,14/Jul/23 05:52,22/Jan/12 17:24,1.1.0,,,,,,0,,,,,,,"It looks like CASSANDRA-2749 broke it:

{noformat}
 WARN 13:02:20,107 Invalid file 'Standard1' in data directory /var/lib/cassandra/data/Keyspace1.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3740,,,,,,,,,,,,,,,"19/Jan/12 16:53;brandon.williams;3752.txt;https://issues.apache.org/jira/secure/attachment/12511136/3752.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224290,,,Sun Jan 22 17:24:19 UTC 2012,,,,,,,,,,"0|i0gnt3:",95287,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"19/Jan/12 16:53;brandon.williams;Patch to use new directory layout, also add stream throttling to prevent an NPE when there is no yaml config (in anticipation of CASSANDRA-3740);;;","22/Jan/12 17:24;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible livelock during commit log playback,CASSANDRA-3751,12538739,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jchakerian,jchakerian,18/Jan/12 05:19,16/Apr/19 09:32,14/Jul/23 05:52,01/Feb/12 19:43,0.8.10,1.0.8,,,,,0,commitlog,,,,,,"In CommitLog.recover, there seems to be the possibility of concurrent inserts to tablesRecovered (a HashSet) in the Runnables instantiated a bit below (line 323 in 1.0.7). This apparently happened during a commit log playback during startup of a node that had not shut down cleanly (the cluster was under heavy load previously and there were several gigabytes of commit logs), resulting in two threads running in perpetuity (2 cores were at 100% from running these threads), preventing the node from coming up. The relevant portion of the stack trace is:

{noformat}
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:25"" prio=10 tid=0x00002aaad01e0800 nid=0x6f62 runnable [0x0000000044d54000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 
INFO   | jvm 1    | 2012/01/16 16:54:42 | ""MutationStage:21"" prio=10 tid=0x00002aaad00a2800 nid=0x6f5e runnable [0x0000000044950000]
INFO   | jvm 1    | 2012/01/16 16:54:42 |    java.lang.Thread.State: RUNNABLE
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashMap.put(HashMap.java:374)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.HashSet.add(HashSet.java:200)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:338)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
INFO   | jvm 1    | 2012/01/16 16:54:42 | 	at java.lang.Thread.run(Thread.java:662)

{noformat}

The most recently modified file in the commit log directory was this entry:
{noformat}
-rw-r----- 1 <redacted> <redacted>    0 Jan 16 16:03 CommitLog-1326758622599.log
{noformat}
though I'm not sure if this was related or not. ",Linux (CentOS 5.7),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/12 23:34;jbellis;3751.txt;https://issues.apache.org/jira/secure/attachment/12512257/3751.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,224242,,,Wed Feb 01 19:43:04 UTC 2012,,,,,,,,,,"0|i0gnsn:",95285,,rbranson,,rbranson,Normal,,,,,,,,,,,,,,,,,"18/Jan/12 05:42;rbranson;Which commitlog_sync option do you have configured on this node?;;;","18/Jan/12 05:52;jchakerian;periodic;;;","27/Jan/12 23:34;jbellis;You're absolutely right, tablesRecovered is not threadsafe but it's mutated by up to concurrent_writes threads during log replay.  Patch attached to switch to NBHS.;;;","27/Jan/12 23:35;jbellis;patch is against 1.0 but applies cleanly to 0.8 as well.;;;","31/Jan/12 00:59;rbranson;Reviewed, +1;;;","01/Feb/12 19:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool.bat double quotes classpath,CASSANDRA-3744,12538403,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,14/Jan/12 16:30,16/Apr/19 09:32,14/Jul/23 05:52,27/Jan/12 19:56,1.0.8,,,Tool/nodetool,,,0,,,,,,,Windows sucks and double quoting things breaks stuff.,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/12 16:31;nickmbailey;0001-Don-t-double-quote-classpath.patch;https://issues.apache.org/jira/secure/attachment/12510587/0001-Don-t-double-quote-classpath.patch","18/Jan/12 22:28;jbellis;3744-v2.txt;https://issues.apache.org/jira/secure/attachment/12511050/3744-v2.txt",,,,,,,,,,,,,,,,2.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223909,,,Fri Jan 27 19:56:44 UTC 2012,,,,,,,,,,"0|i0gnpj:",95271,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Jan/12 23:05;jbellis;Can you give an example of a classpath where quoting breaks it?  It definitely works as-is for me right now.  I believe the purpose of the quoting is to allow paths with spaces, like the common C:\Program Files.;;;","17/Jan/12 00:04;nickmbailey;Right we want to quote the classpath but it is currently getting double quoted which breaks things on windows. When we loop over the jars in the lib dir and call the ':append' block we pass each jar quoted. Then later we quote the entire classpath again when we call java. So we end up with:

{noformat}
""""jar1"";""jar2"";""jar3""""
{noformat}

That will actually break if any of those jars have spaces in the path.;;;","18/Jan/12 22:28;jbellis;I see what you mean.  v2 attached to generalize fix to other .bats.;;;","26/Jan/12 04:30;nickmbailey;lgtm.

Tested all tools with a space in the classpath.;;;","27/Jan/12 19:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOMs because delete operations are not accounted,CASSANDRA-3741,12538312,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,akolyadenko,tivv,tivv,13/Jan/12 12:33,16/Apr/19 09:32,14/Jul/23 05:52,03/May/12 15:46,1.1.1,,,,,,0,,,,,,,"Currently we are moving to new data format where new format is written into new CFs and old one is deleted key-by-key. 
I have started getting OOMs and found out that delete operations are not accounted and so, column families are not flushed (changed == 0 with delete only operations) by storage manager.

This is pull request that fixed this problem for me: https://github.com/apache/cassandra/pull/5",FreeBSD,akolyadenko,hsn,omid,tivv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,akolyadenko,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223818,,,Wed May 22 05:43:18 UTC 2013,,,,,,,,,,"0|i0gno7:",95265,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Jan/12 18:46;jbellis;Thanks, Vitalii!

Unfortunately we can't use that patch as is because adding ops * 20 in there is going to throw off the size calculation for other workloads.

Note that the ""throughput"" size for a deletion is NOT zero (see Column.size implementation).  It sounds like you abruptly changed your workload from doing a bunch of larger inserts, then hit it with a ton of deletes all at once and OOMed before it was able to update its liveRatio estimate.

So the real problem is that if you change workloads dramatically enough, Cassandra's estimates can be off.;;;","13/Jan/12 20:43;hsn;How can you OOM if you replace large inserts with small deletes?;;;","15/Jan/12 08:54;tivv;Throughput size for deletion IS 0 for me. I am not deleting for some columns, but all columns in a key and operation has 0 columns in this case, this means 0 throughput so I did introduce an overhead for memory operation storage that looks good for me (20 bytes is ~ConcurrentMap size).

To note: I was getting OOM on startup/log replay. 

The problem is that I have started delete-only workload from some Column Families and this won't change as all inserts go into another column families: we are moving. And for full-key delete-only throughput is 0;;;","03/May/12 06:20;akolyadenko;Another attempt to fix this: https://github.com/apache/cassandra/pull/10

Please consider my patch. It's very annoying to have Cassandra dying in such situations.;;;","03/May/12 15:36;jbellis;Thanks, Andriy.  You (and Vitalii) are right; whole-row deletions did indeed have zero throughput because of that behavior.

Committed with a change to 12 bytes (long + int from deletion info) to match what we do with Column sizes.  (We measure the serialized size in Memtable.currentThroughput, then multiply by liveRatio to get a better estimate of the in-memory size.  Mixing internal overhead as in CSLM would actually double count that.  I've also introduced CASSANDRA-4215 to clean this up more for 1.2.;;;","03/May/12 15:39;jbellis;(That said, truncate is still the right solution for mass deletes.);;;","03/May/12 20:33;jjordan;Procedural question, for git stuff should we still be attaching patches to JIRA?  Or is a link to the github diff enough?;;;","03/May/12 20:38;jbellis;Intent to contribute in public is enough.  github pull requests (and patches sent to the mailing list for that matter) are fine.

For the purposes of record-keeping though, we like to associate these with Jira tickets.;;;","19/Sep/12 16:56;hsn;was this backported to cassandra 1.0?;;;","19/Sep/12 17:08;jbellis;No, this is a sensitive area of the code and we don't want to risk destabilizing it.;;;","19/Sep/12 18:32;hsn;1.0 is already destabilized by this bug. If you want to have minimal effect, then add just 1 byte to live bytes count for every delete. In non delete only workload, it will have minimal effect.

Its important to have non zero count otherwise it will not be flushed to disk on memory pressure.;;;","24/Sep/12 08:20;hsn;If you do not want to get fixed, then it should be documented as known bug in NEWS.TXT;;;","10/May/13 07:17;akolyadenko;Just would like to report that I have the same behavior with 1.2.4.;;;","22/May/13 05:43;hsn;i retested it. bug from 1.0 do not exists in 1.1 and 1.2. But its still not optimal and can lead to OOM because it do not adds enough bytes count for tombstone to live data count.

If i remember some hardcoded constant was used, it needs to be raised.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
While using BulkOutputFormat  unneccessarily look for the cassandra.yaml file.,CASSANDRA-3740,12538311,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,samarthg1986,samarthg1986,13/Jan/12 12:14,16/Apr/19 09:32,14/Jul/23 05:52,14/Feb/12 13:41,1.1.0,,,,,,0,cassandra,hadoop,mapreduce,,,,"I am trying to use BulkOutputFormat to stream the data from map of Hadoop job. I have set the cassandra related configuration using ConfigHelper ,Also have looked into Cassandra code seems Cassandra has taken care that it should not look for the cassandra.yaml file.
But still when I run the job i get the following error:

{
12/01/13 11:30:04 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
12/01/13 11:30:04 INFO input.FileInputFormat: Total input paths to process : 1
12/01/13 11:30:04 INFO mapred.JobClient: Running job: job_201201130910_0015
12/01/13 11:30:05 INFO mapred.JobClient:  map 0% reduce 0%
12/01/13 11:30:23 INFO mapred.JobClient: Task Id : attempt_201201130910_0015_m_000000_0, Status : FAILED
java.lang.Throwable: Child Error
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 1.
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)

attempt_201201130910_0015_m_000000_0: Cannot locate cassandra.yaml
attempt_201201130910_0015_m_000000_0: Fatal configuration error; unable to start server.
}

Also let me know how can i make this cassandra.yaml file available to Hadoop mapreduce job?",,forsberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3752,,CASSANDRA-3907,,,,,,,,,CASSANDRA-3839,,,,CASSANDRA-3839,,"03/Feb/12 00:20;brandon.williams;0001-Make-DD-the-canonical-partitioner-source.txt;https://issues.apache.org/jira/secure/attachment/12513076/0001-Make-DD-the-canonical-partitioner-source.txt","03/Feb/12 00:20;brandon.williams;0002-Prevent-loading-from-yaml.txt;https://issues.apache.org/jira/secure/attachment/12513077/0002-Prevent-loading-from-yaml.txt","03/Feb/12 14:11;brandon.williams;0003-use-output-partitioner.txt;https://issues.apache.org/jira/secure/attachment/12513129/0003-use-output-partitioner.txt","03/Feb/12 00:20;brandon.williams;0005-BWR-uses-any-if.txt;https://issues.apache.org/jira/secure/attachment/12513080/0005-BWR-uses-any-if.txt",,,,,,,,,,,,,,4.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223817,,,Tue Feb 14 13:55:49 UTC 2012,,,,,,,,,,"0|i0gnnr:",95263,,lenn0x,,lenn0x,Normal,,,,,,,,,,,,,,,,,"13/Jan/12 19:22;brandon.williams;Are you trying this on trunk?  Because specifically patch 0002 on CASSANDRA-3045 addresses this problem.;;;","14/Jan/12 04:33;samarthg1986;I am using ""apache-cassandra-2011-12-27_22-01-50-bin.tar.gz"" build from jenkins . Let me know if I am using the correct binaries . ;;;","14/Jan/12 09:02;samarthg1986;I have checked into the code and I can see following issues:
 1) ""org.apache.cassandra.config.Config"" do not initialize the all the properties and which results into the null pointer exception in a static block of class ""DatabaseDescriptor"" for example ""conf.commitlog_sync""

 2) I cant see any method in ConfigHelper to specify that I am using ""Supercolumn"" and clustername(dont know if cluster name is really needed)

 3) Also there is no method to specify comparator and subcomparator in ConfigHelper and it seems like comparators have been hard coded to ""BytesType""
;;;","14/Jan/12 12:25;samarthg1986;One More issue:

I can set the ""Partitioner"" into jobconf using ""ConfigHelper"" ,but it is no where used to set into the ""DatabaseDescriptor"" and not even initialized into the ""DatabaseDescriptor.initDefaultsOnly()"".
But ""AbstractSSTableSimpleWriter"" uses partioner (at line number 94) which will result into the null pointer.;;;","18/Jan/12 07:32;samarthg1986;Apart From these issues I do not think that we are considering the TTL case in BulkOutputFormat.
Have looked into the code and can only see the ""addColumn()"" method and the ""addExpiringColumn()"" is not used anywhere.;;;","19/Jan/12 17:08;brandon.williams;bq. 1) ""org.apache.cassandra.config.Config"" do not initialize the all the properties and which results into the null pointer exception in a static block of class ""DatabaseDescriptor"" for example ""conf.commitlog_sync""

Patches to address that.

bq. 2) I cant see any method in ConfigHelper to specify that I am using ""Supercolumn""

""mapreduce.output.bulkoutputformat.issuper"" controls that.

bq. 3) Also there is no method to specify comparator and subcomparator in ConfigHelper and it seems like comparators have been hard coded to ""BytesType""

BytesType will sort correctly, the comparators are in the schema on the remote nodes.

bq. Apart From these issues I do not think that we are considering the TTL case in BulkOutputFormat.

CASSANDRA-3754 will handle this.;;;","01/Feb/12 11:03;samarthg1986;I Tested the patches all the patches except the yaml one works fine.
I have checked the Yaml patch and my job still look for the cassandra.yaml file and fails.;;;","01/Feb/12 19:56;brandon.williams;I added a way to get around the yaml, then forgot to make BOF use it.  Updated patches should work.;;;","02/Feb/12 07:39;samarthg1986;Cool! Its Working Perfect with the updated patches.
Can you please explain 
1) what is the significance of ""INPUT_INITIAL_THRIFT_ADDRESS"" for BulkOutPutFormat.
2) What am I suppose to provide there?(If it is needed)
3) Is there any need to provide Listen address of the Hadoop Nodes for BulkOutputFormat if yes How to provide the same?

Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 and IPv6. 
Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.;;;","02/Feb/12 13:41;brandon.williams;bq. what is the significance of ""INPUT_INITIAL_THRIFT_ADDRESS"" for BulkOutPutFormat.

For an output format, this won't be used, it's only for input formats.

bq. Is there any need to provide Listen address of the Hadoop Nodes for BulkOutputFormat if yes How to provide the same?

I'm not sure what you mean, hadoop nodes themselves won't have a listen address, and BOF will discover the cassandra nodes' listen address via thrift.

bq. Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 and IPv6. Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.

Hmm, I can't think of any reason that would work with the yaml, can you give more details of the setup?;;;","02/Feb/12 13:46;forsberg;>Actually we are experiencing the problem while loading the data where it fails to connect if the host the M/R job is running on is dualstack, i.e. has both IPv4 >and IPv6. 
>Also it works when cassandra.yaml is provided ,may be it is reading listen address or something from cassandra.yaml.

Described this problem in more detail in CASSANDRA-3839.;;;","02/Feb/12 14:26;brandon.williams;bq. Described this problem in more detail in CASSANDRA-3839.

I see, let's address this there.;;;","02/Feb/12 19:06;brandon.williams;Two more patches to address CASSANDRA-3839;;;","03/Feb/12 00:20;brandon.williams;Rebased.;;;","03/Feb/12 14:11;brandon.williams;Fix incorrect variable scope causing NPE.;;;","13/Feb/12 16:35;brandon.williams;Samarth/Erik,

How does this patch look?;;;","13/Feb/12 17:11;samarthg1986;First 4 patches working fine.
About the patch related to CASSANDRA-3839 Erik can explain properly.;;;","14/Feb/12 02:55;lenn0x;Brandon,

I just filed CASSANDRA-3906, this fixes counter column NPE. Also we tested out these patches and based on patch: 0004-update-BOF-for-new-dir-layout.txt

That has a scoping issue, where you set File output, so scope becomes local.;;;","14/Feb/12 02:59;brandon.williams;bq. That has a scoping issue, where you set File output, so scope becomes local.

Are you sure you tried the latest patch on this ticket?

Edit: nevermind, I squashed 004 into 003 and never deleted it.;;;","14/Feb/12 05:24;lenn0x;Brandon, other than that, I +1 this patch. If you don't mind reviewing my other patches :) I am about to post another for making compression work with bulk writer too, and it depends on this going in.;;;","14/Feb/12 13:41;brandon.williams;Committed.;;;","14/Feb/12 13:55;forsberg;bq. Samarth/Erik, How does this patch look?

Overall, it's looking good. I haven't tested the latest version of this patch on a large data set yet, hoping to find time to do that tomorrow.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json doesn't work for secondary index sstable due to partitioner mismatch,CASSANDRA-3738,12538304,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,skamio,skamio,13/Jan/12 10:35,16/Apr/19 09:32,14/Jul/23 05:52,20/Jan/12 23:16,1.0.8,,,Feature/2i Index,,,0,tools,,,,,,"sstable2json doesn't work for secondary index sstable in 1.0.6 while it worked in version 0.8.x.


$ bin/sstable2json $DATA/data/Keyspace1/users-hc-1-Data.db 
{
""1111"": [[""birth_year"",""1973"",1326450301786000], [""full_name"",""Patrick Rothfuss"",1326450301782000]],
""1020"": [[""birth_year"",""1975"",1326450301776000], [""full_name"",""Brandon Sanderson"",1326450301716000]]
}

$ bin/sstable2json $DATA/data/Keyspace1/users.users_birth_year_idx-hc-1-Data.db 
Exception in thread ""main"" java.lang.RuntimeException: Cannot open data/Keyspace1/users.users_birth_year_idx-hc-1 because partitioner does not match org.apache.cassandra.dht.RandomPartitioner
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:145)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:123)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:118)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:360)
	at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:373)
	at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:431)


I tested with following sample data via cli:

create keyspace Keyspace1;
use Keyspace1;
create column family users with comparator=UTF8Type and
 column_metadata=[{column_name: full_name, validation_class: UTF8Type},
{column_name: email, validation_class: UTF8Type},
  {column_name: birth_year, validation_class: LongType, index_type: KEYS},
  {column_name: state, validation_class:  UTF8Type, index_type: KEYS}];
set users[1020][full_name] = 'Brandon Sanderson';
set users[1020][birth_year] = 1975;  
set users[1111][full_name] = 'Patrick Rothfuss';     
set users[1111][birth_year] = 1973;
get users where birth_year = 1973;
",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/12 22:50;jbellis;3738-v2.txt;https://issues.apache.org/jira/secure/attachment/12511328/3738-v2.txt","20/Jan/12 21:40;yukim;cassandra-1.0-3738.txt;https://issues.apache.org/jira/secure/attachment/12511324/cassandra-1.0-3738.txt",,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223810,,,Fri Jan 20 23:16:00 UTC 2012,,,,,,,,,,"0|i0gnmv:",95259,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Jan/12 17:33;yukim;In what version do you use to create sstable? If that's 1.0.4, secondary index is created in wrong way.(CASSANDRA-3540)
In that case, you have to drop index and rebuild index first.;;;","16/Jan/12 01:49;skamio;I've tested it with sstable created on 1.0.6.
;;;","20/Jan/12 21:40;yukim;SSTableExport trying to open secondary index sstable with storage partitioner causes this error.

Patch attached to use correct partitioner when exporting secondary index sstable.;;;","20/Jan/12 22:50;jbellis;Can we just move that code into SSTR.open so it's fixed for any use and not just SSTableExport?  v2 attached w/ that approach.;;;","20/Jan/12 23:09;yukim;v2 is much better.
I tested with it and worked as expected.
+1.;;;","20/Jan/12 23:16;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Its impossible to removetoken joining down node,CASSANDRA-3737,12538297,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,tivv,tivv,13/Jan/12 09:32,16/Apr/19 09:32,14/Jul/23 05:52,14/Jan/12 00:42,1.0.8,,,,,,0,,,,,,,"We have a node that incidentaly started to join cluster. Admins made it down quicky, so now it looks :
10.112.0.234    datacenter1 rack1       Down   Joining 46.83 GB        2,90%   15893087653239874101909022095979644640  
And I can't removetoken such a node:

 nodetool -h tap9600 removetoken 15893087653239874101909022095979644640
Exception in thread ""main"" java.lang.UnsupportedOperationException: Token not found.
	at org.apache.cassandra.service.StorageService.removeToken(StorageService.java:2376)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
	at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
",FreeBSD ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/12 23:13;brandon.williams;3737.txt;https://issues.apache.org/jira/secure/attachment/12510540/3737.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223803,,,Sat Jan 14 00:42:52 UTC 2012,,,,,,,,,,"0|i0gnmf:",95257,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"13/Jan/12 18:17;brandon.williams;You shouldn't need to remove this node; it should be removed after a minute or so since it is a fat client.;;;","13/Jan/12 21:50;brandon.williams;bq. You shouldn't need to remove this node; it should be removed after a minute or so since it is a fat client.

But we have a bug introduced by CASSANDRA-957; bootstrapping nodes start in hibernate, which is a dead state, resulting in setHasToken being set to true in handleMajorStateChange:

{noformat}
            epState.setHasToken(true); // fat clients won't have a dead state
{noformat}

That comment is no longer true. :(;;;","13/Jan/12 23:13;brandon.williams;Since the boolean value of hibernate has no bearing, only its presence in STATUS, there is no need for a bootstrapping node to ever use the hibernation state.;;;","14/Jan/12 00:30;vijay2win@yahoo.com;+1;;;","14/Jan/12 00:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-Dreplace_token leaves old node (IP) in the gossip with the token.,CASSANDRA-3736,12538279,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,cywjackson,cywjackson,13/Jan/12 01:13,16/Apr/19 09:32,14/Jul/23 05:52,19/Jan/12 21:08,1.0.8,1.1.0,,,,,1,,,,,,,"https://issues.apache.org/jira/browse/CASSANDRA-957 introduce a -Dreplace_token,

however, the replaced IP keeps on showing up in the Gossiper when starting the replacement node:

{noformat}
 INFO [Thread-2] 2012-01-12 23:59:35,162 CassandraDaemon.java (line 213) Listening for thrift clients...
 INFO [GossipStage:1] 2012-01-12 23:59:35,173 Gossiper.java (line 836) Node /50.56.59.68 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,174 Gossiper.java (line 804) InetAddress /50.56.59.68 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,175 StorageService.java (line 988) Node /50.56.59.68 state jump to normal
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 836) Node /50.56.58.55 has restarted, now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,176 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-12 23:59:35,177 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-12 23:59:45,048 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:00:06,062 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:01:06,320 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:01:06,321 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:01:16,106 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-13 00:01:37,121 Gossiper.java (line 632) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-01-13 00:02:37,352 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-13 00:02:37,353 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-13 00:02:47,158 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,162 Gossiper.java (line 818) InetAddress /50.56.58.55 is now dead.
 INFO [GossipStage:1] 2012-01-13 00:02:50,163 StorageService.java (line 1156) Removing token 122029383590318827259508597176866581733 for /50.56.58.55
{noformat}

in the above, /50.56.58.55 was the replaced IP.

tried adding the ""Gossiper.instance.removeEndpoint(endpoint);"" in the StorageService.java where the message 'Nodes %s and %s have the same token %s.  Ignoring %s"",' seems only have fixed this temporary. Here is a ring output:

{noformat}
riptano@action-quick:~/work/cassandra$ ./bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
50.56.59.68     datacenter1 rack1       Up     Normal  6.67 KB         85.56%  60502102442797279294142560823234402248      
50.56.31.186    datacenter1 rack1       Up     Normal  11.12 KB        14.44%  85070591730234615865843651857942052864 
{noformat}

gossipinfo:
{noformat}
$ ./bin/nodetool -h localhost gossipinfo
/50.56.58.55
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.58.55
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
/50.56.59.68
  LOAD:6835.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:11387.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT
{noformat}

Note that at 1 point earlier it seems to have been removed:

$ ./bin/nodetool -h localhost gossipinfo
/50.56.59.68
  LOAD:13815.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,60502102442797279294142560823234402248
  RELEASE_VERSION:1.0.7-SNAPSHOT
action-quick2/50.56.31.186
  LOAD:13725.0
  SCHEMA:00000000-0000-1000-0000-000000000000
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  RELEASE_VERSION:1.0.7-SNAPSHOT

riptano@action-quick2:~/work/cassandra$  INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 838) Node /50.56.58.55 is now part of the cluster

 INFO [GossipStage:1] 2012-01-13 01:03:30,073 Gossiper.java (line 804) InetAddress /50.56.58.55 is now UP

 INFO [GossipStage:1] 2012-01-13 01:03:30,074 StorageService.java (line 1017) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55",,davidstrauss,scode,tamarfraenkel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/12 03:43;vijay2win@yahoo.com;0001-CASSANDRA-3736.patch;https://issues.apache.org/jira/secure/attachment/12510456/0001-CASSANDRA-3736.patch",,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223785,,,Tue Nov 13 12:16:49 UTC 2012,,,,,,,,,,"0|i0gnlz:",95255,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"13/Jan/12 01:37;vijay2win@yahoo.com;Hi Jackson, Just a clarification is 50.56.58.55 up and cassandra is running?

 INFO [GossipStage:1] 2012-01-12 23:59:35,177 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55

This happens when the replaced node is running or resurrected. ;;;","13/Jan/12 01:50;davidstrauss;Hi Vijay,

I filed the ticket with DataStax that prompted this issue. I'm not 100% certain whether the node we replaced was fully and consistently offline from the point we performed the replacement. I *believe* it was, especially because the -Dreplace_token refuses to work if the node being replaced is online --- and we took no further action to bring the replaced node back (its VM wasn't initializing any network interfaces other than ""lo"").

Even if the replaced node comes back, it shouldn't be allowed to re-join the ring with a token already owned by an ""Up"" node. It should be subjected to the same condition -Dreplace_token is, where the token being used by the new ring member must be owned by a ""Down"" node.

David;;;","13/Jan/12 01:59;davidstrauss;Alternatively, it would be good for Cassandra to provide a convenient (nodetool) way to drop the ""Down"" IP when a token is simultaneously occupied by one ""Up"" IP and at least one ""Down"" IP.;;;","13/Jan/12 02:03;brandon.williams;bq. Alternatively, it would be good for Cassandra to provide a convenient (nodetool) way to drop the ""Down"" IP when a token is simultaneously occupied by one ""Up"" IP and at least one ""Down"" IP.

CASSANDRA-3337 is designed to handle these kinds of situations (where gossip is not doing the right thing naturally);;;","13/Jan/12 02:29;cywjackson;bq. is 50.56.58.55 up and cassandra is running?

no. the Cassandra on 50.56.58.55 was not UP/had shutdown. But the IP is available, though i don't think that matters.

so my test case was simply:
1) start 2 nodes (A , B).  With A being the seed, B bootstrap into it (by specifying a token)
2) stop B (after B had successfully joined)
3) start C with -Dcassandra.replace_token=<B's token>

continuing restarting C (without the replace_token param) could observe the behavior.;;;","13/Jan/12 03:43;vijay2win@yahoo.com;Simple patch to remove from SYSTEM_TABLE/RING_KEY when token is replaced.;;;","13/Jan/12 21:39;cywjackson;fix no good.

and to ensure fix is deployed, checked the compiled class:

$ javap -c -private -classpath ./build/classes/main/ org.apache.cassandra.db.SystemTable | grep ""updateToken(java.net.Inet"" -A10
public static synchronized void updateToken(java.net.InetAddress, org.apache.cassandra.dht.Token);
  Code:
   0:   aload_0
   1:   invokestatic    #51; //Method org/apache/cassandra/utils/FBUtilities.getLocalAddress:()Ljava/net/InetAddress;
   4:   if_acmpne       12
   7:   aload_1
   8:   invokestatic    #52; //Method removeToken:(Lorg/apache/cassandra/dht/Token;)V
   11:  return

to ensure removeToken is added (per the patch)

and the classpath of the jvm is using it:

 INFO 20:32:57,083 Classpath: ./bin/../conf:./bin/*../build/classes/main*:./bin/../build/classes/thrift:./bin/../lib/antlr-3.2.jar:./bin/../lib/avro-1.4.0-fixes.jar:./bin/../lib/avro-1.4.0-sources-fixes.jar:./bin/../lib/commons-cli-1.1.jar:./bin/../lib/commons-codec-1.2.jar:./bin/../lib/commons-lang-2.4.jar:./bin/../lib/compress-lzf-0.8.4.jar:./bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:./bin/../lib/guava-r08.jar:./bin/../lib/high-scale-lib-1.1.2.jar:./bin/../lib/jackson-core-asl-1.4.0.jar:./bin/../lib/jackson-mapper-asl-1.4.0.jar:./bin/../lib/jamm-0.2.5.jar:./bin/../lib/jline-0.9.94.jar:./bin/../lib/json-simple-1.1.jar:./bin/../lib/libthrift-0.6.jar:./bin/../lib/log4j-1.2.16.jar:./bin/../lib/servlet-api-2.5-20081211.jar:./bin/../lib/slf4j-api-1.6.1.jar:./bin/../lib/slf4j-log4j12-1.6.1.jar:./bin/../lib/snakeyaml-1.6.jar:./bin/../lib/snappy-java-1.0.4.1.jar:./bin/../lib/jamm-0.2.5.jar

log from the replacement node:
{noformat}
 INFO 20:34:27,856 Listening for thrift clients...
 INFO 20:35:28,750 Node /50.56.58.55 is now part of the cluster
 INFO 20:35:28,750 InetAddress /50.56.58.55 is now UP
 INFO 20:35:28,751 Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO 20:35:38,841 InetAddress /50.56.58.55 is now dead.
 INFO 20:35:58,852 FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
 INFO 20:36:59,786 Node /50.56.58.55 is now part of the cluster
 INFO 20:36:59,787 InetAddress /50.56.58.55 is now UP
 INFO 20:36:59,787 Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO 20:37:09,887 InetAddress /50.56.58.55 is now dead.
 INFO 20:37:29,898 FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip
{noformat}

;;;","13/Jan/12 22:00;brandon.williams;I suspect we have the same issue as I outlined in CASSANDRA-3737;;;","18/Jan/12 23:53;cywjackson;looks like fix from CASSANDRA-3747 got the fix.

the replacement node would still get this once:
 INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 834) Node /50.56.58.55 is now part of the cluster
 INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 800) InetAddress /50.56.58.55 is now UP
 INFO [GossipStage:1] 2012-01-18 23:45:56,413 StorageService.java (line 1016) Nodes /50.56.58.55 and action-quick2/50.56.31.186 have the same token 85070591730234615865843651857942052864.  Ignoring /50.56.58.55
 INFO [GossipTasks:1] 2012-01-18 23:46:05,805 Gossiper.java (line 814) InetAddress /50.56.58.55 is now dead.
 INFO [GossipTasks:1] 2012-01-18 23:46:26,819 Gossiper.java (line 628) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip

but its quiet after that.

the other node would receive the same info also:

 INFO [GossipTasks:1] 2012-01-18 23:45:57,486 Gossiper.java (line 628) FatClient /50.56.58.55 has been silent for 30000ms, removing from gossip

and the gossipinfo of those nodes are the matching:


$ ./bin/nodetool -h 50.56.31.186 gossipinfo
/50.56.59.68
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:6820.0
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,0
  SCHEMA:00000000-0000-1000-0000-000000000000
action-quick2/50.56.31.186
  RELEASE_VERSION:1.0.7-SNAPSHOT
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
  LOAD:11372.0
  SCHEMA:00000000-0000-1000-0000-000000000000

$ ./bin/nodetool -h 50.56.59.68 gossipinfo
action-quick/50.56.59.68
  SCHEMA:00000000-0000-1000-0000-000000000000
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:6820.0
  RPC_ADDRESS:50.56.59.68
  STATUS:NORMAL,0
/50.56.31.186
  SCHEMA:00000000-0000-1000-0000-000000000000
  RELEASE_VERSION:1.0.7-SNAPSHOT
  LOAD:11372.0
  RPC_ADDRESS:50.56.31.186
  STATUS:NORMAL,85070591730234615865843651857942052864
;;;","19/Jan/12 04:33;vijay2win@yahoo.com;Yes and the fix attached with this ticket will also remove the node from the System table, while replacing hence you wont even see the following message...

>>> INFO [GossipStage:1] 2012-01-18 23:45:56,412 Gossiper.java (line 800) InetAddress /50.56.58.55 is now UP

The problem is that we remove the node after 30 seconds.... Meanwhile the gossip will make the other node know about .55 and hence the message in the other node. 
The patch will fix this by removing the information from the System table in the first place instead of restart which triggering it to reappear. Can you try redoing the test? it doesn't appear back in my tests.;;;","19/Jan/12 19:11;brandon.williams;If CASSANDRA-3747 solved this, then I don't think there's any full solution here worth applying, since this is mostly just a cosmetic problem and not worth introducing a possibly destabilizing change over.  Anyone running into this can use CASSANDRA-3337 to remove it, or avoid replacing tokens.

+1 to this patch for 1.0 and trunk, though.;;;","19/Jan/12 21:08;vijay2win@yahoo.com;Committed both in 1.0 and trunk. Thanks!;;;","13/Nov/12 12:16;tamarfraenkel;I have the same issue with Cassandra 1.0.11 (used DataStax AMI).
I thought it was supposed to be solved already.
I see those messages on the node that was started using -Dcassandra.replace_token=<token>.

From time to time I also see 
{color:blue} 
 INFO [GossipTasks:1] 2012-11-13 12:26:38,195 Gossiper.java (line 818) InetAddress /<dead_node_ip> is now dead.
 INFO [GossipTasks:1] 2012-11-13 12:26:58,203 Gossiper.java (line 632) FatClient /<dead_node_ip> has been silent for 30000ms, removing from gossip
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 Gossiper.java (line 838) Node /<dead_node_ip> is now part of the cluster
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 Gossiper.java (line 804) InetAddress /<dead_node_ip> is now UP
 INFO [GossipStage:1] 2012-11-13 12:27:59,210 StorageService.java (line 1017) Nodes /<dead_node_ip> and /<replacing_node_ip> have the same token 113427455640312821154458202477256070484.  Ignoring /<dead_node_ip>

{color}

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ""Unable to create hard link"" SSTableReaderTest error messages",CASSANDRA-3735,12538275,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Jan/12 23:11,16/Apr/19 09:32,14/Jul/23 05:52,06/Feb/12 16:10,1.1.0,,,,,,0,,,,,,,"Sample failure (on Windows):

{noformat}
    [junit] java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\backups\Standard1-hc-1-Index.db c:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\data\Keyspace1\Standard1-hc-1-Index.db,command error Code: 1, command output: Cannot create a file when that file already exists.
    [junit]
    [junit]     at org.apache.cassandra.utils.CLibrary.exec(CLibrary.java:213)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188)
    [junit]     at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151)
    [junit]     at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)

    [junit]     at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 17:10:17,111 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/12 23:26;jbellis;0001-fix-generation-update-in-loadNewSSTables.patch;https://issues.apache.org/jira/secure/attachment/12510437/0001-fix-generation-update-in-loadNewSSTables.patch","05/Feb/12 02:52;scode;0002-remove-incremental-backups-before-reloading-sstables-v2.patch;https://issues.apache.org/jira/secure/attachment/12513276/0002-remove-incremental-backups-before-reloading-sstables-v2.patch","12/Jan/12 23:26;jbellis;0002-remove-incremental-backups-before-reloading-sstables.patch;https://issues.apache.org/jira/secure/attachment/12510438/0002-remove-incremental-backups-before-reloading-sstables.patch","05/Feb/12 06:17;scode;0003-reset-file-index-generator-on-reset.patch;https://issues.apache.org/jira/secure/attachment/12513285/0003-reset-file-index-generator-on-reset.patch",,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223781,,,Mon Feb 06 16:10:21 UTC 2012,,,,,,,,,,"0|i0gnlj:",95253,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"12/Jan/12 23:26;jbellis;Two parts:

first makes it so loadNewSSTables won't reset sstable generation to a lower value than it currently is.

second removes the (hard linked) sstables from the incremental backups directory, so that when loadNewSSTables links them back in they won't conflict.

patch 2 doesn't actually work on Windows because something is keeping the Data component open.  Java6 just says ""delete failed;"" java7's Files.delete says ""java.nio.file.FileSystemException: build\test\cassandra\data\Keyspace1\backups\Standard1-hc-1-Data.db: The process cannot access the file because it is being used by another process.""  (Even after I prototyped CASSANDRA-3734 to use Files.createLink instead of ProcessBuilder, so that's not the ""other process."")

I'm guessing it's counting something (what?) that opened the original Data, towards the link entry as well.  But, this should be enough to make the messages go away on Linux, which takes a more permissive view of deleting files opened elsewhere.;;;","23/Jan/12 10:23;slebresne;This seems to handle most of the warnings. However (on 1.0 branch):
* SSTableReaderTest doesn't pass on linux either. It ends up with the following trace:
{noformat}
    [junit] Testcase: testPersistentStatisticsFromOlderIndexedSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to delete /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups
    [junit] java.io.IOException: Failed to delete /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:237)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:167)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:260)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsFromOlderIndexedSSTable(SSTableReaderTest.java:251)
{noformat}
* I get a ""Largest generation seen in loaded sstables was 8, which may overlap with native sstable files (generation 8)"" warning during ColumnFamilyStoreTest, followed by  a (not surpising) 'Unable to create hard link', in testSliceByNamesCommandOldMetatada. I believe a quick fix could be to have clearUnsafe reset the generation for the CF. Another solution would be to make loadNewSSTables smarter/less fragile by having it 'reserve' a generation for each of the new file to load from the current generation and rename the loaded files accordingly.;;;","05/Feb/12 02:52;scode;Attaching new version of 0002* that works (but still with the left-overs already mentioned by jbellis/sylvain) post CASSANDRA-2794.;;;","05/Feb/12 06:17;scode;Correction, the conversion to post-2794 did remove the failure of the SSTableReaderTest. Or at least it's no longer happening for me on trunk with the attached patch (v2). Likely because it's only removing specific sstable components given by the iterator, rather than trying to recursively delete backups, but I don't pretend to understand exactly what the history of changes is that caused it to start failing to delete it to begin with.

With repsect to the 'Largest generation seen...' warning, I get that too, but I don't see any subsequent hard link creation failures, nor do I understand why I would if the files are created without using the counter and the quick fix just suppresses the warning? But I'm probably missing something. I do have failing hard linking in ThriftValidationTest though. Maybe this is a side-effect that you're referring to?

In any case, attaching the trivial (if I understood the suggestion correctly) reset patch that supresses the warning.


;;;","06/Feb/12 16:10;slebresne;I still had one remaining link error with v2 in CFSTest, but that was because tests runs with incremental backup and 2 tests were reusing the same CF with just a clearUnsafe in between, so the backup were kept around. I've moved the removal of backups from the SSTableReader.clearAndLoad() method introduced by the patches to CFS.clearUnsafe() directly to avoid this. I took the liberty to commit with that last change. I don't get any link related exceptions anymore.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Once a host has been hinted to, log messages for it repeat every 10 mins even if no hints are delivered",CASSANDRA-3733,12538223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,12/Jan/12 16:39,16/Apr/19 09:32,14/Jul/23 05:52,12/Jan/12 17:03,0.8.10,1.0.7,,,,,0,,,,,,,"{noformat}
 INFO 15:36:03,977 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:36:03,978 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:46:31,248 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:46:31,249 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 15:56:29,448 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 15:56:29,449 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:06:09,949 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:06:09,950 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
 INFO 16:16:21,529 Started hinted handoff for token: 170141183460469231731687303715884105726 with IP: /10.179.111.137
 INFO 16:16:21,530 Finished hinted handoff of 0 rows to endpoint /10.179.111.137
{noformat}

Introduced by CASSANDRA-3554.  The problem is that until a compaction on hints occurs, tombstones are present causing the isEmpty() check to be false.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5068,,,,,,,,,,,,,,"12/Jan/12 16:57;brandon.williams;3733.txt;https://issues.apache.org/jira/secure/attachment/12510388/3733.txt",,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223729,,,Thu Jan 12 17:17:17 UTC 2012,,,,,,,,,,"0|i0gnkn:",95249,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Jan/12 16:43;jbellis;Why doesn't this take care of it?

{code}
.       if (rowsReplayed > 0)
        {
            hintStore.forceFlush();
            try
            {
                CompactionManager.instance.submitMaximal(hintStore, Integer.MAX_VALUE).get();
            }
            catch (Exception e)
            {
                throw new RuntimeException(e);
            }
        }
{code};;;","12/Jan/12 16:51;brandon.williams;Because the flush isn't blocking:

{noformat}
 INFO 16:50:36,248 Started hinted handoff for token: 113427455640312821154458202477256070484 with IP: /10.179.64.227
 INFO 16:50:36,265 Enqueuing flush of Memtable-HintsColumnFamily@1721309039(0/0 serialized/live bytes, 1 ops)
 INFO 16:50:36,266 Writing Memtable-HintsColumnFamily@1721309039(0/0 serialized/live bytes, 1 ops)
 INFO 16:50:36,267 Nothing to compact in HintsColumnFamily.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO 16:50:36,268 Finished hinted handoff of 1 rows to endpoint /10.179.64.227
 INFO 16:50:36,272 Completed flushing /var/lib/cassandra/data/system/HintsColumnFamily-hc-2-Data.db (100 bytes)
{noformat};;;","12/Jan/12 16:57;brandon.williams;Patch to make flushes blocking.;;;","12/Jan/12 17:00;jbellis;+1;;;","12/Jan/12 17:01;jbellis;the tombstone buffering on the next attempt could cause OOMs similar to CASSANDRA-3681, as well.;;;","12/Jan/12 17:03;brandon.williams;Committed.;;;","12/Jan/12 17:05;jbellis;Can you commit to 0.8 too?;;;","12/Jan/12 17:17;brandon.williams;Done in aacbb1ca9c0e7a1992dfc92c096dd885ab149154;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update POM generation after migration to git,CASSANDRA-3732,12538192,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stephenc,slebresne,slebresne,12/Jan/12 13:26,16/Apr/19 09:32,14/Jul/23 05:52,03/Apr/12 16:41,1.0.10,1.1.0,,Packaging,,,0,,,,,,,,,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223698,,,Thu Feb 20 15:46:11 UTC 2014,,,,,,,,,,"0|i0gnjz:",95246,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"03/Apr/12 08:32;stephenc;https://github.com/apache/cassandra/pull/9;;;","03/Apr/12 15:32;slebresne;Why are those information useful?
Basically I have no clue how to test this, and I don't want to commit it without testing if this has a risk of breaking something for someone. Typically, there doesn't seem to be enough information in those metadata to know which git branch this is referring to.;;;","03/Apr/12 15:37;stephenc;Shortcoming of the Maven SCM URI format is that Git branch info is lost.

The current SCM info points to SVN which is just plain wrong at this point!;;;","03/Apr/12 15:39;stephenc;No risk of breaking anything for anyone. You are required to include the scm info in any artifacts that get published to Maven Central... the actual info will not be used by any programs... at best by a human;;;","03/Apr/12 16:41;slebresne;Ok +1 then, committed, thanks
;;;","20/Feb/14 15:46;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/cassandra/pull/9
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unit tests failure,CASSANDRA-3727,12538033,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,slebresne,slebresne,11/Jan/12 09:39,16/Apr/19 09:32,14/Jul/23 05:52,12/Jan/12 09:25,1.0.7,,,Legacy/Testing,,,0,,,,,,,"On current 1.0 branch (and on my machine: Linux), I have the following unit test failures:
* CliTest and EmbeddedCassandraTest: they both first kind of pass (JUnit first prints a message with no failures in it), then hang until JUnit timeout and fails with a 'Timeout occurred'. In other word, the tests themselves are passing, but something they do prevents the process to exit cleanly leading to a JUnit timeout. I don't want to discard that as not a problem, because if something can make the process not exit cleanly, this can be a pain for restarts (and in particular upgrades) and hence would be basically a regression. I'm marking the ticket as blocker (for the release of 1.0.7) mostly because of this one.
* SystemTableTest: throws an assertionError. I haven't checked yet, so that could be an easy one to fix.
* RemoveTest: it fails, saying that '/127.0.0.1:7010 is in use by another process' (consistently). But I have no other process running on port 7010. It's likely just of problem of the test, but it's new and in the meantime removes are not tested.
* I also see a bunch of stack trace with errors like:
{noformat}
    [junit] ERROR 10:01:59,007 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Index.db to /home/mcmanus/Git/cassandra/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Index.db (errno 17)
{noformat}
(with SSTableReaderTest). This does not make the tests fail, but it is still worth investigating. It may be due to CASSANDRA-3101.",,scode,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/12 17:54;jbellis;3727.txt;https://issues.apache.org/jira/secure/attachment/12510212/3727.txt","11/Jan/12 10:24;xedin;CASSANDRA-3727-CliTest-timeout-fix.patch;https://issues.apache.org/jira/secure/attachment/12510171/CASSANDRA-3727-CliTest-timeout-fix.patch",,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223539,,,Thu Jan 19 15:01:37 UTC 2012,,,,,,,,,,"0|i0gnhz:",95237,,,,,Critical,,,,,,,,,,,,,,,,,"11/Jan/12 09:56;xedin;CliTest and others should be timeouting because of newly added shutdown hook.;;;","11/Jan/12 10:07;slebresne;Are you saying that they should be timeouting as in 'having them timeouting is a feature' or are you just pointing out the likely source of the problem? In the latter, do  you remember the ticket that introduced those (or better, have a fix for it)?;;;","11/Jan/12 10:11;xedin;I'm saying that it's likely source of the problem and for CliTest fix would be pretty straightforward, make CliMain to disconnect properly after tests are done (I'm going to attach a patch fixing patch in a few minutes to this ticket), I don't know about other tests tho. ;;;","11/Jan/12 10:24;xedin;fixed CliTest timeout by correctly closing transport connection which allows Cassandra shutdown hook to proceed without waiting for RPC connections to close.;;;","11/Jan/12 14:20;jbellis;Hmm.  Can we fix the thrift shutdown instead to not wait for sockets to be closed nicely?;;;","11/Jan/12 14:22;xedin;I guess we can, but I thought that it's kind of feature that it does wait...;;;","11/Jan/12 15:58;jbellis;Definitely not, otherwise there is no way to shut down if clients stay connected;;;","11/Jan/12 15:59;jbellis;(The thrift shutdown was introduced for CASSANDRA-3335);;;","11/Jan/12 16:03;jbellis;bq. Can we fix the thrift shutdown instead to not wait for sockets to be closed nicely

I'm not sure why this isn't how it already works.  From CustomTThreadPoolServer.WorkerProcess:

{code}
.               while (!stopped_ && processor.process(inputProtocol, outputProtocol))
                {
                    inputProtocol = inputProtocolFactory_.getProtocol(inputTransport);
                    outputProtocol = outputProtocolFactory_.getProtocol(outputTransport);
                }
{code}

In other words, as soon as stopped_ is set (by the stop() method), each thread should finish the current request but not accept more.;;;","11/Jan/12 16:09;xedin;It's easy to test - run cassandra and in other terminal session connect to it using CLI and try Ctrl-C Cassandra server without closing CLI.;;;","11/Jan/12 17:54;jbellis;So, I was over-optimistic in CASSANDRA-3335 when I thought I could get by without a MessagingService shutdown method.  The problem is that although my changes there do work to prevent accepting new connections, and to stop work on existing connections *after the first command post-shutdown*, that's not good enough in this case since the client is just sitting on its connection and never sends another command.

So, this patch renames MS.waitForCallbacks() back to shutdown(), and refuses to add new callbacks after that.  However, the analysis on 3335 that there's no good way to deal with an exception here, so we do this instead in ExpiringMap:

{code}
.   public V put(K key, V value, long timeout)
    {
        if (shutdown)
        {
            // StorageProxy isn't equipped to deal with ""I'm nominally alive, but I can't send any messages out.""
            // So we'll just sit on this thread for until the rest of the server shutdown completes.
            //
            // See comments in CustomTThreadPoolServer.serve, CASSANDRA-3335, and CASSANDRA-3727.
            try
            {
                Thread.sleep(Long.MAX_VALUE);
            }
            catch (InterruptedException e)
            {
                throw new AssertionError(e);
            }
        }
        CacheableObject<V> previous = cache.put(key, new CacheableObject<V>(value, timeout));
        return (previous == null) ? null : previous.getValue();
    }
{code}

Then, we switch the Thrift executor (and all DTPE instances) to use daemon threads, and remove the wait-for-WorkerProcess threads code from CustomTThreadPoolServer.serve:

{code}
.       // Thrift's default shutdown waits for the WorkerProcess threads to complete.  We do not,
        // because doing that allows a client to hold our shutdown ""hostage"" by simply not sending
        // another message after stop is called (since process will block indefinitely trying to read
        // the next meessage header).
        //
        // The ""right"" fix would be to update thrift to set a socket timeout on client connections
        // (and tolerate unintentional timeouts until stopped_ is set).  But this requires deep
        // changes to the code generator, so simply setting these threads to daemon (in our custom
        // CleaningThreadPool) and ignoring them after shutdown is good enough.
        //
        // Remember, our goal on shutdown is not necessarily that each client request we receive
        // gets answered first [to do that, you should redirect clients to a different coordinator
        // first], but rather (1) to make sure that for each update we ack as successful, we generate
        // hints for any non-responsive replicas, and (2) to make sure that we quickly stop
        // accepting client connections so shutdown can continue.  Not waiting for the WorkerProcess
        // threads here accomplishes (2); MessagingService's shutdown method takes care of (1).
        //
        // See CASSANDRA-3335 and CASSANDRA-3727.
{code}

Finally, this patch also updates Memtable's memorymeter thread to use the newly daemonized DTPE for good measure, since there's no reason to ever block shutdown for that either.;;;","11/Jan/12 18:12;brandon.williams;+1;;;","11/Jan/12 18:50;jbellis;committed; leaving open for other test failures;;;","11/Jan/12 19:54;jbellis;SystemTableTest failure taken care of on CASSANDRA-3579;;;","11/Jan/12 20:01;jbellis;bq. RemoveTest: it fails, saying that '/127.0.0.1:7010 is in use by another process' (consistently). 

FWIW, I get a timeout instead (on Windows).;;;","11/Jan/12 20:05;xedin;I get the same thing Sylvain does in RemoveTest : '/127.0.0.1:7010 is in use by another process' on Mac OS X.;;;","11/Jan/12 20:31;brandon.williams;Same 'in use by another process' under linux.  There is definitely no other process.;;;","11/Jan/12 22:04;jbellis;RemoveTest is trying to stop/start MessagingService multiple times in the same suite (see setup/tearDown methods).  My guess is that worked well enough pre-CASSANDRA-3335.  I'll see about making it happy again, although this feels fragile.;;;","11/Jan/12 23:01;jbellis;Pushed 452ddf63c530fc573551e6fc9c79c1a876f11dd0 with the socket teardown code added back.  Now it's hitting {{java.io.IOException: Failed to delete c:\Users\Jonathan\projects\cassandra\git\build\test\cassandra\commitlog\CommitLog-1326319330071.log}}.  Progress?;;;","12/Jan/12 02:46;jbellis;User error...  it was my paused Cassandra instance in my IDE holding that CL segment open.  With that figured out, I also pushed 1e750138177e9cd9cbd6537451a4b5cd301dab3a which allows MS to be restarted by RemoveTest.

All the tests now pass for me, except RecoveryManagerTruncateTest which has failed on Windows for 0.8+.;;;","12/Jan/12 09:25;slebresne;I still had intermittent failure of columnFamilyStoreTest, but because SystemTable.isIndexBuilt() was suffering from the same 'I forgot to expunge tombstones' problem than SystemTable.loadTokens(). I took on myself to commit the same fix for that instance directly (and check no other method had this problem).

So closing this as all tests are now passing.

However I'd be interested to know if anyone else is seeing the 'unable to create link' stack trace during tests, because if so we should probably open another ticket to investigate.;;;","19/Jan/12 14:56;butlermh;Building Cassandra 1.0.7 I get one test failure:

{code}
Class org.apache.cassandra.db.compaction.CompactionsTest
Name	Tests	Errors	Failures	Time(s)	Time Stamp	Host
CompactionsTest	1	1	0	0.000	2012-01-19T12:52:18	mbutler-OptiPlex-990
Tests
Name	Status	Type	Time(s)
testSuperColumnCompactions	Error	Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
	0.001
{code}

and also see exceptions being thrown saying it was unable to create a hard link
{code}

  [junit]  WARN 13:53:33,357 Overriding RING_DELAY to 1000ms
    [junit] ERROR 13:53:39,794 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:39,796 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:39,797 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:39,798 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-2-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-2-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Standard1-hc-2-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Standard1-hc-2-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,501 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,502 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,504 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,505 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,802 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,802 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-2-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-2-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1.626972746864617465-hc-2-Digest.sha1 to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1.626972746864617465-hc-2-Digest.sha1 (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ERROR 13:53:41,804 Unable to create hard link
    [junit] com.sun.jna.LastErrorException: errno was 17
    [junit] 	at org.apache.cassandra.utils.CLibrary.link(Native Method)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:146)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ERROR 13:53:41,805 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.unchecked(FBUtilities.java:689)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to create hard link from build/test/cassandra/data/Keyspace1/Indexed1-hc-1-Filter.db to /home/mbutler/workspace/vscc/vscc-oss-components/cassandra-1.0.7/build/test/cassandra/data/Keyspace1/backups/Indexed1-hc-1-Filter.db (errno 17)
    [junit] 	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:160)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:833)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.runMayThrow(DataTracker.java:161)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	... 8 more
    [junit] ------------- ---------------- ---------------
{code}

Any suggestions on a fix?;;;","19/Jan/12 15:01;jbellis;See CASSANDRA-3735 for the hard link fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh and cassandra-cli show keys differently for data created via stress tool,CASSANDRA-3726,12538024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,cdaw,cdaw,11/Jan/12 07:39,16/Apr/19 09:32,14/Jul/23 05:52,20/Jan/12 19:25,1.0.8,,,,,,0,,,,,,,"{code}

// Run: stress --operation=INSERT --num-keys=5  --columns=2 --consistency-level=QUORUM --column-size=1 --threads=1 --replication-factor=1 --nodes=localhost

// cqlsh
cqlsh:Keyspace1> select * from Standard1;
 KEY,3 | C0,c | C1,c | 
 KEY,0 | 
 KEY,2 | C0,c | C1,c | 
 KEY,1 | C0,c | C1,c | 
 KEY,4 | C0,c | C1,c | 

cqlsh:Keyspace1> describe columnfamily Standard1;

CREATE COLUMNFAMILY Standard1 (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=ascii AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=1.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replicate_on_write=True;


// cassandra-cli
[default@Keyspace1] list Standard1;    
Using default limit of 100
-------------------
RowKey: 33
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)
-------------------
RowKey: 30
-------------------
RowKey: 32
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 31
=> (column=C0, value=63, timestamp=1326259960704)
=> (column=C1, value=63, timestamp=1326259960704)
-------------------
RowKey: 34
=> (column=C0, value=63, timestamp=1326259960705)
=> (column=C1, value=63, timestamp=1326259960705)

[default@Keyspace1] describe Standard1;
    ColumnFamily: Standard1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/12 16:54;thepaul;3726.patch.2.txt;https://issues.apache.org/jira/secure/attachment/12511273/3726.patch.2.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223530,,,Fri Jan 20 19:25:01 UTC 2012,,,,,,,,,,"0|i0gnhj:",95235,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Jan/12 16:52;thepaul;Added a bunch to cqlsh's output formatting so that certain types (in particular, blob) are shown in a more human-readable way. Used colors (when enabled) to help distinguish hex strings from text strings, and to distinguish control characters in text strings.

The attached patch contains the changes, or the 3726 branch in my github fork can be used:

https://github.com/thepaul/cassandra/commits/3726;;;","20/Jan/12 00:01;jbellis;Can you rebase on top of 1.0?  I think 3586/3587 conflict.;;;","20/Jan/12 16:54;thepaul;So rebased, on top of cassandra-1.0 instead of trunk since that's where you seem to be putting it. New branch on my github is ""rebased/3726/1"", or new patch attached.

https://github.com/thepaul/cassandra/commits/rebased/3726/1;;;","20/Jan/12 17:38;jbellis;1e6defcaba90b7313d87c2626cbb6ae418b40567 still doesn't apply cleanly to 1.0 for me.

Incidentally, why is switching ""out=sys.stdout"" to ""out=None"" and then special casing none in the method body an improvement?  Are you expecting stdout to change?;;;","20/Jan/12 17:39;jbellis;... the new squashed patch applies fine to 1.0 though, so no need to rebase again.;;;","20/Jan/12 17:52;thepaul;sys.stdout can change, yeah, and that might happen depending on how we decide to implement output capture vis à vis CASSANDRA-3479. Note that's different from just dup2-ing another stream onto fd 0.

bq. 1e6defcaba90b7313d87c2626cbb6ae418b40567 still doesn't apply cleanly to 1.0 for me.

Are you sure? Its only parent is fc6103966cbe90c96e75f4b52c10376b9df468d7, which is currently the head of cassandra-1.0 on the git-wip-us.apache.org repo.;;;","20/Jan/12 19:25;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh missing help for INSERT,CASSANDRA-3718,12537933,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,10/Jan/12 18:03,16/Apr/19 09:32,14/Jul/23 05:52,10/Jan/12 20:23,1.0.7,,,Legacy/Tools,,,0,cqlsh,,,,,,this must have been overlooked.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 19:10;thepaul;3718.patch.txt;https://issues.apache.org/jira/secure/attachment/12510081/3718.patch.txt",,,,,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223439,,,Tue Jan 10 20:23:07 UTC 2012,,,,,,,,,,"0|i0gnef:",95221,,,,,Low,,,,,,,,,,,,,,,,,"10/Jan/12 19:10;thepaul;Fixed in https://github.com/thepaul/cassandra/commit/25f5a63dc3f0a34266d041bcb66a0e2a5ab7c7d9 ; patch attached here.;;;","10/Jan/12 20:23;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Show Schema; inserts an extra comma in column_metadata",CASSANDRA-3714,12537832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,j.casares,j.casares,09/Jan/12 23:43,16/Apr/19 09:32,14/Jul/23 05:52,10/Jan/12 17:27,1.0.7,,,,,,0,cli,,,,,,"create column family inode
  with column_type = 'Standard'
  and comparator = 'DynamicCompositeType(t=>org.apache.cassandra.db.marshal.TimeUUIDType,s=>org.apache.cassandra.db.marshal.UTF8Type,b=>org.apache.cassandra.db.marshal.BytesType)'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 1000000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 60
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and comment = 'Stores file meta data'
  and column_metadata = [
    {column_name : 'b@70617468',
    validation_class : BytesType,
    index_name : 'path',
    index_type : 0,
},
    {column_name : 'b@73656e74696e656c',
    validation_class : BytesType,
    index_name : 'sentinel',
    index_type : 0,
},
    {column_name : 'b@706172656e745f70617468',
    validation_class : BytesType,
    index_name : 'parent_path',
    index_type : 0,
}];

That's that was outputted when I ran show schema. When I tried it on a new cluster, it failed since the commas after 'index_type: 0' were present.

Proposed fixes:
1. Allow trailing commas
2. Do not output trailing commas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 00:45;yukim;CASSANDRA-1.0-3714.txt;https://issues.apache.org/jira/secure/attachment/12509982/CASSANDRA-1.0-3714.txt",,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223338,,,Tue Jan 10 17:27:59 UTC 2012,,,,,,,,,,"0|i0gncf:",95212,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"10/Jan/12 00:44;j.casares;Deleting comment. Wrong window. :);;;","10/Jan/12 00:45;yukim;Confirmed bug only on cassandra-1.0 branch.
Patch attached to eliminate last comma from hash representation.;;;","10/Jan/12 00:53;j.casares;Tested in 0.8.8 and saw this:

{noformat}
create column family inode
  with column_type = 'Standard'
  and comparator = 'DynamicCompositeType(t=>org.apache.cassandra.db.marshal.TimeUUIDType,b=>org.apache.cassandra.db.marshal.BytesType,s=>org.apache.cassandra.db.marshal.UTF8Type)'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and memtable_operations = 0.29062499999999997
  and memtable_throughput = 62
  and memtable_flush_after = 1440
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and column_metadata = [
    {column_name : 'b@70617468',
    validation_class : BytesType,
    index_name : 'path',
    index_type : 0},
    {column_name : 'b@73656e74696e656c',
    validation_class : BytesType,
    index_name : 'sentinel',
    index_type : 0},
    {column_name : 'b@706172656e745f70617468',
    validation_class : BytesType,
    index_name : 'parent_path',
    index_type : 0}];
{noformat}

0.8.8 is not affected.

Edit: Checked 0.8.9 and the cassandra-0.8 branch as well and saw the same thing.;;;","10/Jan/12 17:27;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't cleanup after I moved a token.,CASSANDRA-3712,12537765,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,hnicol,hnicol,09/Jan/12 14:37,16/Apr/19 09:32,14/Jul/23 05:52,13/Feb/12 23:30,1.0.8,,,,,,0,,,,,,,"Before cleanup failed, I moved one node's token.
My cluster had 10GB data on 2 nodes. Data repartition was bad, tokens were 165[...] and 155[...].
I moved 155 to 075[...], then adjusted to 076[...]. The moves were correctly processed, with no exception.
But then, when I wanted to cleanup, it failed and keeps failing, on both nodes.

Other maintenance procedures like repair, compact or scrub work.
All the data is in the URLs CF.

Example session log:
nodetool cleanup fails:
$ ./nodetool --host cnode1 cleanup
Error occured during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError
 at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
 at java.util.concurrent.FutureTask.get(FutureTask.java:83)
 at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
 at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:237)
 at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:958)
 at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1604)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
 at sun.rmi.transport.Transport$1.run(Transport.java:159)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError
 at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
 at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
 at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
 at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
 at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
 at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
 at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 ... 3 more


The server log looks like this:
 INFO [CompactionExecutor:260] 2012-01-09 14:08:41,716 CompactionManager.java (line 702) Cleaning up SSTableReader(path='/ke/cassandra/data/kev3/URLs-hc-457-Data.db')
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,220 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 156787206 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,226 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:47,236 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [pool-1-thread-1] 2012-01-09 14:08:51,003 Memtable.java (line 180) CFS(Keyspace='kev3', ColumnFamily='URLs.URLs_1_idx') liveRatio is 7.692510757866615 (just-counted was 4.512127842861816).  calculation took 8648ms for 97329 columns
 INFO [FlushWriter:23] 2012-01-09 14:08:54,360 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-143-Data.db (26375495 bytes)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:55,566 GCInspector.java (line 123) GC for ParNew: 206 ms for 1 collections, 934108624 used; max is 2034237440
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,289 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 188842513 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,297 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:57,297 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:57,619 GCInspector.java (line 123) GC for ParNew: 402 ms for 2 collections, 981893424 used; max is 2034237440
 INFO [FlushWriter:23] 2012-01-09 14:09:05,944 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-144-Data.db (31755390 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 174605041 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
 INFO [FlushWriter:23] 2012-01-09 14:09:06,447 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
ERROR [CompactionExecutor:260] 2012-01-09 14:09:06,448 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:260,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
	at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
	at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)




","java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

Ubuntu 10.04.2 LTS 64-Bit
RAM: 2GB / 1GB free
Data partition: 80% free on the most used server.",psanford,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/12 15:44;yukim;0001-Add-flush-and-cleanup-race-test.patch;https://issues.apache.org/jira/secure/attachment/12513825/0001-Add-flush-and-cleanup-race-test.patch","10/Feb/12 18:46;yukim;0002-Acquire-lock-when-updating-index.patch;https://issues.apache.org/jira/secure/attachment/12514136/0002-Acquire-lock-when-updating-index.patch","10/Feb/12 21:26;jbellis;3712-v3.txt;https://issues.apache.org/jira/secure/attachment/12514159/3712-v3.txt",,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223271,,,Mon Feb 13 23:30:35 UTC 2012,,,,,,,,,,"0|i0gnbj:",95208,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Jan/12 23:16;jbellis;We'll look into the root cause, but in the meantime, restarting the node should fix this.;;;","10/Jan/12 13:32;hnicol;I restarted my nodes, but it didn't fix this.;;;","10/Jan/12 17:30;jbellis;Then you'll probably need to drop your indexes, then rebuild after the cleanup.;;;","11/Jan/12 17:07;hnicol;Thanks, that fixed cleanup.;;;","08/Feb/12 15:44;yukim;When updating/deleting CFS backed secondary index, currently it doesn't acquire Memtable lock. So when flush and cleanup occurred at the same time on indexed column family, there is a chance of getting this AssertionError.

Test case attached to reproduce the same error. Note that test does not fail always, so you may run several times to see the error.

To fix this, I added CFS#applyDirect method which just acquire and release lock before/after CFS update, and call it from KeysIndex.

Both patches are for 1.0 branch.;;;","08/Feb/12 23:47;jbellis;A couple comments:

- I can't get the new test to fail after a dozen tries. If there isn't a way to make it more robust (say, with explicit sleeps) maybe we should just leave that out.
- Currently the switch locking is done by the callers of the SIM methods, i.e., Table.apply and Table.indexRow. Locking at the column level is not sufficient there, but doing it in both places is redundant. So maybe the right place to lock here would be in the doCleanupCompaction method.;;;","10/Feb/12 18:52;yukim;bq. I can't get the new test to fail after a dozen tries. If there isn't a way to make it more robust (say, with explicit sleeps) maybe we should just leave that out.

In my env, it fails 1/3 or 1/4 try. I cannot think of better test program, so you can leave it out.

bq. Currently the switch locking is done by the callers of the SIM methods, i.e., Table.apply and Table.indexRow. Locking at the column level is not sufficient there, but doing it in both places is redundant. So maybe the right place to lock here would be in the doCleanupCompaction method.

You are right. Previous patch acquires lock too often. I placed lock/unlock inside doCleanupCompaction in newer patch.
In order to do that, I have to expose Table.switchlock to public, but I don't know if that is the right way.;;;","10/Feb/12 21:26;jbellis;The new patch acquires the switchlock for the entire cleanup, so no flushes can happen.  For large sstables this could cause a problem.  How about attached v3, is that sufficient for it to be safe?;;;","13/Feb/12 18:05;yukim;I ran my unit test enough and I see no error.
+1;;;","13/Feb/12 23:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsustainable Thread Accumulation in ParallelCompactionIterable.Reducer ThreadPoolExecutor,CASSANDRA-3711,12537700,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,maedhroz,maedhroz,08/Jan/12 23:46,16/Apr/19 09:32,14/Jul/23 05:52,10/Jan/12 08:29,1.0.7,,,,,,0,compaction,memory_leak,threading,threads,,,"With multithreaded compaction enabled, it looks like Reducer creates a new thread pool for every compaction.  These pools seem to just sit around - i.e. ""executor.shutdown()"" never gets called and the Threads live forever waiting for tasks that will never come.  For instance...


Name: CompactionReducer:1
State: TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@72938aea
Total blocked: 0  Total waited: 1

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1043)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1103)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
java.lang.Thread.run(Thread.java:722)
","- Linux version 2.6.32-71.29.1.el6.x86_64 (mockbuild@c6b5.bsys.dev.centos.org) (gcc version 4.4.4 20100726 (Red Hat 4.4.4-13) (GCC) ) #1 SMP Mon Jun 27 19:49:27 BST 2011
- java version ""1.7.0_01"" / Java(TM) SE Runtime Environment (build 1.7.0_01-b08) / Java HotSpot(TM) 64-Bit Server VM (build 21.1-b02, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/12 09:53;maedhroz;3711.2.txt;https://issues.apache.org/jira/secure/attachment/12509889/3711.2.txt","09/Jan/12 04:12;jbellis;3711.txt;https://issues.apache.org/jira/secure/attachment/12509875/3711.txt",,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,223206,,,Tue Jan 10 08:29:22 UTC 2012,,,,,,,,,,"0|i0gnb3:",95206,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"09/Jan/12 04:12;jbellis;You're absolutely right.  Patch attached.;;;","09/Jan/12 09:50;maedhroz;Would it be useful to just create the executor once and reuse it for all compactions, then get rid of it in a shutdown hook or something similar?  I've attached a patch, 3711.2.txt, that does roughly that, if you want to take a look.;;;","09/Jan/12 09:53;maedhroz;I guess I should grant the license, just in case ;);;;","09/Jan/12 17:37;jbellis;That changes the behavior, though, since now you have a single pool of AvailableProcessors across all compactions, rather than one per compaction.  Increasing the thread count proportionate to concurrent compactions setting isn't a silver bullet either, since you would still have the possibility of a large compaction ""starving"" out smaller ones, which was the problem that concurrent compactions were introduced to solve.;;;","09/Jan/12 18:28;maedhroz;Yeah, I think you're right.  In retrospect, my idea would only have been useful (maybe) if the compactions were CPU-bound, which I guess is not the case.

I can't wait for this release, because I can stop doing rolling restarts every few days!;;;","10/Jan/12 08:29;slebresne;+1 on v1, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SelectStatement start/end key are not set correctly when a key alias is involved,CASSANDRA-3700,12537365,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,05/Jan/12 17:32,16/Apr/19 09:32,14/Jul/23 05:52,10/Jan/12 18:56,1.0.7,,,Legacy/CQL,,,0,cql,,,,,,"start/end key are set by antlr in WhereClause, but this depends on the ""KEY"" keyword.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 18:37;jbellis;3700-case-insensitivity.txt;https://issues.apache.org/jira/secure/attachment/12510077/3700-case-insensitivity.txt","06/Jan/12 21:07;xedin;CASSANDRA-3700.patch;https://issues.apache.org/jira/secure/attachment/12509714/CASSANDRA-3700.patch",,,,,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222873,,,Tue Jan 10 18:56:46 UTC 2012,,,,,,,,,,"0|i0gn6f:",95185,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Jan/12 17:32;jbellis;My suggestion would be to have antlr just generate a list of expressions and let QueryProcessor determine which are for the key and which are for columns.;;;","05/Jan/12 23:00;jbellis;This also affects 0.8, but if people haven't hit the problem there yet then they're probably not going to.  (And the workaround of ""just use KEY instead"" is straightforward.)  Let's fix this in 1.0+.;;;","06/Jan/12 16:08;jbellis;EntityType is basically obsolete now isn't it?;;;","06/Jan/12 16:11;xedin;we will need it in the situation where KEY is involved, we probably can use default alias if it's guaranteed to be KEY tho...;;;","06/Jan/12 16:20;jbellis;Right, it either has to be the key alias or KEY.;;;","06/Jan/12 16:32;slebresne;Is there a real advantage to allow the use of key alias *or* KEY. That is, why don't we just allow the key alias, and have this be KEY (which we already do) by default for CF created through thrift without a proper key_alias (case that is hopefully bound to disappear). The fact that we reserve KEY (and key too I believe) is kind of limiting in that you cannot use KEY for some other column.
Besides, the KEY notation will be slightly less meaningful post-2474 because it will not necessarily correspond to the primary key anymore (i.e, it may correspond only to the first column column that compose the PRIMARY KEY).;;;","06/Jan/12 16:44;jbellis;bq. why don't we just allow the key alias, and have this be KEY (which we already do) by default for CF created through thrift without a proper key_alias 

That makes a lot of sense to me.;;;","06/Jan/12 21:08;xedin;updated patch with does not special-case KEY and removes EntityType.;;;","06/Jan/12 23:06;jbellis;I think this is obsolete now (in QP.process) since we use the ""realKeyAlias"" in WhereClause now to check if in fact an identifier is a key reference:

{code}
.               if (select.getKeys().size() > 0)
                    validateKeyAlias(metadata, select.getKeyAlias());
{code};;;","06/Jan/12 23:19;xedin;<key> IN (.., ..., ...) condition is treated directly without relaying on extractKeysFromColumns() so we still need this check in such case.;;;","07/Jan/12 05:37;jbellis;I guess that's okay, although we will probably want generalize IN to allow <column> IN (...) as well at some point.

+1 on this patch;;;","07/Jan/12 14:40;xedin;Committed.;;;","10/Jan/12 18:35;jbellis;Ugh, we should actually continue to allow using KEY in 1.0.x.  Working on a patch for that and case-insensitivity for the aliases.;;;","10/Jan/12 18:48;xedin;+1;;;","10/Jan/12 18:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding another datacenter's node results in 0 rows returned on first datacenter,CASSANDRA-3696,12537228,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,j.casares,j.casares,04/Jan/12 19:02,16/Apr/19 09:32,14/Jul/23 05:52,05/Jan/12 22:42,0.8.10,1.0.7,,,,,0,,,,,,,"On Cassandra-1.0.5:
1. Create a node in C* with a fresh installation and create a keyspace on that node with one column family -

CREATE KEYSPACE test 
WITH placement_strategy = 'SimpleStrategy' 
and strategy_options={replication_factor:1};

use test; 
create column family cf1;

2. Insert values into cf1 -

set cf1[ascii('k')][ascii('c')] = ascii('v');

get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

3. update the strategy options from simple to networktopology with {Cassandra:1, Backup:1} 
4. read from cf1 to make sure the options change doesn't affect anything -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
=> (column=63, value=v, timestamp=1325689630397000) 
Returned 1 results.

5. start a second node in the Backup datacenter 
6. read from cf1 again (on the first node) -

consistencylevel as LOCAL_QUORUM; 
get cf1[ascii('k')]; 
Returned 0 results.

After about 60 seconds, ""get cf1[ascii('k')]"" started to return results again. 

Also, when running at a CL of ONE on 1.0's head, we were able to see issues as well.

But, if more than one node was added to the second datacenter, then replication_strategy is changed, it seems okay.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/12 03:10;jbellis;3696.txt;https://issues.apache.org/jira/secure/attachment/12509499/3696.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222737,,,Thu Jan 05 22:42:28 UTC 2012,,,,,,,,,,"0|i0gn4n:",95177,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,"04/Jan/12 19:24;jbellis;ONE is expected to not return results in this situation, if it happens to query the new node.  But LOCAL_QUORUM against the first node should be fine.

Can you post the debug log of a query failing at LOCAL_QUORUM?  1.0 head preferred but 1.0.5 is fine if you already have it.;;;","05/Jan/12 01:13;brandon.williams;I can reproduce this reliably, but only if read repair is on.  With it off, everything works.  Here is failed read:

{noformat}
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303637', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 ReadCallback.java (line 77) Blockfor/repair is 1/true; setting up requests to /10.179.64.227,/10.179.111.137
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,523 StorageProxy.java (line 624) reading data from /10.179.64.227
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,524 StorageProxy.java (line 644) reading digest from /10.179.111.137
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,526 ResponseVerbHandler.java (line 44) Processing response on a callback from 4145@/10.179.111.137
DEBUG [RequestResponseStage:10] 2012-01-05 01:00:51,526 ResponseVerbHandler.java (line 44) Processing response on a callback from 4144@/10.179.64.227
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,526 AbstractRowResolver.java (line 66) Preprocessed digest response
DEBUG [RequestResponseStage:10] 2012-01-05 01:00:51,526 AbstractRowResolver.java (line 66) Preprocessed data response
DEBUG [RequestResponseStage:9] 2012-01-05 01:00:51,527 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,527 StorageProxy.java (line 672) Read: 3 ms.
{noformat}

In this case, 10.179.65.102 and 10.179.111.137 are in the first datacenter, and 10.179.64.227 is in the second.  Both DCs have an RF of one, .102 is always the coordinator and the client is using a single connection.

For comparison, here is a successful local read where read repair did not fire (set to 10%):
{noformat}

DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303632', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,519 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 ReadCallback.java (line 77) Blockfor/repair is 1/false; setting up requests to cassandra-1/10.179.65.102
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,520 StorageProxy.java (line 619) reading data locally
DEBUG [ReadStage:93] 2012-01-05 01:00:51,520 StorageProxy.java (line 763) LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='303632', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG [ReadStage:93] 2012-01-05 01:00:51,520 CollationController.java (line 192) collectAllData
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 0 of 5: C0:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 1 of 5: C1:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 2 of 5: C2:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 3 of 5: C3:false:34@1325724968371
DEBUG [ReadStage:93] 2012-01-05 01:00:51,521 SliceQueryFilter.java (line 123) collecting 4 of 5: C4:false:34@1325724968371
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,522 StorageProxy.java (line 672) Read: 2 ms.
{noformat}

And one where it asked .137 for data (but did not leave the DC or repair):

{noformat}
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 CassandraServer.java (line 323) get_slice
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 StorageProxy.java (line 603) Command/ConsistencyLevel is SliceFromReadCommand(table='Keyspace1', key='303731', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)/LOCAL_QUORUM
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for cassandra-1/10.179.65.102, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,514 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 ReadCallback.java (line 77) Blockfor/repair is 1/false; setting up requests to /10.179.111.137
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,515 StorageProxy.java (line 624) reading data from /10.179.111.137
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 ResponseVerbHandler.java (line 44) Processing response on a callback from 4143@/10.179.111.137
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 AbstractRowResolver.java (line 66) Preprocessed data response
DEBUG [RequestResponseStage:12] 2012-01-05 01:00:51,517 PropertyFileSnitch.java (line 90) Could not find end point information for /10.179.111.137, will use default
DEBUG [pool-2-thread-7] 2012-01-05 01:00:51,517 StorageProxy.java (line 672) Read: 2 ms.
{noformat};;;","05/Jan/12 03:10;jbellis;Fix attached.  The problem is we were only pulling out the local-dc replicas when RR was enabled.  We still need to force a local-DC replica to the front of the endpoints list otherwise, since that will get the data read.;;;","05/Jan/12 15:33;jbellis;(patch is against 1.0 but probably needed by 0.8 too.);;;","05/Jan/12 18:38;vijay2win@yahoo.com;+1;;;","05/Jan/12 22:42;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassCastException during hinted handoff,CASSANDRA-3694,12537198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,04/Jan/12 15:15,16/Apr/19 09:32,14/Jul/23 05:52,04/Jan/12 16:13,1.1.0,,,,,,0,,,,,,,"{noformat}
ERROR 08:51:00,200 Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.dht.BigIntegerToken cannot be cast to org.apache.cassandra.db.RowPosition
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1286)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1356)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:351)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/12 15:23;slebresne;3694.patch;https://issues.apache.org/jira/secure/attachment/12509411/3694.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222707,,,Wed Jan 04 16:13:46 UTC 2012,,,,,,,,,,"0|i0gn3j:",95172,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Jan/12 15:23;slebresne;Fix attached.

For the record, this was introduced by CASSANDRA-3554. I think we should try to make sure we use generics for Range object now as this would typically have caught this one.;;;","04/Jan/12 16:07;jbellis;+1;;;","04/Jan/12 16:13;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
strange values of pending tasks with compactionstats (below 0),CASSANDRA-3693,12537183,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,zenek_kraweznik0,zenek_kraweznik0,04/Jan/12 12:53,16/Apr/19 09:32,14/Jul/23 05:52,11/Jan/12 07:56,1.0.7,,,Legacy/Tools,,,0,compaction,,,,,,"during scrub:

Every 2.0s: for i in 1 2 3; do nodetool -h 192.168.2.$i compactionstats; done                                                                                                                                     Wed Jan  4 13:48:13 2012

pending tasks: 2147483646
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     28034971475     72393139120    38.73%
pending tasks: -2147483647
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     24575687282     72385305067    33.95%
pending tasks: 0

","linux, oracle java 1.6.26",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 23:58;jbellis;3693.txt;https://issues.apache.org/jira/secure/attachment/12510128/3693.txt",,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222692,,,Wed Jan 11 10:34:01 UTC 2012,,,,,,,,,,"0|i0gn33:",95170,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Jan/12 09:10;zenek_kraweznik0;Also my cassandra is doing compactions on two nodes in loop. restart of cassandra service doesn't fix this. After restart compaction is also started.;;;","05/Jan/12 23:12;jbellis;Are you using LeveledCompaction?

How much data do you have on disk?  How many sstables?;;;","08/Jan/12 21:13;zenek_kraweznik0;I'm using leveled compaction (with 4096mb limit, but with compression sstables are smaller).

There's a 80GB per node (3 nodes, replication factor=3).

# find /var/lib/cassandra/data -type f | wc -l
273
#

=> 273 sstables

compactionstats looks now like this:

pending tasks: -2147483648
          compaction type        keyspace   column family bytes compacted     bytes total  progress
               Compaction         Archive        Messages     35050352366               0       n/a

I wonder why 0 bytes total is visible on this node (nodetool ring is reporting 37.18GB).

After every compactions bytes total is about 73xxxxxxxxx (i guess it is not compress data size), but this value isn't saved anywhere.;;;","10/Jan/12 23:58;jbellis;Patch to keep compaction task count arithmetic as a long until after the division by sstable size has been done.;;;","11/Jan/12 07:56;slebresne;+1, committed;;;","11/Jan/12 10:34;zenek_kraweznik0;ok, thanks. I'll chech it after ugprade to 1.0.7

And second question from my last comment: ""I wonder why 0 bytes total is visible on this node during compaction (nodetool ring is reporting 37.18GB)."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionStrategy is broken because of generation pre-allocation in LeveledManifest.,CASSANDRA-3691,12537079,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,xedin,xedin,03/Jan/12 17:01,16/Apr/19 09:32,14/Jul/23 05:52,03/Jan/12 19:34,1.1.0,,,,,,0,lcs,,,,,,"LeveledManifest constructor has the following code:

{code}
for (int i = 0; i < generations.length; i++)
{
    generations[i] = new ArrayList<SSTableReader>();
    lastCompactedKeys[i] = new DecoratedKey(cfs.partitioner.getMinimumToken(), null);
}
{code}

But in the DecoratedKey constructor we have:

{code}
assert token != null && key != null && key.remaining() > 0;
{code}

so when you tried to create a CF with LeveledCompressionStrategy that will result in 

{noformat}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:865)
	at org.apache.cassandra.thrift.CassandraServer.system_add_keyspace(CassandraServer.java:953)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace.process(Cassandra.java:4103)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3078)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:857)
	... 7 more
Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:770)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:209)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:300)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:281)
	at org.apache.cassandra.db.Table.initCf(Table.java:339)
	at org.apache.cassandra.db.Table.<init>(Table.java:288)
	at org.apache.cassandra.db.Table.open(Table.java:117)
	at org.apache.cassandra.db.migration.AddKeyspace.applyModels(AddKeyspace.java:72)
	at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
	at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:850)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.cassandra.config.CFMetaData.createCompactionStrategyInstance(CFMetaData.java:752)
	... 14 more
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:55)
	at org.apache.cassandra.db.compaction.LeveledManifest.<init>(LeveledManifest.java:79)
	at org.apache.cassandra.db.compaction.LeveledManifest.create(LeveledManifest.java:85)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.<init>(LeveledCompactionStrategy.java:74)
	... 19 more
ERROR 19:52:44,029 Fatal exception in thread Thread[MigrationStage:1,5,main]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/12 18:47;slebresne;3691.patch;https://issues.apache.org/jira/secure/attachment/12509326/3691.patch",,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222589,,,Tue Jan 03 19:34:53 UTC 2012,,,,,,,,,,"0|i0gn27:",95166,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"03/Jan/12 18:47;slebresne;Oops, missed this one. Patch attached to fix.;;;","03/Jan/12 19:34;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming CommitLog backup,CASSANDRA-3690,12536930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,03/Jan/12 00:39,16/Apr/19 09:32,14/Jul/23 05:52,09/Apr/12 16:24,1.1.1,,,Legacy/Tools,,,0,,,,,,,"Problems with the current SST backups
1) The current backup doesn't allow us to restore point in time (within a SST)
2) Current SST implementation needs the backup to read from the filesystem and hence additional IO during the normal operational Disks
3) in 1.0 we have removed the flush interval and size when the flush will be triggered per CF, 
          For some use cases where there is less writes it becomes increasingly difficult to time it right.
4) Use cases which needs BI which are external (Non cassandra), needs the data in regular intervals than waiting for longer or unpredictable intervals.

Disadvantages of the new solution
1) Over head in processing the mutations during the recover phase.
2) More complicated solution than just copying the file to the archive.

Additional advantages:
Online and offline restore.
Close to live incremental backup.

Note: If the listener agent gets restarted, it is the agents responsibility to Stream the files missed or incomplete.

There are 3 Options in the initial implementation:
1) Backup -> Once a socket is connected we will switch the commit log and send new updates via the socket.
2) Stream -> will take the absolute path of the file and will read the file and send the updates via the socket.
3) Restore -> this will get the serialized bytes and apply's the mutation.

Side NOTE: (Not related to this patch as such) The agent which will take incremental backup is planned to be open sourced soon (Name: Priam).",,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/12 23:11;vijay2win@yahoo.com;0001-CASSANDRA-3690-v2.patch;https://issues.apache.org/jira/secure/attachment/12517636/0001-CASSANDRA-3690-v2.patch","07/Apr/12 01:00;vijay2win@yahoo.com;0001-CASSANDRA-3690-v4.patch;https://issues.apache.org/jira/secure/attachment/12521798/0001-CASSANDRA-3690-v4.patch","08/Apr/12 02:51;vijay2win@yahoo.com;0001-CASSANDRA-3690-v5.patch;https://issues.apache.org/jira/secure/attachment/12521871/0001-CASSANDRA-3690-v5.patch","24/Jan/12 05:42;vijay2win@yahoo.com;0001-Make-commitlog-recycle-configurable.patch;https://issues.apache.org/jira/secure/attachment/12511632/0001-Make-commitlog-recycle-configurable.patch","24/Jan/12 05:42;vijay2win@yahoo.com;0002-support-commit-log-listener.patch;https://issues.apache.org/jira/secure/attachment/12511633/0002-support-commit-log-listener.patch","24/Jan/12 05:42;vijay2win@yahoo.com;0003-helper-jmx-methods.patch;https://issues.apache.org/jira/secure/attachment/12511634/0003-helper-jmx-methods.patch","24/Jan/12 05:42;vijay2win@yahoo.com;0004-external-commitlog-with-sockets.patch;https://issues.apache.org/jira/secure/attachment/12511635/0004-external-commitlog-with-sockets.patch","24/Jan/12 05:42;vijay2win@yahoo.com;0005-cmmiting-comments-to-yaml.patch;https://issues.apache.org/jira/secure/attachment/12511636/0005-cmmiting-comments-to-yaml.patch","09/Apr/12 15:04;jbellis;3690-v6.txt;https://issues.apache.org/jira/secure/attachment/12521959/3690-v6.txt",,,,,,,,,9.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222525,,,Mon Apr 09 16:24:43 UTC 2012,,,,,,,,,,"0|i0gn1r:",95164,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Jan/12 23:38;jbellis;The socket business sounds complicated.  CASSANDRA-1602 is a lot more straightforward, I'd recommend starting with that.;;;","05/Jan/12 23:52;vijay2win@yahoo.com;Hi Jonathan, But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... 
While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery. 

Something like copying the data to S3 in amazon and copying right back for the node for recovery. (this backup will also be used for test cluster refresh for prod data and BI which is completely a different system)
Recovery in most case are loose of instance or the whole cluster (Virtual machines).;;;","24/Jan/12 05:34;vijay2win@yahoo.com;0001 => Adds a configuration so we can avoid recycling in case some one wants to copy the files across to another location like a archive logs
0002 => Adds CommitLogListener, implementation can recive the updates to the commitlogs.
0003 => helper JMX in case the user wants to query the active CL's
0004 => this can go to the tools folder/we dont need to commit it to the core.;;;","09/Feb/12 23:35;jbellis;bq. But there is additional IO which the server has to do to copy the archive logs to a different location (not locally)... While streaming the Commit log back to the server we have to copy it first and then read it back which is also a over head in recovery.

What if you mounted the archive logs via s3fs?;;;","21/Feb/12 23:11;jasobrown;We spent some time looking into s3fs, and ran into problems between fuse (which s3fs depends on) and mmap. I put together a small java app to simulate writing to local disc vs. s3fs using mmapp'ed and non-mmap'ed files. (Note most of my java sample code is based on CommitLogSegment's implementation.) We found the non-mmap'ed files wrote to s3fs without a hitch, but writing to the mmap'ed S3 mount failed. The failure was different between OpenJDK 6 vs. Sun JDK, but they were both SIGBUS errors. Also, I ran the tests on my local ubuntu box running Linux 3.0.0-12, fuse version (fusermount --version) at 2.8.4. 

At this point, it seems like s3fs isn't as viable as one would hope.



;;;","22/Feb/12 19:38;jbellis;That does rule out using s3fs in read/write mode, but I imagine that would be a pretty bad idea from a latency standpoint anyway.  But in the context of just mounting log files for replay/recover, CommitLog uses RandomAccessReader which is ordinary buffered i/o.;;;","22/Feb/12 19:50;vijay2win@yahoo.com;Starting to think... What we really want is a Async Triggers CASSANDRA-1311 which listens for all the updates + a way to restore the data with mutation before starting the node. In someways thats what the original patch was trying to do will it make sense to merge these two efforts?;;;","22/Feb/12 20:46;jbellis;I'm skeptical of trying to do this on top of triggers.  First, CASSANDRA-1311 seems to lean towards coordinator-level triggers rather than replica-level.  Second, I don't think it makes sense for a Trigger-level API to deal with raw bytes, which would mean losing efficiency from having to re-serialize RowMutations.

I like the postgresql approach: http://www.postgresql.org/docs/9.1/static/continuous-archiving.html -- briefly, you configure an {{archive_command}} that tells it how you want it to copy full log segments off-server when full, and set up a recovery.conf file when you want to recover, which includes a {{restore_command}} that is the inverse of the archive command.

The main difference is that postgresql's default wal segment size is 16MB, which gives them a finer resolution than our 128MB.  I can't think of a reason we can't lower ours, though.;;;","08/Mar/12 23:11;vijay2win@yahoo.com;Hi Jonathan,

Attached patch does exactly what we discussed here. Its almost the same as PostgreSQL :) 

In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.

Plz let me know.;;;","14/Mar/12 04:29;jbellis;I don't see any uses of CommitLogRecover outside of CommitLog, which makes me think this is an unrelated refactoring.  Is that correct?  If so, let's split that out of this ticket.;;;","14/Mar/12 04:37;vijay2win@yahoo.com;Hi Jonathan, The refactor is mainly for the following:

>>> In addition we can start the node with -Dcassandra.join_ring=false and then use JMX to restore files one by one via JMX.
This allows us to start the recovery process before all the files are downloaded from S3/External source. I can do that it in another ticket, let me know. Thanks!;;;","05/Apr/12 21:45;jbellis;I've made a bunch of minor changes and pushed to https://github.com/jbellis/cassandra/branches/3690-v3.

I noticed that we need to wait for the archive to finish whether we end up recycling or not.  Seems to me it would be simpler to continue to always recycle, but (as we have here) wait for the archive first.  So archive can copy off to s3 or whatever directly, instead of ln somewhere else as an intermediate step.  Total i/o will be lower and commitlog will create extra segments if needed in the meantime.

Maybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?;;;","05/Apr/12 21:53;jbellis;Also: would be nice to get rid of the new Thread / busywait archive dance.  If we used an ExecutorService instead, we could add the Future to the segment and just say segment.waitForArchive(), no looping.;;;","07/Apr/12 00:58;vijay2win@yahoo.com;Hi Jonathan, Attached patch incorporates all the recommended changes.... except

>>> Maybe we should also have a restore_list_segments command as well, so we can query s3 (again for instance) directly and have restore_command pull from there, rather than requiring a local directory?
IMHO. It might be better if we have a streaming API to list and stream the data in... otherwise we need have to download to the local FS anyways, So it will be better to incrementally download and use the JMX to restore the files independently (example: A external agent), that may be a simple solution for now..... If the user has a NFS mount it will work even better all he needs to do is to ""ln -s"" location and he is done :)

Plz note that i also removed the requirement to turn off recycling for backup (as recommended), but i left that as configurable because it will good to have unique names in the backup sometimes so we dont overwrite :);;;","07/Apr/12 01:40;jbellis;bq. it will good to have unique names in the backup sometimes so we dont overwrite 

If you think about it, ""target filename"" is just a suggestion... you'd be free to ignore it and generate a different filename (incorporating timestamp for instance, or even a uuid) in the archive script.;;;","07/Apr/12 01:51;vijay2win@yahoo.com;Hi Jonathan, Agree, would you like to remove the configuration to disable recycle (which this patch added)? let me know... Thanks!;;;","07/Apr/12 20:43;jbellis;I do think we should make recycling always-on; it's a non-negligible performance win and so far we don't have a use case that requires disabling it.;;;","08/Apr/12 02:51;vijay2win@yahoo.com;Hi Jonathan, 
v5 removes the recycle related changes and
Added 2 JMX (getActiveSegmentNames and getArchivingSegmentNames)

(list all files) - (getActiveSegmentNames) will provide a view of orphan files which failed archiving...;;;","09/Apr/12 15:04;jbellis;v6 attached.  The primary changes made are fixes to the Future logic: the only way you'll get a null Future back is if no archive tack was submitted; if it errors out, you'll get an ExecutionException when you call get(), but never a null.

Updated getArchivingSegmentNames javadoc to emphasize that it does NOT include failed archive attempts. Not sure if this is what was intended.;;;","09/Apr/12 15:33;vijay2win@yahoo.com;+1;;;","09/Apr/12 16:24;jbellis;committed w/ some final improvements to yaml comments;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local range scans are not run on the read stage,CASSANDRA-3687,12536810,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Jan/12 14:53,16/Apr/19 09:32,14/Jul/23 05:52,16/Aug/12 20:30,1.2.0 beta 1,,,,,,0,,,,,,,"Running directly on the client request/StorageProxy thread means we're now allowing one range scan per thrift thread instead of one per read stage thread [which may be more, or less, depending on thrift server mode], and it bypasses the ""drop hopeless requests"" overcapacity protection built in there. ",,jbellis,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222493,,,Thu Aug 16 20:30:28 UTC 2012,,,,,,,,,,"0|i0gn0f:",95158,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,"02/Jan/12 14:53;jbellis;Look at SP.LocalReadRunnable for how to do this safely. Simplest fix would be to just continue routing all range scans over MessagingService.;;;","14/May/12 22:53;jbellis;Patch pushed to https://github.com/jbellis/cassandra/branches/3687.  (Simple fix, but Jira attach file is broken.)

Generalizes AbstractRowResolver.injectPreProcessed into ReadCallback.response(TMessage) and adds LocalRangeSliceRunnable.;;;","18/May/12 00:54;vijay2win@yahoo.com;+10, one thing which i noticed was will it be better to remove (coz the localnode might not be in position 0).

{code}
if (handler.endpoints.size() == 1 && handler.endpoints.get(0).equals(FBUtilities.getBroadcastAddress())
{code}

and add it to 

{code}
for (InetAddress endpoint : handler.endpoints)
{
    if (endpoint.equals(FBUtilities.getBroadcastAddress())
    {
        logger.debug(""reading data locally"");
        StageManager.getStage(Stage.READ).execute(new LocalRangeSliceRunnable(nodeCmd, handler));
    }
{code}

similar to fetchRows?;;;","23/May/12 21:31;jbellis;We've already checked that size == 1, so position 0 is the only possible place for it to be. :);;;","12/Jul/12 03:58;vijay2win@yahoo.com;+1;;;","16/Aug/12 20:30;jbellis;rebased and committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
