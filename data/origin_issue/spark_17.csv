Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Required),Inward issue link (Supercedes),Inward issue link (Supercedes),Inward issue link (Supercedes),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
R DataFrame API for sortWithinPartitions,SPARK-22924,13127708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,29/Dec/17 20:09,12/Dec/22 18:10,14/Jul/23 06:30,30/Dec/17 17:50,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 30 17:50:28 UTC 2017,,,,,,,,,,"0|i3odi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/17 20:10;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/20118;;;","30/Dec/17 17:50;gurwls223;Issue resolved by pull request 20118
[https://github.com/apache/spark/pull/20118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"R sql functions for current_date, current_timestamp, rtrim/ltrim/trim with trimString",SPARK-22920,13127588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,28/Dec/17 21:45,29/Dec/17 20:07,14/Jul/23 06:30,29/Dec/17 20:06,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 21:47:04 UTC 2017,,,,,,,,,,"0|i3ocrj:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"28/Dec/17 21:47;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/20105;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shouldn't bias towards build right if user does not specify,SPARK-22916,13127446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fengliu@databricks.com,liufeng.ee@gmail.com,liufeng.ee@gmail.com,28/Dec/17 00:48,29/Jan/18 02:03,14/Jul/23 06:30,29/Dec/17 10:49,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"This is an issue very similar to SPARK-22489. When there are no broadcast hints, the current spark strategies will prefer to build right, without considering the sizes of the two sides. To reproduce:

{code:java}
import org.apache.spark.sql.execution.joins.BroadcastHashJoinExec

spark.createDataFrame(Seq((1, ""4""), (2, ""2""))).toDF(""key"", ""value"").createTempView(""table1"")
spark.createDataFrame(Seq((1, ""1""), (2, ""2""), (3, ""3""))).toDF(""key"", ""value"").createTempView(""table2"")

val bl = sql(s""SELECT * FROM table1 t1 JOIN table2 t2 ON t1.key = t2.key"").queryExecution.executedPlan
{code}

The plan is going to broadcast right side (`t2`), even though it is larger.
",,apachespark,liufeng.ee@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 29 02:03:03 UTC 2018,,,,,,,,,,"0|i3obvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/17 00:55;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/20099;;;","29/Jan/18 02:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20420;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ChiSqSelectorModel, GaussianMixtureModel save implementation for Row order issues",SPARK-22905,13127308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,27/Dec/17 04:38,23/Aug/18 23:27,14/Jul/23 06:30,29/Dec/17 01:33,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,MLlib,,,,,0,,,,,,,,,"Currently, in `ChiSqSelectorModel`, save:
{code}
spark.createDataFrame(dataArray).repartition(1).write...
{code}
The default partition number used by createDataFrame is ""defaultParallelism"",
Current RoundRobinPartitioning won't guarantee the ""repartition"" generating the same order result with local array. We need fix it.",,apachespark,josephkb,podongfeng,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 23:27:07 UTC 2018,,,,,,,,,,"0|i3ob1b:",9223372036854775807,,,,,josephkb,,,,,,,,,,,,,,,,,,,"27/Dec/17 04:42;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/20088;;;","29/Dec/17 01:33;josephkb;Resolved by https://github.com/apache/spark/pull/20088;;;","29/Dec/17 02:08;podongfeng;Many other models are saved in the same way {{sparkSession.createDataFrame(...).repartition(1).write.parquet}}, are they needed to be fixed?;;;","29/Dec/17 03:08;weichenxu123;[~podongfeng] Some of them only including one row to save so there's no bug, some case including row-number column and when reading it will sort to get stable order. But I am not sure I miss some cases, it will great if you help check.;;;","29/Dec/17 06:06;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/20113;;;","29/Dec/17 06:07;podongfeng;[~WeichenXu123] I made a check and found that same issue exists in {{GaussianMixtureModel}}, otherwise looks fine.;;;","29/Dec/17 18:08;josephkb;Merged https://github.com/apache/spark/pull/20113 to fix GaussianMixtureModel issue too.;;;","13/Aug/18 20:05;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/22079;;;","23/Aug/18 23:27;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/22211;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add non-deterministic to Python UDF,SPARK-22901,13127158,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,smilegator,smilegator,26/Dec/17 06:31,06/Jan/18 09:19,14/Jul/23 06:30,26/Dec/17 14:40,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,Add a new API for Python UDF to allow users to change the determinism from deterministic to non-deterministic. ,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 06 09:19:04 UTC 2018,,,,,,,,,,"0|i3oa3z:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"26/Dec/17 10:29;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19929;;;","06/Jan/18 09:19;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/20173;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OneVsRestModel transform on streaming data failed.,SPARK-22899,13127095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,25/Dec/17 10:20,28/Dec/17 01:32,14/Jul/23 06:30,28/Dec/17 01:32,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,Structured Streaming,,,,0,,,,,,,,,"OneVsRestModel transform on streaming data failed.
Because of it persisting the input dataset, which streaming do not support.",,apachespark,josephkb,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22888,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 01:32:08 UTC 2017,,,,,,,,,,"0|i3o9q7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/17 10:25;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/20077;;;","28/Dec/17 01:32;josephkb;Resolved by https://github.com/apache/spark/pull/20077;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when use udf,SPARK-22891,13126949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fengliu@databricks.com,hellodoge,hellodoge,23/Dec/17 07:12,29/Dec/17 07:09,14/Jul/23 06:30,29/Dec/17 07:09,2.2.0,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"In my application,i use multi threads. Each thread has a SparkSession and use SparkSession.sqlContext.udf.register to register my udf. Sometimes there throws exception like this:

{code:java}
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1062)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:137)
	at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:136)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:136)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:133)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:207)
	at org.apache.spark.sql.SQLContext.udf(SQLContext.scala:203)
	at com.game.data.stat.clusterTask.tools.standard.IpConverterRegister$.run(IpConverterRegister.scala:63)
	at 
	... 20 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newSession(HiveClientImpl.scala:789)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newSession(HiveClientImpl.scala:79)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.resourceLoader$lzycompute(HiveSessionStateBuilder.scala:45)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.resourceLoader(HiveSessionStateBuilder.scala:44)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:61)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:35)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:289)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1059)
	... 20 more
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException
	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:744)
	at org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1391)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:210)
	... 34 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException
	at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:769)
	at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:736)
	... 36 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.isCompatibleWith(HiveMetaStoreClient.java:287)
	at sun.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy25.isCompatibleWith(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:206)
	at org.apache.hadoop.hive.ql.session.SessionState.setAuthorizerV2Config(SessionState.java:765)
	... 37 more

{code}

Also, i use apache hive 2.1.1 in my cluster.
When i use spark 2.1.x, the exception above never happends again.
",hadoop 2.7.2,apachespark,hellodoge,liufeng.ee@gmail.com,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 29 01:09:21 UTC 2017,,,,,,,,,,"0|i3o8tz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/17 14:21;srowen;It sounds like it was resolved, then, by some other change?
It also looks like a Hive bug, related to https://issues.apache.org/jira/browse/HIVE-11935;;;","25/Dec/17 06:30;hellodoge;It happends in spark 2.2.x, not in spark 2.1.x.;;;","26/Dec/17 14:40;srowen;It looks like a Hive problem, primarily? just not sure if it's something that can be resolved in Spark. How do you reproduce it?;;;","29/Dec/17 00:48;liufeng.ee@gmail.com;This is definitely caused by the race from https://issues.apache.org/jira/browse/HIVE-11935. 

In spark 2.1, spark creates the `metadataHive` lazily until `addJar`(https://github.com/apache/spark/blob/branch-2.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionState.scala#L40), so this can only be triggered by concurrent `addJar` (can't imagine this will happen in practice)

In spark 2.2, the `metadataHive` creation is tied to the `resourceLoader` creation (see the stack trace), so it starts to be triggered by new spark session creation. In https://github.com/apache/spark/pull/20029, I'm trying to make an argument that it is safe to remove the new hive client creation. Besides this change, I think we should also make the hive client creation thread safe: https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L251

;;;","29/Dec/17 01:04;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/20109;;;","29/Dec/17 01:09;liufeng.ee@gmail.com;A side note: if we don't want to merge https://github.com/apache/spark/pull/20029, we should make the creation of hive client lazy inside the HiveSessionResourceLoader: https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveSessionStateBuilder.scala#L123 as we know the hive client creation is expensive, so it does not make sense to materialize it if we don't use it. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRAN checks can fail if older Spark install exists,SPARK-22889,13126927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,23/Dec/17 00:25,23/Dec/17 18:30,14/Jul/23 06:30,23/Dec/17 18:29,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SparkR,,,,,0,,,,,,,,,"Since all CRAN checks go through the same machine, if there is an older partial download or partial install of Spark left behind the tests fail. One solution is to overwrite the install files when running tests. ",,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 23 00:33:04 UTC 2017,,,,,,,,,,"0|i3o8p3:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.0,,,,,,,,,,"23/Dec/17 00:33;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/20060;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly build fails for a high user id,SPARK-22875,13126763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jira.shegalov,jira.shegalov,jira.shegalov,22/Dec/17 08:49,28/Dec/17 17:45,14/Jul/23 06:30,28/Dec/17 17:44,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,"{code}
./build/mvn package -Pbigtop-dist -DskipTests
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single (dist) on project spark-assembly_2.11: Execution dist of goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single failed: user id '123456789' is too big ( > 2097151 ). -> [Help 1]
{code}

",,apachespark,jira.shegalov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 17:44:32 UTC 2017,,,,,,,,,,"0|i3o7ov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/17 09:11;apachespark;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/20055;;;","28/Dec/17 17:44;srowen;Issue resolved by pull request 20055
[https://github.com/apache/spark/pull/20055];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes dockerfile path needs update,SPARK-22866,13126714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,foxish,foxish,22/Dec/17 03:04,17/May/20 18:25,14/Jul/23 06:30,22/Dec/17 05:03,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,,0,,,,,,,,,Minor bug in dockerfile preventing successful builds of the default driver and executor images,,apachespark,foxish,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 22 05:03:39 UTC 2017,,,,,,,,,,"0|i3o7dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/17 03:07;apachespark;User 'foxish' has created a pull request for this issue:
https://github.com/apache/spark/pull/20051;;;","22/Dec/17 05:03;mridulm80;Issue resolved by pull request 20051
[https://github.com/apache/spark/pull/20051];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: ExecutorAllocationManagerSuite ""cancel pending executors when no longer needed""",SPARK-22864,13126654,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,21/Dec/17 21:36,29/Dec/17 16:52,14/Jul/23 06:30,29/Dec/17 16:52,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Failed in one PR run:

{noformat}
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 0 did not equal -1
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
	at org.apache.spark.ExecutorAllocationManagerSuite$$anonfun$9.apply(ExecutorAllocationManagerSuite.scala:271)
	at org.apache.spark.ExecutorAllocationManagerSuite$$anonfun$9.apply(ExecutorAllocationManagerSuite.scala:247)
{noformat}

Logs have a more interesting exception:

{noformat}
17/12/21 04:26:10.846 pool-1-thread-1-ScalaTest-running-ExecutorAllocationManagerSuite INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 5)
17/12/21 04:26:10.858 spark-listener-group-appStatus ERROR AsyncEventQueue: Listener AppStatusListener threw an exception
java.lang.UnsupportedOperationException: duration() called on unfinished task
	at org.apache.spark.scheduler.TaskInfo.duration(TaskInfo.scala:118)
	at org.apache.spark.status.AppStatusListener$$anonfun$onTaskEnd$1.apply(AppStatusListener.scala:468)
	at org.apache.spark.status.AppStatusListener$$anonfun$onTaskEnd$1.apply(AppStatusListener.scala:435)
	at scala.Option.foreach(Option.scala:257)
{noformat}

Seems like a race since it passed several attempts locally.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 29 16:52:05 UTC 2017,,,,,,,,,,"0|i3o70n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/17 21:43;vanzin;The exception seems to also happen on success (and is probably easy to fix).;;;","21/Dec/17 22:53;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20050;;;","29/Dec/17 16:52;irashid;Issue resolved by pull request 20050
[https://github.com/apache/spark/pull/20050];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docs on lazy elimination of columns missing from an encoder.,SPARK-22862,13126638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,21/Dec/17 19:47,22/Dec/17 05:40,14/Jul/23 06:30,22/Dec/17 05:40,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.2,2.3.0,,,SQL,,,,,0,,,,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 19:50:05 UTC 2017,,,,,,,,,,"0|i3o6x3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/17 19:50;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/20048;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLAppStatusListener should track all stages in multi-job executions,SPARK-22861,13126599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,21/Dec/17 16:53,21/Dec/17 23:38,14/Jul/23 06:30,21/Dec/17 23:38,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,Web UI,,,,0,,,,,,,,,"The SQLAppStatusListener introduced in SPARK-20652 did not track all stages from a multi-job SQL execution, which should be fixed.

Reported by [~carsonwang] on that PR",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 23:38:26 UTC 2017,,,,,,,,,,"0|i3o6of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/17 16:58;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/20047;;;","21/Dec/17 23:38;vanzin;Issue resolved by pull request 20047
[https://github.com/apache/spark/pull/20047];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark workers log ssl passwords passed to the executors,SPARK-22860,13126584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,QAFelixK,QAFelixK,21/Dec/17 15:50,05/Dec/19 23:51,14/Jul/23 06:30,26/Feb/19 22:50,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Spark Core,,,,,2,,,,,,,,,"The workers log the spark.ssl.keyStorePassword and spark.ssl.trustStorePassword passed by cli to the executor processes. The ExecutorRunner should escape passwords to not appear in the worker's log files in INFO level. In this example, you can see my 'SuperSecretPassword' in a worker log:

{code}
17/12/08 08:04:12 INFO ExecutorRunner: Launch command: ""/global/myapp/oem/jdk/bin/java"" ""-cp"" ""/global/myapp/application/myapp_software/thing_loader_lib/core-repository-model-zzz-1.2.3-SNAPSHOT.jar
[...]
:/global/myapp/application/spark-2.1.1-bin-hadoop2.7/jars/*"" ""-Xmx16384M"" ""-Dspark.authenticate.enableSaslEncryption=true"" ""-Dspark.ssl.keyStorePassword=SuperSecretPassword"" ""-Dspark.ssl.keyStore=/global/myapp/application/config/ssl/keystore.jks"" ""-Dspark.ssl.trustStore=/global/myapp/application/config/ssl/truststore.jks"" ""-Dspark.ssl.enabled=true"" ""-Dspark.driver.port=39927"" ""-Dspark.ssl.protocol=TLS"" ""-Dspark.ssl.trustStorePassword=SuperSecretPassword"" ""-Dspark.authenticate=true"" ""-Dmyapp_IMPORT_DATE=2017-10-30"" ""-Dmyapp.config.directory=/global/myapp/application/config"" ""-Dsolr.httpclient.builder.factory=com.company.myapp.loader.auth.LoaderConfigSparkSolrBasicAuthConfigurer"" ""-Djavax.net.ssl.trustStore=/global/myapp/application/config/ssl/truststore.jks"" ""-XX:+UseG1GC"" ""-XX:+UseStringDeduplication"" ""-Dthings.loader.export.zzz_files=false"" ""-Dlog4j.configuration=file:/global/myapp/application/config/spark-executor-log4j.properties"" ""-XX:+HeapDumpOnOutOfMemoryError"" ""-XX:+UseStringDeduplication"" ""org.apache.spark.executor.CoarseGrainedExecutorBackend"" ""--driver-url"" ""spark://CoarseGrainedScheduler@192.168.0.1:39927"" ""--executor-id"" ""2"" ""--hostname"" ""192.168.0.1"" ""--cores"" ""4"" ""--app-id"" ""app-20171208080412-0000"" ""--worker-url"" ""spark://Worker@192.168.0.1:59530""
{code}",,apachespark,kabhwan,mgaido,QAFelixK,toopt4,tooptoop4,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26998,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 05 23:37:26 UTC 2019,,,,,,,,,,"0|i3o6l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/18 23:30;apachespark;User 'tooptoop4' has created a pull request for this issue:
https://github.com/apache/spark/pull/21514;;;","11/Feb/19 22:48;toopt4;gentle ping, fix waiting to be committed;;;","17/Feb/19 11:47;toopt4;Not just in worker log, but in 'ps -ef' process list;;;","18/Feb/19 00:46;kabhwan;If we concern only about logging them into log file (boundary of this issue) we can try to remove them, but if we also concern about showing them into process list, that is a bit different issue.

If I'm not mistaken, we'll have to pass them to CoarseGrainedExecutorBackend at any way, because driver cannot pass these values which executor needs them to connect to driver. Adding level of security doesn't help, because we need to pass any security information to CoarseGrainedExecutorBackend to start from.;;;","18/Feb/19 08:00;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23820;;;","18/Feb/19 08:00;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/23820;;;","18/Feb/19 08:04;kabhwan;I'm proposing to just redact these values from log message first, because complexities of both are quite different. Given we need to deal with cli arguments, I had to create my own logic instead of taking up existing PR. (Existing PR just removed them in arguments which I wonder it works well.) Sorry about that.;;;","22/Feb/19 21:38;tooptoop4;[~kabhwan]  spark.ssl.keyStorePassword and  spark.ssl.keyPassword don't need to be passed to  CoarseGrainedExecutorBackend. Only  spark.ssl.trustStorePassword is used;;;","26/Feb/19 22:50;vanzin;Issue resolved by pull request 23820
[https://github.com/apache/spark/pull/23820];;;","05/Dec/19 23:37;toopt4;[~kabhwan] can this go in 2.4.5?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sbt publishLocal under scala 2.12 fails due to invalid javadoc comments in tags package,SPARK-22855,13126438,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,easel,easel,easel,21/Dec/17 03:07,21/Dec/17 16:09,14/Jul/23 06:30,21/Dec/17 16:08,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,Attempting to use sbt publishLocal on a 2.12 build fails due to scaladoc. ,,apachespark,easel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 16:08:49 UTC 2017,,,,,,,,,,"0|i3o5on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/17 03:09;easel;For example:
{noformat}
[spark] SBT_OPTS=-Dscala-2.12 sbt                                                                                                                            master  ✱
[info] Loading global plugins from /Users/erik/.sbt/0.13/plugins
[info] Loading project definition from /Users/erik/Projects/spark/project
[info] Compiling 1 Scala source to /Users/erik/Projects/spark/project/target/scala-2.10/sbt-0.13/classes...
[info] Resolving key references (16934 settings) ...
[info] Set current project to spark-parent (in build file:/Users/erik/Projects/spark/)
> ++2.12.4
[info] Setting version to 2.12.4
[info] Reapplying settings...
[info] Set current project to spark-parent (in build file:/Users/erik/Projects/spark/)
> tags/publishLocal
[warn] Binary version (2.11) for dependency org.scala-lang#scala-library;2.11.8
[warn] 	in org.apache.spark#spark-tags_2.12;2.3.0-SNAPSHOT differs from Scala binary version in project (2.12).
[info] Wrote /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT.pom
[info] :: delivering :: org.apache.spark#spark-tags_2.12;2.3.0-SNAPSHOT :: 2.3.0-SNAPSHOT :: integration :: Wed Dec 20 22:07:06 EST 2017
[info] Main Scala API documentation to /Users/erik/Projects/spark/common/tags/target/scala-2.12/api...
[info] 	delivering ivy file to /Users/erik/Projects/spark/common/tags/target/scala-2.12/ivy-2.3.0-SNAPSHOT.xml
[info] Compiling 2 Scala sources and 6 Java sources to /Users/erik/Projects/spark/common/tags/target/scala-2.12/classes...
[warn] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/AlphaComponent.java:33: Implementation restriction: subclassing Classfile does not
[warn] make your annotation visible at runtime.  If that is what
[warn] you want, you must write the annotation class in Java.
[warn] public @interface AlphaComponent {}
[warn]                   ^
[warn] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java:35: Implementation restriction: subclassing Classfile does not
[warn] make your annotation visible at runtime.  If that is what
[warn] you want, you must write the annotation class in Java.
[warn] public @interface DeveloperApi {}
[warn]                   ^
[warn] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java:36: Implementation restriction: subclassing Classfile does not
[warn] make your annotation visible at runtime.  If that is what
[warn] you want, you must write the annotation class in Java.
[warn] public @interface Experimental {}
[warn]                   ^
[error] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java:34: inner classes cannot be classfile annotations
[error]   public @interface Stable {};
[error]                     ^
[error] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java:41: inner classes cannot be classfile annotations
[error]   public @interface Evolving {};
[error]                     ^
[error] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java:48: inner classes cannot be classfile annotations
[error]   public @interface Unstable {};
[error]                     ^
[warn] /Users/erik/Projects/spark/common/tags/src/main/java/org/apache/spark/annotation/Private.java:41: Implementation restriction: subclassing Classfile does not
[warn] make your annotation visible at runtime.  If that is what
[warn] you want, you must write the annotation class in Java.
[warn] public @interface Private {}
[warn]                   ^
[info] No documentation generated with unsuccessful compiler run
[warn] four warnings found
[error] three errors found
[info] Packaging /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT.jar ...
[info] Done packaging.
[error] (tags/compile:doc) Scaladoc generation failed
[error] Total time: 5 s, completed Dec 20, 2017 10:07:11 PM
>
{noformat};;;","21/Dec/17 03:23;apachespark;User 'easel' has created a pull request for this issue:
https://github.com/apache/spark/pull/20042;;;","21/Dec/17 03:23;easel;With the fix:

{noformat}
> ++2.12.4
[info] Setting version to 2.12.4
[info] Reapplying settings...
[info] Set current project to spark-parent (in build file:/Users/erik/Projects/spark/)
> tags/publishLocal
[info] Packaging /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT-sources.jar ...
[info] Updating {file:/Users/erik/Projects/spark/}tags...
[warn] Binary version (2.11) for dependency org.scala-lang#scala-library;2.11.8
[warn] 	in org.apache.spark#spark-tags_2.12;2.3.0-SNAPSHOT differs from Scala binary version in project (2.12).
[info] Done packaging.
[info] Wrote /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT.pom
[warn] Multiple dependencies with the same organization/name but different versions. To avoid conflict, pick one version:
[warn]  * org.scala-lang:scala-library:(2.12.4, 2.11.8)
[info] Resolving org.scala-lang#scala-library;2.12.4 ...
[success] created output: /Users/erik/Projects/spark/common/tags/target
[info] Resolving jline#jline;2.14.5 ...
[info] Done updating.
[info] :: delivering :: org.apache.spark#spark-tags_2.12;2.3.0-SNAPSHOT :: 2.3.0-SNAPSHOT :: integration :: Wed Dec 20 22:23:16 EST 2017
[info] 	delivering ivy file to /Users/erik/Projects/spark/common/tags/target/scala-2.12/ivy-2.3.0-SNAPSHOT.xml
[info] Main Scala API documentation to /Users/erik/Projects/spark/common/tags/target/scala-2.12/api...
[info] Compiling 2 Scala sources and 6 Java sources to /Users/erik/Projects/spark/common/tags/target/scala-2.12/classes...
model contains 10 documentable templates
[info] Main Scala API documentation successful.
[info] Packaging /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT-javadoc.jar ...
[info] Done packaging.
[info] Packaging /Users/erik/Projects/spark/common/tags/target/scala-2.12/spark-tags_2.12-2.3.0-SNAPSHOT.jar ...
[info] Done packaging.
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT.pom
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-tags_2.12 to /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT.pom
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-tags_2.12 to /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT.jar
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT-sources.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-tags_2.12 to /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT-sources.jar
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT-javadoc.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-tags_2.12 to /Users/erik/.m2/repository/org/apache/spark/spark-tags_2.12/2.3.0-SNAPSHOT/spark-tags_2.12-2.3.0-SNAPSHOT-javadoc.jar
[info] 	published spark-tags_2.12 to /Users/erik/.ivy2/local/org.apache.spark/spark-tags_2.12/2.3.0-SNAPSHOT/poms/spark-tags_2.12.pom
[info] 	published spark-tags_2.12 to /Users/erik/.ivy2/local/org.apache.spark/spark-tags_2.12/2.3.0-SNAPSHOT/jars/spark-tags_2.12.jar
[info] 	published spark-tags_2.12 to /Users/erik/.ivy2/local/org.apache.spark/spark-tags_2.12/2.3.0-SNAPSHOT/srcs/spark-tags_2.12-sources.jar
[info] 	published spark-tags_2.12 to /Users/erik/.ivy2/local/org.apache.spark/spark-tags_2.12/2.3.0-SNAPSHOT/docs/spark-tags_2.12-javadoc.jar
[info] 	published ivy to /Users/erik/.ivy2/local/org.apache.spark/spark-tags_2.12/2.3.0-SNAPSHOT/ivys/ivy.xml
[success] Total time: 1 s, completed Dec 20, 2017 10:23:18 PM
{noformat};;;","21/Dec/17 16:08;srowen;Issue resolved by pull request 20042
[https://github.com/apache/spark/pull/20042];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AppStatusListener should get Spark version by SparkListenerLogStart,SPARK-22854,13126426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,cloud_fan,cloud_fan,21/Dec/17 02:12,22/Dec/17 01:26,14/Jul/23 06:30,22/Dec/17 01:26,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,,,apachespark,cloud_fan,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 22 01:26:11 UTC 2017,,,,,,,,,,"0|i3o5lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/17 20:50;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20049;;;","22/Dec/17 01:26;cloud_fan;Issue resolved by pull request 20049
[https://github.com/apache/spark/pull/20049];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sbt publishLocal fails due to -Xlint:unchecked flag passed to javadoc,SPARK-22852,13126398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,easel,easel,easel,20/Dec/17 22:22,21/Dec/17 15:39,14/Jul/23 06:30,21/Dec/17 15:38,2.2.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,The subject pretty much says it. PublishLocal fails from sbt due to javadoc receiving -Xlint:unchecked. ,,apachespark,easel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 15:38:36 UTC 2017,,,,,,,,,,"0|i3o5fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 22:34;apachespark;User 'easel' has created a pull request for this issue:
https://github.com/apache/spark/pull/20040;;;","20/Dec/17 22:35;easel;There's a one-liner PR to resolve this at https://github.com/apache/spark/pull/20040

For reference, the current failure looks like this:

{noformat}
> launcher/publishLocal
[info] Packaging /Users/erik/Projects/spark/launcher/target/scala-2.11/spark-launcher_2.11-2.3.0-SNAPSHOT-sources.jar ...
[info] Done packaging.
[warn] Skipped generating '<exclusion/>' for com.sun.jersey#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[warn] Skipped generating '<exclusion/>' for com.sun.jersey.jersey-test-framework#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[warn] Skipped generating '<exclusion/>' for com.sun.jersey.contribs#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[info] Wrote /Users/erik/Projects/spark/launcher/target/scala-2.11/spark-launcher_2.11-2.3.0-SNAPSHOT.pom
[info] :: delivering :: org.apache.spark#spark-launcher_2.11;2.3.0-SNAPSHOT :: 2.3.0-SNAPSHOT :: integration :: Wed Dec 20 17:35:35 EST 2017
[info] 	delivering ivy file to /Users/erik/Projects/spark/launcher/target/scala-2.11/ivy-2.3.0-SNAPSHOT.xml
[info] Main Java API documentation to /Users/erik/Projects/spark/launcher/target/scala-2.11/api...
[error] javadoc: error - invalid flag: -Xlint:unchecked
[info] Usage: javadoc [options] [packagenames] [sourcefiles] [@files]
[info]   -overview <file>                 Read overview documentation from HTML file
[info]   -public                          Show only public classes and members
[info]   -protected                       Show protected/public classes and members (default)
[info]   -package                         Show package/protected/public classes and members
[info]   -private                         Show all classes and members
[info]   -help                            Display command line options and exit
[info]   -doclet <class>                  Generate output via alternate doclet
[info]   -docletpath <path>               Specify where to find doclet class files
[info]   -sourcepath <pathlist>           Specify where to find source files
[info]   -classpath <pathlist>            Specify where to find user class files
[info]   -cp <pathlist>                   Specify where to find user class files
[info]   -exclude <pkglist>               Specify a list of packages to exclude
[info]   -subpackages <subpkglist>        Specify subpackages to recursively load
[info]   -breakiterator                   Compute first sentence with BreakIterator
[info]   -bootclasspath <pathlist>        Override location of class files loaded
[info]                                    by the bootstrap class loader
[info]   -source <release>                Provide source compatibility with specified release
[info]   -extdirs <dirlist>               Override location of installed extensions
[info]   -verbose                         Output messages about what Javadoc is doing
[info]   -locale <name>                   Locale to be used, e.g. en_US or en_US_WIN
[info]   -encoding <name>                 Source file encoding name
[info]   -quiet                           Do not display status messages
[info]   -J<flag>                         Pass <flag> directly to the runtime system
[info]   -X                               Print a synopsis of nonstandard options and exit
[info]
[info] Provided by Standard doclet:
[info]   -d <directory>                   Destination directory for output files
[info]   -use                             Create class and package usage pages
[info]   -version                         Include @version paragraphs
[info]   -author                          Include @author paragraphs
[info]   -docfilessubdirs                 Recursively copy doc-file subdirectories
[info]   -splitindex                      Split index into one file per letter
[info]   -windowtitle <text>              Browser window title for the documentation
[info]   -doctitle <html-code>            Include title for the overview page
[info]   -header <html-code>              Include header text for each page
[info]   -footer <html-code>              Include footer text for each page
[info]   -top    <html-code>              Include top text for each page
[info]   -bottom <html-code>              Include bottom text for each page
[info]   -link <url>                      Create links to javadoc output at <url>
[info]   -linkoffline <url> <url2>        Link to docs at <url> using package list at <url2>
[info]   -excludedocfilessubdir <name1>:.. Exclude any doc-files subdirectories with given name.
[info]   -group <name> <p1>:<p2>..        Group specified packages together in overview page
[info]   -nocomment                       Suppress description and tags, generate only declarations.
[info]   -nodeprecated                    Do not include @deprecated information
[info]   -noqualifier <name1>:<name2>:... Exclude the list of qualifiers from the output.
[info]   -nosince                         Do not include @since information
[info]   -notimestamp                     Do not include hidden time stamp
[info]   -nodeprecatedlist                Do not generate deprecated list
[info]   -notree                          Do not generate class hierarchy
[info]   -noindex                         Do not generate index
[info]   -nohelp                          Do not generate help link
[info]   -nonavbar                        Do not generate navigation bar
[info]   -serialwarn                      Generate warning about @serial tag
[info]   -tag <name>:<locations>:<header> Specify single argument custom tags
[info]   -taglet                          The fully qualified name of Taglet to register
[info]   -tagletpath                      The path to Taglets
[info]   -charset <charset>               Charset for cross-platform viewing of generated documentation.
[info]   -helpfile <file>                 Include file that help link links to
[info]   -linksource                      Generate source in HTML
[info]   -sourcetab <tab length>          Specify the number of spaces each tab takes up in the source
[info]   -keywords                        Include HTML meta tags with package, class and member info
[info]   -stylesheetfile <path>           File to change style of the generated documentation
[info]   -docencoding <name>              Specify the character encoding for the output
[info] 1 error
[error] (launcher/compile:doc) javadoc returned nonzero exit code
[error] Total time: 0 s, completed Dec 20, 2017 5:35:35 PM
{noformat};;;","20/Dec/17 22:37;easel;With the fix:

{noformat}
> launcher/publishLocal
[warn] Skipped generating '<exclusion/>' for com.sun.jersey#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[warn] Skipped generating '<exclusion/>' for com.sun.jersey.jersey-test-framework#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[warn] Skipped generating '<exclusion/>' for com.sun.jersey.contribs#*. Dependency exclusion should have both 'org' and 'module' to comply with Maven POM's schema.
[info] Wrote /Users/erik/Projects/spark/launcher/target/scala-2.11/spark-launcher_2.11-2.3.0-SNAPSHOT.pom
[info] :: delivering :: org.apache.spark#spark-launcher_2.11;2.3.0-SNAPSHOT :: 2.3.0-SNAPSHOT :: integration :: Wed Dec 20 17:37:21 EST 2017
[info] Compiling 2 Scala sources and 6 Java sources to /Users/erik/Projects/spark/common/tags/target/scala-2.11/classes...
[info] 	delivering ivy file to /Users/erik/Projects/spark/launcher/target/scala-2.11/ivy-2.3.0-SNAPSHOT.xml
[info] Packaging /Users/erik/Projects/spark/common/tags/target/scala-2.11/spark-tags_2.11-2.3.0-SNAPSHOT.jar ...
[info] Done packaging.
[info] Main Java API documentation to /Users/erik/Projects/spark/launcher/target/scala-2.11/api...
[info] Compiling 16 Java sources to /Users/erik/Projects/spark/launcher/target/scala-2.11/classes...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/ChildProcAppHandle.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/CommandBuilderUtils.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/FilteredObjectInputStream.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/LauncherConnection.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/LauncherProtocol.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/LauncherServer.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/Main.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/NamedThreadFactory.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/OutputRedirector.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/SparkAppHandle.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/SparkClassCommandBuilder.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/SparkLauncher.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitOptionParser.java...
[info] Loading source file /Users/erik/Projects/spark/launcher/src/main/java/org/apache/spark/launcher/package-info.java...
[info] Constructing Javadoc information...
[info] Standard Doclet version 1.8.0_152
[info] Building tree for all the packages and classes...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/SparkAppHandle.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/SparkAppHandle.Listener.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/SparkAppHandle.State.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/SparkLauncher.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/package-frame.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/package-summary.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/org/apache/spark/launcher/package-tree.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/constant-values.html...
[info] Building index for all the packages and classes...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/overview-tree.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/index-all.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/deprecated-list.html...
[info] Building index for all classes...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/allclasses-frame.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/allclasses-noframe.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/index.html...
[info] Generating /Users/erik/Projects/spark/launcher/target/scala-2.11/api/help-doc.html...
[info] Main Java API documentation successful.
[info] Packaging /Users/erik/Projects/spark/launcher/target/scala-2.11/spark-launcher_2.11-2.3.0-SNAPSHOT-javadoc.jar ...
[info] Done packaging.
[info] Packaging /Users/erik/Projects/spark/launcher/target/scala-2.11/spark-launcher_2.11-2.3.0-SNAPSHOT.jar ...
[info] Done packaging.
[info] 	published spark-launcher_2.11 to /Users/erik/.ivy2/local/org.apache.spark/spark-launcher_2.11/2.3.0-SNAPSHOT/poms/spark-launcher_2.11.pom
[info] 	published spark-launcher_2.11 to /Users/erik/.ivy2/local/org.apache.spark/spark-launcher_2.11/2.3.0-SNAPSHOT/jars/spark-launcher_2.11.jar
[info] 	published spark-launcher_2.11 to /Users/erik/.ivy2/local/org.apache.spark/spark-launcher_2.11/2.3.0-SNAPSHOT/srcs/spark-launcher_2.11-sources.jar
[info] 	published spark-launcher_2.11 to /Users/erik/.ivy2/local/org.apache.spark/spark-launcher_2.11/2.3.0-SNAPSHOT/docs/spark-launcher_2.11-javadoc.jar
[info] 	published ivy to /Users/erik/.ivy2/local/org.apache.spark/spark-launcher_2.11/2.3.0-SNAPSHOT/ivys/ivy.xml
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT.pom
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-launcher_2.11 to /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT.pom
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-launcher_2.11 to /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT.jar
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT-sources.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-launcher_2.11 to /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT-sources.jar
[warn] Attempting to overwrite /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT-javadoc.jar
[warn] 	This usage is deprecated and will be removed in sbt 1.0.
[info] 	published spark-launcher_2.11 to /Users/erik/.m2/repository/org/apache/spark/spark-launcher_2.11/2.3.0-SNAPSHOT/spark-launcher_2.11-2.3.0-SNAPSHOT-javadoc.jar
[success] Total time: 4 s, completed Dec 20, 2017 5:37:25 PM
{noformat};;;","21/Dec/17 15:38;srowen;Issue resolved by pull request 20040
[https://github.com/apache/spark/pull/20040];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor page in SHS does not show driver,SPARK-22850,13126360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,20/Dec/17 19:22,16/Aug/18 15:12,14/Jul/23 06:30,04/Jan/18 22:19,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"This bug is sort of related to SPARK-22836.

Starting with Spark 2.2 (at least), the event logs generated by Spark do not contain a {{SparkListenerBlockManagerAdded}} entry for the driver. That means when applications are replayed in the SHS, the driver is not listed in the executors page.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25120,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 22:19:56 UTC 2018,,,,,,,,,,"0|i3o57b:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"20/Dec/17 19:50;vanzin;Sorry for the noise, this is actually a bug in the initialization of the listener bus caused by SPARK-18838, so it only affects 2.3 (I had it in an internal 2.2 branch).;;;","20/Dec/17 20:31;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20039;;;","04/Jan/18 22:19;irashid;Issue resolved by pull request 20039
[https://github.com/apache/spark/pull/20039];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ivy.retrieve pattern should also consider `classifier`,SPARK-22849,13126332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,20/Dec/17 16:40,20/Dec/17 19:25,14/Jul/23 06:30,20/Dec/17 19:25,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"If this pattern for instance doesn't has the [type] or [classifier] token, Ivy will download the source/javadoc artifacts to the same file as the regular jar.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 17:04:25 UTC 2017,,,,,,,,,,"0|i3o513:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 16:54;srowen;I think this is the same, or closely related to, an issue I wrestled with along with Burak a while ago in https://issues.apache.org/jira/browse/SPARK-20075
You can see some of my work at https://github.com/srowen/spark/commit/41267020701be4877be352e1678113bd9870ec12 but I failed. It was hard!;;;","20/Dec/17 17:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20037;;;","20/Dec/17 17:04;smilegator;Not sure whether they are related, but my fix is very trivial.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table's owner property in hive metastore is null,SPARK-22846,13126258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,xwc3504,xwc3504,xwc3504,20/Dec/17 11:56,27/Dec/17 02:11,14/Jul/23 06:30,27/Dec/17 02:09,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"I met this issue after upgrading from spark 2.0.1 to spark 2.2.1.

when creating table using spark sql or spark thriftserver, it occured that the table's owner info in metastore is null, which may cause other issue on table authentication. It may be a bug.

After digging into the code, I found that in class HiveClientImpl:

{code:java}
private val userName = state.getAuthenticator.getUserName
{code}

the result of state.getAuthenticator.getUserName is null, which would cause all operation on tables have a null username, such as method toHiveTable:

{code:java}
 def toHiveTable(table: CatalogTable, userName: Option[String] = None): HiveTable = {
{code}

my create table command:  create table  datapm.test_xwc9(id string,name string);



","spark 2.2.1, hive 0.14, hadoop 2.6.0",apachespark,cloud_fan,dongjoon,xwc3504,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 11:58;xwc3504;talbe_owner_null.png;https://issues.apache.org/jira/secure/attachment/12903025/talbe_owner_null.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 27 02:09:23 UTC 2017,,,,,,,,,,"0|i3o4kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 13:16;apachespark;User 'BruceXu1991' has created a pull request for this issue:
https://github.com/apache/spark/pull/20034;;;","21/Dec/17 17:19;dongjoon;Hi, [~xwc3504].

When I used 2.2.1, it works. Could you describe your situation more specifically?

{code}
scala> spark.version
res0: String = 2.2.1

scala> sql(""CREATE TABLE spark_22846(a INT)"")

scala> sql(""DESCRIBE FORMATTED spark_22846"").show
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                   a|                 int|   null|
|                    |                    |       |
|# Detailed Table ...|                    |       |
|            Database|             default|       |
|               Table|         spark_22846|       |
|               Owner|            dongjoon|       |
{code};;;","27/Dec/17 02:09;cloud_fan;Issue resolved by pull request 20034
[https://github.com/apache/spark/pull/20034];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R localCheckpoint API,SPARK-22843,13126232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,felixcheung,felixcheung,20/Dec/17 09:36,12/Dec/22 18:10,14/Jul/23 06:30,28/Dec/17 11:18,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22649,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 11:18:07 UTC 2017,,,,,,,,,,"0|i3o4ev:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"22/Dec/17 06:23;felixcheung;any taker on this for 2.3.0?;;;","25/Dec/17 05:16;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/20073;;;","28/Dec/17 11:18;gurwls223;Issue resolved by pull request 20073
[https://github.com/apache/spark/pull/20073];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session timeout checker does not work in SessionManager,SPARK-22837,13126082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,20/Dec/17 01:52,24/Jan/18 18:08,14/Jul/23 06:30,24/Jan/18 18:08,2.0.2,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, 
{code:java}
SessionManager.init
{code}
 will not be called, the config 
{code:java}
HIVE_SERVER2_SESSION_CHECK_INTERVAL HIVE_SERVER2_IDLE_SESSION_TIMEOUT HIVE_SERVER2_IDLE_SESSION_CHECK_OPERATION
{code}
of session timeout checker can not be loaded, it cause the session timeout checker does not work.",,apachespark,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 02:29:08 UTC 2017,,,,,,,,,,"0|i3o3hr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 02:29;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20025;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors page is not showing driver logs links,SPARK-22836,13126073,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,20/Dec/17 00:37,28/Dec/17 21:41,14/Jul/23 06:30,28/Dec/17 21:41,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,,"-I think this was mainly caused by SPARK-15951; that changed modified the executors page to read data from the REST API, and in 2.1 and 2.2 the REST API does not return the driver as an executor. So no information about the driver is shown in that page at all.- (Edit: bug is unrelated to the aforementioned bug.)

In 2.3 the driver executor is listed, but it is doesn't have any log links.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 28 21:41:36 UTC 2017,,,,,,,,,,"0|i3o3fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 18:35;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20038;;;","20/Dec/17 19:06;vanzin;I posted a patch for 2.3.

For 2.2, it seems to work fine in a live application. It also works with an event log created by Spark 2.0. But it does not show up with an event log created by Spark 2.2.;;;","20/Dec/17 19:19;vanzin;The 2.2 issue is different (and also affects 2.3 aside from my patch above), I'll file a separate bug.;;;","28/Dec/17 21:41;irashid;Issue resolved by pull request 20038
[https://github.com/apache/spark/pull/20038];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make insert commands have real children to fix UI issues,SPARK-22834,13125934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,19/Dec/17 14:15,29/Dec/17 07:30,14/Jul/23 06:30,29/Dec/17 07:30,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"With https://github.com/apache/spark/pull/19474, children of insert commands in UI are missing. To fix it, create a new physical plan `DataWritingCommandExec` to exec `DataWritingCommand` with children.",,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 29 07:30:13 UTC 2017,,,,,,,,,,"0|i3o2lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/17 14:38;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/20020;;;","29/Dec/17 07:30;cloud_fan;Issue resolved by pull request 20020
[https://github.com/apache/spark/pull/20020];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid throwing OutOfMemoryError in case of exception in spill,SPARK-22827,13125757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sitalkedia@gmail.com,sitalkedia@gmail.com,sitalkedia@gmail.com,19/Dec/17 00:24,08/Nov/18 05:25,14/Jul/23 06:30,20/Dec/17 04:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Currently, the task memory manager throws an OutofMemory error when there is an IO exception happens in spill() - https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java#L194. Similarly there any many other places in code when if a task is not able to acquire memory due to an exception we throw an OutofMemory error which kills the entire executor and hence failing all the tasks that are running on that executor instead of just failing one single task. ",,apachespark,cloud_fan,sitalkedia@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 05:25:52 UTC 2018,,,,,,,,,,"0|i3o1hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/17 00:40;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/20014;;;","20/Dec/17 04:21;cloud_fan;Issue resolved by pull request 20014
[https://github.com/apache/spark/pull/20014];;;","08/Nov/18 05:25;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22969;;;","08/Nov/18 05:25;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/22969;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results of Casting Array to String,SPARK-22825,13125712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,smilegator,smilegator,18/Dec/17 20:50,05/Jan/18 06:04,14/Jul/23 06:30,05/Jan/18 06:04,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"{code}
val df = spark.range(10).select('id.cast(""integer"")).agg(collect_list('id).as('ids))       df.write.saveAsTable(""t"")
sql(""SELECT cast(ids as String) FROM t"").show(false)
{code}
The output is like
{code}
+------------------------------------------------------------------+
|ids                                                               |
+------------------------------------------------------------------+
|org.apache.spark.sql.catalyst.expressions.UnsafeArrayData@8bc285df|
+------------------------------------------------------------------+
{code}
",,apachespark,cloud_fan,maropu,mengxr,rxin,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 05 06:04:47 UTC 2018,,,,,,,,,,"0|i3o187:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/17 06:42;maropu;[~smilegator] you're taking on this now?;;;","19/Dec/17 06:43;rxin;[~maropu] you should :)
;;;","19/Dec/17 06:45;maropu;ok;;;","19/Dec/17 16:58;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/20024;;;","05/Jan/18 06:04;cloud_fan;Issue resolved by pull request 20024
[https://github.com/apache/spark/pull/20024];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Structured Streaming Source trait breaking change,SPARK-22824,13125656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,mmaitre,mmaitre,18/Dec/17 16:19,20/Dec/17 18:43,14/Jul/23 06:30,20/Dec/17 18:43,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"org.apache.spark.sql.execution.streaming.Offset was moved to org.apache.spark.sql.sources.v2.reader.Offset on the Source trait:
https://github.com/apache/spark/commit/f8c7c1f21aa9d1fd38b584ca8c4adf397966e9f7#diff-56453fdb4dc2d7666c7ab237cb102189

This broke our custom sources that are called in Spark jobs running on Azure Databricks 3.4 since the Maven package does not match the hosted Spark bits. We use the following Maven version to be able to deploy on Azure Databricks:

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>2.3.0-SNAPSHOT</version>
        </dependency>
",Azure Databricks 3.4,apachespark,joseph.torres,marmbrus,mmaitre,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 18:43:27 UTC 2017,,,,,,,,,,"0|i3o0vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/17 17:02;srowen;[~zsxwing] [~joseph.torres] was that meant to be an internal API?;;;","18/Dec/17 21:18;marmbrus;This is technically an internal API (as is all of [org.apache.spark.sql.execution|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/package.scala]).  That said, I think we should do our best to preserve compatibility until there is a {{@Stable}} public alternative (which is the goal of the V2 project).

We'll open a PR to try and fix this.;;;","18/Dec/17 22:04;apachespark;User 'joseph-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/20012;;;","20/Dec/17 18:43;zsxwing;Issue resolved by pull request 20012
[https://github.com/apache/spark/pull/20012];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download page - updating package type does nothing,SPARK-22819,13125463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,yohannj,yohannj,17/Dec/17 14:22,17/Dec/17 16:50,14/Jul/23 06:30,17/Dec/17 16:50,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Documentation,,,,,0,,,,,,,,,"How to reproduce:
Go to the download page: [https://spark.apache.org/downloads.html].
Update the package type

What happens:
Nothing

What did I expect:
The link to download spark is updated

--

Inspecting the source code, that drop-down menu calls onPackageSelect() when a new value is selected.
However downloads.js does not contain that method. Though it contains the method onVersionSelect().

--

I'm not yet familiar with the Spark repository, that's why I'm creating this jira without a Pull Request.",,yohannj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22525,,,,,,SPARK-22525,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 17 15:51:30 UTC 2017,,,,,,,,,,"0|i3nzp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/17 14:38;yohannj;I initially searched for JIRA in the backlog only.
After looking for the downloads.js file and its history, I ended up on https://issues.apache.org/jira/browse/SPARK-22525

Sry for the duplicate jira.;;;","17/Dec/17 15:51;srowen;No you're correct; actually the latest docs release somehow un-did the fix for the issue in https://github.com/apache/spark-website/pull/75 . It might have been generated before the fix or something and we didn't catch it. I'll re-patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
csv escape of quote escape,SPARK-22818,13125458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ep1804,ep1804,ep1804,17/Dec/17 10:56,28/Dec/17 23:38,14/Jul/23 06:30,28/Dec/17 23:38,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"A DataFrame is stored in CSV format and loaded again. When there's backslash followed by quotation mark, csv reading seems to make an error.

This issue was raised before in https://issues.apache.org/jira/browse/SPARK-19834 and postponed due to a bug in its dependency. Now it is resolved and this issue can be reopened.

",,apachespark,ep1804,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,SPARK-19834,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 18 01:47:03 UTC 2017,,,,,,,,,,"0|i3nznz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/17 01:47;apachespark;User 'ep1804' has created a pull request for this issue:
https://github.com/apache/spark/pull/20004;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use fixed testthat version for SparkR tests in AppVeyor,SPARK-22817,13125425,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,16/Dec/17 15:47,12/Dec/22 18:10,14/Jul/23 06:30,17/Dec/17 05:42,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SparkR,,,,,0,,,,,,,,,"We happened to access to the internal {{run_tests}} - https://github.com/r-lib/testthat/blob/v1.0.2/R/test-package.R#L62-L75. 

https://github.com/apache/spark/blob/master/R/pkg/tests/run-all.R#L58

This seems removed out in 2.0.0.

",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 09:08:02 UTC 2018,,,,,,,,,,"0|i1u557:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/17 16:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/20003;;;","17/Dec/17 05:42;gurwls223;Fixed in https://github.com/apache/spark/pull/20003.;;;","15/Feb/18 09:08;felixcheung;I should have caught this - -we need to fix the test because it will fail in CRAN - another option is to fix the dependency version in DESCRIPTION file-

scratch that. in CRAN we are calling test_package, which works fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keep PromotePrecision in Optimized Plans,SPARK-22815,13125399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,16/Dec/17 09:09,19/Dec/17 14:17,14/Jul/23 06:30,19/Dec/17 14:17,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"We could get incorrect results by running DecimalPrecision twice. 
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 19 14:17:56 UTC 2017,,,,,,,,,,"0|i3nzbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/17 09:11;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20000;;;","19/Dec/17 14:17;cloud_fan;Issue resolved by pull request 20000
[https://github.com/apache/spark/pull/20000];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-tests.py fails when /usr/sbin/lsof does not exist,SPARK-22813,13125386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,16/Dec/17 02:03,12/Dec/22 18:10,14/Jul/23 06:30,18/Dec/17 22:35,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Running ./dev/run-tests.py for mvn on OS that does not have /usr/sbin/lsof (e.g. /usr/bin/lsof) gives the error

{code}
/bin/sh: 1: /usr/sbin/lsof: not found

Usage:
 kill [options] <pid> [...]

Options:
 <pid> [...]            send signal to every <pid> listed
 -<signal>, -s, --signal <signal>
                        specify the <signal> to be sent
 -l, --list=[<signal>]  list all signal names, or convert one to a name
 -L, --table            list all signal names in a nice table

 -h, --help     display this help and exit
 -V, --version  output version information and exit

For more details see kill(1).
Traceback (most recent call last):
  File ""./dev/run-tests.py"", line 626, in <module>
    main()
  File ""./dev/run-tests.py"", line 597, in main
    build_apache_spark(build_tool, hadoop_version)
  File ""./dev/run-tests.py"", line 389, in build_apache_spark
    build_spark_maven(hadoop_version)
  File ""./dev/run-tests.py"", line 329, in build_spark_maven
    exec_maven(profiles_and_goals)
  File ""./dev/run-tests.py"", line 270, in exec_maven
    kill_zinc_on_port(zinc_port)
  File ""./dev/run-tests.py"", line 258, in kill_zinc_on_port
    subprocess.check_call(cmd, shell=True)
  File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '/usr/sbin/lsof -P |grep 3156 | grep LISTEN | awk '{ print $2; }' | xargs kill' returned non-zero exit status 123
{code}",,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 18 22:35:31 UTC 2017,,,,,,,,,,"0|i3nz8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/17 02:07;srowen;https://issues.apache.org/jira/browse/SPARK-22377;;;","16/Dec/17 02:18;kiszk;Thank you for pointing the PR that we worked. I was not able to remember the number.;;;","16/Dec/17 02:31;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19998;;;","18/Dec/17 22:35;gurwls223;Issue resolved by pull request 19998
[https://github.com/apache/spark/pull/19998];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing cran-check on master ,SPARK-22812,13125384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,falaki,falaki,16/Dec/17 01:45,12/Dec/22 18:10,14/Jul/23 06:30,18/Dec/17 08:39,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SparkR,,,,,0,,,,,,,,,"When I run {{R/run-tests.sh}} or {{R/check-cran.sh}} I get the following failure message:

{code}
* checking CRAN incoming feasibility ...Error in .check_package_CRAN_incoming(pkgdir) :
  dims [product 22] do not match the length of object [0]
{code}

cc [~felixcheung] have you experienced this error before?",,falaki,felixcheung,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24152,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 18 19:30:11 UTC 2017,,,,,,,,,,"0|i1u552:zzx",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/17 04:36;felixcheung;Not exactly... what’s the environment? Seems like something is wrong connecting/pulling from CRAN.




;;;","16/Dec/17 21:04;falaki;Do you know what is being checked in that step? Is it trying to reach a CRAN server?;;;","18/Dec/17 08:39;gurwls223;This is fixed via [~viirya]'s request to CRAN sysadmin team. Please see https://github.com/apache/spark/pull/20005.;;;","18/Dec/17 19:30;yhuai;Thank you guys!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.ml.tests is missing a py4j import.,SPARK-22811,13125348,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,15/Dec/17 22:24,12/Dec/22 18:10,14/Jul/23 06:30,16/Dec/17 01:58,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,PySpark,,,,0,,,,,,,,,"This bug isn't getting caught because the relevant code only gets run if the test environment does not have Hive.

https://github.com/apache/spark/blob/46776234a49742e94c64897322500582d7393d35/python/pyspark/ml/tests.py#L1866",,apachespark,bago.amirbekian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 16 01:58:00 UTC 2017,,,,,,,,,,"0|i3nz07:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"15/Dec/17 22:26;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/19997;;;","16/Dec/17 01:58;gurwls223;Issue resolved by pull request 19997
[https://github.com/apache/spark/pull/19997];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark is sensitive to imports with dots,SPARK-22809,13125314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,CricketScience,CricketScience,15/Dec/17 19:35,24/Oct/18 00:16,14/Jul/23 06:30,23/Oct/18 22:50,2.2.0,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.2,2.4.0,,,,PySpark,,,,,0,,,,,,,,,"User code can fail with dotted imports.  Here's a repro script.

{noformat}
import numpy as np
import pandas as pd
import pyspark
import scipy.interpolate
import scipy.interpolate as scipy_interpolate
import py4j

scipy_interpolate2 = scipy.interpolate

sc = pyspark.SparkContext()
spark_session = pyspark.SQLContext(sc)

#######################################################
# The details of this dataset are irrelevant          #
# Sorry if you'd have preferred something more boring #
#######################################################
x__ = np.linspace(0,10,1000)
freq__ = np.arange(1,5)
x_, freq_ = np.ix_(x__, freq__)
y = np.sin(x_ * freq_).ravel()
x = (x_ * np.ones(freq_.shape)).ravel()
freq = (np.ones(x_.shape) * freq_).ravel()
df_pd = pd.DataFrame(np.stack([x,y,freq]).T, columns=['x','y','freq'])
df_sk = spark_session.createDataFrame(df_pd)
assert(df_sk.toPandas() == df_pd).all().all()

try:
    import matplotlib.pyplot as plt
    for f, data in df_pd.groupby(""freq""):
        plt.plot(*data[['x','y']].values.T)
    plt.show()
except:
    print(""I guess we can't plot anything"")

def mymap(x, interp_fn):
    df = pd.DataFrame.from_records([row.asDict() for row in list(x)])
    return interp_fn(df.x.values, df.y.values)(np.pi)

df_by_freq = df_sk.rdd.keyBy(lambda x: x.freq).groupByKey()

result = df_by_freq.mapValues(lambda x: mymap(x, scipy_interpolate.interp1d)).collect()
assert(np.allclose(np.array(zip(*result)[1]), np.zeros(len(freq__)), atol=1e-6))

try:
    result = df_by_freq.mapValues(lambda x: mymap(x, scipy.interpolate.interp1d)).collect()
    raise Excpetion(""Not going to reach this line"")
except py4j.protocol.Py4JJavaError, e:
    print(""See?"")

result = df_by_freq.mapValues(lambda x: mymap(x, scipy_interpolate2.interp1d)).collect()
assert(np.allclose(np.array(zip(*result)[1]), np.zeros(len(freq__)), atol=1e-6))

# But now it works!
result = df_by_freq.mapValues(lambda x: mymap(x, scipy.interpolate.interp1d)).collect()
assert(np.allclose(np.array(zip(*result)[1]), np.zeros(len(freq__)), atol=1e-6))
{noformat}",,bryanc,cloud_fan,CricketScience,dongjoon,holden,ueshin,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23159,,,,,,,,,,,SPARK-23159,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 24 00:15:38 UTC 2018,,,,,,,,,,"0|i3nysn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/17 19:51;srowen;It's not clear what the problem is? what does this output?
Spark isn't managing imports, so I am not yet clear how this is a Spark-related issue.;;;","15/Dec/17 20:39;CricketScience;Outputs: When I run it, it plots a picture and prints ""See?""

This is certainly ""unexpected behavior"" for me.
{noformat}
> import a.b
> import a.b as a_b
> a_b2 = a.b
> your_function(a_b)
Yay!
>your_function(a.b)
Boo!
>your_function(a_b2)
Yay!
>your_function(a.b)
Yay!
{noformat}

The problem is that when people port code to pyspark they're going to have errors until they go through and update imports to avoid this.  If it's possible to trigger this from a library (which I don't know if it is), that might be hard to work around.
;;;","16/Dec/17 15:01;srowen;What is the error? shouldn't this fail on any dot import if so, and can we just see that error?
I still don't see how this is a Spark issue vs Python interpreter issue, but even there, not sure why this import type would be an issue.
I don't think it's an import thing.;;;","22/Dec/17 17:35;srowen;It's still not clear this isn't some other error swallowed by error-handling in your example. I say that because you show this construct works, the second time, so doesn't really seem to be related to the import. Spark doesn't handle imports anyway; it's the interpreter. At worst you have a workaround to whatever this is. If you have a specific change to propose that pinpoints or explains what's happening, that's attribute to Spark, reopen.;;;","06/Jan/18 02:17;holden;After further investigation this turns out to be an issue maye have been fixed upstream in cloud pickle (see 0.4.1 in https://github.com/cloudpipe/cloudpickle/blob/fb3a80f4aa8e76098b4cebd0dc8ff2331424e53d/CHANGES.md ). This issue only presents when serializing from an IPython/Jupyter/Zeppelin notebook as the import ends up being in the module space where as in regular console this is not the case.

We can verify this is purely a cloudpickle/pickling/serialization issue by serializing a simple function with the version of cloudpickle PySpark ships with, like the one shown above, opening a new python shell and doing pickle loads (after importing pickle & cloudpickle).

The solution would be to update cloud pickle, or stop copying cloud pickles source code and just use it as a regular pypi dependency. Since we're at the start of a release I think a copy update is probably the best path forward to try and get this in for Spark 2.3.

Thanks [~CricketScience] for the pair programming/debugging session on this.

cc [~rgbkrk];;;","06/Jan/18 02:21;holden;Note we used a simple 
{code:python}
def foo(x):
    return scipy.interpolate.interp1d(x)
cloudpickle.dumps(foo)
{code}

To do the dump and verify the issue.;;;","06/Jan/18 02:40;CricketScience;Much shorter version

{code:python}
import cloudpickle
import pyspark
import py4j
sc = pyspark.SparkContext()
rdd = sc.parallelize([(1,2)])

import scipy.interpolate
def foo(*ards, **kwd):
    scipy.interpolate.interp1d
try:
    rdd.mapValues(foo).collect()
except py4j.protocol.Py4JJavaError, err:
    print(""it errored"")
    
import scipy.interpolate as scipy_interpolate
def bar(*ards, **kwd):
    scipy_interpolate.interp1d
rdd.mapValues(bar).collect()
print(""worked"")
rdd.mapValues(foo).collect()
print(""worked"")
{code}
;;;","23/Jan/18 09:09;ueshin;[~holdenk] Hi, any updates on this?
 I might miss something, but I couldn't reproduce the problem with the current master in my local environment.
 Thanks!;;;","23/Jan/18 17:01;CricketScience;You've got to run it in ipython or zeppelin


;;;","23/Jan/18 22:44;dongjoon;Since this is not a regression, can we move the target version from 2.3.0 into 2.3.1, [~holdenk] and [~ueshin]?;;;","24/Jan/18 04:29;ueshin;I agree with [~dongjoon].

[~CricketScience] Ah, I tried in Jupyter but it should work, right?

Btw, will [https://github.com/apache/spark/pull/20373] fix this issue? Could someone take a look at the pr please?;;;","24/Jan/18 06:09;holden;[~ueshin]: we can push this out to 2.3.1 given we are already in the RC process for 2.3.0. I'm not sure if this is fixed in the 0.4.2 upgrade (I think it would be in 0.5.2 but we're only partially upgrading). Already looking at the upgrade PR.;;;","24/Jan/18 06:10;holden;oh wait it should work in 0.4.2, I'll poke at that PR more.;;;","26/Jan/18 00:23;srowen;Might duplicate SPARK-23159 but I wasn't sure.;;;","27/Mar/18 18:18;holden;This _should_ be resolved by SPARK-23169 but I'll double check when I've got some cycles set aside this Friday..;;;","25/May/18 18:12;vanzin;I'm removing 2.3.1 since it doesn't seem there's any activity here. Please re-add if you plan to work on this for that release.;;;","23/Oct/18 02:13;cloud_fan;is this issue fixed? Anyway I'm removing the target version, since it's not a blocker and we are not able to fix it before 2.4.0.;;;","23/Oct/18 22:49;bryanc;I confirmed that I could reproduce in IPython with Spark branch-2.3 and did not have the issue with branch-2.4. I think we can close this issue
{noformat}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.1-SNAPSHOT
      /_/

Using Python version 3.6.6 (default, Oct 12 2018 14:08:43)
SparkSession available as 'spark'.

In [1]: import pyspark.cloudpickle
   ...: import pyspark
   ...: import py4j
   ...: rdd = sc.parallelize([(1,2)])
   ...: import scipy.interpolate                                                                                             

In [2]: import scipy.interpolate
   ...: def foo(*ards, **kwd):
   ...:     scipy.interpolate.interp1d
   ...: try:
   ...:     rdd.mapValues(foo).collect()
   ...: except py4j.protocol.Py4JJavaError as err:
   ...:     print(""it errored"")
   ...: import scipy.interpolate as scipy_interpolate
   ...: def bar(*ards, **kwd):
   ...:     scipy_interpolate.interp1d
   ...: rdd.mapValues(bar).collect()
   ...: print(""worked"")
   ...: rdd.mapValues(foo).collect()
   ...: print(""worked"")                                                                                                      
worked                                                                          
worked{noformat}

{noformat}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.3-SNAPSHOT
      /_/

Using Python version 3.6.6 (default, Oct 12 2018 14:08:43)
SparkSession available as 'spark'.

In [1]: import pyspark.cloudpickle 
   ...: import pyspark 
   ...: import py4j 
   ...: rdd = sc.parallelize([(1,2)]) 
   ...: import scipy.interpolate                                                                                             

In [2]: import scipy.interpolate 
   ...: def foo(*ards, **kwd): 
   ...:     scipy.interpolate.interp1d 
   ...: try: 
   ...:     rdd.mapValues(foo).collect() 
   ...: except py4j.protocol.Py4JJavaError as err: 
   ...:     print(""it errored"") 
   ...: import scipy.interpolate as scipy_interpolate 
   ...: def bar(*ards, **kwd): 
   ...:     scipy_interpolate.interp1d 
   ...: rdd.mapValues(bar).collect() 
   ...: print(""worked"") 
   ...: rdd.mapValues(foo).collect() 
   ...: print(""worked"")                                                                                                      
18/10/23 15:39:54 ERROR Executor: Exception in task 7.0 in stage 0.0 (TID 7)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 196, in main
    process()
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 191, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""/home/bryan/git/spark/python/pyspark/rdd.py"", line 1951, in <lambda>
    map_values_fn = lambda kv: (kv[0], f(kv[1]))
  File ""<ipython-input-2-d3edf966b095>"", line 3, in foo
AttributeError: module 'scipy' has no attribute 'interpolate'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:197)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:238)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:156)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[Stage 0:>                                                          (0 + 8) / 8]18/10/23 15:39:54 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 196, in main
    process()
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 191, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/bryan/git/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""/home/bryan/git/spark/python/pyspark/rdd.py"", line 1951, in <lambda>
    map_values_fn = lambda kv: (kv[0], f(kv[1]))
  File ""<ipython-input-2-d3edf966b095>"", line 3, in foo
AttributeError: module 'scipy' has no attribute 'interpolate'

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:197)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:238)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:156)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

18/10/23 15:39:54 ERROR TaskSetManager: Task 7 in stage 0.0 failed 1 times; aborting job
it errored
worked                                                                          
worked
{noformat};;;","23/Oct/18 23:11;dongjoon;Hi, [~bryanc]. It seems that the test occurs in `branch-2.2`. Could you confirm 2.3.2, too?;;;","24/Oct/18 00:15;bryanc;Sure, I probably shouldn't have tested out of the branches. Running tests again from IPython with Python 3.6.6:

 

*v2.2.2* - Error is raised

*v2.3.2* - Working

*v2.4.0-rc4* - Working

 

From those results, it seems like SPARK-21070 most likely fixed it;;;",,,,,,,,,,,,,,,,,,,
Memory leak in Spark Thrift Server,SPARK-22793,13125114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,15/Dec/17 07:16,17/Jan/18 13:13,14/Jul/23 06:30,06/Jan/18 10:11,2.0.2,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"1. Start HiveThriftServer2.
2. Connect to thriftserver through beeline.
3. Close the beeline.
4. repeat step2 and step 3 for several times, which caused the leak of Memory.

we found there are many directories never be dropped under the path
{code:java}
hive.exec.local.scratchdir
{code} and 
{code:java}
hive.exec.scratchdir
{code} , as we know the scratchdir has been added to deleteOnExit when it be created. So it means that the cache size of FileSystem deleteOnExit will keep increasing until JVM terminated.

In addition, we use 
{code:java}
jmap -histo:live [PID]
{code} to printout the size of objects in HiveThriftServer2 Process, we can find the object ""org.apache.spark.sql.hive.client.HiveClientImpl"" and ""org.apache.hadoop.hive.ql.session.SessionState"" keep increasing even though we closed all the beeline connections, which caused the leak of Memory.


",,apachespark,dongjoon,maropu,mgaido,toopt4,xwc3504,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15401,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 08:09:05 UTC 2017,,,,,,,,,,"0|i3nxkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/17 08:09;zuo.tingbing9;
{code:java}
lazy val metadataHive: HiveClient = sharedState.metadataHive.newSession()
{code}

HiveClient has been created by 
{code:java}
sharedState.metadataHive
{code}
but will be created again in
{code:java}
.newSession()
{code}

;;;","15/Dec/17 08:55;mgaido;Have you tried if the problem still exists in current master branch?;;;","15/Dec/17 09:36;zuo.tingbing9;ok, i will try to check it in master branch. Thanks.;;;","15/Dec/17 10:25;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19989;;;","15/Dec/17 10:31;zuo.tingbing9;yes the master branch also has this problem.;;;","20/Dec/17 08:09;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/20029;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redact Output of Explain,SPARK-22791,13125112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,15/Dec/17 07:02,19/Dec/17 14:13,14/Jul/23 06:30,19/Dec/17 14:13,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When calling explain on a query, the output can contain sensitive information. We should provide an admin/user to redact such information.
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 15 07:05:04 UTC 2017,,,,,,,,,,"0|i3nxk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/17 07:05;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19985;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HdfsUtils.getOutputStream uses non-existent Hadoop conf ""hdfs.append.support""",SPARK-22788,13125037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,vanzin,vanzin,vanzin,14/Dec/17 20:38,20/Dec/17 17:31,14/Jul/23 06:30,20/Dec/17 17:31,1.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,DStreams,,,,,0,,,,,,,,,"Code: 

{noformat}
        if (conf.getBoolean(""hdfs.append.support"", false) || dfs.isInstanceOf[RawLocalFileSystem]) {
          dfs.append(dfsPath)
        } else {
          throw new IllegalStateException(""File exists and there is no append support!"")
        }
{noformat}

This makes the exception to be thrown if you enable {{writeAheadLog.closeFileAfterWrite}} with HDFS.

The correct config, from {{DFSConfigKeys}}, is {{dfs.support.append}}, and the default value is true.",,apachespark,irashid,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 17:31:33 UTC 2017,,,,,,,,,,"0|i3nx3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/17 21:10;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19983;;;","20/Dec/17 17:31;irashid;Issue resolved by pull request 19983
[https://github.com/apache/spark/pull/19983];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigEntry's default value should actually be a value,SPARK-22779,13124810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,rxin,rxin,13/Dec/17 22:43,21/Aug/18 18:59,14/Jul/23 06:30,14/Dec/17 06:47,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,ConfigEntry's config value right now shows a human readable message. In some places in SQL we actually rely on default value for real to be setting the values. ,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 18:59:05 UTC 2018,,,,,,,,,,"0|i3nvpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/17 22:46;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19973;;;","14/Dec/17 02:53;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19974;;;","21/Aug/18 18:59;apachespark;User 'GregOwen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22174;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes scheduler at master failing to run applications successfully,SPARK-22778,13124774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,liyinan926,foxish,foxish,13/Dec/17 20:14,17/May/20 18:23,14/Jul/23 06:30,14/Dec/17 22:03,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,,0,,,,,,,,,"Building images based on master and deploying Spark PI results in the following error.

2017-12-13 19:57:19 INFO  SparkContext:54 - Successfully stopped SparkContext
Exception in thread ""main"" org.apache.spark.SparkException: Could not parse Master URL: 'k8s:https://xx.yy.zz.ww'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2741)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:496)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2490)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:927)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:918)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:918)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
2017-12-13 19:57:19 INFO  ShutdownHookManager:54 - Shutdown hook called
2017-12-13 19:57:19 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-b47515c2-6750-4a37-aa68-6ee12da5d2bd

This is likely an artifact seen because of changes in master, or our submission code in the reviews. We haven't seen this on our fork. Hopefully once integration tests are ported against upstream/master, we will catch these issues earlier. ",,apachespark,foxish,liyinan926,mcheah,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 14 22:03:27 UTC 2017,,,,,,,,,,"0|i3nvhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/17 20:15;foxish;[~mcheah] [~kimoonkim] [~ifilonenko] PTAL;;;","13/Dec/17 20:18;mcheah;Think that URI should be 'k8s://https://xx.yy.zz.ww' - notice the extra slashes?;;;","13/Dec/17 20:21;foxish;I submitted it as `k8s://https://xx.yy.zz.ww` to spark submit. However, it seems there is some change in how the validation of said URL occurs on the client-side - which makes us strip out the k8s and add it back in the above format. That might be at fault here. 

Here's my full spark-submit command:

bin/spark-submit \
  --deploy-mode cluster \
  --class org.apache.spark.examples.SparkPi \
  --master k8s://https://xx.yy.zz.ww \
  --conf spark.executor.instances=5 \
  --conf spark.app.name=spark-pi \
  --conf spark.kubernetes.driver.docker.image=foxish/spark-driver:spark-k8s-master-13dec-11-56 \
  --conf spark.kubernetes.executor.docker.image=foxish/spark-executor:spark-k8s-master-13dec-11-56 \
  local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0-SNAPSHOT.jar;;;","13/Dec/17 20:23;liyinan926;Just some background on this. The validation and parsing of k8s master url has been moved to SparkSubmit as being suggested in the review. The parsed master URL (https://... for example) is appended a {{k8s}} prefix after the parsing to satisfy {{KubernetesClusterManager}}, whose {{canCreate}} method is based on if the master URL starts {{k8s}}. That's why you see the {{k8s:}} prefix. The issue seems that in the driver pod {{SparkContext}} could not find {{KubernetesClusterManager}} based on the debug messages I added. The code that triggered the error (with the debugging I added) is as follows:

{code:java}
private def getClusterManager(url: String): Option[ExternalClusterManager] = {
    val loader = Utils.getContextOrSparkClassLoader
    val serviceLoaders =
      ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala
    serviceLoaders.foreach { loader =>
      logInfo(s""Found the following external cluster manager: $loader"")
    }

    val filteredServiceLoaders = serviceLoaders.filter(_.canCreate(url))
    if (filteredServiceLoaders.size > 1) {
      throw new SparkException(
        s""Multiple external cluster managers registered for the url $url: $serviceLoaders"")
    } else if (filteredServiceLoaders.isEmpty) {
      logWarning(s""No external cluster manager registered for url $url"")
    }
    filteredServiceLoaders.headOption
  }
{code}

And I got the following:
{code:java}
No external cluster manager registered for url k8s:https://35.226.8.173
{code}
;;;","13/Dec/17 20:27;mcheah;The {{canCreate}} method for {{KubernetesClusterManager}} should match that URI. The primary possibility I can think of is that the {{KubernetesClusterManager}} isn't being service loaded at all, which would imply that {{spark-kubernetes}} isn't on the classpath. Can we verify that the Docker image contains all of the correct jars?;;;","13/Dec/17 20:41;foxish;I've verified that the image contains the right jars.
One more thing that changed from underneath us is https://github.com/apache/spark/pull/19631. Not sure yet if that's related.;;;","13/Dec/17 20:45;mcheah;I see the problem. We're missing the {{META-INF.services}} file that tells service loaders to include our implementation. See https://github.com/apache-spark-on-k8s/spark/blob/branch-2.2-kubernetes/resource-managers/kubernetes/core/src/main/resources/META-INF/services/org.apache.spark.scheduler.ExternalClusterManager.;;;","13/Dec/17 20:45;mcheah;And then notice we don't even have a {{resources}} directory on master: https://github.com/apache/spark/tree/master/resource-managers/kubernetes/core/src.;;;","13/Dec/17 20:46;liyinan926;Ah, yes, the PR missed that. OK, I'm gonna give that a try and submit a PR to fix it.;;;","13/Dec/17 20:49;foxish;Excellent - thanks for the quick debug Matt. Now waiting for confirmation that the fix is sufficient. I also suggest we fix the URL format to prepend `k8s://` as opposed to just `k8s:` in the interest of having a URL looking more well-formed. ;;;","13/Dec/17 21:03;liyinan926;Just verified that the fix worked. I'm gonna send a PR soon.;;;","13/Dec/17 21:14;apachespark;User 'liyinan926' has created a pull request for this issue:
https://github.com/apache/spark/pull/19972;;;","14/Dec/17 22:03;vanzin;Issue resolved by pull request 19972
[https://github.com/apache/spark/pull/19972];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Docker container built for Kubernetes doesn't allow running entrypoint.sh,SPARK-22777,13124770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,foxish,foxish,foxish,13/Dec/17 19:59,17/May/20 18:23,14/Jul/23 06:30,18/Dec/17 23:32,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Kubernetes,Spark Core,,,,0,,,,,,,,,"Default docker images built thrown an error when trying to run on a cluster. The error looks like the following:


```
  9s		9s		1	kubelet, gke-jupytercluster2-default-pool-6be20085-4nm4spec.containers{spark-kubernetes-driver}	Warning		Failed			Error: failed to start container ""spark-kubernetes-driver"": Error response from daemon: {""message"":""oci runtime error: container_linux.go:247: starting container process caused \""exec: \\\""/opt/entrypoint.sh\\\"": permission denied\""\n""}
```

The fix is probably just changing permissions for the entrypoint script in the default docker image.",,apachespark,foxish,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 18 23:32:03 UTC 2017,,,,,,,,,,"0|i3nvgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/17 20:00;foxish;[~liyinan926] [~ssuchter] [~kimoonkim];;;","18/Dec/17 10:29;apachespark;User 'foxish' has created a pull request for this issue:
https://github.com/apache/spark/pull/20007;;;","18/Dec/17 23:32;vanzin;Issue resolved by pull request 20007
[https://github.com/apache/spark/pull/20007];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When driver stopping, there is errors: ""Could not find CoarseGrainedScheduler"" and ""RpcEnv already stopped""",SPARK-22769,13124595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sqlwindspeaker,KaiXinXIaoLei,KaiXinXIaoLei,13/Dec/17 07:52,17/Dec/20 14:57,14/Jul/23 06:30,17/Dec/20 14:57,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"I run ""spark-sql --master yarn --num-executors 1000 -f createTable.sql"". When task is finished, there is a error: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler. I think the log level should be warning, not error.
{noformat}
17/12/12 18:30:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/12 18:30:16 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:134)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:180)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
{noformat}

and another error is :
{noformat}
17/12/12 18:20:44 INFO MemoryStore: MemoryStore cleared
17/12/12 18:20:44 INFO BlockManager: BlockManager stopped
17/12/12 18:20:44 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/12 18:20:44 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
        at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:134)
        at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:570)
        at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:180)
        at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
{noformat}",,apachespark,chengbing.liu,KaiXinXIaoLei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22770,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 14:57:10 UTC 2020,,,,,,,,,,"0|i3nudz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/17 12:44;apachespark;User 'KaiXinXiaoLei' has created a pull request for this issue:
https://github.com/apache/spark/pull/19965;;;","13/Dec/17 14:16;apachespark;User 'KaiXinXiaoLei' has created a pull request for this issue:
https://github.com/apache/spark/pull/19968;;;","08/Dec/20 01:29;apachespark;User 'sqlwindspeaker' has created a pull request for this issue:
https://github.com/apache/spark/pull/30658;;;","10/Dec/20 02:12;sqlwindspeaker;The original reporter gave up work on this，i make a new change for this;;;","17/Dec/20 14:57;srowen;Issue resolved by pull request 30658
[https://github.com/apache/spark/pull/30658];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: SparkContextSuite ""Cancelling stages/jobs with custom reasons""",SPARK-22764,13124496,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,12/Dec/17 21:41,20/Jan/20 05:01,14/Jul/23 06:30,13/Dec/17 22:06,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Saw this in a PR builder:

{noformat}
[info] - Cancelling stages/jobs with custom reasons. *** FAILED *** (135 milliseconds)
[info]   Expected exception org.apache.spark.SparkException to be thrown, but no exception was thrown (SparkContextSuite.scala:531)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
[info]   at org.scalatest.Assertions$class.intercept(Assertions.scala:822)
[info]   at org.scalatest.FunSuite.intercept(FunSuite.scala:1560)

{noformat}

From the logs, the job is finishing before the test code cancels it:

{noformat}
17/12/12 11:00:41.680 Executor task launch worker for task 1 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 703 bytes result sent to driver
17/12/12 11:00:41.681 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 13 ms on localhost (executor driver) (1/1)
17/12/12 11:00:41.681 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/12/12 11:00:41.681 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (apply at Assertions.scala:805) finished in 0.066 s
17/12/12 11:00:41.681 pool-1-thread-1-ScalaTest-running-SparkContextSuite INFO DAGScheduler: Job 1 finished: apply at Assertions.scala:805, took 0.066946 s
17/12/12 11:00:41.682 spark-listener-group-shared INFO DAGScheduler: Asked to cancel job 1
{noformat}
",,apachespark,irashid,kabhwan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 05:01:18 UTC 2020,,,,,,,,,,"0|i3ntrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/17 22:16;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19956;;;","13/Dec/17 22:06;irashid;Issue resolved by pull request 19956
[https://github.com/apache/spark/pull/19956];;;","20/Jan/20 05:01;kabhwan;This test failure comes in again, but given it's for 3.0 and the error message is different, I'll file a new issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filters can be combined iff both are deterministic,SPARK-22759,13124248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,12/Dec/17 00:51,17/May/20 17:58,14/Jul/23 06:30,27/Jul/19 18:05,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Optimizer,SQL,,,,0,,,,,,,,,The query execution/optimization does not guarantee the expressions are evaluated in order. We only can combine them if and only if both are deterministic. We need to update the optimizer rule: CombineFilters.,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 12 00:52:04 UTC 2017,,,,,,,,,,"0|i3ns93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/17 00:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19947;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expression (946-885)*1.0/946 < 0.1 and (946-885)*1.000/946 < 0.1 return different results,SPARK-22755,13124164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,KevinZwx,KevinZwx,11/Dec/17 16:34,13/Dec/17 02:11,14/Jul/23 06:30,12/Dec/17 18:56,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,,,,,SQL,,,,,0,,,,,,,,,"both of the following sql statements
{code:sql}
select ((946-885)*1.000/946 < 0.1)
{code}
and
{code:sql}
select ((946-885)*1.0/946 < 0.100)
{code}
return true, while the following statement 

{code:sql}
select ((946-885)*1.0/946 < 0.1)
{code}
returns false



",,KevinZwx,ksunitha,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21332,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 13 02:11:16 UTC 2017,,,,,,,,,,"0|i3nrqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/17 16:59;srowen;I assume the problem is precision, that the last expression is using only one decimal place. Mathematically it should be true, but, I'm not expert enough on SQL semantics to know whether the final expression is actually supposed to be true.;;;","11/Dec/17 20:01;ksunitha;I just tried this in sql on the trunk codeline, and both those statements return true: 
{code}
  test(""SPARK-22755"") {
    spark.sql(""select ((946-885)*1.0/946 < 0.1)"").show
    spark.sql(""select ((946-885)*1.0/946 < 0.100)"").show
  }

+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|(CAST((CAST((CAST(CAST((946 - 885) AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(1.0 AS DECIMAL(11,1))) AS DECIMAL(13,1)) / CAST(CAST(946 AS DECIMAL(10,0)) AS DECIMAL(13,1))) AS DECIMAL(24,12)) < CAST(0.1 AS DECIMAL(24,12)))|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                         true|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|(CAST((CAST((CAST(CAST((946 - 885) AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(1.0 AS DECIMAL(11,1))) AS DECIMAL(13,1)) / CAST(CAST(946 AS DECIMAL(10,0)) AS DECIMAL(13,1))) AS DECIMAL(24,12)) < CAST(0.100 AS DECIMAL(24,12)))|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                           true|
+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

{code};;;","12/Dec/17 01:54;KevinZwx;Thanks for reply. In hive and presto the result is supposed to be true. 

I tried another time in both spark sql repl and thrift-server, the statement `select ((946-885)*1.0/946 < 0.1` still returns false, and I used spark 2.2 instead of the trunk codeline


{code:sql}
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|(CAST((CAST((CAST(CAST((946 - 885) AS DECIMAL(10,0)) AS DECIMAL(11,1)) * CAST(1.0 AS DECIMAL(11,1))) AS DECIMAL(13,1)) / CAST(CAST(946 AS DECIMAL(13,1)) AS DECIMAL(13,1))) AS DECIMAL(13,1)) < CAST(0.1 AS DECIMAL(13,1)))|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                      false|
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{code}

and I found out my physical plan is different from yours in the decimal precision and scale, is there any configuration influences? or the bug has been fixed by some issue? 
;;;","12/Dec/17 18:11;ksunitha;
A) TRUNK:
- My trunk codeline is sync'd up to Dec 7, commit 2d4c2b0bdf89badf25f2d1d98903125e48e7cd5c
- That said,  I tried these statements now in spark sql repl and the answer is true.   
- I have not set any configuration, it is just the defaults in my dev env. 

B) Spark 2.2:
- I downloaded the spark 2.2 binaries and tried the queries in spark-sql repl and I can repro the issue. 
{code}
select ((946-885)*1.0/946 < 0.1) -> returns false
select ((946-885)*1.0/946 < 0.100)  -> returns true
{code}

So it looks like some fix has gone in as I cannot repro it on trunk.   
I am not sure which issue has fixed it but just wanted to add this info for now. ;;;","12/Dec/17 18:49;ksunitha;SPARK-21332 fixed this issue.   
The changes for it are also in Spark-2.2.1 that is released on Dec 1.  I tried it on Spark-2.2.1 and the queries return true. 

;;;","13/Dec/17 02:11;KevinZwx;[~ksunitha] Thanks, it helps a lot;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Error in query: grouping_id() can only be used with GroupingSets/Cube/Rollup;",SPARK-22748,13123966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,cenyuhai,cenyuhai,10/Dec/17 07:22,07/Mar/21 06:58,14/Jul/23 06:30,05/Mar/21 07:42,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,SQL,,,,,0,bulk-closed,,,,,,,,"sql
{code}
create table temp.test_grouping_replace(a int, b int);
select grouping__id from (select a, b, count(1) , grouping__id from temp.test_grouping_replace group by a, b grouping sets(a, b))
{code}
Exception
{code}
Error in query: grouping_id() can only be used with GroupingSets/Cube/Rollup;;
!Project [grouping_id() AS grouping__id#78]
+- Aggregate [a#76, b#77, spark_grouping_id#73], [a#76, b#77, count(1) AS count(1)#72L, spark_grouping_id#73 AS grouping__id#71]
   +- Expand [List(a#69, b#70, a#74, null, 1), List(a#69, b#70, null, b#75, 2)], [a#69, b#70, a#76, b#77, spark_grouping_id#73]
      +- Project [a#69, b#70, a#69 AS a#74, b#70 AS b#75]
         +- MetastoreRelation temp, test_grouping_replace
{code}

this is a FOLLOW-UP issue of SPARK-21055",,apachespark,cenyuhai,maropu,smurakozi,sqlwindspeaker,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 04:41:22 UTC 2021,,,,,,,,,,"0|i3nqin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/17 11:45;yumwang;Do you mean this?
{code:sql}
select gid from (select a, b, count(1), grouping__id as gid from temp.test_grouping_replace group by a, b grouping sets(a, b));
{code};;;","10/Dec/17 11:54;cenyuhai; your sql is ok;;;","15/Dec/20 13:12;sqlwindspeaker;This issue did not be fixed;;;","15/Dec/20 13:32;apachespark;User 'sqlwindspeaker' has created a pull request for this issue:
https://github.com/apache/spark/pull/30781;;;","16/Dec/20 01:10;sqlwindspeaker;[~yumwang]，[~hyukjin.kwon] could u take a look of this issue?;;;","05/Mar/21 04:40;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31751;;;","05/Mar/21 04:41;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31751;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BytesToBytesMap peak memory usage not accurate after reset(),SPARK-22721,13123292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,06/Dec/17 20:23,07/Dec/17 21:31,14/Jul/23 06:30,07/Dec/17 12:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"BytesToBytesMap doesn't update peak memory usage before shrinking back to initial capacity in reset(), so after a disk spill one never knows what was the size of hash table was before spilling.",,apachespark,codingcat,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 21:31:04 UTC 2017,,,,,,,,,,"0|i3nmd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/17 20:27;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/19915;;;","07/Dec/17 21:31;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/19923;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM caused by the memory contention and memory leak in TaskMemoryManager,SPARK-22713,13123128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,eyalfa,jerrylead,jerrylead,06/Dec/17 09:00,16/Sep/18 03:22,14/Jul/23 06:30,13/Aug/18 12:56,2.1.1,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Shuffle,Spark Core,,,,0,,,,,,,,,"The pdf version of this issue with high-quality figures is available at https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/report/OOM-TaskMemoryManager.pdf.

*[Abstract]* 

I recently encountered an OOM error in a PageRank application (_org.apache.spark.examples.SparkPageRank_). After profiling the application, I found the OOM error is related to the memory contention in shuffle spill phase. Here, the memory contention means that a task tries to release some old memory consumers from memory for keeping the new memory consumers. After analyzing the OOM heap dump, I found the root cause is a memory leak in _TaskMemoryManager_. Since memory contention is common in shuffle phase, this is a critical bug/defect. In the following sections, I will use the application dataflow, execution log, heap dump, and source code to identify the root cause.


*[Application]* 

This is a PageRank application from Spark’s example library. The following figure shows the application dataflow. The source code is available at \[1\].

!https://raw.githubusercontent.com/JerryLead/Misc/master/OOM-TasksMemoryManager/figures/PageRankDataflow.png|width=100%!

*[Failure symptoms]*

This application has a map stage and many iterative reduce stages. An OOM error occurs in a reduce task (Task-28) as follows.

!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/Stage.png?raw=true|width=100%!

!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/task.png?raw=true|width=100%!
 
*[OOM root cause identification]*

Each executor has 1 CPU core and 6.5GB memory, so it only runs one task at a time. After analyzing the application dataflow, error log, heap dump, and source code, I found the following steps lead to the OOM error. 

=> The MemoryManager found that there is not enough memory to cache the _links:ShuffledRDD_ (rdd-5-28, red circles in the dataflow figure).
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/ShuffledRDD.png?raw=true|width=100%!

=> The task needs to shuffle twice (1st shuffle and 2nd shuffle in the dataflow figure).
=> The task needs to generate two _ExternalAppendOnlyMap_ (E1 for 1st shuffle and E2 for 2nd shuffle) in sequence.
=> The 1st shuffle begins and ends. E1 aggregates all the shuffled data of 1st shuffle and achieves 3.3 GB.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/FirstShuffle.png?raw=true|width=100%!
=> The 2nd shuffle begins. E2 is aggregating the shuffled data of 2nd shuffle, and finding that there is not enough memory left. This triggers the memory contention.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/SecondShuffle.png?raw=true|width=100%!
=> To handle the memory contention, the _TaskMemoryManager_ releases E1 (spills it onto disk) and assumes that the 3.3GB space is free now.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/MemoryContention.png?raw=true|width=100%!
=> E2 continues to aggregates the shuffled records of 2nd shuffle. However, E2 encounters an OOM error while shuffling.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/OOMbefore.png?raw=true|width=100%!
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/OOMError.png?raw=true|width=100%!

*[Guess]* 
The task memory usage below reveals that there is not memory drop down. So, the cause may be that the 3.3GB _ExternalAppendOnlyMap_ (E1) is not actually released by the _TaskMemoryManger_. 
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/GCFigure.png?raw=true|width=100%!


*[Root cause]* 
After analyzing the heap dump, I found the guess is right (the 3.3GB _ExternalAppendOnlyMap_ is actually not released). The 1.6GB object is _ExternalAppendOnlyMap (E2)_.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/heapdump.png?raw=true|width=100%!

*[Question]* 
Why the released _ExternalAppendOnlyMap_ is still in memory?
The source code of _ExternalAppendOnlyMap_ shows that the _currentMap_ (_AppendOnlyMap_) has been set to _null_ when the spill action is finished.
!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/SourceCode.png?raw=true|width=100%!

*[Root cause in the source code]* I further analyze the reference chain of unreleased _ExternalAppendOnlyMap_. The reference chain shows that the 3.3GB _ExternalAppendOnlyMap_ is still referenced by the _upstream/readingIterator_ and further referenced by _TaskMemoryManager_ as follows. So, the root cause in the source code is that the _ExternalAppendOnlyMap_ is still referenced by other iterators (setting the _currentMap_ to _null_ is not enough).

!https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/figures/References.png?raw=true|width=100%!

*[Potential solution]*

Setting the _upstream/readingIterator_ to _null_ after the _forceSpill_() action. I will try this solution in these days.

[References]
[1] PageRank source code. https://github.com/JerryLead/SparkGC/blob/master/src/main/scala/applications/graph/PageRank.scala
[2] Task execution log. https://github.com/JerryLead/Misc/blob/master/OOM-TasksMemoryManager/log/TaskExecutionLog.txt 



",,apachespark,cloud_fan,desmoon,Dhruve Ashar,eyalfa,felixcheung,huasanyelao,java8964,jerrylead,kiszk,Kotomi,suj1th,toopt4,viirya,wxl24life,xwc3504,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 13 14:10:51 UTC 2018,,,,,,,,,,"0|i3nld3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/May/18 04:54;eyalfa;[~jerrylead], excellent investigation and description of the issue, I'll open a PR shortly.;;;","19/May/18 05:12;apachespark;User 'eyalfa' has created a pull request for this issue:
https://github.com/apache/spark/pull/21369;;;","20/May/18 09:32;eyalfa;[~jerrylead], what's the relation to SPARK-22286? does solving this at least partially solve that as well?;;;","13/Aug/18 12:56;cloud_fan;Issue resolved by pull request 21369
[https://github.com/apache/spark/pull/21369];;;","13/Aug/18 14:10;eyalfa;[~jerrylead], can you please test this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use `buildReaderWithPartitionValues` in native OrcFileFormat,SPARK-22712,13123122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,06/Dec/17 08:42,07/Dec/17 13:09,14/Jul/23 06:30,07/Dec/17 13:09,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"To support vectorization in native OrcFileFormat, we need to use `buildReaderWithPartitionValues` instead of `buildReader`.",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 13:09:17 UTC 2017,,,,,,,,,,"0|i3nlbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/17 08:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19907;;;","07/Dec/17 13:09;cloud_fan;Issue resolved by pull request 19907
[https://github.com/apache/spark/pull/19907];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigBuilder.fallbackConf doesn't trigger onCreate function,SPARK-22710,13123107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,06/Dec/17 07:08,06/Dec/17 18:11,14/Jul/23 06:30,06/Dec/17 18:11,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"I was looking at the config code today and found that configs defined using ConfigBuilder.fallbackConf didn't trigger onCreate function. We should fix it. This doesn't require backporting since we currently have no configs that use it.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 07:13:04 UTC 2017,,,,,,,,,,"0|i3nl8f:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"06/Dec/17 07:13;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucketizer.transform incorrectly drops row containing NaN,SPARK-22700,13122850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,05/Dec/17 11:44,22/Feb/18 01:27,14/Jul/23 06:30,13/Dec/17 07:10,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,ML,,,,,0,,,,,,,,,"{code}
import org.apache.spark.ml.feature._

val df = spark.createDataFrame(Seq((2.3, 3.0), (Double.NaN, 3.0), (6.7, Double.NaN))).toDF(""a"", ""b"")

val splits = Array(Double.NegativeInfinity, 3.0, Double.PositiveInfinity)

val bucketizer: Bucketizer = new Bucketizer().setInputCol(""a"").setOutputCol(""aa"").setSplits(splits)

bucketizer.setHandleInvalid(""skip"")

scala> df.show
+---+---+
|  a|  b|
+---+---+
|2.3|3.0|
|NaN|3.0|
|6.7|NaN|
+---+---+

scala> bucketizer.transform(df).show
+---+---+---+
|  a|  b| aa|
+---+---+---+
|2.3|3.0|0.0|
+---+---+---+

{code}

When {{handleInvalid}} is set {{skip}}, the last item in input is incorrectly droped, though colum 'b' is not an input column",,apachespark,josephkb,mlnick,podongfeng,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 22 01:27:24 UTC 2018,,,,,,,,,,"0|i3njnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/17 11:51;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/19894;;;","13/Dec/17 07:10;mlnick;Issue resolved by pull request 19894
[https://github.com/apache/spark/pull/19894];;;","08/Feb/18 01:42;weichenxu123;[~podongfeng] Have you checked other transformers with handleInvalid option for the similar issue ?;;;","08/Feb/18 03:27;podongfeng;[~WeichenXu123] I have checked others, and them seems ok;;;","08/Feb/18 03:35;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/20539;;;","22/Feb/18 01:27;josephkb;Resolved for branch-2.2 via https://github.com/apache/spark/pull/20539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DROP TABLE IF EXISTS should not show AnalysisException,SPARK-22686,13122755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,05/Dec/17 05:01,06/Dec/17 02:54,14/Jul/23 06:30,06/Dec/17 02:54,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,,0,,,,,,,,,"During SPARK-22488 to Fix the view resolution issue, there occurs a regression at 2.2.1 and master branch like the following.

{code}
scala> spark.version
res2: String = 2.2.1

scala> sql(""DROP TABLE IF EXISTS t"").show
17/12/04 21:01:06 WARN DropTableCommand: org.apache.spark.sql.AnalysisException: Table or view not found: t;
org.apache.spark.sql.AnalysisException: Table or view not found: t;
{code}",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 02:54:12 UTC 2017,,,,,,,,,,"0|i3nj2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/17 05:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19888;;;","06/Dec/17 02:54;cloud_fan;Issue resolved by pull request 19888
[https://github.com/apache/spark/pull/19888];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accumulator should only be updated once for each task in result stage,SPARK-22681,13122565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carsonwang,carsonwang,carsonwang,04/Dec/17 12:31,05/Dec/17 17:15,14/Jul/23 06:30,05/Dec/17 17:15,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"As the doc says ""For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value.""
But currently the code doesn't guarantee this. ",,apachespark,carsonwang,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 17:15:37 UTC 2017,,,,,,,,,,"0|i3nhxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/17 12:36;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19877;;;","05/Dec/17 17:15;vanzin;Issue resolved by pull request 19877
[https://github.com/apache/spark/pull/19877];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid iterating all partition paths when spark.sql.hive.verifyPartitionPath=true,SPARK-22676,13122410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,03/Dec/17 06:59,08/Jan/20 09:48,14/Jul/23 06:30,17/Apr/18 13:53,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,SQL,,,,,0,,,,,,,,,"In current code, it will scanning all partition paths when spark.sql.hive.verifyPartitionPath=true.
e.g. table like below:
CREATE TABLE `test`(
  `id` int,
  `age` int,
  `name` string)
PARTITIONED BY (
  `A` string,
  `B` string)
load data local inpath '/tmp/data1' into table test partition(A='00', B='00')
load data local inpath '/tmp/data1' into table test partition(A='01', B='01')
load data local inpath '/tmp/data1' into table test partition(A='10', B='10')
load data local inpath '/tmp/data1' into table test partition(A='11', B='11')

If I query with SQL -- ""select * from test where year=2017 and month=12 and day=03"", current code will scan all partition paths including '/data/A=00/B=00', '/data/A=00/B=00', '/data/A=01/B=01', '/data/A=10/B=10', '/data/A=11/B=11'. It costs much time and memory cost. ",,apachespark,cloud_fan,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 03:40:05 UTC 2018,,,,,,,,,,"0|i3ngzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/17 07:11;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/19868;;;","17/Apr/18 13:53;cloud_fan;Issue resolved by pull request 19868
[https://github.com/apache/spark/pull/19868];;;","18/Apr/18 03:40;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/21091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CodegenContext.splitExpressions() creates incorrect results with global variable arguments ,SPARK-22668,13122126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,kiszk,kiszk,01/Dec/17 08:53,21/Dec/17 16:22,14/Jul/23 06:30,21/Dec/17 16:22,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"{{CodegenContext.splitExpressions()}} creates incorrect results with arguments that were declared as global variable.

{code}
class Test {
  int global1;

  void splittedFunction(int global1) {
    ...
    global1 = 2;
  }

  void apply() {
    global1 = 1;
    ...
    splittedFunction(global1);
    // global1 should be 2
  }
}
{code}",,apachespark,cloud_fan,kiszk,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 16:22:19 UTC 2017,,,,,,,,,,"0|i3nf9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/17 08:54;kiszk;I am working for this.;;;","02/Dec/17 06:10;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19865;;;","19/Dec/17 15:50;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/20021;;;","21/Dec/17 16:22;cloud_fan;Issue resolved by pull request 20021
[https://github.com/apache/spark/pull/20021];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to prune columns after rewriting predicate subquery,SPARK-22662,13121855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,30/Nov/17 08:57,20/Oct/18 01:43,14/Jul/23 06:30,05/Dec/17 23:16,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"As a simple example:
{code}
spark-sql> create table base (a int, b int) using parquet;
Time taken: 0.066 seconds
spark-sql> create table relInSubq ( x int, y int, z int) using parquet;
Time taken: 0.042 seconds
spark-sql> explain select a from base where a in (select x from relInSubq);
== Physical Plan ==
*Project [a#83]
+- *BroadcastHashJoin [a#83], [x#85], LeftSemi, BuildRight
   :- *FileScan parquet default.base[a#83,b#84] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://100.0.0.4:9000/wzh/base], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<a:int,b:int>
   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))
      +- *Project [x#85]
         +- *FileScan parquet default.relinsubq[x#85] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://100.0.0.4:9000/wzh/relinsubq], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<x:int>
{code}

We only need column `a` in table `base`, but all columns (`a`, `b`) are fetched.
",,apachespark,maropu,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25784,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 30 09:04:04 UTC 2017,,,,,,,,,,"0|i3ndl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/17 09:04;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/19855;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail task instead of complete task silently in PythonRunner during shutdown,SPARK-22655,13121744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,icexelloss,icexelloss,icexelloss,29/Nov/17 22:04,12/Dec/22 18:11,14/Jul/23 06:30,08/Dec/17 11:44,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"We have observed in our production environment that during Spark shutdown, if there are some active tasks, sometimes they will complete with incorrect results. We've tracked down the issue to a PythonRunner where it is returning partial result instead of throwing exception during Spark shutdown. 

I think the better way to handle this is to have these tasks fail instead of complete with partial results (complete with partial is always bad IMHO)",,apachespark,icexelloss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 08 11:44:57 UTC 2017,,,,,,,,,,"0|i3ncwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 22:07;icexelloss;PR: https://github.com/apache/spark/pull/19852;;;","29/Nov/17 22:08;apachespark;User 'icexelloss' has created a pull request for this issue:
https://github.com/apache/spark/pull/19852;;;","08/Dec/17 11:44;gurwls223;Issue resolved by pull request 19852
[https://github.com/apache/spark/pull/19852];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retry download of Spark from ASF mirror in HiveExternalCatalogVersionsSuite,SPARK-22654,13121712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,29/Nov/17 19:04,12/Dec/22 18:10,14/Jul/23 06:30,30/Nov/17 16:22,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SQL,Tests,,,,0,,,,,,,,,"HiveExternalCatalogVersionsSuite has failed a few times apparently after failing to download Spark tarballs from a particular mirror. This could be mitigated with some retry logic, at least.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23489,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 30 16:22:45 UTC 2017,,,,,,,,,,"0|i3ncpr:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.0,,,,,,,,,,"29/Nov/17 19:07;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19851;;;","30/Nov/17 16:22;gurwls223;Issue resolved by pull request 19851
[https://github.com/apache/spark/pull/19851];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executorAddress registered in CoarseGrainedSchedulerBackend.executorDataMap is null,SPARK-22653,13121678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,29/Nov/17 15:51,17/May/20 17:48,14/Jul/23 06:30,01/Dec/17 02:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"In CoarseGrainedSchedulerBackend.RegisterExecutor the executor data address (executorRef.address) can be null.

 val data = new ExecutorData(executorRef, executorRef.address, hostname,
            cores, cores, logUrls)

At this point the executorRef.address can be null, there is actually code above it that handles this case:

 // If the executor's rpc env is not listening for incoming connections, `hostPort`
          // will be null, and the client connection should be used to contact the executor.
          val executorAddress = if (executorRef.address != null) {
              executorRef.address
            } else {
              context.senderAddress
            }

But it doesn't use executorAddress when it creates the ExecutorData.

This causes removeExecutor to never remove it properly from addressToExecutorId.

            addressToExecutorId -= executorInfo.executorAddress

This is also a memory leak and can also call onDisconnected to call disableExecutor when it shouldn't.


",,apachespark,cloud_fan,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 01 02:54:58 UTC 2017,,,,,,,,,,"0|i3nci7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 15:51;tgraves;will have patch up shortly;;;","29/Nov/17 18:02;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/19849;;;","29/Nov/17 18:05;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/19850;;;","01/Dec/17 02:54;cloud_fan;Issue resolved by pull request 19850
[https://github.com/apache/spark/pull/19850];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling ImageSchema.readImages initiate multiple Hive clients,SPARK-22651,13121642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,29/Nov/17 13:14,12/Dec/22 18:10,14/Jul/23 06:30,02/Dec/17 02:56,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,PySpark,,,,0,,,,,,,,,"While playing with images, I realised calling {{ImageSchema.readImages}} multiple times seems attempting to create multiple Hive clients.

{code}
from pyspark.ml.image import ImageSchema
data_path = 'data/mllib/images/kittens'
_ = ImageSchema.readImages(data_path, recursive=True, dropImageFailures=True).collect()
_ = ImageSchema.readImages(data_path, recursive=True, dropImageFailures=True).collect()
{code}

{code}
...
org.datanucleus.exceptions.NucleusDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@742f639f, see the next exception for details.
...
	at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)
...
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
...
	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:180)
...
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:348)
	at org.apache.spark.ml.image.ImageSchema$$anonfun$readImages$2$$anonfun$apply$1.apply(ImageSchema.scala:253)
...
Caused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@742f639f, see the next exception for details.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 121 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /.../spark/metastore_db.
...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/ml/image.py"", line 190, in readImages
    dropImageFailures, float(sampleRatio), seed)
  File ""/.../spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py"", line 1160, in __call__
  File ""/.../spark/python/pyspark/sql/utils.py"", line 69, in deco
    raise AnalysisException(s.split(': ', 1)[1], stackTrace)
pyspark.sql.utils.AnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;'
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 02 02:56:09 UTC 2017,,,,,,,,,,"0|i1u552:zzv",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 13:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19845;;;","02/Dec/17 02:56;gurwls223;Issue resolved by pull request 19845
[https://github.com/apache/spark/pull/19845];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the createdTempDir will not be deleted if an exception occurs,SPARK-22642,13121556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,29/Nov/17 06:14,11/Dec/17 19:36,14/Jul/23 06:30,11/Dec/17 19:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"We found staging directories will not be dropped sometimes in our production environment.
The createdTempDir will not be deleted if an exception occurs, we should delete createdTempDir in finally.

Refer to SPARK-18703。
",,apachespark,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 19:36:36 UTC 2017,,,,,,,,,,"0|i3nbrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 06:26;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19841;;;","29/Nov/17 13:15;srowen;This is hardly critical;;;","11/Dec/17 19:36;srowen;Issue resolved by pull request 19841
[https://github.com/apache/spark/pull/19841];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CatalogImpl.refresh() has quadratic complexity for a view,SPARK-22637,13121445,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,28/Nov/17 21:05,29/Nov/17 05:43,14/Jul/23 06:30,29/Nov/17 00:04,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{{org.apache.spark.sql.internal.CatalogImpl.refreshTable}} uses {{foreach(..)}} to refresh all tables in a view. This traverses all nodes in the subtree and calls {{LogicalPlan.refresh()}} on these nodes. However {{LogicalPlan.refresh()}} is also refreshing its children, as a result refreshing a large view can be quite expensive.",,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 28 21:15:05 UTC 2017,,,,,,,,,,"0|i3nb2f:",9223372036854775807,,,,,,,,,,,,,2.2.2,2.3.0,,,,,,,,,,"28/Nov/17 21:15;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/19837;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException again while reading ORC files containing special characters,SPARK-22635,13121367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,28/Nov/17 16:48,12/Dec/22 18:10,14/Jul/23 06:30,30/Nov/17 16:24,2.2.0,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,SQL,,,,,0,,,,,,,,,"SPARK-22146 fix the issue only for the {{inferSchema}}, ie. only for the schema inference, but it doesn't fix the problem when actually reading the data. Thus nearly the same exception happens when someone tries to use the data.

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 98, host-172-22-127-77.example.com, executor 3): java.io.FileNotFoundException: File does not exist: hdfs://XXX/tmp/aaa/start=2017-11-27%2009%253A30%253A00/part-00000-c1477c9f-9d48-4341-89de-81056b6b618e.c000.snappy.orc
It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,dongjoon,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 30 16:24:41 UTC 2017,,,,,,,,,,"0|i3nal3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 09:52;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19844;;;","30/Nov/17 16:24;gurwls223;Issue resolved by pull request 19844
[https://github.com/apache/spark/pull/19844];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD.unpersist can cause fatal exception when used with dynamic allocation,SPARK-22618,13121055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bradkaiser,bradkaiser,bradkaiser,27/Nov/17 18:39,29/Mar/18 14:11,14/Jul/23 06:30,07/Dec/17 13:05,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"If you use rdd.unpersist() with dynamic allocation, then an executor can be deallocated while your rdd is being removed, which will throw an uncaught exception killing your job. 

I looked into different ways of preventing this error from occurring but couldn't come up with anything that wouldn't require a big change. I propose the best fix is just to catch and log IOExceptions in unpersist() so they don't kill your job. This will match the effective behavior when executors are lost from dynamic allocation in other parts of the code.

In the worst case scenario I think this could lead to RDD partitions getting left on executors after they were unpersisted, but this is probably better than the whole job failing. I think in most cases the IOException would be due to the executor dieing for some reason, which is effectively the same result as unpersisting the rdd from that executor anyway.

I noticed this exception in a job that loads a 100GB dataset on a cluster where we use dynamic allocation heavily. Here is the relevant stack trace

java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:276)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)
Exception in thread ""main"" org.apache.spark.SparkException: Exception thrown in awaitResult:
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
        at org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:131)
        at org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:1806)
        at org.apache.spark.rdd.RDD.unpersist(RDD.scala:217)
        at com.ibm.sparktc.sparkbench.workload.exercise.CacheTest.doWorkload(CacheTest.scala:62)
        at com.ibm.sparktc.sparkbench.workload.Workload$class.run(Workload.scala:40)
        at com.ibm.sparktc.sparkbench.workload.exercise.CacheTest.run(CacheTest.scala:33)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially$1.apply(SuiteKickoff.scala:78)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially$1.apply(SuiteKickoff.scala:78)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$.com$ibm$sparktc$sparkbench$workload$SuiteKickoff$$runSerially(SuiteKickoff.scala:78)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$2.apply(SuiteKickoff.scala:52)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$$anonfun$2.apply(SuiteKickoff.scala:47)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.Range.foreach(Range.scala:160)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
        at com.ibm.sparktc.sparkbench.workload.SuiteKickoff$.run(SuiteKickoff.scala:47)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially$1.apply(MultipleSuiteKickoff.scala:24)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially$1.apply(MultipleSuiteKickoff.scala:24)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$.com$ibm$sparktc$sparkbench$workload$MultipleSuiteKickoff$$runSuitesSerially(MultipleSuiteKickoff.scala:24)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$run$1.apply(MultipleSuiteKickoff.scala:13)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$$anonfun$run$1.apply(MultipleSuiteKickoff.scala:10)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at com.ibm.sparktc.sparkbench.workload.MultipleSuiteKickoff$.run(MultipleSuiteKickoff.scala:10)
        at com.ibm.sparktc.sparkbench.cli.CLIKickoff$.main(CLIKickoff.scala:16)
        at com.ibm.sparktc.sparkbench.cli.CLIKickoff.main(CLIKickoff.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:843)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:188)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:218)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:127)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:221)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:899)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:276)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
        at java.lang.Thread.run(Thread.java:748)",,apachespark,bradkaiser,cloud_fan,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23806,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 14:11:17 UTC 2018,,,,,,,,,,"0|i3n8nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/17 18:40;bradkaiser;I have a PR for this forthcoming.;;;","28/Nov/17 19:58;apachespark;User 'brad-kaiser' has created a pull request for this issue:
https://github.com/apache/spark/pull/19836;;;","07/Dec/17 13:05;cloud_fan;Issue resolved by pull request 19836
[https://github.com/apache/spark/pull/19836];;;","27/Mar/18 21:58;tgraves;thanks for fixing this, hitting it now in spark 2.2, I think this same issue can happen with broadcast variables if its told to wait, did you happen to look at that at the same time?  ;;;","28/Mar/18 04:58;cloud_fan;Looks like we can apply the same fix to `Broadcast.unpersist`. Do you want to send a PR to fix? thanks!;;;","28/Mar/18 14:54;tgraves;I'll file a separate Jira for it and put up a pr;;;","29/Mar/18 14:11;bradkaiser;Yeah the fix for broadcaset unpersist should be basically the same. Thanks Thomas.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle more cases in PropagateEmptyRelation ,SPARK-22615,13120917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,27/Nov/17 12:39,29/Nov/17 17:18,14/Jul/23 06:30,29/Nov/17 17:18,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, in the optimize rule `PropagateEmptyRelation`, the following cases is not handled:
1. empty relation as right child in left outer join
2. empty relation as left child in right outer join
3. empty relation as right child in left semi join
4. empty relation as right child in left anti join


case #1 and #2 can be treated as Cartesian product and cause exception.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 27 12:49:05 UTC 2017,,,,,,,,,,"0|i3n7t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/17 12:49;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19825;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set large stack size consistently for tests to avoid StackOverflowError,SPARK-22607,13120738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,25/Nov/17 18:24,26/Nov/17 13:46,14/Jul/23 06:30,26/Nov/17 13:46,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,Build,Tests,,,,0,,,,,,,,,"I was seeing this error while testing the 2.2.1 RC:

{code}
OrderingSuite:
...
- GenerateOrdering with ShortType
*** RUN ABORTED ***
java.lang.StackOverflowError: 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:370) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541) 
at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)
...
{code}

This doesn't seem to happen on Jenkins, for whatever reason. It seems like we set JVM flags for tests inconsistently, and in particular, only set a 4MB stack size for surefire, not scalatest-maven-plugin. Adding {{-Xss4m}} made the test pass for me.

We can also make sure that all of these pass {{-ea}} consistently.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 26 13:46:04 UTC 2017,,,,,,,,,,"0|i3n6pb:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"25/Nov/17 18:27;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19820;;;","26/Nov/17 13:46;srowen;Issue resolved by pull request 19820
[https://github.com/apache/spark/pull/19820];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutputMetrics empty for DataFrame writes,SPARK-22605,13120683,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cloud_fan,jason.white,jason.white,24/Nov/17 22:42,14/Feb/22 08:42,14/Jul/23 06:30,29/Nov/17 11:19,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"I am trying to use the SparkListener interface to hook up some custom monitoring for some of our critical jobs. Among the first metrics I would like is an output row count & size metric. I'm using PySpark and the Py4J interface to implement the listener.

I am able to see the recordsRead and bytesRead metrics via the taskEnd.taskMetrics().inputMetrics().recordsRead() and .bytesRead() methods. taskEnd.taskMetrics().outputMetrics().recordsWritten() and .bytesWritten() are always 0. I see similar output if I use the stageCompleted event instead.

To trigger execution, I am using df.write.parquet(path). If I use df.rdd.saveAsTextFile(path) instead, the counts and bytes are correct.

Another clue that this bug is deeper in Spark SQL is that the Spark Application Master doesn't show the Output Size / Records column with df.write.parquet or df.write.text, but does with df.rdd.saveAsTextFile. Since the Spark Application Master also gets its output via the Listener interface, this would seem related.

There is a related PR: https://issues.apache.org/jira/browse/SPARK-21882, but I believe this to be a distinct issue.",,apachespark,cloud_fan,dongjoon,jason.white,mgaido,zoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 14 08:42:21 UTC 2022,,,,,,,,,,"0|i3n6d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/17 15:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19833;;;","29/Nov/17 11:19;cloud_fan;Issue resolved by pull request 19833
[https://github.com/apache/spark/pull/19833];;;","14/Feb/22 08:42;zoli;Not solved, still an issue in Spark v3.1.2 for structured streaming.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenerateOrdering shouldn't change ctx.INPUT_ROW,SPARK-22591,13120463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,23/Nov/17 12:07,24/Nov/17 21:20,14/Jul/23 06:30,24/Nov/17 10:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{{GenerateOrdering}} changes {{ctx.INPUT_ROW}} but doesn't restore the original value. Since {{ctx.INPUT_ROW}} is used when generating codes, it is risky to change it arbitrarily.
",,apachespark,cloud_fan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 24 10:47:41 UTC 2017,,,,,,,,,,"0|i3n50f:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"23/Nov/17 12:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19800;;;","24/Nov/17 10:47;cloud_fan;Issue resolved by pull request 19800
[https://github.com/apache/spark/pull/19800];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broadcast thread propagates the localProperties to task,SPARK-22590,13120442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ajithshetty,ajithshetty,ajithshetty,23/Nov/17 10:50,18/Feb/20 00:41,14/Jul/23 06:30,17/Feb/20 18:31,2.2.0,2.4.4,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Spark Core,,,,,0,bulk-closed,,,,,,,,"Local properties set via sparkContext are not available as TaskContext properties when executing parallel jobs and threadpools have idle threads

Explanation: 
 When executing parallel jobs via {{BroadcastExchangeExec}}, the {{relationFuture}} is evaluated via a seperate thread. The threads inherit the {{localProperties}} from sparkContext as they are the child threads.
 These threads are controlled via the executionContext (thread pools). Each Thread pool has a default {{keepAliveSeconds}} of 60 seconds for idle threads. 
 Scenarios where the thread pool has threads which are idle and reused for a subsequent new query, the thread local properties will not be inherited from spark context (thread properties are inherited only on thread creation) hence end up having old or no properties set. This will cause taskset properties to be missing when properties are transferred by child thread via {{sparkContext.runJob/submitJob}}

Attached is a test-case to simulate this behavior",,ajithshetty,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/17 10:51;ajithshetty;TestProps.scala;https://issues.apache.org/jira/secure/attachment/12899041/TestProps.scala",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 17 18:31:00 UTC 2020,,,,,,,,,,"0|i3n4vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/20 16:55;ajithshetty;Adding Fix;;;","17/Feb/20 18:31;cloud_fan;Issue resolved by pull request 27266
[https://github.com/apache/spark/pull/27266];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark job fails if fs.defaultFS and application jar are different url,SPARK-22587,13120391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,merlin,prabhujoseph,prabhujoseph,23/Nov/17 06:17,23/Oct/19 16:15,14/Jul/23 06:30,11/Jan/18 04:03,1.6.3,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Submit,,,,,0,,,,,,,,,"Spark Job fails if the fs.defaultFs and url where application jar resides are different and having same scheme,

spark-submit  --conf spark.master=yarn-cluster wasb://XXX/tmp/test.py

core-site.xml fs.defaultFS is set to wasb:///YYY. Hadoop list works (hadoop fs -ls) works for both the url XXX and YYY.

{code}
Exception in thread ""main"" java.lang.IllegalArgumentException: Wrong FS: wasb://XXX/tmp/test.py, expected: wasb://YYY 
at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:665) 
at org.apache.hadoop.fs.azure.NativeAzureFileSystem.checkPath(NativeAzureFileSystem.java:1251) 
at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:485) 
at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:396) 
at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:507) 
at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:660) 
at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:912) 
at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:172) 
at org.apache.spark.deploy.yarn.Client.run(Client.scala:1248) 
at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1307) 
at org.apache.spark.deploy.yarn.Client.main(Client.scala) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.lang.reflect.Method.invoke(Method.java:498) 
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:751) 
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) 
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) 
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) 
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) 
{code}

The code Client.copyFileToRemote tries to resolve the path of application jar (XXX) from the FileSystem object created using fs.defaultFS url (YYY) instead of the actual url of application jar.

val destFs = destDir.getFileSystem(hadoopConf)
val srcFs = srcPath.getFileSystem(hadoopConf)

getFileSystem will create the filesystem based on the url of the path and so this is fine. But the below lines of code tries to get the srcPath (XXX url) from the destFs (YYY url) and so it fails.

var destPath = srcPath
val qualifiedDestPath = destFs.makeQualified(destPath)


",,apachespark,jerryshao,merlin,mgaido,prabhujoseph,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-15070,HADOOP-15094,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 00:02:04 UTC 2017,,,,,,,,,,"0|i3n4kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/17 10:19;srowen;Hm, but if the src and dest FS are different, it overwrites destPath to be a path relative to destDir. I am not sure if that is the actual problem.
Is it that compareFs believes incorrectly that these represent the same FS?
If so then I do wonder if it makes sense to always set {{destPath = new Path(destDir, destName.getOrElse(srcPath.getName()))}}

This is some old logic from Sandy; maybe [~vanzin] or [~steve_l] has an opinion on the logic here.
https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L356;;;","24/Nov/17 06:51;jerryshao;[~prabhujoseph], I think it is because the logic of comparing two FS {{compareFs}} is not worked as expected for wasb, it identifies these two FSs as the same FS, but in fact they're two FSs. that's why the following {{makeQualified}} will throw an exception.;;;","27/Nov/17 11:39;stevel@apache.org;Jerry had already pulled me in for this; it's one of those little ""pits of semantics"" you can get pulled into, ""the incident pit"" as they call it in SCUBA.

summary: You need a case insensitive check for schema and serInfo too, maybe even port. 
# Allow for null though. 
# an consider using {{FileSystem.makeQualified(path)}} as the safety check

What does Hadoop get up to?
* FileSystem.checkPath does a full check of (scheme, authority), with the auth of the canonicalized URI (including default ports) (so hdfs://namnode/ and hdfs://namenode:9820/ refer to the same FS. That code dates from 2008, so should be considered normative.
* S3AFilesSystem.checkPath only looks at hostnames, because it tries to strip out user:password from Paths for security reasons
* Wasb uses FileSystem.checkPath, but does some hacks to also handle an older scheme of ""asv"". I wouldn't worry about that little detail though
* {{AbstractFileSystem.checkPath}} (the FileContext implementation code) doesn't check auth, it looks at host and mentions the fact that on a file:// reference the host may be null. Raises {{InvalidPathException}} (subclass of {{IllegalArgumentException}} if its unhappy.

Overall then: check auth with an .equalsIgnoreCase(), allow for null. Worry about default ports if you really want to. 

Filed HADOOP-15070 to cover this whole area better in docs & tests, should make the FileContext/FileSystem checks consistent and raise the same InvalidPathException.

One thing to consider is adding to the FS APIs some predicate {{isFileSystemPath(Path p)}} to do the validation without the overhead of exception throwing, and implement it in one single (consistent) place. Wouldn't be there until Hadoop 3.1 though, so not of any immediate benefit.

thank you for bringing this undocumented, unspecified, untested and inconsistent logic to my attention :)
;;;","27/Nov/17 12:00;stevel@apache.org;See also [FileSystem.CACHE.Key.isEqual()|https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3530] is how the filesystem cache compares things (scheme + auth + UGI of owner of the FS).

FileSystem doesn't implement it's own .equals operator, nor do any subclasses: they rely on Object.equals. So if you compare that then the cache will return the same FS instance for both (assuming the conf didn't disable caching). 

You could probably implement a reliable check as follows

{code}
    try {
      srcFs.makeQualified(destFs.getWorkingDir())))
      return true;
    } catch {
      case IllegalArgumentException =>
        return false;
    }
{code}

Not ideal, as the false response is expensive, but given if the two filesystems are different the client will copy the file, then the cost of raising an exception is lost in the noise;;;","01/Dec/17 00:49;merlin;we can update the compareFS by considering the authority. 
https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1442

The PR is sent out. 
https://github.com/apache/spark/pull/19885
;;;","05/Dec/17 00:02;apachespark;User 'merlintang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19885;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Url encoding of jar path expected?,SPARK-22585,13120325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dubovsky,dubovsky,dubovsky,22/Nov/17 20:52,12/Dec/22 18:10,14/Jul/23 06:30,30/Nov/17 01:25,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"I am calling {code}sparkContext.addJar{code} method with path to a local jar I want to add. Example:
{code}/home/me/.coursier/cache/v1/https/artifactory.com%3A443/path/to.jar{code}. As a result I get an exception saying
{code}
Failed to add /home/me/.coursier/cache/v1/https/artifactory.com%3A443/path/to.jar to Spark environment. Stacktrace:
java.io.FileNotFoundException: Jar /home/me/.coursier/cache/v1/https/artifactory.com:443/path/to.jar not found
{code}
Important part to notice here is that colon character is url encoded in path I want to use but exception is complaining about path in decoded form. This is caused by this line of code from implementation ([see here|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/SparkContext.scala#L1833]):
{code}
case null | ""file"" => addJarFile(new File(uri.getPath))
{code}
It uses [getPath|https://docs.oracle.com/javase/7/docs/api/java/net/URI.html#getPath()] method of [java.net.URI|https://docs.oracle.com/javase/7/docs/api/java/net/URI.html] which url decodes the path. I believe method [getRawPath|https://docs.oracle.com/javase/7/docs/api/java/net/URI.html#getRawPath()] should be used here which keeps path string in original form.

I tend to see this as a bug since I want to use my dependencies resolved from artifactory with port directly. Is there some specific reason for this or can we fix this?

Thanks",,apachespark,dubovsky,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 30 01:25:20 UTC 2017,,,,,,,,,,"0|i3n45r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/17 22:53;srowen;Does the real file name contain ""%3A443"" or "":443""? ;;;","23/Nov/17 08:36;dubovsky;Real file path is exactly the same as one I am passing into add jar. It contains ""%3A443"".;;;","23/Nov/17 10:31;srowen;Hm, I think the issue is rather than the path needs to be encoded before becoming part of the URI. The URI {{file:/home/me/.coursier/cache/v1/https/artifactory.com%3A443/path/to.jar}} is not one that names your file; it's {{file:/home/me/.coursier/cache/v1/https/artifactory.com%253A443/path/to.jar}} (escaped %). That may be a workaround here.
{{/home/me/.coursier/cache/v1/https/artifactory.com%3A443/path/to.jar}} is however a valid and correct local path to the file, and that's what the argument is meant to be. So the code should do the encoding. I'm aware that there are a number of places that probably turn paths into URIs, so would be best to try to update all issues of this form. I think it's low-risk as URI encoding won't do anything for most paths, and where it does, it's probably essential.;;;","23/Nov/17 11:54;dubovsky;I am not sure about adding encoding step into implementation of addJar method. It's not about encoding whole path as a string since you want to keep some characters literally (':', '/' possibly others). So the code would first need to parse the path to get only path segments and encode those. This most probably leads to using URI again at which point this starts to be circular problem. Moreover I am not sure what is the point of encoding path segments only to ask URI to decode it...

I also think that it makes sense to decode the segment only inside of a logic accessing a value of that segment. If I work with url/path as a whole I want to keep it parsable and therefore keep special characters encoded. This is the thinking I would personally use to decide which version (getPath/getRawPath) should be used in particular scenarios across spark code base even though I must admit I have very little insight into these other URI usecases :);;;","23/Nov/17 14:25;srowen;Yes, I mean escaping the path only; the host and scheme and so on may use reserved characters for their intended usage. I think that's the context here where it's just paths being turned into local file URIs.

A URI is the right representation in this code but needs to represent the right path. I don't see any issue with that, nor decoding. It's not round-tripping for nothing.

I don't think the representation depends on usage. The URI's representation simply needs to correctly represent the resource. That's not quite happening here, and it's because special chars aren't escaped in the right places.;;;","23/Nov/17 15:14;dubovsky;I just tried how URI behaves on some examples and learned that it is doing something else then I think. So yes either encoding a path or using getRawPath are both valid solutions to me. Should I create PR for this or what is next step?;;;","27/Nov/17 14:19;dubovsky;[~srowen] I'd like to understand more about solution you prefer. So by addJar scaladoc these schemes are supported: null, file, local, hdfs, http, https, ftp. Now semantics of argument to addJar is the same as for URI single arg constructor (with exception of local scheme). This means for example that path is expected to be url encoded.

Here as a fix we want to remove requirement for path to be encoded. Do we want to do this for all supported schemes named above? Or only for null, file and local?

After we sync in this question I will post a code suggestion how to handle this. Discussion over code will be more fruitful I believe.;;;","27/Nov/17 15:30;srowen;URI encoding applies to all of those URI schemes. I don't think they even differ in semantics for the URI path part.

I would expect that where users supply URIs, that they are already valid URIs -- properly encoded.
Where users supply a local file path, there is no reason to expect that path is URI-encoded. A path is not a URI. Hence if it is made into a URI, it needs to be encoded somehow.;;;","28/Nov/17 15:18;apachespark;User 'james64' has created a pull request for this issue:
https://github.com/apache/spark/pull/19834;;;","28/Nov/17 15:19;dubovsky;I have created a PR where we can agree on a code to address this.;;;","30/Nov/17 01:25;gurwls223;Issue resolved by pull request 19834
[https://github.com/apache/spark/pull/19834];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
First delegation token renewal time is not 75% of renewal time in Mesos,SPARK-22583,13120295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kalvinnchau,kalvinnchau,kalvinnchau,22/Nov/17 18:19,25/Nov/17 13:33,14/Jul/23 06:30,25/Nov/17 13:32,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Mesos,,,,,0,,,,,,,,,"The first renewal time of the delegation tokens is the exact renewal time. This could lead to a situation where the tokens don't make it to executors in time, and the executors will be working with expired tokens (and Exception out).

The subsequent renewal times are correctly set to 75% of total renewal time. The initial renewal time just needs to be set to 75% of the renewal time.",,apachespark,kalvinnchau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 25 13:32:41 UTC 2017,,,,,,,,,,"0|i3n3z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/17 18:22;apachespark;User 'kalvinnchau' has created a pull request for this issue:
https://github.com/apache/spark/pull/19798;;;","25/Nov/17 13:32;srowen;Issue resolved by pull request 19798
[https://github.com/apache/spark/pull/19798];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
executor page blacklist status should update with TaskSet level blacklisting,SPARK-22577,13120044,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,tgraves,tgraves,21/Nov/17 19:09,17/May/20 17:48,14/Jul/23 06:30,24/Jan/18 17:37,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"right now the executor blacklist status only updates with the BlacklistTracker after a task set has finished and propagated the blacklisting to the application level. We should change that to show at the taskset level as well. Without this it can be very confusing to the user why things aren't running.
",,apachespark,attilapiros,irashid,jerryshao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23189,,,,,,,,,,,,,,"11/Jan/18 17:12;attilapiros;app_blacklisting.png;https://issues.apache.org/jira/secure/attachment/12905713/app_blacklisting.png","19/Jan/18 14:54;attilapiros;node_blacklisting_for_stage.png;https://issues.apache.org/jira/secure/attachment/12906833/node_blacklisting_for_stage.png","09/Jan/18 13:36;attilapiros;stage_blacklisting.png;https://issues.apache.org/jira/secure/attachment/12905283/stage_blacklisting.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 17:37:22 UTC 2018,,,,,,,,,,"0|i3n2fj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 18:10;irashid;[~tgraves] what exactly were you thinking?

I remember when first looking at this I wasn't really sure what to show within a taskset -- from my experience, whenever you have a bad taskset, which eventually fails, you end up getting a bunch of executor blacklisting.  This is totally fine, but I worried about loudly warning, as it would be misleading -- the users would think the issue was bad executors, when in fact its just a bad taskset.

I'm guessing you want this to show up in the stage UI -- what did you have in mind?;;;","20/Dec/17 18:19;tgraves;Yes I was thinking the stage UI (and perhaps executor UI). The executors table there already has a blacklist column it would be nice to fill in. People find it confusing that it says 0 when really the executor is blacklisted at the taskset level.  So perhaps it can be confusing either way but I would rather show users what is actually going on.   
;;;","20/Dec/17 18:28;irashid;OK that sounds reasonable.  I'd be happy to look at some proposed UI changes.

I have a feeling I will be against any changes to the executor UI, as it'll be too confusing, but probably good for somebody to take a shot at it and see what we think.

I'm guessing you don't actually have cycles to do this, and this is up for grabs?;;;","20/Dec/17 18:30;tgraves;correct (unfortunately don't time), anyone can take it that has time;;;","28/Dec/17 18:25;attilapiros;I started working on this issue;;;","09/Jan/18 13:38;attilapiros;Uploading screenshot where stage level blacklisting is show for a selected stage.
See the ""Aggregated Metrics by Executor"" section at the bottom of picture.  ;;;","09/Jan/18 14:43;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/20203;;;","24/Jan/18 17:37;irashid;Issue resolved by pull request 20203
[https://github.com/apache/spark/pull/20203];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong request causing Spark Dispatcher going inactive,SPARK-22574,13119981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gschiavon,gschiavon,gschiavon,21/Nov/17 15:51,13/Dec/17 21:38,14/Jul/23 06:30,13/Dec/17 21:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,Mesos,Spark Submit,,,,0,,,,,,,,,"When submitting a wrong _CreateSubmissionRequest_ to Spark Dispatcher is causing a bad state of Dispatcher and making it inactive as a mesos framework.

The class CreateSubmissionRequest initialise its arguments to null as follows:

{code:title=CreateSubmissionRequest.scala|borderStyle=solid}
  var appResource: String = null
  var mainClass: String = null
  var appArgs: Array[String] = null
  var sparkProperties: Map[String, String] = null
  var environmentVariables: Map[String, String] = null
{code}

There are some checks of these variables but not in all of them, for example in appArgs and environmentVariables. 

If you don't set _appArgs_ it will cause the following error: 
{code:title=error|borderStyle=solid}
17/11/21 14:37:24 INFO MesosClusterScheduler: Reviving Offers.
Exception in thread ""Thread-22"" java.lang.NullPointerException
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.getDriverCommandValue(MesosClusterScheduler.scala:444)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.buildDriverCommand(MesosClusterScheduler.scala:451)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.org$apache$spark$scheduler$cluster$mesos$MesosClusterScheduler$$createTaskInfo(MesosClusterScheduler.scala:538)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1.apply(MesosClusterScheduler.scala:570)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler$$anonfun$scheduleTasks$1.apply(MesosClusterScheduler.scala:555)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.scheduleTasks(MesosClusterScheduler.scala:555)
	at org.apache.spark.scheduler.cluster.mesos.MesosClusterScheduler.resourceOffers(MesosClusterScheduler.scala:621)
{code}

Because it's trying to access to it without checking whether is null or not.

 ",,apachespark,gschiavon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 13 21:37:53 UTC 2017,,,,,,,,,,"0|i1u552:zzi",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/17 16:05;apachespark;User 'Gschiavon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19793;;;","12/Dec/17 19:47;vanzin;Issue resolved by pull request 19793
[https://github.com/apache/spark/pull/19793];;;","13/Dec/17 00:06;vanzin;(Commit above was reverted.);;;","13/Dec/17 13:27;apachespark;User 'Gschiavon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19966;;;","13/Dec/17 21:37;vanzin;Issue resolved by pull request 19966
[https://github.com/apache/spark/pull/19966];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell does not re-initialize on :replay,SPARK-22572,13119904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mpetruska,mpetruska,mpetruska,21/Nov/17 11:05,12/Dec/22 18:10,14/Jul/23 06:30,22/Nov/17 12:36,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Shell,,,,,0,,,,,,,,,"Spark-shell does not run the re-initialization script when a `:replay` command is issued:

{code}
$ ./bin/spark-shell 
17/11/21 12:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://192.168.1.3:4040
Spark context available as 'sc' (master = local[*], app id = local-1511262066013).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.0-SNAPSHOT
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_74)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@77bb916f

scala> :replay
Replaying: sc
<console>:12: error: not found: value sc
       sc
       ^


scala> sc
<console>:12: error: not found: value sc
       sc
       ^

scala>
{code}",,apachespark,mpetruska,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 22 12:36:10 UTC 2017,,,,,,,,,,"0|i3n1kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/17 13:39;srowen;Yes, I think this is just how it is. The way Spark is initialized by injecting some code into the shell init means it's not available for replay. Have a look at the integration and see if you have bright ideas, but I recall concluding it wouldn't work.;;;","21/Nov/17 13:56;apachespark;User 'mpetruska' has created a pull request for this issue:
https://github.com/apache/spark/pull/19791;;;","21/Nov/17 13:57;mpetruska;[~srowen]: Can I ask you to look at https://github.com/apache/spark/pull/19791, please?;;;","22/Nov/17 12:36;gurwls223;Issue resolved by pull request 19791
[https://github.com/apache/spark/pull/19791];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
history server: handle exception on opening corrupted listing.ldb,SPARK-22559,13119563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,20/Nov/17 07:06,24/Nov/17 14:08,14/Jul/23 06:30,24/Nov/17 14:08,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Currently history server v2 failed to start if listing.ldb is corrupted.
Need to handle exception on opening listing.ldb.",,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 24 14:08:14 UTC 2017,,,,,,,,,,"0|i3mzhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/17 07:14;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19786;;;","24/Nov/17 14:08;cloud_fan;Issue resolved by pull request 19786
[https://github.com/apache/spark/pull/19786];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use ThreadSignaler explicitly,SPARK-22557,13119536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,20/Nov/17 00:27,12/Dec/22 18:10,14/Jul/23 06:30,20/Nov/17 04:32,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,ScalaTest 3.0 uses an implicit `Signaler`. This PR makes it sure all Spark tests uses `ThreadSignaler` explicitly which has the same default behavior of interrupting a thread on the JVM in ScalaTest 2.2.x. This will reduce potential flakiness inside the other test suites.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 04:32:26 UTC 2017,,,,,,,,,,"0|i3mzbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/17 00:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19784;;;","20/Nov/17 04:32;gurwls223;Issue resolved by pull request 19784
[https://github.com/apache/spark/pull/19784];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect nested AND expression pushed down to JDBC data source,SPARK-22548,13119381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jliwork,jliwork,jliwork,18/Nov/17 01:11,24/Nov/17 21:20,14/Jul/23 06:30,22/Nov/17 04:13,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"Let’s say I have a JDBC data source table ‘foobar’ with 3 rows:

NAME		THEID
==================
fred            	1
mary          	2
joe 'foo' ""bar""    3

This query returns incorrect result. 
SELECT * FROM foobar WHERE (THEID > 0 AND TRIM(NAME) = 'mary') OR (NAME = 'fred')

It’s supposed to return:
fred             	1
mary          	2

But it returns
fred            	1
mary          	2
joe 'foo' ""bar""    3

This is because one leg of the nested AND predicate, TRIM(NAME) = 'mary’, can not be pushed down but is lost during JDBC push down filter translation. The same translation method is also called by Data Source V2. I have a fix for this issue and will open a PR. ",,apachespark,jliwork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 18 01:18:04 UTC 2017,,,,,,,,,,"0|i3mycv:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"18/Nov/17 01:18;apachespark;User 'jliwork' has created a pull request for this issue:
https://github.com/apache/spark/pull/19776;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileStreamSource should use its own hadoop conf to call globPathIfNecessary,SPARK-22544,13119060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,16/Nov/17 23:29,24/Nov/17 21:17,14/Jul/23 06:30,17/Nov/17 23:35,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 23:31:03 UTC 2017,,,,,,,,,,"0|i3mwdr:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"16/Nov/17 23:31;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HighlyCompressedMapStatus's avgSize is incorrect,SPARK-22540,13118836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,16/Nov/17 08:13,24/Nov/17 21:16,14/Jul/23 06:30,17/Nov/17 13:55,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"The calculation of HighlyCompressedMapStatus's avgSize is incorrect. 
Currently, it looks like ""sum of small blocks / count of all non empty blocks"", the count of all non empty blocks not only contains small blocks, which contains huge blocks number also, but we need the count of small blocks only.",,apachespark,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 08:23:04 UTC 2017,,,,,,,,,,"0|i3mv07:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"16/Nov/17 08:23;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/19765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLTransformer.transform(inputDataFrame) uncaches inputDataFrame,SPARK-22538,13118803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,MBALearnsToCode,MBALearnsToCode,16/Nov/17 04:58,24/Nov/17 21:17,14/Jul/23 06:30,17/Nov/17 16:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,ML,PySpark,SQL,Web UI,,0,,,,,,,,,"When running the below code on PySpark v2.2.0, the cached input DataFrame df disappears from SparkUI after SQLTransformer.transform(...) is called on it.

I don't yet know whether this is only a SparkUI bug, or the input DataFrame df is indeed unpersisted from memory. If the latter is true, this can be a serious bug because any new computation using new_df would have to re-do all the work leading up to df.

{code}
import pandas
import pyspark
from pyspark.ml.feature import SQLTransformer

spark = pyspark.sql.SparkSession.builder.getOrCreate()

df = spark.createDataFrame(pandas.DataFrame(dict(x=[-1, 0, 1])))

# after below step, SparkUI Storage shows 1 cached RDD
df.cache(); df.count()

# after below step, cached RDD disappears from SparkUI Storage
new_df = SQLTransformer(statement='SELECT * FROM __THIS__').transform(df)
{code}
",,apachespark,cloud_fan,MBALearnsToCode,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 17 16:44:23 UTC 2017,,,,,,,,,,"0|i3musv:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"17/Nov/17 04:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19772;;;","17/Nov/17 16:44;cloud_fan;Issue resolved by pull request 19772
[https://github.com/apache/spark/pull/19772];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PythonRunner.MonitorThread should give the task a little time to finish before killing the python worker,SPARK-22535,13118755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,16/Nov/17 00:34,24/Nov/17 21:16,14/Jul/23 06:30,16/Nov/17 05:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,PySpark,,,,,0,,,,,,,,,,,apachespark,ueshin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 19:03:08 UTC 2017,,,,,,,,,,"0|i3mui7:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"16/Nov/17 00:41;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19762;;;","16/Nov/17 05:22;ueshin;Issue resolved by pull request 19762
[https://github.com/apache/spark/pull/19762];;;","16/Nov/17 19:03;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkConfigProvider does not handle deprecated config keys,SPARK-22533,13118658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,15/Nov/17 21:33,20/Nov/17 11:46,14/Jul/23 06:30,20/Nov/17 11:45,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Noticed that when you read a config using a {{ConfigBuilder}}-created constant using {{SparkConf.get}}, that does not respect the deprecated keys declared in {{SparkConf.configsWithAlternatives}}.

For example, adding this test to {{SparkConfSuite}} generates a failure:

{code}
    conf.set(""spark.history.fs.cleaner.interval.seconds"", ""42"")
    assert(conf.get(MAX_LOG_AGE_S) === 42L)
{code}

While here it would also be nice to handle this TODO from {{SparkConf.configsWithAlternatives}}:

{noformat}
   * TODO: consolidate it with `ConfigBuilder.withAlternative`.
{noformat}",,apachespark,cloud_fan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 11:45:37 UTC 2017,,,,,,,,,,"0|i3mtwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/17 22:30;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19760;;;","20/Nov/17 11:45;cloud_fan;Issue resolved by pull request 19760
[https://github.com/apache/spark/pull/19760];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark download page doesn't update package name based package type,SPARK-22525,13118489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,holden,holden,15/Nov/17 09:44,17/Dec/17 16:50,14/Jul/23 06:30,15/Nov/17 15:21,2.1.1,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Documentation,,,,,0,,,,,,,,,See http://spark.apache.org/downloads.html / https://github.com/apache/spark-website/commit/853627da67322b6b57d7c3b8ddc9fc150576a4fc#commitcomment-25623876,,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22819,,,,,,SPARK-22819,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 15 15:21:06 UTC 2017,,,,,,,,,,"0|i3msv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/17 15:21;srowen;Resolved by https://github.com/apache/spark-website/pull/75;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CSV Read breaks: When ""multiLine"" = ""true"", if ""comment"" option is set as last line's first character",SPARK-22516,13118180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,smurakozi,crkumaresh24,crkumaresh24,14/Nov/17 08:57,12/Dec/22 18:10,14/Jul/23 06:30,06/Dec/17 21:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,csvparser,,,,,,,,"Try to read attached CSV file with following parse properties,

scala> *val csvFile = spark.read.option(""header"",""true"").option(""inferSchema"", ""true"").option(""parserLib"", ""univocity"").option(""comment"", ""c"").csv(""hdfs://localhost:8020/test
CommentChar.csv"");   *                                                                                                                                                        
csvFile: org.apache.spark.sql.DataFrame = [a: string, b: string]                                                                                                             
                                                                                                                                                                             
scala> csvFile.show                                                                                                                                                          
+---+---+                                                                                                                                                                    
|  a|  b|                                                                                                                                                                    
+---+---+                                                                                                                                                                    
+---+---+   

{color:#8eb021}*Noticed that it works fine.*{color}

If we add an option ""multiLine"" = ""true"", it fails with below exception. This happens only if we pass ""comment"" == input dataset's last line's first character

scala> val csvFile = *spark.read.option(""header"",""true"").{color:red}{color:#d04437}option(""multiLine"",""true""){color}{color}.option(""inferSchema"", ""true"").option(""parserLib"", ""univocity"").option(""comment"", ""c"").csv(""hdfs://localhost:8020/testCommentChar.csv"");*
17/11/14 14:26:17 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)
com.univocity.parsers.common.TextParsingException: java.lang.IllegalArgumentException - Unable to skip 1 lines from line 2. End of input reached
Parser Configuration: CsvParserSettings:
        Auto configuration enabled=true
        Autodetect column delimiter=false
        Autodetect quotes=false
        Column reordering enabled=true
        Empty value=null
        Escape unquoted values=false
        Header extraction enabled=null
        Headers=null
        Ignore leading whitespaces=false
        Ignore trailing whitespaces=false
        Input buffer size=128
        Input reading on separate thread=false
        Keep escape sequences=false
        Keep quotes=false
        Length of content displayed on error=-1
        Line separator detection enabled=false
        Maximum number of characters per column=-1
        Maximum number of columns=20480
        Normalize escaped line separators=true
        Null value=
        Number of records to read=all
        Processor=none
        Restricting data in exceptions=false
        RowProcessor error handler=null
        Selected fields=none
        Skip empty lines=true
        Unescaped quote handling=STOP_AT_DELIMITERFormat configuration:
        CsvFormat:
                Comment character=c
                Field delimiter=,
                Line separator (normalized)=\n
                Line separator sequence=\r\n
                Quote character=""
                Quote escape character=\
                Quote escape escape character=null
Internal state when error was thrown: line=3, column=0, record=1, charIndex=19
        at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:475)
        at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.next(UnivocityParser.scala:281)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:444)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Unable to skip 1 lines from line 2. End of input reached
        at com.univocity.parsers.common.input.AbstractCharInputReader.skipLines(AbstractCharInputReader.java:262)
        at com.univocity.parsers.common.AbstractParser.processComment(AbstractParser.java:96)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:440)
        ... 24 more
17/11/14 14:26:17 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.IllegalArgumentException - Unable to skip 1 lines from line 2. End of input reached
Parser Configuration: CsvParserSettings:
        Auto configuration enabled=true
        Autodetect column delimiter=false
        Autodetect quotes=false
        Column reordering enabled=true
        Empty value=null
        Escape unquoted values=false
        Header extraction enabled=null
        Headers=null
        Ignore leading whitespaces=false
        Ignore trailing whitespaces=false
        Input buffer size=128
        Input reading on separate thread=false
        Keep escape sequences=false
        Keep quotes=false
        Length of content displayed on error=-1
        Line separator detection enabled=false
        Maximum number of characters per column=-1
        Maximum number of columns=20480
        Normalize escaped line separators=true
        Null value=
        Number of records to read=all
        Processor=none
        Restricting data in exceptions=false
        RowProcessor error handler=null
        Selected fields=none
        Skip empty lines=true
        Unescaped quote handling=STOP_AT_DELIMITERFormat configuration:
        CsvFormat:
                Comment character=c
                Field delimiter=,
                Line separator (normalized)=\n
                Line separator sequence=\r\n
                Quote character=""
                Quote escape character=\
                Quote escape escape character=null
Internal state when error was thrown: line=3, column=0, record=1, charIndex=19
        at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:475)
        at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.next(UnivocityParser.scala:281)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:444)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Unable to skip 1 lines from line 2. End of input reached
        at com.univocity.parsers.common.input.AbstractCharInputReader.skipLines(AbstractCharInputReader.java:262)
        at com.univocity.parsers.common.AbstractParser.processComment(AbstractParser.java:96)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:440)
        ... 24 more

17/11/14 14:26:17 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost, executor driver): com.univocity.parsers.common.TextParsingException: java.lang.IllegalArgumentException - Unable to skip 1 lines from line 2. End of input reached
Parser Configuration: CsvParserSettings:
        Auto configuration enabled=true
        Autodetect column delimiter=false
        Autodetect quotes=false
        Column reordering enabled=true
        Empty value=null
        Escape unquoted values=false
        Header extraction enabled=null
        Headers=null
        Ignore leading whitespaces=false
        Ignore trailing whitespaces=false
        Input buffer size=128
        Input reading on separate thread=false
        Keep escape sequences=false
        Keep quotes=false
        Length of content displayed on error=-1
        Line separator detection enabled=false
        Maximum number of characters per column=-1
        Maximum number of columns=20480
        Normalize escaped line separators=true
        Null value=
        Number of records to read=all
        Processor=none
        Restricting data in exceptions=false
        RowProcessor error handler=null
        Selected fields=none
        Skip empty lines=true
        Unescaped quote handling=STOP_AT_DELIMITERFormat configuration:
        CsvFormat:
                Comment character=c
                Field delimiter=,
                Line separator (normalized)=\n
                Line separator sequence=\r\n
                Quote character=""
                Quote escape character=\
                Quote escape escape character=null
Internal state when error was thrown: line=3, column=0, record=1, charIndex=19
        at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:475)
        at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.next(UnivocityParser.scala:281)
        at scala.collection.Iterator$$anon$12.next(Iterator.scala:444)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Unable to skip 1 lines from line 2. End of input reached
        at com.univocity.parsers.common.input.AbstractCharInputReader.skipLines(AbstractCharInputReader.java:262)
        at com.univocity.parsers.common.AbstractParser.processComment(AbstractParser.java:96)
        at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:440)
        ... 24 more

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
  at org.apache.spark.rdd.RDD.take(RDD.scala:1327)
  at org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:224)
  at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:62)
  at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:177)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:177)
  at scala.Option.orElse(Option.scala:289)
  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:176)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:412)
  ... 48 elided
Caused by: com.univocity.parsers.common.TextParsingException: java.lang.IllegalArgumentException - Unable to skip 1 lines from line 2. End of input reached
Parser Configuration: CsvParserSettings:
        Auto configuration enabled=true
        Autodetect column delimiter=false
        Autodetect quotes=false
        Column reordering enabled=true
        Empty value=null
        Escape unquoted values=false
        Header extraction enabled=null
        Headers=null
        Ignore leading whitespaces=false
        Ignore trailing whitespaces=false
        Input buffer size=128
        Input reading on separate thread=false
        Keep escape sequences=false
        Keep quotes=false
        Length of content displayed on error=-1
        Line separator detection enabled=false
        Maximum number of characters per column=-1
        Maximum number of columns=20480
        Normalize escaped line separators=true
        Null value=
        Number of records to read=all
        Processor=none
        Restricting data in exceptions=false
        RowProcessor error handler=null
        Selected fields=none
        Skip empty lines=true
        Unescaped quote handling=STOP_AT_DELIMITERFormat configuration:
        CsvFormat:
                Comment character=c
                Field delimiter=,
                Line separator (normalized)=\n
                Line separator sequence=\r\n
                Quote character=""
                Quote escape character=\
                Quote escape escape character=null
Internal state when error was thrown: line=3, column=0, record=1, charIndex=19
  at com.univocity.parsers.common.AbstractParser.handleException(AbstractParser.java:339)
  at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:475)
  at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anon$1.next(UnivocityParser.scala:281)
  at scala.collection.Iterator$$anon$12.next(Iterator.scala:444)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)
  at scala.collection.Iterator$class.foreach(Iterator.scala:893)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
  at scala.collection.AbstractIterator.to(Iterator.scala:1336)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1354)
  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
  at org.apache.spark.scheduler.Task.run(Task.scala:108)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: Unable to skip 1 lines from line 2. End of input reached
  at com.univocity.parsers.common.input.AbstractCharInputReader.skipLines(AbstractCharInputReader.java:262)
  at com.univocity.parsers.common.AbstractParser.processComment(AbstractParser.java:96)
  at com.univocity.parsers.common.AbstractParser.parseNext(AbstractParser.java:440)
  ... 24 more

scala>
",,apachespark,crkumaresh24,mgaido,smurakozi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/17 09:00;crkumaresh24;testCommentChar.csv;https://issues.apache.org/jira/secure/attachment/12897519/testCommentChar.csv","20/Nov/17 12:01;crkumaresh24;test_file_without_eof_char.csv;https://issues.apache.org/jira/secure/attachment/12898472/test_file_without_eof_char.csv",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 21:22:31 UTC 2017,,,,,,,,,,"0|i3mqyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/17 09:04;crkumaresh24;[~hyukjin.kwon]: Need your help here :);;;","17/Nov/17 15:01;mgaido;not sure why but this is caused by the fact that your file contains ""CR LF"" as line separator instead of only LF;;;","20/Nov/17 12:00;crkumaresh24;[~mgaido]: Even after I replaced all 'CR LF' to 'LF', still in the below case, the error is thrown.

 -> When the file doesn't have 'LF' as the last character in its last line  i.e. EOF
 (Note: All other lines in the file ends with LF) character

Attached the failing file 'test_file_without_eof_char.csv' for your reference.

Is it something the problem with the parser or the input data (which doesn't have any line ending as its last character) ?;;;","21/Nov/17 15:29;mgaido;[~crkumaresh24] I can't reproduce the issue with the new file you have uploaded. I am running on a OSX, maybe it depends on the OS:

{code}
scala> val a = spark.read.option(""header"",""true"").option(""inferSchema"", ""true"").option(""multiLine"", ""true"").option(""comment"", ""c"").option(""parserLib"", ""univocity"").csv(""/Users/mgaido/Downloads/test_file_without_eof_char.csv"")
a: org.apache.spark.sql.DataFrame = [abc: string, def: string]

scala> a.show
+---+---+
|abc|def|
+---+---+
|ghi|jkl|
+---+---+
{code};;;","22/Nov/17 09:10;gurwls223;This can be reproduced by:

{code}
spark.read.option(""header"",""true"").option(""inferSchema"", ""true"").option(""multiLine"", ""true"").option(""comment"", ""g"").csv(""test_file_without_eof_char.csv"").show()
{code}

The root cause seems from Univocity parser. I filed an issue there - https://github.com/uniVocity/univocity-parsers/issues/213

BTW, let's keep the description and reproducer clean as possible as we can. I was actually about to say the same things above ^ but realised it's a separate issue after multiple close looks. ;;;","22/Nov/17 11:24;gurwls223;Seems fixed in 2.5.9. We could probably bump up Univocity library.;;;","22/Nov/17 13:34;smurakozi;I'm a newbie, I would be happy to happy to work on it. Would it be ok for you [~hyukjin.kwon]?;;;","22/Nov/17 14:53;gurwls223;Sure. Please go ahead. Probably, you could refer the changes here - https://github.com/apache/spark/pull/19113/files. I opened a PR bumping the version of Univocity library before in order to to resolve an issue fixed in higher version of it. We probably also need a test case too likewise.;;;","06/Dec/17 08:18;apachespark;User 'smurakozi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19906;;;","06/Dec/17 21:22;vanzin;Issue resolved by pull request 19906
[https://github.com/apache/spark/pull/19906];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update maven central repo address,SPARK-22511,13118078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,felixcheung,felixcheung,13/Nov/17 21:48,24/Nov/17 21:14,14/Jul/23 06:30,15/Nov/17 00:00,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Build,,,,,0,,,,,,,,,"As a part of building 2.2.1, we hit an issue with sonatype
https://issues.sonatype.org/browse/MVNCENTRAL-2870
to workaround, we switch the address to repo.maven.apache.org, in branch-2.2.

we should decide if we keep that or revert after 2.2.1 is released
",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 15 00:00:04 UTC 2017,,,,,,,,,,"0|i3mqbr:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"14/Nov/17 02:57;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19742;;;","15/Nov/17 00:00;srowen;Issue resolved by pull request 19742
[https://github.com/apache/spark/pull/19742];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix setup of SPARK_HOME variable on Windows,SPARK-22495,13117717,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jsnowacki,gurwls223,,11/Nov/17 06:50,12/Dec/22 18:10,14/Jul/23 06:30,23/Nov/17 03:50,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,PySpark,Windows,,,,0,,,,,,,,,"On Windows, pip installed pyspark is unable to find out the spark home. There is already proposed change, sufficient details and discussions in https://github.com/apache/spark/pull/19370 and SPARK-18136
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18136,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 23 19:33:04 UTC 2017,,,,,,,,,,"0|i3mo3r:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"13/Nov/17 19:56;apachespark;User 'jsnowacki' has created a pull request for this issue:
https://github.com/apache/spark/pull/19370;;;","23/Nov/17 03:50;gurwls223;Fixed in https://github.com/apache/spark/pull/19370

and needs a manual backport.;;;","23/Nov/17 19:33;apachespark;User 'jsnowacki' has created a pull request for this issue:
https://github.com/apache/spark/pull/19807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shouldn't change broadcast join buildSide if user clearly specified,SPARK-22489,13117483,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,10/Nov/17 05:54,31/Jan/18 16:48,14/Jul/23 06:30,30/Nov/17 23:38,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"How to reproduce:

{code:java}
import org.apache.spark.sql.execution.joins.BroadcastHashJoinExec

spark.createDataFrame(Seq((1, ""4""), (2, ""2""))).toDF(""key"", ""value"").createTempView(""table1"")
spark.createDataFrame(Seq((1, ""1""), (2, ""2""))).toDF(""key"", ""value"").createTempView(""table2"")

val bl = sql(s""SELECT /*+ MAPJOIN(t1) */ * FROM table1 t1 JOIN table2 t2 ON t1.key = t2.key"").queryExecution.executedPlan

println(bl.children.head.asInstanceOf[BroadcastHashJoinExec].buildSide)
{code}

The result is {{BuildRight}}, but should be {{BuildLeft}}.",,apachespark,Samwel,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 01 09:08:04 UTC 2017,,,,,,,,,,"0|i3mmo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/17 06:04;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19714;;;","28/Nov/17 11:03;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19831;;;","01/Dec/17 09:08;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19858;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The view resolution in the SparkSession internal table() API ,SPARK-22488,13117465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,10/Nov/17 04:15,12/Nov/17 22:16,14/Jul/23 06:30,11/Nov/17 17:25,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"The current internal `table()` API of `SparkSession` bypasses the Analyzer and directly calls `sessionState.catalog.lookupRelation` API. This skips the view resolution logics in our Analyzer rule `ResolveRelations`. This internal API is widely used by various DDL commands or the other internal APIs.

Users might get the strange error caused by view resolution when the default database is different.
```
Table or view not found: t1; line 1 pos 14
org.apache.spark.sql.AnalysisException: Table or view not found: t1; line 1 pos 14
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
```
",,apachespark,felixcheung,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 11 18:38:03 UTC 2017,,,,,,,,,,"0|i3mmk7:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"10/Nov/17 04:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19713;;;","11/Nov/17 18:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19723;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No usages of HIVE_EXECUTION_VERSION found in whole spark project,SPARK-22487,13117454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,10/Nov/17 02:36,10/Nov/17 21:25,14/Jul/23 06:30,10/Nov/17 11:01,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Actually there is no hive client for executions in spark now and there are no usages of HIVE_EXECUTION_VERSION found in whole spark project.  HIVE_EXECUTION_VERSION is set by `spark.sql.hive.version`, which is still set internally in some places or by users,  this may confuse developers and users with HIVE_METASTORE_VERSION(spark.sql.hive.metastore.version)",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 21:25:05 UTC 2017,,,,,,,,,,"0|i3mmhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/17 03:12;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19712;;;","10/Nov/17 11:01;cloud_fan;Issue resolved by pull request 19712
[https://github.com/apache/spark/pull/19712];;;","10/Nov/17 21:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PySpark DataFrame.write.csv(quote="""") uses nullchar as quote",SPARK-22484,13117360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jbollen,jbollen,jbollen,09/Nov/17 18:23,12/Dec/22 18:11,14/Jul/23 06:30,28/Nov/17 01:14,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Input/Output,PySpark,,,,0,,,,,,,,,"[Documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=save#pyspark.sql.DataFrame) of DataFrame.write.csv() says that setting the quote parameter to an empty string should turn off quoting.  Instead, it uses the [null character](https://en.wikipedia.org/wiki/Null_character) as the quote.  ",,apachespark,jbollen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 28 01:14:58 UTC 2017,,,,,,,,,,"0|i3mlwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/17 15:22;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19814;;;","28/Nov/17 01:14;gurwls223;Issue resolved by pull request 19814
[https://github.com/apache/spark/pull/19814];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SaveIntoDataSourceCommand logs jdbc credentials,SPARK-22479,13117270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,onursatici,onursatici,onursatici,09/Nov/17 13:49,13/Oct/18 21:11,14/Jul/23 06:30,16/Nov/17 11:51,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"JDBC credentials are not redacted in plans including a 'SaveIntoDataSourceCommand'.

Steps to reproduce:
{code}
spark-shell --packages org.postgresql:postgresql:42.1.1
{code}

{code}
import org.apache.spark.sql.execution.QueryExecution
import org.apache.spark.sql.util.QueryExecutionListener
val listener = new QueryExecutionListener {
  override def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit = {}
  override def onSuccess(funcName: String, qe: QueryExecution, duration: Long): Unit = {
    System.out.println(qe.toString())
  }
}
spark.listenerManager.register(listener)
spark.range(100).write.format(""jdbc"").option(""url"", ""jdbc:postgresql:sparkdb"").option(""password"", ""pass"").option(""driver"", ""org.postgresql.Driver"").option(""dbtable"", ""test"").save()
{code}
The above will yield the following plan:

{code}
== Parsed Logical Plan ==
SaveIntoDataSourceCommand jdbc, Map(dbtable -> test10, driver -> org.postgresql.Driver, url -> jdbc:postgresql:sparkdb, password -> pass), ErrorIfExists
   +- Range (0, 100, step=1, splits=Some(8))

== Analyzed Logical Plan ==
SaveIntoDataSourceCommand jdbc, Map(dbtable -> test10, driver -> org.postgresql.Driver, url -> jdbc:postgresql:sparkdb, password -> pass), ErrorIfExists
   +- Range (0, 100, step=1, splits=Some(8))

== Optimized Logical Plan ==
SaveIntoDataSourceCommand jdbc, Map(dbtable -> test10, driver -> org.postgresql.Driver, url -> jdbc:postgresql:sparkdb, password -> pass), ErrorIfExists
   +- Range (0, 100, step=1, splits=Some(8))

== Physical Plan ==
ExecutedCommand
   +- SaveIntoDataSourceCommand jdbc, Map(dbtable -> test10, driver -> org.postgresql.Driver, url -> jdbc:postgresql:sparkdb, password -> pass), ErrorIfExists
         +- Range (0, 100, step=1, splits=Some(8))
{code}




",,aash,apachespark,onursatici,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25726,,,,,SPARK-23567,SPARK-23850,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 02 15:56:32 UTC 2018,,,,,,,,,,"0|i3mld3:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"09/Nov/17 13:54;onursatici;will submit a pr;;;","09/Nov/17 14:54;apachespark;User 'onursatici' has created a pull request for this issue:
https://github.com/apache/spark/pull/19708;;;","09/Nov/17 19:19;aash;Completely agree that credentials shouldn't be in the toString since query plans are logged in many places.  This looks like it brings SaveIntoDataSourceCommand more in-line with JdbcRelation, which also currently redacts credentials from its toString to avoid them being written to logs.;;;","15/Nov/17 23:18;apachespark;User 'onursatici' has created a pull request for this issue:
https://github.com/apache/spark/pull/19761;;;","02/Mar/18 15:44;tgraves;[~aash] [~onursatici] this seems to have redacted user names as well as the passwords.   We specifically added the User: field to the UI and now its being blocked, which is makes debugging harder.  The user name does not seem like something that needs to be redacted by default.  what is the reasoning behind that? 

Note that at least on yarn there are other ways to easily see the username on the UI (like the Resource Paths) so its definitely not a complete solution anyway.;;;","02/Mar/18 15:56;tgraves;Also the example above shows the password, but the password should have been already redacted, this pr excluded url, user, and username. Was the password not being redacted for some reason?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datasets generate random values for null primitive types,SPARK-22472,13117007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,kuzemchik,kuzemchik,08/Nov/17 15:34,01/Dec/17 07:22,14/Jul/23 06:30,10/Nov/17 06:02,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,release-notes,,,,,,,,"Not sure if it ever were reported.

{code}
scala> val s = sc.parallelize(Seq[Option[Long]](None,Some(1L),Some(5))).toDF(""v"")
s: org.apache.spark.sql.DataFrame = [v: bigint]

scala> s.show(false)
+----+
|v   |
+----+
|null|
|1   |
|5   |
+----+


scala> s.as[Long].map(v => v*2).show(false)
+-----+
|value|
+-----+
|-2   |
|2    |
|10   |
+-----+


scala> s.select($""v""*2).show(false)
+-------+
|(v * 2)|
+-------+
|null   |
|2      |
|10     |
+-------+
{code}",,apachespark,cloud_fan,felixcheung,kiszk,kuzemchik,manojmallela,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 01 07:22:17 UTC 2017,,,,,,,,,,"0|i3mjr3:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"08/Nov/17 16:28;srowen;Not sure it's random, but, somehow the value is being construed as -1 before being doubled. It's odd because {{s.as[Long].show(false)}} is fine. Try {{s.as[Long].map(_.toString).show(false)}} to see it more directly.

I admit I don't know much about the internals; is someone more informed able to weigh in on why null would become -1 here?

I think part of the issue is that you're converting null to a primitive long, but would expect 0 if anything, or an exception.;;;","08/Nov/17 16:37;kuzemchik;My assumption is that it is because it goes into Tungsten to do multiplication on Dataset, and probably tries to get value of null pointer.

I would expect 0 or exception too.;;;","08/Nov/17 16:56;mgaido;Two things:
 1 - if you use {{as\[Option\[Long\]\]}}, it works fine;
 2 - actually if you collect the Dataset, the value for {{null}} is {{0}}, but with the transformation, there is this bad result.

Then I am not sure about the right approach here. Because maybe the best thing would be to force using {{Option\[Long\]}} when the value is {{nullable}}, but this might be too restrictive and may break compatibility.;;;","08/Nov/17 17:05;kuzemchik;I'm using Option[Long] as a workaround, but it is kinda scary to leave things as is and hope that you gonna catch it on review when anyone else is using datasets.

I think spark should warn(or even error with some config parameter set) when you converting nullable DataFrame column into non-optional type.

Currently if you do that with non-primitive type, you most likely gonna net NPE, and will have to handle this use case anyway.


In my opinion current implicit behavior cause much more harm. We talking about bad results without any notification.;;;","08/Nov/17 17:10;kiszk;Thank you for reporting this behavior.

When I checked the generated code and source code, it is currently-expected behavior. In other words, if an Option object is {{null}} or an Option value is {{empty}}, {{-1}} is passed to a lambda function. I will check why {{-1}} was used as a value for these cases.;;;","08/Nov/17 17:59;kiszk;From [the initial version|https://github.com/apache/spark/commit/9e66a53c9955285a85c19f55c3ef62db2e1b868a#diff-94a1f59bcc9b6758c4ca874652437634R227] of this conversion (about two years ago), this conversion returns {{-1}}...;;;","08/Nov/17 18:19;srowen;This doesn't look right ...

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L596

{code}
  def defaultValue(jt: String): String = jt match {
    case JAVA_BOOLEAN => ""false""
    case JAVA_BYTE => ""(byte)-1""
    case JAVA_SHORT => ""(short)-1""
    case JAVA_INT => ""-1""
    case JAVA_LONG => ""-1L""
    case JAVA_FLOAT => ""-1.0f""
    case JAVA_DOUBLE => ""-1.0""
    case _ => ""null""
  }
{code}

The default for any uninitialized primitive type is 0 in the JVM.

[~cloud_fan] I think you were the last to touch this, but didn't write it. Is this really -1 on purpose for another reason?;;;","09/Nov/17 13:52;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19707;;;","29/Nov/17 05:33;felixcheung;This is going out in 2.2.1 - do we need a rel note on this change?;;;","29/Nov/17 06:47;cloud_fan;yea I think we should, this may turn a runnable query(with an unreasonable result) to a failed query.;;;","01/Dec/17 07:07;felixcheung;I guess it's too late to add to http://spark.apache.org/docs/latest/sql-programming-guide.html#migration-guide (and we don't seem to document this in patch release there anyway)

I guess I'll just add this to the website on the actual release announcement like
http://spark.apache.org/releases/spark-release-2-1-2.html
or 
http://spark.apache.org/releases/spark-release-2-1-0.html#known-issues

sounds good?
;;;","01/Dec/17 07:22;cloud_fan;sounds good, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLListener consumes much memory causing OutOfMemoryError,SPARK-22471,13117002,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tashoyan,tashoyan,tashoyan,08/Nov/17 15:26,24/Nov/17 21:14,14/Jul/23 06:30,13/Nov/17 21:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,,,,,SQL,Web UI,,,,1,memory-leak,sql,,,,,,,"_SQLListener_ may grow very large when Spark runs complex multi-stage requests. The listener tracks metrics for all stages in __stageIdToStageMetrics_ hash map. _SQLListener_ has some means to cleanup this hash map regularly, but this is not enough. Precisely, the method _trimExecutionsIfNecessary_ ensures that __stageIdToStageMetrics_ does not have metrics for very old data; this method runs on each execution completion.
However, if an execution has many stages, _SQLListener_ keeps adding new entries to __stageIdToStageMetrics_ without calling _trimExecutionsIfNecessary_. The hash map may grow to enormous size.
Strictly speaking, it is not a memory leak, because finally _trimExecutionsIfNecessary_ cleans the hash map. However, the driver program has high odds to crash with OutOfMemoryError (and it does).","Spark 2.2.0, Linux",ajbozarth,apachespark,dongjoon,felixcheung,tashoyan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/17 15:26;tashoyan;SQLListener_retained_size.png;https://issues.apache.org/jira/secure/attachment/12896669/SQLListener_retained_size.png","08/Nov/17 15:27;tashoyan;SQLListener_stageIdToStageMetrics_retained_size.png;https://issues.apache.org/jira/secure/attachment/12896670/SQLListener_stageIdToStageMetrics_retained_size.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Tue Nov 14 00:46:11 UTC 2017,,,,,,,,,,"0|i3mjpz:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"08/Nov/17 18:08;vanzin;My patch for SPARK-20652 greatly reduces the memory usage of the SQL listener. But it's not backportable to 2.2.;;;","08/Nov/17 22:13;apachespark;User 'tashoyan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19700;;;","08/Nov/17 22:15;tashoyan;I am coming up with a simple fix, backportable to 2.2.;;;","09/Nov/17 21:53;apachespark;User 'tashoyan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19711;;;","13/Nov/17 21:50;vanzin;Issue resolved by pull request 19711
[https://github.com/apache/spark/pull/19711];;;","13/Nov/17 22:05;dongjoon;Thank you for merging, [~vanzin].

Although this is late for RC1, ping [~felixcheung] anyway.;;;","13/Nov/17 22:08;vanzin;If RC1 has been cut then there probably should be a 2.2.2 version in jira.;;;","14/Nov/17 00:46;felixcheung;yes, RC1 has been cut.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accuracy problem in comparison with string and numeric ,SPARK-22469,13116914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liutang123,liutang123,liutang123,08/Nov/17 09:12,27/Sep/19 12:43,14/Jul/23 06:30,15/Nov/17 17:04,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{code:sql}
select '1.5' > 0.5; // Result is NULL in Spark but is true in Hive.
{code}

IIUC, we can cast string as double like Hive.",,apachespark,felixcheung,liutang123,xiaojuwu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21646,,,SPARK-29274,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 04 14:19:04 UTC 2018,,,,,,,,,,"0|i3mj6f:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"08/Nov/17 09:36;apachespark;User 'liutang123' has created a pull request for this issue:
https://github.com/apache/spark/pull/19692;;;","04/Mar/18 14:19;xiaojuwu;[~liutang123] cast Decimal to Double is possible to lose precision, why did you say ""There is no proper decimal type we can pick, using double type is the best we can do."" ? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_CONF_DIR is not is set by Spark's launch scripts with default value,SPARK-22466,13116854,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,08/Nov/17 01:56,12/Dec/22 18:10,14/Jul/23 06:30,09/Nov/17 05:33,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Submit,,,,,0,,,,,,,,,"If we don't explicitly set `SPARK_CONF_DIR` in `spark-env.sh`, it is only set in `sbin/spark-config.sh` with a default value which is used for spark daemons, but applications with spark-submit will miss this  environment variable at runtime.",,apachespark,Qin Yao,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25934,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 05:33:40 UTC 2017,,,,,,,,,,"0|i3mit3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/17 02:35;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19688;;;","09/Nov/17 05:33;gurwls223;Issue resolved by pull request 19688
[https://github.com/apache/spark/pull/19688];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cogroup of two disproportionate RDDs could lead into 2G limit BUG,SPARK-22465,13116757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,amitkumar,amitkumar,07/Nov/17 17:46,23/Jan/19 02:29,14/Jul/23 06:30,24/Dec/17 19:15,1.0.0,1.0.1,1.0.2,1.1.0,1.1.1,1.2.0,1.2.1,1.2.2,1.3.0,1.3.1,1.4.0,1.4.1,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.1.2,2.2.0,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"While running my spark pipeline, it failed with the following exception
{noformat}
2017-11-03 04:49:09,776 [Executor task launch worker for task 58670] ERROR org.apache.spark.executor.Executor  - Exception in task 630.0 in stage 28.0 (TID 58670)
java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869)
	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103)
	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303)
	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105)
	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:469)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:705)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:324)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}


After debugging I found that the issue lies with how spark handles cogroup of two RDDs.
Here is the relevant code from apache spark
{noformat}
 /**
   * For each key k in `this` or `other`, return a resulting RDD that contains a tuple with the
   * list of values for that key in `this` as well as `other`.
   */
  def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] = self.withScope {
    cogroup(other, defaultPartitioner(self, other))
  }


/**
   * Choose a partitioner to use for a cogroup-like operation between a number of RDDs.
   *
   * If any of the RDDs already has a partitioner, choose that one.
   *
   * Otherwise, we use a default HashPartitioner. For the number of partitions, if
   * spark.default.parallelism is set, then we'll use the value from SparkContext
   * defaultParallelism, otherwise we'll use the max number of upstream partitions.
   *
   * Unless spark.default.parallelism is set, the number of partitions will be the
   * same as the number of partitions in the largest upstream RDD, as this should
   * be least likely to cause out-of-memory errors.
   *
   * We use two method parameters (rdd, others) to enforce callers passing at least 1 RDD.
   */
  def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = {
    val rdds = (Seq(rdd) ++ others)
    val hasPartitioner = rdds.filter(_.partitioner.exists(_.numPartitions > 0))
    if (hasPartitioner.nonEmpty) {
      hasPartitioner.maxBy(_.partitions.length).partitioner.get
    } else {
      if (rdd.context.conf.contains(""spark.default.parallelism"")) {
        new HashPartitioner(rdd.context.defaultParallelism)
      } else {
        new HashPartitioner(rdds.map(_.partitions.length).max)
      }
    }
  }

{noformat}

Given this  suppose we have two  pair RDDs.
RDD1 : A small RDD which fewer data and partitions
RDD2: A huge RDD which has loads of data and partitions

Now in the code if we were to have a cogroup
{noformat}
val RDD3 = RDD1.cogroup(RDD2)
{noformat}

there is a case where this could lead to the SPARK-6235 Bug which is If RDD1 has a partitioner when it is being called into a cogroup. This is because the cogroups partitions are then decided by the partitioner and could lead to the huge RDD2 being shuffled into a small number of partitions.

One way is probably to add a safety check here that would ignore the partitioner if the number of partitions on the two RDDs are very different in magnitude.



",,amitkumar,apachespark,mridulm80,suj1th,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 27 14:25:03 UTC 2017,,,,,,,,,,"0|i3mi7j:",9223372036854775807,,,,,tgraves,,,,,,,,,,,,,,,,,,,"07/Nov/17 20:54;srowen;Is this not indeed just the 2G limit again?
You can work around this by repartitioning the larger RDD, right?;;;","09/Nov/17 21:37;tgraves;Its not strictly the 2G limit.  He did hit that but he hit it because of the default behavior of cogroup.  I think this jira was filed to look at that to make the behavior better.  So I think the last couple sentences in the description refer to that.

;;;","15/Dec/17 10:44;suj1th;Hi [~tgraves], is there a plan to resolve this behaviour of cogroup, outside of the umbrella ticket for fixing 2G limit ([SPARK-6235]). I wish to chip in if that is the case. Thank you.;;;","15/Dec/17 14:03;tgraves;I don't have time at the moment to work on this so if you want to pick it up that would be great.;;;","15/Dec/17 14:10;suj1th;Would something along the lines of  'add a safety-check that ignores the partitioner if the number of partitions on the RDDs are very different in magnitude', as the reporter suggests, be a satisfactory solution? Any pointers here would be very helpful.;;;","15/Dec/17 14:34;tgraves;Yes I think that makes sense.  ;;;","16/Dec/17 12:57;apachespark;User 'sujithjay' has created a pull request for this issue:
https://github.com/apache/spark/pull/20002;;;","24/Dec/17 19:15;mridulm80;Issue resolved by pull request 20002
[https://github.com/apache/spark/pull/20002];;;","27/Dec/17 14:25;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/20091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
<=> is not supported by Hive metastore partition predicate pushdown,SPARK-22464,13116591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Nov/17 07:08,12/Nov/17 22:17,14/Jul/23 06:30,07/Nov/17 20:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,<=> is not supported by Hive metastore partition predicate pushdown. We should forbid it. ,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 11 19:42:03 UTC 2017,,,,,,,,,,"0|i3mh6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/17 07:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19682;;;","11/Nov/17 19:42;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19724;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing hadoop/hive/hbase/etc configuration files in SPARK_CONF_DIR to distributed archive,SPARK-22463,13116562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,07/Nov/17 02:40,17/May/20 18:14,14/Jul/23 06:30,09/Nov/17 08:23,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"When I ran self contained sql apps, such as
{code:java}
import org.apache.spark.sql.SparkSession

object ShowHiveTables {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName(""Show Hive Tables"")
      .enableHiveSupport()
      .getOrCreate()
    spark.sql(""show tables"").show()
    spark.stop()
  }
}
{code}
with **yarn cluster** mode and `hive-site.xml` correctly within `$SPARK_HOME/conf`,they failed to connect the right hive metestore for not seeing hive-site.xml in AM/Driver's classpath.

Although submitting them with `--files/--jars local/path/to/hive-site.xml` or puting it to `$HADOOP_CONF_DIR/YARN_CONF_DIR` can make these apps works well in cluster mode as client mode, according to the official doc, see @ http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables
> Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.

We may respect these configuration files too or modify the doc for hive-tables in cluster mode.

",,apachespark,cloud_fan,Qin Yao,sudheer0553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 08:23:04 UTC 2017,,,,,,,,,,"0|i3mh07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/17 02:42;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19663;;;","09/Nov/17 08:23;cloud_fan;Issue resolved by pull request 19663
[https://github.com/apache/spark/pull/19663];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL metrics missing after foreach operation on dataframe,SPARK-22462,13116560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,juliuszsompolski,juliuszsompolski,07/Nov/17 02:16,11/Nov/17 11:35,14/Jul/23 06:30,11/Nov/17 11:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"No SQL metrics are visible in the SQL tab of SparkUI when foreach is executed on the DataFrame.
e.g.
{code}
sql(""select * from range(10)"").collect()
sql(""select * from range(10)"").foreach(a => Unit)
sql(""select * from range(10)"").foreach(a => println(a))
{code}
See collect.png vs. foreach.png",,apachespark,cloud_fan,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/17 02:19;juliuszsompolski;collect.png;https://issues.apache.org/jira/secure/attachment/12896326/collect.png","07/Nov/17 02:19;juliuszsompolski;foreach.png;https://issues.apache.org/jira/secure/attachment/12896325/foreach.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 11 11:34:52 UTC 2017,,,,,,,,,,"0|i3mgzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/17 04:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19689;;;","11/Nov/17 11:34;cloud_fan;Issue resolved by pull request 19689
[https://github.com/apache/spark/pull/19689];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalShuffleClient.close() should check null,SPARK-22454,13116367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yumwang,yumwang,yumwang,06/Nov/17 12:11,17/May/20 18:31,14/Jul/23 06:30,07/Nov/17 08:31,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Shuffle,Spark Core,,,,0,,,,,,,,,"
{noformat}
17/11/06 20:08:05 ERROR Utils: Uncaught exception in thread main
java.lang.NullPointerException
	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152)
	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1407)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:89)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1849)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:587)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2320)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:288)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 07 08:31:45 UTC 2017,,,,,,,,,,"0|i3mftb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/17 12:16;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19670;;;","07/Nov/17 08:31;srowen;Resolved by https://github.com/apache/spark/pull/19670;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Optimizer causing StringIndexerModel's indexer UDF to throw ""Unseen label"" exception incorrectly for filtered data.",SPARK-22446,13116111,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,IdRatherBeCoding,IdRatherBeCoding,04/Nov/17 00:57,06/Mar/18 03:01,14/Jul/23 06:30,08/Nov/17 11:18,2.0.2,2.1.2,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,Optimizer,,,,0,,,,,,,,,"In the following, the `indexer` UDF defined inside the `org.apache.spark.ml.feature.StringIndexerModel.transform` method throws an ""Unseen label"" error, despite the label not being present in the transformed DataFrame.

Here is the definition of the indexer UDF in the transform method:

{code:java}
    val indexer = udf { label: String =>
      if (labelToIndex.contains(label)) {
        labelToIndex(label)
      } else {
        throw new SparkException(s""Unseen label: $label."")
      }
    }
{code}

We can demonstrate the error with a very simple example DataFrame.

{code:java}
scala> import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.StringIndexer

scala> // first we create a DataFrame with three cities

scala> val df = List(
     | (""A"", ""London"", ""StrA""),
     | (""B"", ""Bristol"", null),
     | (""C"", ""New York"", ""StrC"")
     | ).toDF(""ID"", ""CITY"", ""CONTENT"")
df: org.apache.spark.sql.DataFrame = [ID: string, CITY: string ... 1 more field]

scala> df.show
+---+--------+-------+
| ID|    CITY|CONTENT|
+---+--------+-------+
|  A|  London|   StrA|
|  B| Bristol|   null|
|  C|New York|   StrC|
+---+--------+-------+


scala> // then we remove the row with null in CONTENT column, which removes Bristol

scala> val dfNoBristol = finalStatic.filter($""CONTENT"".isNotNull)
dfNoBristol: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ID: string, CITY: string ... 1 more field]

scala> dfNoBristol.show
+---+--------+-------+
| ID|    CITY|CONTENT|
+---+--------+-------+
|  A|  London|   StrA|
|  C|New York|   StrC|
+---+--------+-------+


scala> // now create a StringIndexer for the CITY column and fit to dfNoBristol

scala> val model = {
     | new StringIndexer()
     | .setInputCol(""CITY"")
     | .setOutputCol(""CITYIndexed"")
     | .fit(dfNoBristol)
     | }
model: org.apache.spark.ml.feature.StringIndexerModel = strIdx_f5afa23333fb

scala> // the StringIndexerModel has only two labels: ""London"" and ""New York""

scala> str.labels foreach println
London
New York

scala> // transform our DataFrame to add an index column

scala> val dfWithIndex = model.transform(dfNoBristol)
dfWithIndex: org.apache.spark.sql.DataFrame = [ID: string, CITY: string ... 2 more fields]

scala> dfWithIndex.show
+---+--------+-------+-----------+
| ID|    CITY|CONTENT|CITYIndexed|
+---+--------+-------+-----------+
|  A|  London|   StrA|        0.0|
|  C|New York|   StrC|        1.0|
+---+--------+-------+-----------+
{code}

The unexpected behaviour comes when we filter `dfWithIndex` for `CITYIndexed` equal to 1.0 and perform an action. The `indexer` UDF in `transform` method throws an exception reporting unseen label ""Bristol"". This is irrational behaviour as far as the user of the API is concerned, because there is no such value as ""Bristol"" when do show all rows of `dfWithIndex`:

{code:java}
scala> dfWithIndex.filter($""CITYIndexed"" === 1.0).count
17/11/04 00:33:41 ERROR Executor: Exception in task 1.0 in stage 20.0 (TID 40)
org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) => double)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Unseen label: Bristol.  To handle unseen labels, set Param handleInvalid to keep.
	at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:222)
	at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)
	... 13 more
{code}

To understand what is happening here, note that an action is triggered when we call `StringIndexer.fit()`, before the `CITYIndexed === 1` filter is applied, so the StringIndexerModel sees only London and New York, as expected. Now compare the query plans for `dfWithIndex` and `dfWithIndex.filter($""CITYIndexed"" === 1.0)`:

{noformat}
scala> dfWithIndex.explain
== Physical Plan ==
*Project [_1#3 AS ID#7, _2#4 AS CITY#8, _3#5 AS CONTENT#9, UDF(_2#4) AS CITYIndexed#159]
+- *Filter isnotnull(_3#5)
   +- LocalTableScan [_1#3, _2#4, _3#5]

scala> dfWithIndex.filter($""CITYIndexed"" === 1.0).explain
== Physical Plan ==
*Project [_1#3 AS ID#7, _2#4 AS CITY#8, _3#5 AS CONTENT#9, UDF(_2#4) AS CITYIndexed#159]
+- *Filter (isnotnull(_3#5) && (UDF(_2#4) = 1.0))
   +- LocalTableScan [_1#3, _2#4, _3#5]
{noformat}

Note that in the latter, the query plan has pushed the filter `$""CITYIndexed"" === 1.0` back to be performed at the same stage as our null filter (`Filter (isnotnull(_3#5) && (UDF(_2#4) = 1.0))`).

With a debugger I have seen that both operands of `&&` are executed on each row of `df`: `isnotnull(_3#5)` and `UDF(_2#4) = 1.0`. Therefore, the UDF is passed the label `Bristol` despite isnotnull returning false for that row.

If we cache the DataFrame `dfNoBristol` immediately after creating it, then there is no longer an error because the optimizer does not attempt to call the UDF on unseen data. The fact that we get different results depending on whether or not we call cache is a cause for concern.

I have seen similar issues with pure SparkSql DataFrame operations when the DAG gets complicated (many joins, and aggregations). These are harder to isolate to such a simple example, but I plan to report them in the near future.","spark-shell, local mode, macOS Sierra 10.12.6",apachespark,cloud_fan,IdRatherBeCoding,josephkb,kiszk,mgaido,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 06 03:01:26 UTC 2018,,,,,,,,,,"0|i3me8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/17 04:36;viirya;For this special case, the simplest workaround is to set {{handleInvalid}} as keep. Actually the another predicate {{isnotnull(_3#5)}} can filter the row out if the UDF doesn't cause error with {{handleInvalid}} as keep.

The problem is happened at the optimizer when pushing predicates down through projection. For the catalyst expressions, applying on the supposedly filtered out data is not a problem because other predicates should filter it out.

UDFs are special case because they can possibly cause runtime exception when applying on unexcepted data. It is not always safe to push down such predicates.

However, because not all UDFs are not safe to push down, we may not want to disable all pushdown UDF predicates. Currently I think we should let such UDFs as non-deterministic and disable pushdown for it.

;;;","06/Nov/17 04:49;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19662;;;","08/Nov/17 11:18;cloud_fan;Issue resolved by pull request 19662
[https://github.com/apache/spark/pull/19662];;;","08/Feb/18 01:27;josephkb;[~viirya] Did you confirm this is an issue in Spark 2.2 or earlier?;;;","08/Feb/18 05:19;viirya;Yes, this is an issue in Spark 2.2. For earlier version, let me check it.;;;","08/Feb/18 06:06;viirya;2.0 and 2.1 also have this issue.;;;","02/Mar/18 19:54;josephkb;I see.  Do you have a sense of how hard it would be to backport this fix (definitely to 2.2 and maybe to 2.1 if it's easy)?;;;","03/Mar/18 11:14;viirya;This fix uses an new API  {{asNondeterministic}} of {{UserDefinedFunction. }}{{asNondeterministic}} is added since 2.3.0. If we want to backport this fix, we need to backport the API too. It is not hard but it involves SQL codes. Should we backport it because of this fix?;;;","05/Mar/18 18:44;josephkb;Maybe not then unless someone complains?;;;","06/Mar/18 03:01;viirya;Yeah, sounds good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregatedDialect doesn't override quoteIdentifier and other methods in JdbcDialects,SPARK-22443,13116060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxing,liuhb86,liuhb86,03/Nov/17 21:24,05/Nov/17 06:14,14/Jul/23 06:30,05/Nov/17 06:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"The AggregatedDialect only implements canHandle, getCatalystType, getJDBCType. It doesn't implement other methods in JdbcDialect. 
So if multiple Dialects are registered with the same driver, the implementation of these methods will not be taken and the default implementation in JdbcDialect will be used.

Example:

{code:java}
package example

import java.util.Properties

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects}
import org.apache.spark.sql.types.{DataType, MetadataBuilder}

object AnotherMySQLDialect extends JdbcDialect {
  override def canHandle(url : String): Boolean = url.startsWith(""jdbc:mysql"")

  override def getCatalystType(
                                sqlType: Int, typeName: String, size: Int, md: MetadataBuilder): Option[DataType] = {
    None
  }

  override def quoteIdentifier(colName: String): String = {
    s""`$colName`""
  }
}

object App {
  def main(args: Array[String]) {
    val spark = SparkSession.builder.master(""local"").appName(""Simple Application"").getOrCreate()
    JdbcDialects.registerDialect(AnotherMySQLDialect)
    val jdbcUrl = s""jdbc:mysql://host:port/db?user=user&password=password""
    spark.read.jdbc(jdbcUrl, ""badge"", new Properties()).show()
  }
}
{code}

will throw an exception. 

{code:none}
17/11/03 17:08:39 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.sql.SQLDataException: Cannot determine value type from string 'id'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:530)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:513)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:505)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:479)
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:489)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:89)
	at com.mysql.cj.jdbc.result.ResultSetImpl.getLong(ResultSetImpl.java:853)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:409)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8.apply(JdbcUtils.scala:408)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:330)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:312)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.mysql.cj.core.exceptions.DataConversionException: Cannot determine value type from string 'id'
	at com.mysql.cj.core.io.StringConverter.createFromBytes(StringConverter.java:121)
	at com.mysql.cj.core.io.MysqlTextValueDecoder.decodeByteArray(MysqlTextValueDecoder.java:232)
	at com.mysql.cj.mysqla.result.AbstractResultsetRow.decodeAndCreateReturnValue(AbstractResultsetRow.java:124)
	at com.mysql.cj.mysqla.result.AbstractResultsetRow.getValueFromBytes(AbstractResultsetRow.java:225)
	at com.mysql.cj.mysqla.result.ByteArrayRow.getValue(ByteArrayRow.java:84)
	at com.mysql.cj.jdbc.result.ResultSetImpl.getNonStringValueFromRow(ResultSetImpl.java:630)
	... 24 more
{code}

Though the quoteIdentifier is correctly implemented in Spark's MySQLDialect and our AnotherMySQLDialect.
",,apachespark,liuhb86,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 05 06:14:44 UTC 2017,,,,,,,,,,"0|i3mdxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/17 22:16;srowen;Good catch. I suppose that this and getTableExistsQuery and getSchemaQuery need to return the value from the first dialect?;;;","04/Nov/17 09:15;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19658;;;","04/Nov/17 16:50;liuhb86;[~srowen] Thanks for the quick response!

I think returning the first dialect is an acceptable solution. But I was wondering whether it could be better?

Suppose the first dialect doesn't override,  e.g., the quoteIdentifier method, but the second dialect overrides it. Naturally, using the implementation in the second dialect is better.  But in the current implementation, it will use the default implementation in the base JdbcDialect class.

Maybe we can derive new dialects from another base class which returns null(I hate null, but wrap with Option will change external API) for the string methods? And in AggregatedDialect, it can return the first non-null result. If all the dialects return null, then it returns the default implementation in NoopDialect (the trivial concrete object derived from JdbcDialect).

Just my two cents.;;;","04/Nov/17 17:57;srowen;The semantics here are already odd. The methods might return values from different implementations already, and there's no reason to expect they're consistent or compatible. There's also not a way to know if a method is overridden (short of reflection).

I'm not actually sure what the use case is for this, but assume it intends to wrap a dialect, with another dialect as fallback. If so, it makes sense to prefer the first dialect's value, as it's also the first implementation that returns a non-None value, always.

Returning the first non-null value seems reasonable too, OK.;;;","04/Nov/17 22:56;liuhb86;In our case, we want to map MySQL YEAR to ShortType instead of DateType. Ideally, I'd like to write a custom dialect by overriding the getCatalystType method. But it doesn't work because it breaks the quoteIdentifier implemented in the predefined MySQLDialect.

We have a workaround. The custom dialect needs to override all other methods and redirect to the implementation in MySQLDialect. Then we unregister MySQLDialect and register our custom dialect. It's not robust because if there are new methods added to JdbcDialect in the future, it may break again.
;;;","05/Nov/17 06:14;smilegator;It sounds your custom dialect is a good solution for your scenario. You can customize it based on your requirement.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema generated by Product Encoder doesn't match case class field name when using non-standard characters,SPARK-22442,13116054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mikelsanvi,mikelsanvi,03/Nov/17 20:54,13/Nov/17 06:32,14/Jul/23 06:30,09/Nov/17 10:55,2.0.2,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,1,,,,,,,,,"Product encoder encodes special characters wrongly when field name contains certain nonstandard characters.

For example for:
{quote}
case class MyType(`field.1`: String, `field 2`: String)
{quote}
we will get the following schema

{quote}
root
 |-- field$u002E1: string (nullable = true)
 |-- field$u00202: string (nullable = true)
{quote}

As a consequence of this issue a DataFrame with the correct schema can't be converted to a Dataset using .as[MyType]",,apachespark,cloud_fan,glenn.strycker@gmail.com,mgaido,mikelsanvi,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 13 06:32:04 UTC 2017,,,,,,,,,,"0|i3mdvz:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"06/Nov/17 01:57;viirya;I tried on latest master branch. It can work with {{as[MyType]}}.

{code}
scala> val df = Seq(MyType(""a"", ""b""), MyType(""c"", ""d"")).toDF
df: org.apache.spark.sql.DataFrame = [field$u002E1: string, field$u00202: string]
scala> df.as[MyType].collect
res7: Array[MyType] = Array(MyType(a,b), MyType(c,d))
{code}

;;;","06/Nov/17 02:03;mikelsanvi;yes, that will work but it wont work for the correct schema that will be inferred if you read directly from json:

spark.read.json(path).as[MyType]

it won't work because the inferred schema will be
[field.1: string, field 2: string];;;","06/Nov/17 09:10;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19664;;;","09/Nov/17 10:55;cloud_fan;Issue resolved by pull request 19664
[https://github.com/apache/spark/pull/19664];;;","12/Nov/17 22:18;mikelsanvi;are you planning to make a patch to version 2.2.x with this bug fix?;;;","12/Nov/17 23:28;cloud_fan;I don't have a strong preference. [~viirya] how hard it is to backport it to 2.2?;;;","13/Nov/17 02:06;viirya;I think it is easy. Let me prepare a backport PR for it.;;;","13/Nov/17 02:53;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19734;;;","13/Nov/17 03:56;viirya;cc [~felixcheung] This will be backported to 2.2.;;;","13/Nov/17 06:32;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19736;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jdbc write fails to set default mode,SPARK-22437,13115892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mgaido,abridgett,abridgett,03/Nov/17 11:25,12/Dec/22 18:10,14/Jul/23 06:30,04/Nov/17 08:03,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Java API,,,,,0,,,,,,,,,"With this bit of code:
{code}
        df.write.jdbc(jdbc_url, table)
{code}

We see this error:
{code}
09:54:22 2017-11-03 09:54:19,985 INFO       File ""/opt/spark220/python/lib/pyspark.zip/pyspark/sql/readwriter.py"", line 820, in jdbc
09:54:22 2017-11-03 09:54:19,985 INFO       File ""/opt/spark220/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
09:54:22 2017-11-03 09:54:19,986 INFO       File ""/opt/spark220/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco
09:54:22 2017-11-03 09:54:19,986 INFO       File ""/opt/spark220/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value
09:54:22 2017-11-03 09:54:19,987 INFO     py4j.protocol.Py4JJavaError: An error occurred while calling o106.jdbc.
09:54:22 2017-11-03 09:54:19,987 INFO     : scala.MatchError: null
09:54:22 2017-11-03 09:54:19,987 INFO     	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:62)
09:54:22 2017-11-03 09:54:19,987 INFO     	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:472)
09:54:22 2017-11-03 09:54:19,987 INFO     	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
...
{code}

This seems to be that ""mode"" isn't correctly picking up the ""error"" default as listed in https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame

Note that if it was erroring because of existing data it'd say ""SaveMode: ErrorIfExists."" as seen in ./sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider.scala).

Changing our code to this worked:
{code}
        df.write.jdbc(jdbc_url, table, mode='append')
{code}",python3,abridgett,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23684,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 04 08:03:05 UTC 2017,,,,,,,,,,"0|i3mcvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/17 15:10;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19654;;;","03/Nov/17 15:15;abridgett;good grief that was fast!;;;","04/Nov/17 08:03;gurwls223;Issue resolved by pull request 19654
[https://github.com/apache/spark/pull/19654];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating Permanent view with illegal type,SPARK-22431,13115766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ksunitha,hvanhovell,hvanhovell,02/Nov/17 22:15,28/Nov/17 21:09,14/Jul/23 06:30,28/Nov/17 21:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"It is possible in Spark SQL to create a permanent view that uses an nested field with an illegal name.

For example if we create the following view:
{noformat}
create view x as select struct('a' as `$q`, 1 as b) q
{noformat}
A simple select fails with the following exception:
{noformat}
select * from x;

org.apache.spark.SparkException: Cannot recognize hive type string: struct<$q:string,b:int>
  at org.apache.spark.sql.hive.client.HiveClientImpl$.fromHiveColumn(HiveClientImpl.scala:812)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
...
{noformat}
Dropping the view isn't possible either:
{noformat}
drop view x;

org.apache.spark.SparkException: Cannot recognize hive type string: struct<$q:string,b:int>
  at org.apache.spark.sql.hive.client.HiveClientImpl$.fromHiveColumn(HiveClientImpl.scala:812)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:378)
...
{noformat}",,apachespark,hvanhovell,jmchung,ksunitha,mgaido,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 14 17:05:07 UTC 2017,,,,,,,,,,"0|i3mc3z:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"13/Nov/17 12:30;ksunitha;*Observations:*
I ran a few tests with the STRUCT containing a `$a` and for the following scenarios:
a) create table, b) create view, c) create datasource table against the hive and in-memory catalog.   

*A. Hive Catalog*
+1. Create Table (CTAS) - illegal type+   - Results in Error
{code:java}
spark-sql> CREATE TABLE t AS SELECT STRUCT('a' AS `$a`, 1 AS b) q;
17/11/10 22:50:45 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Error in query: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: name expected at the position 7 of 'struct<$a:string,b:int>' but '$' is found.;
{code}

+2 Create Table – illegal type+  -	Results in Error

{code:java}
CREATE TABLE t(q STRUCT<`$a`:INT,col2:STRING>, i1 INT);
Error in query: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: name expected at the position 7 of 'struct<$a:int,col2:string>:int' but '$' is found.;
{code}

+3 Create DataSourceTable  - illegal type+  - 
Try to store it in hive compatible way if possible, if this fails, then it tries to  store the metadata in Spark SQL format.   This is successful
With Parquet, there is error trying to store in hive compatible way so it falls back to persisting the metadata in Spark SQL specific format. 
{code:java}
CREATE TABLE t(q STRUCT<`$a`:INT,col2:STRING>, i1 INT) USING PARQUET;	17/11/10 22:52:40 WARN HiveExternalCatalog: Could not persist `default`.`t` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.IllegalArgumentException: Error: name expected at the position 7 of 'struct<$a:int,col2:string>:int' but '$' is found.
at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:720)
Caused by: java.lang.IllegalArgumentException: Error: name expected at the position 7 of 'struct<$a:int,col2:string>:int' but '$' is found.
{code}
Retrieving the table metadata –  OK

{code:java}
select * from t;
Time taken: 0.912 seconds
spark-sql> describe formatted t;
q	struct<$a:int,col2:string>	NULL
i1	int	NULL
		
# Detailed Table Information		
Database	default	
Table	t	
Owner	ksunitha	
Created Time	Fri Nov 10 22:52:40 IST 2017	
Last Access	Thu Jan 01 05:30:00 IST 1970	
Created By	Spark 2.3.0-SNAPSHOT	
Type	MANAGED	
Provider	PARQUET	
Table Properties	[transient_lastDdlTime=1510334560]	
Location	file:/Users/ksunitha/projects/trunk/spark/spark-warehouse/t	
Serde Library	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	
InputFormat	org.apache.hadoop.mapred.SequenceFileInputFormat	
OutputFormat	org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat	
Storage Properties	[serialization.format=1]	
Time taken: 0.071 seconds, Fetched 18 row(s)
{code}

+4. Create View - illegal type+ 
Creation successful.
Retrieving the view metadata – fails, so select, drop fail.

{code:java}
CREATE VIEW t AS SELECT STRUCT('a' AS `$a`, 1 AS b) q;
Time taken: 0.036 seconds
spark-sql> select * from t;
17/11/10 22:57:22 ERROR SparkSQLDriver: Failed in [select * from t]
org.apache.spark.SparkException: Cannot recognize hive type string: struct<$a:string,b:int>
{code}

--
*B. InMemoryCatalog*
+1.Create Table -  illegal type+
N/A – Hive Support is needed 

+2. Create DataSourceTable  - illegal type+
OK/Successful
{code:java}
CREATE TABLE t(q STRUCT<`$a`:INT,col2:STRING>, i1 INT) USING PARQUET
{code}
Retrieving the table metadata – Select Query – OK

+3. Create View - illegal type+
Creation successful.
{code:java}
CREATE VIEW t AS SELECT STRUCT('a' AS `$a`, 1 AS b) q
{code}
Retrieving the view metadata and select query – OK 


----
*Cause:*
# When you store the table metadata with provider as Hive, then it stores the schema that has a struct with illegal name in the catalog. 
a.	If *table metadata* is being stored, the underlying serde initialization catches any illegal parameters and throws an exception. 
b.	If *view metadata* is being stored, then hive has some special case logic and will not trigger any checks on the illegal parameters so the metadata for the view gets stored(/created) in the Hive metastore. But when retrieving a hive table’s metadata, Spark will make sure that it can read the schema and it is compatible with Spark and so when it retrieves the information from the Hive Metastore, it checks that the column datatype can be parsed by Spark.  Spark is not able to parse the struct with the illegal name because it is not quoted (backticked). 
# When you store the table metadata for a non-hive provider, then there are no checks done which is why the create and the select statements work fine.  
# CatalystSqlParser.parseDataType cannot parse the schema when the struct has an illegal name without the backtick. 

As the tests show, there are differences in the behavior.  

*Some possible solutions:*
# Fix the hive table/view codepath to check whether the schema datatype is parseable by Spark before persisting it in the metastore. This change is localized to HiveClientImpl to do the check similar to the check in FromHiveColumn.  This is fail-fast and we will avoid the scenario where we write something to the metastore that we are unable to read it back ( ie the scenario in this jira description)
# Improve storing and handling of the nested datatypes that have quoted (backtick identifiers) – This would need more changes after more investigation/input.
# The tests show that there are differences in the behavior when using the in-memory versus the hive catalog.  Resolve inconsistent behavior ?

What do we want the desired behavior to be ?   Please share your thoughts/comments.  Thanks.;;;","13/Nov/17 12:38;hvanhovell;[~ksunitha] Thanks for the thorough analysis! I think this is mainly a Hive metastore issue (that does not support column names with weird characters), and so I think we should keep this localized to the Hive code. I would go with option '1' for now.;;;","13/Nov/17 12:45;ksunitha;Thanks for the response.   Option 1 sounds good.  I'll go ahead and create a PR for the same.;;;","13/Nov/17 15:49;hvanhovell;I look forward to the PR :);;;","14/Nov/17 17:05;apachespark;User 'skambha' has created a pull request for this issue:
https://github.com/apache/spark/pull/19747;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unknown tag warnings when building R docs with Roxygen 6.0.1,SPARK-22430,13115748,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rekhajoshm,TV4Fun,TV4Fun,02/Nov/17 20:50,05/Mar/18 17:32,14/Jul/23 06:30,05/Mar/18 17:32,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Documentation,,,,,0,,,,,,,,,"When building R docs using create-rd.sh with Roxygen 6.0.1, a large number of unknown tag warnings are generated:

{noformat}
Warning: @export [schema.R#33]: unknown tag
Warning: @export [schema.R#53]: unknown tag
Warning: @export [schema.R#63]: unknown tag
Warning: @export [schema.R#80]: unknown tag
Warning: @export [schema.R#123]: unknown tag
Warning: @export [schema.R#141]: unknown tag
Warning: @export [schema.R#216]: unknown tag
Warning: @export [generics.R#388]: unknown tag
Warning: @export [generics.R#403]: unknown tag
Warning: @export [generics.R#407]: unknown tag
Warning: @export [generics.R#414]: unknown tag
Warning: @export [generics.R#418]: unknown tag
Warning: @export [generics.R#422]: unknown tag
Warning: @export [generics.R#428]: unknown tag
Warning: @export [generics.R#432]: unknown tag
Warning: @export [generics.R#438]: unknown tag
Warning: @export [generics.R#442]: unknown tag
Warning: @export [generics.R#446]: unknown tag
Warning: @export [generics.R#450]: unknown tag
Warning: @export [generics.R#454]: unknown tag
Warning: @export [generics.R#459]: unknown tag
Warning: @export [generics.R#467]: unknown tag
Warning: @export [generics.R#475]: unknown tag
Warning: @export [generics.R#479]: unknown tag
Warning: @export [generics.R#483]: unknown tag
Warning: @export [generics.R#487]: unknown tag
Warning: @export [generics.R#498]: unknown tag
Warning: @export [generics.R#502]: unknown tag
Warning: @export [generics.R#506]: unknown tag
Warning: @export [generics.R#512]: unknown tag
Warning: @export [generics.R#518]: unknown tag
Warning: @export [generics.R#526]: unknown tag
Warning: @export [generics.R#530]: unknown tag
Warning: @export [generics.R#534]: unknown tag
Warning: @export [generics.R#538]: unknown tag
Warning: @export [generics.R#542]: unknown tag
Warning: @export [generics.R#549]: unknown tag
Warning: @export [generics.R#556]: unknown tag
Warning: @export [generics.R#560]: unknown tag
Warning: @export [generics.R#567]: unknown tag
Warning: @export [generics.R#571]: unknown tag
Warning: @export [generics.R#575]: unknown tag
Warning: @export [generics.R#579]: unknown tag
Warning: @export [generics.R#583]: unknown tag
Warning: @export [generics.R#587]: unknown tag
Warning: @export [generics.R#591]: unknown tag
Warning: @export [generics.R#595]: unknown tag
Warning: @export [generics.R#599]: unknown tag
Warning: @export [generics.R#603]: unknown tag
Warning: @export [generics.R#607]: unknown tag
Warning: @export [generics.R#611]: unknown tag
Warning: @export [generics.R#615]: unknown tag
Warning: @export [generics.R#619]: unknown tag
Warning: @export [generics.R#623]: unknown tag
Warning: @export [generics.R#627]: unknown tag
Warning: @export [generics.R#631]: unknown tag
Warning: @export [generics.R#635]: unknown tag
Warning: @export [generics.R#639]: unknown tag
Warning: @export [generics.R#643]: unknown tag
Warning: @export [generics.R#647]: unknown tag
Warning: @export [generics.R#654]: unknown tag
Warning: @export [generics.R#658]: unknown tag
Warning: @export [generics.R#663]: unknown tag
Warning: @export [generics.R#667]: unknown tag
Warning: @export [generics.R#672]: unknown tag
Warning: @export [generics.R#676]: unknown tag
Warning: @export [generics.R#680]: unknown tag
Warning: @export [generics.R#684]: unknown tag
Warning: @export [generics.R#690]: unknown tag
Warning: @export [generics.R#696]: unknown tag
Warning: @export [generics.R#702]: unknown tag
Warning: @export [generics.R#706]: unknown tag
Warning: @export [generics.R#710]: unknown tag
Warning: @export [generics.R#716]: unknown tag
Warning: @export [generics.R#720]: unknown tag
Warning: @export [generics.R#726]: unknown tag
Warning: @export [generics.R#730]: unknown tag
Warning: @export [generics.R#734]: unknown tag
Warning: @export [generics.R#738]: unknown tag
Warning: @export [generics.R#742]: unknown tag
Warning: @export [generics.R#750]: unknown tag
Warning: @export [generics.R#754]: unknown tag
Warning: @export [generics.R#758]: unknown tag
Warning: @export [generics.R#766]: unknown tag
Warning: @export [generics.R#770]: unknown tag
Warning: @export [generics.R#774]: unknown tag
Warning: @export [generics.R#778]: unknown tag
Warning: @export [generics.R#782]: unknown tag
Warning: @export [generics.R#786]: unknown tag
Warning: @export [generics.R#790]: unknown tag
Warning: @export [generics.R#794]: unknown tag
Warning: @export [generics.R#799]: unknown tag
Warning: @export [generics.R#803]: unknown tag
Warning: @export [generics.R#807]: unknown tag
Warning: @export [generics.R#813]: unknown tag
Warning: @export [generics.R#817]: unknown tag
Warning: @export [generics.R#821]: unknown tag
Warning: @export [generics.R#827]: unknown tag
Warning: @export [generics.R#831]: unknown tag
Warning: @export [generics.R#835]: unknown tag
Warning: @export [generics.R#839]: unknown tag
Warning: @export [generics.R#843]: unknown tag
Warning: @export [generics.R#847]: unknown tag
Warning: @export [generics.R#851]: unknown tag
Warning: @export [generics.R#855]: unknown tag
Warning: @export [generics.R#859]: unknown tag
Warning: @export [generics.R#863]: unknown tag
Warning: @export [generics.R#867]: unknown tag
Warning: @export [generics.R#871]: unknown tag
Warning: @export [generics.R#876]: unknown tag
Warning: @export [generics.R#880]: unknown tag
Warning: @export [generics.R#884]: unknown tag
Warning: @export [generics.R#890]: unknown tag
Warning: @export [generics.R#894]: unknown tag
Warning: @export [generics.R#898]: unknown tag
Warning: @export [generics.R#902]: unknown tag
Warning: @export [generics.R#906]: unknown tag
Warning: @export [generics.R#912]: unknown tag
Warning: @export [generics.R#917]: unknown tag
Warning: @export [generics.R#922]: unknown tag
Warning: @export [generics.R#927]: unknown tag
Warning: @export [generics.R#934]: unknown tag
Warning: @export [generics.R#938]: unknown tag
Warning: @export [generics.R#943]: unknown tag
Warning: @export [generics.R#948]: unknown tag
Warning: @export [generics.R#953]: unknown tag
Warning: @export [generics.R#958]: unknown tag
Warning: @export [generics.R#963]: unknown tag
Warning: @export [generics.R#968]: unknown tag
Warning: @export [generics.R#973]: unknown tag
Warning: @export [generics.R#978]: unknown tag
Warning: @export [generics.R#982]: unknown tag
Warning: @export [generics.R#987]: unknown tag
Warning: @export [generics.R#992]: unknown tag
Warning: @export [generics.R#997]: unknown tag
Warning: @export [generics.R#1002]: unknown tag
Warning: @export [generics.R#1007]: unknown tag
Warning: @export [generics.R#1012]: unknown tag
Warning: @export [generics.R#1017]: unknown tag
Warning: @export [generics.R#1022]: unknown tag
Warning: @export [generics.R#1027]: unknown tag
Warning: @export [generics.R#1032]: unknown tag
Warning: @export [generics.R#1037]: unknown tag
Warning: @export [generics.R#1042]: unknown tag
Warning: @export [generics.R#1047]: unknown tag
Warning: @export [generics.R#1052]: unknown tag
Warning: @export [generics.R#1057]: unknown tag
Warning: @export [generics.R#1062]: unknown tag
Warning: @export [generics.R#1067]: unknown tag
Warning: @export [generics.R#1072]: unknown tag
Warning: @export [generics.R#1077]: unknown tag
Warning: @export [generics.R#1082]: unknown tag
Warning: @export [generics.R#1087]: unknown tag
Warning: @export [generics.R#1092]: unknown tag
Warning: @export [generics.R#1097]: unknown tag
Warning: @export [generics.R#1102]: unknown tag
Warning: @export [generics.R#1107]: unknown tag
Warning: @export [generics.R#1112]: unknown tag
Warning: @export [generics.R#1117]: unknown tag
Warning: @export [generics.R#1122]: unknown tag
Warning: @export [generics.R#1127]: unknown tag
Warning: @export [generics.R#1132]: unknown tag
Warning: @export [generics.R#1137]: unknown tag
Warning: @export [generics.R#1142]: unknown tag
Warning: @export [generics.R#1147]: unknown tag
Warning: @export [generics.R#1153]: unknown tag
Warning: @export [generics.R#1158]: unknown tag
Warning: @export [generics.R#1163]: unknown tag
Warning: @export [generics.R#1168]: unknown tag
Warning: @export [generics.R#1173]: unknown tag
Warning: @export [generics.R#1177]: unknown tag
Warning: @export [generics.R#1182]: unknown tag
Warning: @export [generics.R#1187]: unknown tag
Warning: @export [generics.R#1192]: unknown tag
Warning: @export [generics.R#1197]: unknown tag
Warning: @export [generics.R#1202]: unknown tag
Warning: @export [generics.R#1207]: unknown tag
Warning: @export [generics.R#1212]: unknown tag
Warning: @export [generics.R#1217]: unknown tag
Warning: @export [generics.R#1222]: unknown tag
Warning: @export [generics.R#1227]: unknown tag
Warning: @export [generics.R#1232]: unknown tag
Warning: @export [generics.R#1237]: unknown tag
Warning: @export [generics.R#1242]: unknown tag
Warning: @export [generics.R#1248]: unknown tag
Warning: @export [generics.R#1253]: unknown tag
Warning: @export [generics.R#1258]: unknown tag
Warning: @export [generics.R#1262]: unknown tag
Warning: @export [generics.R#1267]: unknown tag
Warning: @export [generics.R#1272]: unknown tag
Warning: @export [generics.R#1276]: unknown tag
Warning: @export [generics.R#1281]: unknown tag
Warning: @export [generics.R#1286]: unknown tag
Warning: @export [generics.R#1291]: unknown tag
Warning: @export [generics.R#1296]: unknown tag
Warning: @export [generics.R#1301]: unknown tag
Warning: @export [generics.R#1306]: unknown tag
Warning: @export [generics.R#1311]: unknown tag
Warning: @export [generics.R#1316]: unknown tag
Warning: @export [generics.R#1321]: unknown tag
Warning: @export [generics.R#1326]: unknown tag
Warning: @export [generics.R#1331]: unknown tag
Warning: @export [generics.R#1336]: unknown tag
Warning: @export [generics.R#1342]: unknown tag
Warning: @export [generics.R#1347]: unknown tag
Warning: @export [generics.R#1352]: unknown tag
Warning: @export [generics.R#1357]: unknown tag
Warning: @export [generics.R#1362]: unknown tag
Warning: @export [generics.R#1367]: unknown tag
Warning: @export [generics.R#1372]: unknown tag
Warning: @export [generics.R#1377]: unknown tag
Warning: @export [generics.R#1382]: unknown tag
Warning: @export [generics.R#1387]: unknown tag
Warning: @export [generics.R#1392]: unknown tag
Warning: @export [generics.R#1397]: unknown tag
Warning: @export [generics.R#1402]: unknown tag
Warning: @export [generics.R#1407]: unknown tag
Warning: @export [generics.R#1412]: unknown tag
Warning: @export [generics.R#1417]: unknown tag
Warning: @export [generics.R#1422]: unknown tag
Warning: @export [generics.R#1427]: unknown tag
Warning: @export [generics.R#1432]: unknown tag
Warning: @export [generics.R#1437]: unknown tag
Warning: @export [generics.R#1442]: unknown tag
Warning: @export [generics.R#1447]: unknown tag
Warning: @export [generics.R#1452]: unknown tag
Warning: @export [generics.R#1457]: unknown tag
Warning: @export [generics.R#1462]: unknown tag
Warning: @export [generics.R#1467]: unknown tag
Warning: @export [generics.R#1472]: unknown tag
Warning: @export [generics.R#1477]: unknown tag
Warning: @export [generics.R#1482]: unknown tag
Warning: @export [generics.R#1487]: unknown tag
Warning: @export [generics.R#1492]: unknown tag
Warning: @export [generics.R#1497]: unknown tag
Warning: @export [generics.R#1502]: unknown tag
Warning: @export [generics.R#1507]: unknown tag
Warning: @export [generics.R#1512]: unknown tag
Warning: @export [generics.R#1517]: unknown tag
Warning: @export [generics.R#1522]: unknown tag
Warning: @export [generics.R#1527]: unknown tag
Warning: @export [generics.R#1532]: unknown tag
Warning: @export [generics.R#1537]: unknown tag
Warning: @export [generics.R#1542]: unknown tag
Warning: @export [generics.R#1547]: unknown tag
Warning: @export [generics.R#1552]: unknown tag
Warning: @export [generics.R#1557]: unknown tag
Warning: @export [generics.R#1562]: unknown tag
Warning: @export [generics.R#1570]: unknown tag
Warning: @export [generics.R#1578]: unknown tag
Warning: @export [generics.R#1584]: unknown tag
Warning: @export [generics.R#1588]: unknown tag
Warning: @export [generics.R#1592]: unknown tag
Warning: @export [generics.R#1596]: unknown tag
Warning: @export [generics.R#1601]: unknown tag
Warning: @export [generics.R#1606]: unknown tag
Warning: @export [generics.R#1610]: unknown tag
Warning: @export [generics.R#1614]: unknown tag
Warning: @export [generics.R#1618]: unknown tag
Warning: @export [generics.R#1622]: unknown tag
Warning: @export [generics.R#1626]: unknown tag
Warning: @export [generics.R#1630]: unknown tag
Warning: @export [generics.R#1634]: unknown tag
Warning: @export [generics.R#1638]: unknown tag
Warning: @export [generics.R#1642]: unknown tag
Warning: @export [generics.R#1647]: unknown tag
Warning: @export [generics.R#1652]: unknown tag
Warning: @export [generics.R#1656]: unknown tag
Warning: @export [generics.R#1660]: unknown tag
Warning: @export [generics.R#1664]: unknown tag
Warning: @export [generics.R#1668]: unknown tag
Warning: @export [generics.R#1672]: unknown tag
Warning: @export [generics.R#1676]: unknown tag
Warning: @export [generics.R#1683]: unknown tag
Warning: @export [generics.R#1690]: unknown tag
Warning: @export [generics.R#1694]: unknown tag
Warning: @export [generics.R#1698]: unknown tag
Warning: @export [generics.R#1702]: unknown tag
Warning: @export [generics.R#1706]: unknown tag
Warning: @export [generics.R#1710]: unknown tag
Warning: @export [column.R#33]: unknown tag
Warning: @export [column.R#60]: unknown tag
Warning: @export [column.R#138]: unknown tag
Warning: @export [column.R#274]: unknown tag
Warning: @export [column.R#300]: unknown tag
Warning: @export [column.R#322]: unknown tag
Warning: @export [column.R#352]: unknown tag
Warning: @export [group.R#34]: unknown tag
Warning: @export [group.R#52]: unknown tag
Warning: @export [group.R#67]: unknown tag
Warning: @export [group.R#91]: unknown tag
Warning: @export [group.R#154]: unknown tag
Warning: @export [group.R#206]: unknown tag
Warning: @export [group.R#220]: unknown tag
Warning: @export [DataFrame.R#40]: unknown tag
Warning: @export [DataFrame.R#72]: unknown tag
Warning: @export [DataFrame.R#92]: unknown tag
Warning: @export [DataFrame.R#118]: unknown tag
Warning: @export [DataFrame.R#141]: unknown tag
Warning: @export [DataFrame.R#173]: unknown tag
Warning: @export [DataFrame.R#204]: unknown tag
Warning: @export [DataFrame.R#236]: unknown tag
Warning: @export [DataFrame.R#264]: unknown tag
Warning: @export [DataFrame.R#291]: unknown tag
Warning: @export [DataFrame.R#383]: unknown tag
Warning: @export [DataFrame.R#440]: unknown tag
Warning: @export [DataFrame.R#489]: unknown tag
Warning: @export [DataFrame.R#516]: unknown tag
Warning: @export [DataFrame.R#547]: unknown tag
Warning: @export [DataFrame.R#576]: unknown tag
Warning: @export [DataFrame.R#607]: unknown tag
Warning: @export [DataFrame.R#637]: unknown tag
Warning: @export [DataFrame.R#665]: unknown tag
Warning: @export [DataFrame.R#703]: unknown tag
Warning: @export [DataFrame.R#740]: unknown tag
Warning: @export [DataFrame.R#789]: unknown tag
Warning: @export [DataFrame.R#821]: unknown tag
Warning: @export [DataFrame.R#852]: unknown tag
Warning: @export [DataFrame.R#883]: unknown tag
Warning: @export [DataFrame.R#904]: unknown tag
Warning: @export [DataFrame.R#928]: unknown tag
Warning: @export [DataFrame.R#955]: unknown tag
Warning: @export [DataFrame.R#996]: unknown tag
Warning: @export [DataFrame.R#1053]: unknown tag
Warning: @export [DataFrame.R#1086]: unknown tag
Warning: @export [DataFrame.R#1110]: unknown tag
Warning: @export [DataFrame.R#1136]: unknown tag
Warning: @export [DataFrame.R#1221]: unknown tag
Warning: @export [DataFrame.R#1245]: unknown tag
Warning: @export [DataFrame.R#1274]: unknown tag
Warning: @export [DataFrame.R#1299]: unknown tag
Warning: @export [DataFrame.R#1351]: unknown tag
Warning: @export [DataFrame.R#1393]: unknown tag
Warning: @export [DataFrame.R#1452]: unknown tag
Warning: @export [DataFrame.R#1511]: unknown tag
Warning: @export [DataFrame.R#1568]: unknown tag
Warning: @export [DataFrame.R#1665]: unknown tag
Warning: @export [DataFrame.R#1939]: unknown tag
Warning: @export [DataFrame.R#1984]: unknown tag
Warning: @export [DataFrame.R#2016]: unknown tag
Warning: @export [DataFrame.R#2029]: unknown tag
Warning: @export [DataFrame.R#2058]: unknown tag
Warning: @export [DataFrame.R#2089]: unknown tag
Warning: @export [DataFrame.R#2128]: unknown tag
Warning: @export [DataFrame.R#2199]: unknown tag
Warning: @export [DataFrame.R#2223]: unknown tag
Warning: @export [DataFrame.R#2249]: unknown tag
Warning: @export [DataFrame.R#2294]: unknown tag
Warning: @export [DataFrame.R#2320]: unknown tag
Warning: @export [DataFrame.R#2353]: unknown tag
Warning: @export [DataFrame.R#2374]: unknown tag
Warning: @export [DataFrame.R#2417]: unknown tag
Warning: @export [DataFrame.R#2466]: unknown tag
Warning: @export [DataFrame.R#2518]: unknown tag
Warning: @export [DataFrame.R#2566]: unknown tag
Warning: @export [DataFrame.R#2706]: unknown tag
Warning: @export [DataFrame.R#2727]: unknown tag
Warning: @export [DataFrame.R#2754]: unknown tag
Warning: @export [DataFrame.R#2787]: unknown tag
Warning: @export [DataFrame.R#2820]: unknown tag
Warning: @export [DataFrame.R#2848]: unknown tag
Warning: @export [DataFrame.R#2857]: unknown tag
Warning: @export [DataFrame.R#2893]: unknown tag
Warning: @export [DataFrame.R#2928]: unknown tag
Warning: @export [DataFrame.R#2961]: unknown tag
Warning: @export [DataFrame.R#2999]: unknown tag
Warning: @export [DataFrame.R#3055]: unknown tag
Warning: @export [DataFrame.R#3101]: unknown tag
Warning: @export [DataFrame.R#3132]: unknown tag
Warning: @export [DataFrame.R#3152]: unknown tag
Warning: @export [DataFrame.R#3380]: unknown tag
Warning: @export [DataFrame.R#3408]: unknown tag
Warning: @export [DataFrame.R#3427]: unknown tag
Warning: @export [DataFrame.R#3562]: unknown tag
Warning: @export [DataFrame.R#3592]: unknown tag
Warning: @export [DataFrame.R#3626]: unknown tag
Warning: @export [DataFrame.R#3652]: unknown tag
Warning: @export [DataFrame.R#3696]: unknown tag
Warning: @export [DataFrame.R#3756]: unknown tag
Warning: @export [DataFrame.R#3784]: unknown tag
Warning: @export [DataFrame.R#3819]: unknown tag
Warning: @export [DataFrame.R#3852]: unknown tag
Warning: @export [DataFrame.R#3876]: unknown tag
Warning: @export [DataFrame.R#3907]: unknown tag
Warning: @export [SQLContext.R#127]: unknown tag
Warning: @export [SQLContext.R#167]: unknown tag
Warning: @export [SQLContext.R#195]: unknown tag
Warning: @export [SQLContext.R#298]: unknown tag
Warning: @export [SQLContext.R#308]: unknown tag
Warning: @export [SQLContext.R#346]: unknown tag
Warning: @export [SQLContext.R#375]: unknown tag
Warning: @export [SQLContext.R#427]: unknown tag
Warning: @export [SQLContext.R#448]: unknown tag
Warning: @export [SQLContext.R#470]: unknown tag
Warning: @export [SQLContext.R#494]: unknown tag
Warning: @export [SQLContext.R#526]: unknown tag
Warning: @export [SQLContext.R#560]: unknown tag
Warning: @export [SQLContext.R#595]: unknown tag
Warning: @export [SQLContext.R#685]: unknown tag
Warning: @export [SQLContext.R#736]: unknown tag
Warning: @export [WindowSpec.R#32]: unknown tag
Warning: @export [WindowSpec.R#48]: unknown tag
Warning: @export [WindowSpec.R#67]: unknown tag
Warning: @export [WindowSpec.R#101]: unknown tag
Warning: @export [WindowSpec.R#117]: unknown tag
Warning: @export [WindowSpec.R#146]: unknown tag
Warning: @export [WindowSpec.R#178]: unknown tag
Warning: @export [WindowSpec.R#206]: unknown tag
Warning: @export [catalog.R#38]: unknown tag
Warning: @export [catalog.R#75]: unknown tag
Warning: @export [catalog.R#114]: unknown tag
Warning: @export [catalog.R#144]: unknown tag
Warning: @export [catalog.R#171]: unknown tag
Warning: @export [catalog.R#197]: unknown tag
Warning: @export [catalog.R#229]: unknown tag
Warning: @export [catalog.R#255]: unknown tag
Warning: @export [catalog.R#280]: unknown tag
Warning: @export [catalog.R#308]: unknown tag
Warning: @export [catalog.R#328]: unknown tag
Warning: @export [catalog.R#351]: unknown tag
Warning: @export [catalog.R#374]: unknown tag
Warning: @export [catalog.R#407]: unknown tag
Warning: @export [catalog.R#437]: unknown tag
Warning: @export [catalog.R#467]: unknown tag
Warning: @export [catalog.R#494]: unknown tag
Warning: @export [catalog.R#516]: unknown tag
Warning: @export [context.R#312]: unknown tag
Warning: @export [context.R#327]: unknown tag
Warning: @export [context.R#348]: unknown tag
Warning: @export [context.R#395]: unknown tag
Warning: @export [context.R#416]: unknown tag
Warning: @export [context.R#435]: unknown tag
Warning: @export [functions.R#240]: unknown tag
Warning: @export [functions.R#263]: unknown tag
Warning: @export [functions.R#278]: unknown tag
Warning: @export [functions.R#292]: unknown tag
Warning: @export [functions.R#315]: unknown tag
Warning: @export [functions.R#334]: unknown tag
Warning: @export [functions.R#349]: unknown tag
Warning: @export [functions.R#366]: unknown tag
Warning: @export [functions.R#382]: unknown tag
Warning: @export [functions.R#406]: unknown tag
Warning: @export [functions.R#420]: unknown tag
Warning: @export [functions.R#438]: unknown tag
Warning: @export [functions.R#452]: unknown tag
Warning: @export [functions.R#467]: unknown tag
Warning: @export [functions.R#479]: unknown tag
Warning: @export [functions.R#510]: unknown tag
Warning: @export [functions.R#529]: unknown tag
Warning: @export [functions.R#553]: unknown tag
Warning: @export [functions.R#594]: unknown tag
Warning: @export [functions.R#613]: unknown tag
Warning: @export [functions.R#627]: unknown tag
Warning: @export [functions.R#645]: unknown tag
Warning: @export [functions.R#661]: unknown tag
Warning: @export [functions.R#676]: unknown tag
Warning: @export [functions.R#695]: unknown tag
Warning: @export [functions.R#717]: unknown tag
Warning: @export [functions.R#735]: unknown tag
Warning: @export [functions.R#750]: unknown tag
Warning: @export [functions.R#764]: unknown tag
Warning: @export [functions.R#778]: unknown tag
Warning: @export [functions.R#792]: unknown tag
Warning: @export [functions.R#815]: unknown tag
Warning: @export [functions.R#839]: unknown tag
Warning: @export [functions.R#853]: unknown tag
Warning: @export [functions.R#867]: unknown tag
Warning: @export [functions.R#890]: unknown tag
Warning: @export [functions.R#925]: unknown tag
Warning: @export [functions.R#938]: unknown tag
Warning: @export [functions.R#967]: unknown tag
Warning: @export [functions.R#993]: unknown tag
Warning: @export [functions.R#1011]: unknown tag
Warning: @export [functions.R#1025]: unknown tag
Warning: @export [functions.R#1039]: unknown tag
Warning: @export [functions.R#1053]: unknown tag
Warning: @export [functions.R#1067]: unknown tag
Warning: @export [functions.R#1081]: unknown tag
Warning: @export [functions.R#1095]: unknown tag
Warning: @export [functions.R#1135]: unknown tag
Warning: @export [functions.R#1149]: unknown tag
Warning: @export [functions.R#1175]: unknown tag
Warning: @export [functions.R#1189]: unknown tag
Warning: @export [functions.R#1212]: unknown tag
Warning: @export [functions.R#1228]: unknown tag
Warning: @export [functions.R#1242]: unknown tag
Warning: @export [functions.R#1256]: unknown tag
Warning: @export [functions.R#1270]: unknown tag
Warning: @export [functions.R#1285]: unknown tag
Warning: @export [functions.R#1300]: unknown tag
Warning: @export [functions.R#1320]: unknown tag
Warning: @export [functions.R#1334]: unknown tag
Warning: @export [functions.R#1348]: unknown tag
Warning: @export [functions.R#1366]: unknown tag
Warning: @export [functions.R#1381]: unknown tag
Warning: @export [functions.R#1395]: unknown tag
Warning: @export [functions.R#1409]: unknown tag
Warning: @export [functions.R#1421]: unknown tag
Warning: @export [functions.R#1435]: unknown tag
Warning: @export [functions.R#1449]: unknown tag
Warning: @export [functions.R#1463]: unknown tag
Warning: @export [functions.R#1480]: unknown tag
Warning: @export [functions.R#1510]: unknown tag
Warning: @export [functions.R#1524]: unknown tag
Warning: @export [functions.R#1538]: unknown tag
Warning: @export [functions.R#1564]: unknown tag
Warning: @export [functions.R#1578]: unknown tag
Warning: @export [functions.R#1592]: unknown tag
Warning: @export [functions.R#1611]: unknown tag
Warning: @export [functions.R#1625]: unknown tag
Warning: @export [functions.R#1640]: unknown tag
Warning: @export [functions.R#1655]: unknown tag
Warning: @export [functions.R#1675]: unknown tag
Warning: @export [functions.R#1696]: unknown tag
Warning: @export [functions.R#1712]: unknown tag
Warning: @export [functions.R#1750]: unknown tag
Warning: @export [functions.R#1761]: unknown tag
Warning: @export [functions.R#1775]: unknown tag
Warning: @export [functions.R#1790]: unknown tag
Warning: @export [functions.R#1805]: unknown tag
Warning: @export [functions.R#1819]: unknown tag
Warning: @export [functions.R#1833]: unknown tag
Warning: @export [functions.R#1848]: unknown tag
Warning: @export [functions.R#1862]: unknown tag
Warning: @export [functions.R#1876]: unknown tag
Warning: @export [functions.R#1890]: unknown tag
Warning: @export [functions.R#1904]: unknown tag
Warning: @export [functions.R#1919]: unknown tag
Warning: @export [functions.R#1935]: unknown tag
Warning: @export [functions.R#1959]: unknown tag
Warning: @export [functions.R#1975]: unknown tag
Warning: @export [functions.R#1998]: unknown tag
Warning: @export [functions.R#2016]: unknown tag
Warning: @export [functions.R#2033]: unknown tag
Warning: @export [functions.R#2048]: unknown tag
Warning: @export [functions.R#2062]: unknown tag
Warning: @export [functions.R#2081]: unknown tag
Warning: @export [functions.R#2110]: unknown tag
Warning: @export [functions.R#2130]: unknown tag
Warning: @export [functions.R#2149]: unknown tag
Warning: @export [functions.R#2159]: unknown tag
Warning: @export [functions.R#2178]: unknown tag
Warning: @export [functions.R#2196]: unknown tag
Warning: @export [functions.R#2239]: unknown tag
Warning: @export [functions.R#2261]: unknown tag
Warning: @export [functions.R#2284]: unknown tag
Warning: @export [functions.R#2299]: unknown tag
Warning: @export [functions.R#2312]: unknown tag
Warning: @export [functions.R#2333]: unknown tag
Warning: @export [functions.R#2347]: unknown tag
Warning: @export [functions.R#2364]: unknown tag
Warning: @export [functions.R#2387]: unknown tag
Warning: @export [functions.R#2401]: unknown tag
Warning: @export [functions.R#2417]: unknown tag
Warning: @export [functions.R#2433]: unknown tag
Warning: @export [functions.R#2450]: unknown tag
Warning: @export [functions.R#2466]: unknown tag
Warning: @export [functions.R#2484]: unknown tag
Warning: @export [functions.R#2499]: unknown tag
Warning: @export [functions.R#2520]: unknown tag
Warning: @export [functions.R#2562]: unknown tag
Warning: @export [functions.R#2613]: unknown tag
Warning: @export [functions.R#2630]: unknown tag
Warning: @export [functions.R#2647]: unknown tag
Warning: @export [functions.R#2662]: unknown tag
Warning: @export [functions.R#2676]: unknown tag
Warning: @export [functions.R#2686]: unknown tag
Warning: @export [functions.R#2703]: unknown tag
Warning: @export [functions.R#2732]: unknown tag
Warning: @export [functions.R#2748]: unknown tag
Warning: @export [functions.R#2771]: unknown tag
Warning: @export [functions.R#2794]: unknown tag
Warning: @export [functions.R#2809]: unknown tag
Warning: @export [functions.R#2819]: unknown tag
Warning: @export [functions.R#2829]: unknown tag
Warning: @export [functions.R#2845]: unknown tag
Warning: @export [functions.R#2874]: unknown tag
Warning: @export [functions.R#2900]: unknown tag
Warning: @export [functions.R#2921]: unknown tag
Warning: @export [functions.R#2938]: unknown tag
Warning: @export [functions.R#2963]: unknown tag
Warning: @export [functions.R#2987]: unknown tag
Warning: @export [functions.R#3005]: unknown tag
Warning: @export [functions.R#3026]: unknown tag
Warning: @export [functions.R#3037]: unknown tag
Warning: @export [functions.R#3051]: unknown tag
Warning: @export [functions.R#3069]: unknown tag
Warning: @export [functions.R#3083]: unknown tag
Warning: @export [functions.R#3097]: unknown tag
Warning: @export [functions.R#3111]: unknown tag
Warning: @export [functions.R#3125]: unknown tag
Warning: @export [functions.R#3143]: unknown tag
Warning: @export [functions.R#3158]: unknown tag
Warning: @export [functions.R#3173]: unknown tag
Warning: @export [functions.R#3194]: unknown tag
Warning: @export [functions.R#3212]: unknown tag
Warning: @export [functions.R#3232]: unknown tag
Warning: @export [functions.R#3247]: unknown tag
Warning: @export [functions.R#3270]: unknown tag
Warning: @export [functions.R#3293]: unknown tag
Warning: @export [functions.R#3318]: unknown tag
Warning: @export [functions.R#3339]: unknown tag
Warning: @export [functions.R#3367]: unknown tag
Warning: @export [functions.R#3400]: unknown tag
Warning: @export [functions.R#3435]: unknown tag
Warning: @export [functions.R#3453]: unknown tag
Warning: @export [install.R#62]: unknown tag
Warning: @export [jvm.R#39]: unknown tag
Warning: @export [jvm.R#73]: unknown tag
Warning: @export [jvm.R#104]: unknown tag
Warning: @export [mllib_classification.R#25]: unknown tag
Warning: @export [mllib_classification.R#32]: unknown tag
Warning: @export [mllib_classification.R#39]: unknown tag
Warning: @export [mllib_classification.R#46]: unknown tag
Warning: @export [mllib_classification.R#86]: unknown tag
Warning: @export [mllib_classification.R#135]: unknown tag
Warning: @export [mllib_classification.R#150]: unknown tag
Warning: @export [mllib_classification.R#173]: unknown tag
Warning: @export [mllib_classification.R#261]: unknown tag
Warning: @export [mllib_classification.R#365]: unknown tag
Warning: @export [mllib_classification.R#393]: unknown tag
Warning: @export [mllib_classification.R#408]: unknown tag
Warning: @export [mllib_classification.R#449]: unknown tag
Warning: @export [mllib_classification.R#508]: unknown tag
Warning: @export [mllib_classification.R#529]: unknown tag
Warning: @export [mllib_classification.R#544]: unknown tag
Warning: @export [mllib_classification.R#576]: unknown tag
Warning: @export [mllib_classification.R#615]: unknown tag
Warning: @export [mllib_classification.R#639]: unknown tag
Warning: @export [mllib_classification.R#653]: unknown tag
Warning: @export [mllib_clustering.R#24]: unknown tag
Warning: @export [mllib_clustering.R#31]: unknown tag
Warning: @export [mllib_clustering.R#38]: unknown tag
Warning: @export [mllib_clustering.R#45]: unknown tag
Warning: @export [mllib_clustering.R#72]: unknown tag
Warning: @export [mllib_clustering.R#121]: unknown tag
Warning: @export [mllib_clustering.R#148]: unknown tag
Warning: @export [mllib_clustering.R#164]: unknown tag
Warning: @export [mllib_clustering.R#185]: unknown tag
Warning: @export [mllib_clustering.R#212]: unknown tag
Warning: @export [mllib_clustering.R#255]: unknown tag
Warning: @export [mllib_clustering.R#295]: unknown tag
Warning: @export [mllib_clustering.R#310]: unknown tag
Warning: @export [mllib_clustering.R#340]: unknown tag
Warning: @export [mllib_clustering.R#389]: unknown tag
Warning: @export [mllib_clustering.R#417]: unknown tag
Warning: @export [mllib_clustering.R#435]: unknown tag
Warning: @export [mllib_clustering.R#462]: unknown tag
Warning: @export [mllib_clustering.R#500]: unknown tag
Warning: @export [mllib_clustering.R#562]: unknown tag
Warning: @export [mllib_clustering.R#600]: unknown tag
Warning: @export [mllib_clustering.R#615]: unknown tag
Warning: @export [mllib_clustering.R#630]: unknown tag
Warning: @export [mllib_fpm.R#24]: unknown tag
Warning: @export [mllib_fpm.R#49]: unknown tag
Warning: @export [mllib_fpm.R#113]: unknown tag
Warning: @export [mllib_fpm.R#129]: unknown tag
Warning: @export [mllib_fpm.R#142]: unknown tag
Warning: @export [mllib_fpm.R#157]: unknown tag
Warning: @export [mllib_recommendation.R#24]: unknown tag
Warning: @export [mllib_recommendation.R#57]: unknown tag
Warning: @export [mllib_recommendation.R#120]: unknown tag
Warning: @export [mllib_recommendation.R#141]: unknown tag
Warning: @export [mllib_recommendation.R#157]: unknown tag
Warning: @export [mllib_regression.R#25]: unknown tag
Warning: @export [mllib_regression.R#32]: unknown tag
Warning: @export [mllib_regression.R#39]: unknown tag
Warning: @export [mllib_regression.R#89]: unknown tag
Warning: @export [mllib_regression.R#214]: unknown tag
Warning: @export [mllib_regression.R#247]: unknown tag
Warning: @export [mllib_regression.R#293]: unknown tag
Warning: @export [mllib_regression.R#327]: unknown tag
Warning: @export [mllib_regression.R#341]: unknown tag
Warning: @export [mllib_regression.R#366]: unknown tag
Warning: @export [mllib_regression.R#415]: unknown tag
Warning: @export [mllib_regression.R#432]: unknown tag
Warning: @export [mllib_regression.R#447]: unknown tag
Warning: @export [mllib_regression.R#480]: unknown tag
Warning: @export [mllib_regression.R#520]: unknown tag
Warning: @export [mllib_regression.R#540]: unknown tag
Warning: @export [mllib_regression.R#553]: unknown tag
Warning: @export [mllib_stat.R#24]: unknown tag
Warning: @export [mllib_stat.R#56]: unknown tag
Warning: @export [mllib_stat.R#98]: unknown tag
Warning: @export [mllib_stat.R#121]: unknown tag
Warning: @export [mllib_tree.R#24]: unknown tag
Warning: @export [mllib_tree.R#31]: unknown tag
Warning: @export [mllib_tree.R#38]: unknown tag
Warning: @export [mllib_tree.R#45]: unknown tag
Warning: @export [mllib_tree.R#52]: unknown tag
Warning: @export [mllib_tree.R#59]: unknown tag
Warning: @export [mllib_tree.R#181]: unknown tag
Warning: @export [mllib_tree.R#263]: unknown tag
Warning: @export [mllib_tree.R#277]: unknown tag
Warning: @export [mllib_tree.R#287]: unknown tag
Warning: @export [mllib_tree.R#299]: unknown tag
Warning: @export [mllib_tree.R#312]: unknown tag
Warning: @export [mllib_tree.R#321]: unknown tag
Warning: @export [mllib_tree.R#336]: unknown tag
Warning: @export [mllib_tree.R#345]: unknown tag
Warning: @export [mllib_tree.R#402]: unknown tag
Warning: @export [mllib_tree.R#480]: unknown tag
Warning: @export [mllib_tree.R#494]: unknown tag
Warning: @export [mllib_tree.R#504]: unknown tag
Warning: @export [mllib_tree.R#516]: unknown tag
Warning: @export [mllib_tree.R#529]: unknown tag
Warning: @export [mllib_tree.R#538]: unknown tag
Warning: @export [mllib_tree.R#554]: unknown tag
Warning: @export [mllib_tree.R#563]: unknown tag
Warning: @export [mllib_tree.R#615]: unknown tag
Warning: @export [mllib_tree.R#688]: unknown tag
Warning: @export [mllib_tree.R#702]: unknown tag
Warning: @export [mllib_tree.R#712]: unknown tag
Warning: @export [mllib_tree.R#724]: unknown tag
Warning: @export [mllib_tree.R#737]: unknown tag
Warning: @export [mllib_tree.R#746]: unknown tag
Warning: @export [mllib_tree.R#762]: unknown tag
Warning: @export [mllib_tree.R#771]: unknown tag
Warning: @export [mllib_utils.R#35]: unknown tag
Warning: @export [mllib_utils.R#52]: unknown tag
Warning: @export [mllib_utils.R#79]: unknown tag
Warning: @export [sparkR.R#39]: unknown tag
Warning: @export [sparkR.R#88]: unknown tag
Warning: @export [sparkR.R#107]: unknown tag
Warning: @export [sparkR.R#274]: unknown tag
Warning: @export [sparkR.R#302]: unknown tag
Warning: @export [sparkR.R#351]: unknown tag
Warning: @export [sparkR.R#434]: unknown tag
Warning: @export [stats.R#41]: unknown tag
Warning: @export [stats.R#67]: unknown tag
Warning: @export [stats.R#96]: unknown tag
Warning: @export [stats.R#128]: unknown tag
Warning: @export [stats.R#172]: unknown tag
Warning: @export [stats.R#209]: unknown tag
Warning: @export [streaming.R#32]: unknown tag
Warning: @export [streaming.R#49]: unknown tag
Warning: @export [streaming.R#74]: unknown tag
Warning: @export [streaming.R#89]: unknown tag
Warning: @export [streaming.R#108]: unknown tag
Warning: @export [streaming.R#133]: unknown tag
Warning: @export [streaming.R#154]: unknown tag
Warning: @export [streaming.R#181]: unknown tag
Warning: @export [streaming.R#206]: unknown tag
Warning: @export [utils.R#112]: unknown tag
Warning: @export [window.R#33]: unknown tag
Warning: @export [window.R#56]: unknown tag
Warning: @export [window.R#82]: unknown tag
Warning: @export [window.R#105]: unknown tag
{noformat}
This is because Roxygen 6 now only loads tags for the roclets it is using, and the rd roclet does nothing with the export tag. This is described here https://github.com/klutometis/roxygen/issues/571. This is an incredibly annoying change to Roxygen, but if we don't generate namespace files, then there is no need for the extra export tags in our R sources.",Roxygen 6.0.1,apachespark,felixcheung,TV4Fun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 04 03:46:04 UTC 2018,,,,,,,,,,"0|i3mbzz:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,,,,,,,"03/Nov/17 08:28;srowen;I see it too, because I have Roxygen 6.0.1 locally. [~felixcheung] have you seen this? I wonder if we could all use 6.0.1, but that's separate.
Do you know of a fix?;;;","03/Nov/17 15:13;felixcheung;I am seeing it too. I think we can just remove the tag but Jenkins is running an older version of roxygen2 and our earlier attempt to update it didn't go well (we have a JIRA on this). Will need to be very careful with this.

;;;","04/Feb/18 03:46;apachespark;User 'rekhajoshm' has created a pull request for this issue:
https://github.com/apache/spark/pull/20501;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming checkpointing code does not retry after failure due to NullPointerException,SPARK-22429,13115672,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tmgstev,tmgstev,tmgstev,02/Nov/17 16:47,05/Nov/17 09:12,14/Jul/23 06:30,05/Nov/17 09:11,1.6.3,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,DStreams,,,,,0,,,,,,,,,"CheckpointWriteHandler has a built in retry mechanism. However SPARK-14930/SPARK-13693 put in a fix to de-allocate the fs object, yet initialises it in the wrong place for the while loop, and therefore on attempt 2 it fails with a NPE.",,apachespark,tmgstev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 05 09:11:22 UTC 2017,,,,,,,,,,"0|i3mbj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/17 17:54;srowen;Sounds straightforward -- feel free to open a pull request.;;;","02/Nov/17 19:59;apachespark;User 'tmgstevens' has created a pull request for this issue:
https://github.com/apache/spark/pull/19645;;;","02/Nov/17 20:00;tmgstev;[~srowen] I've raised a PR against branch-2.2. master would not compile for me (before I made changes), but the patch should apply cleanly on there too.;;;","03/Nov/17 08:44;srowen;[~tmgstev] it will have to be vs. master. Master and all branches seem fine though: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/ What do you see?;;;","05/Nov/17 09:11;srowen;Issue resolved by pull request 19645
[https://github.com/apache/spark/pull/19645];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
createDataFrame from a pandas.DataFrame reads datetime64 values as longs,SPARK-22417,13115492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,01/Nov/17 22:41,09/Nov/17 04:53,14/Jul/23 06:30,07/Nov/17 20:32,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,PySpark,,,,,0,,,,,,,,,"When trying to create a Spark DataFrame from an existing Pandas DataFrame using {{createDataFrame}}, columns with datetime64 values are converted as long values.  This is only when the schema is not specified.  

{code}
In [2]: import pandas as pd
   ...: from datetime import datetime
   ...: 

In [3]: pdf = pd.DataFrame({""ts"": [datetime(2017, 10, 31, 1, 1, 1)]})

In [4]: df = spark.createDataFrame(pdf)

In [5]: df.show()
+-------------------+
|                 ts|
+-------------------+
|1509411661000000000|
+-------------------+


In [6]: df.schema
Out[6]: StructType(List(StructField(ts,LongType,true)))
{code}

Spark should interpret a datetime64\[D\] value to DateType and other datetime64 values to TImestampType.",,apachespark,bryanc,cloud_fan,kiszk,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20791,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 04:53:14 UTC 2017,,,,,,,,,,"0|i3mafb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/17 15:32;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/19646;;;","07/Nov/17 20:32;cloud_fan;Issue resolved by pull request 19646
[https://github.com/apache/spark/pull/19646];;;","09/Nov/17 04:53;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19704;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive spill for Pyspark UDF when a row has shrunk,SPARK-22410,13115420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,cstenac,cstenac,01/Nov/17 17:44,04/Nov/17 12:11,14/Jul/23 06:30,04/Nov/17 12:11,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"Hi,

The following code processes 900KB of data and outputs around 2MB of data. However, to process it, Spark needs to spill roughly 12 GB of data.

{code:python}
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

ss = SparkSession.builder.getOrCreate()

# Create a few lines of data (5 lines).
# Each line is made of a string, and an array of 10000 strings
# Total size of data is around 900 KB

lines_of_file = [ ""this is a line"" for x in xrange(10000) ]
file_obj = [ ""this_is_a_foldername/this_is_a_filename"", lines_of_file ]
data = [ file_obj for x in xrange(5) ]

# Make a two-columns dataframe out of it
small_df = ss.sparkContext.parallelize(data).map(lambda x : (x[0], x[1])).toDF([""file"", ""lines""])

# We then explode the array, so we now have 50000 rows in the dataframe, with 2 columns, the 2nd 
# column now has only ""this is a line"" as content
exploded = small_df.select(""file"", explode(""lines""))

print(""Exploded"")
print(exploded.explain())

# Now, just process it with a trivial Pyspark UDF that touches the first column
# (the one which was not an array)

def split_key(s):
    return s.split(""/"")[1]
split_key_udf = udf(split_key, StringType())

with_filename = exploded.withColumn(""filename"", split_key_udf(""file""))

# As expected, explain plan is very simple (BatchEval -> Explode -> Project -> ScanExisting)
print(with_filename.explain())

# Getting the head will spill around 12 GB of data
print(with_filename.head())
{code}

The spill happens in the HybridRowQueue that is used to merge the part that went through the Python worker and the part that didn't.

The problem comes from the fact that when it is added to the HybridRowQueue, the UnsafeRow has a totalSizeInBytes of ~240000 (seen by adding debug message in HybridRowQueue), whereas, since it's after the explode, the actual size of the row should be in the ~60 bytes range.

My understanding is that the row has retained the size it consumed *prior* to the explode (at that time, the size of each of the 5 rows was indeed ~240000 bytes.

A workaround is to do exploded.cache() before calling the UDF. The fact of going through the InMemoryColumnarTableScan ""resets"" the wrongful size of the UnsafeRow.

Thanks!",Reproduced on up-to-date master,apachespark,cloud_fan,codingcat,cstenac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 04 12:11:35 UTC 2017,,,,,,,,,,"0|i3m9zb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/17 10:49;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19642;;;","04/Nov/17 12:11;cloud_fan;Issue resolved by pull request 19642
[https://github.com/apache/spark/pull/19642];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark version tag is wrong on PyPi,SPARK-22406,13115289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holden,kerrick-lyft,kerrick-lyft,01/Nov/17 02:04,06/Jan/18 00:47,14/Jul/23 06:30,06/Jan/18 00:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,,,,PySpark,,,,,0,,,,,,,,,"On pypi.python.org, the pyspark package is tagged with version {{2.2.0.post0}}: https://pypi.python.org/pypi/pyspark/2.2.0

However, when you install the package, it has version {{2.2.0}}.

This has really annoying consequences: if you try {{pip install pyspark==2.2.0}}, it won't work. Instead you have to do {{pip install pyspark==2.2.0.post0}}. Then, if you later run the same command ({{pip install pyspark==2.2.0.post0}}), it won't recognize the existing pyspark installation (because it has version {{2.2.0}}) and instead will reinstall it, which is very slow because pyspark is a large package.

This can happen if you add a new package to a {{requirements.txt}} file; you end up waiting a lot longer than necessary because every time you run {{pip install -r requirements.txt}} it reinstalls pyspark.

Can you please change the package on PyPi to have the version {{2.2.0}}?",,felixcheung,holden,holden.karau@gmail.com,kerrick-lyft,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 06 00:47:22 UTC 2018,,,,,,,,,,"0|i3m967:",9223372036854775807,,,,,holden.karau@gmail.com,,,,,,,,2.2.1,,,,,,,,,,,"01/Nov/17 06:26;viirya;cc [~holdenk];;;","05/Nov/17 07:50;holden;Due to restrictions on PyPI, no. We can try and fix this in 2.2.1 however.;;;","11/Nov/17 06:39;felixcheung;is this still being targeted for 2.2.1?;;;","11/Nov/17 09:13;holden.karau@gmail.com;Yes, although this should be fixed in the documented upload process, it
just has to be run at the end of the release to be verified closed.

On Fri, Nov 10, 2017 at 10:40 PM Felix Cheung (JIRA) <jira@apache.org>

-- 
Cell : 425-233-8271
;;;","05/Jan/18 23:15;srowen;[~holden.karau@gmail.com] [~felixcheung] is this bit actually done now?;;;","06/Jan/18 00:46;holden;Yes. I'll close this.;;;","06/Jan/18 00:47;holden;We've posted 2.2.1 & 2.1.2 to PyPi with out the post issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StructuredKafkaWordCount example fails in YARN cluster mode,SPARK-22403,13113180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wypoon,wypoon,wypoon,30/Oct/17 22:55,10/Nov/17 00:22,14/Jul/23 06:30,10/Nov/17 00:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"When I run the StructuredKafkaWordCount example in YARN client mode, it runs fine. However, when I run it in YARN cluster mode, the application errors during initialization, and dies after the default number of YARN application attempts. In the AM log, I see
{noformat}
17/10/30 11:34:52 INFO execution.SparkSqlParser: Parsing command: CAST(value AS STRING)
17/10/30 11:34:53 ERROR streaming.StreamMetadata: Error writing stream metadata StreamMetadata(b71ca714-a7a1-467f-96aa-023375964429) to /data/yarn/nm/usercache/systest/appcache/application_1508800814252_0047/container_1508800814252_0047_01_000001/tmp/temporary-b5ced4ae-32e0-4725-b905-aad679aec9b5/metadata
org.apache.hadoop.security.AccessControlException: Permission denied: user=systest, access=WRITE, inode=""/"":hdfs:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:397)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:256)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:194)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1842)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1826)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1785)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:315)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2313)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2257)
...
        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1235)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1214)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1152)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:458)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:455)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:469)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:396)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1103)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1083)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:972)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:960)
	at org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:76)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$6.apply(StreamExecution.scala:116)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$6.apply(StreamExecution.scala:114)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:114)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:240)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:282)
	at org.apache.spark.examples.sql.streaming.StructuredKafkaWordCount$.main(StructuredKafkaWordCount.scala:79)
	at org.apache.spark.examples.sql.streaming.StructuredKafkaWordCount.main(StructuredKafkaWordCount.scala)
{noformat}
Looking at StreamingQueryManager#createQuery, we have
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L198
{code}
    val checkpointLocation = userSpecifiedCheckpointLocation.map { ...
      ...
    }.orElse {
      ...
    }.getOrElse {
      if (useTempCheckpointLocation) {
        // Delete the temp checkpoint when a query is being stopped without errors.
        deleteCheckpointOnStop = true
        Utils.createTempDir(namePrefix = s""temporary"").getCanonicalPath
      } else {
        ...
      }
    }
{code}
And Utils.createTempDir has
{code}
  def createTempDir(
      root: String = System.getProperty(""java.io.tmpdir""),
      namePrefix: String = ""spark""): File = {
    val dir = createDirectory(root, namePrefix)
    ShutdownHookManager.registerShutdownDeleteDir(dir)
    dir
  }
{code}
In client mode, java.io.tmpdir is set to ""/tmp"", which also exists in HDFS and has permissions 1777. In cluster mode, java.io.tmpdir is set in the YARN AM to ""$PWD/tmp"", where PWD is ""$\{yarn.nodemanager.local-dirs\}/usercache/$\{user\}/appcache/application_$\{appid\}/container_$\{contid\}"".
The problem is that Spark is using java.io.tmpdir, which is a path in the local filesystem, as a path in HDFS. When that path is ""/tmp"", which happens to exist in HDFS, no problem arises, but that is just by coincidence.",,apachespark,wypoon,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 04:33:03 UTC 2017,,,,,,,,,,"0|i3lw5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/17 23:01;wypoon;The simplest change that will solve the problem in this particular scenario, is to change the Utils.createTempDir(namePrefix = s""temporary"") call in https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L208 to Utils.createTempDir(root = ""/tmp"", namePrefix = ""temporary"").
In my view, using ""/tmp"" is not worse, in fact is better, than using System.getProperty(""java.io.tmpdir"").
However, others may know of better solutions.;;;","31/Oct/17 18:23;zsxwing;Yeah, Spark creates a temp directory for you. You can set ""checkpointLocation"" by yourself to avoid this issue. I don't know if there is an API to create a temp directory for all types of file systems.;;;","31/Oct/17 18:38;wypoon;I realize that in a production application, one would set checkpointLocation and avoid this issue. However, there is evidently a problem in the code that handles the case when checkpointLocation is not set and a temporary checkpoint location is created. Also, the StructuredKafkaWordCount example does not accept a parameter for setting the checkpointLocation.
;;;","31/Oct/17 18:52;zsxwing;Yeah, feel free to submit a PR to improve the example.;;;","09/Nov/17 04:33;apachespark;User 'wypoon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19703;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing 2.1.2 tag in git,SPARK-22401,13113121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holden,dreamy_chillwave,dreamy_chillwave,30/Oct/17 19:11,02/Nov/17 19:12,14/Jul/23 06:30,02/Nov/17 19:12,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,,,,,Build,Deploy,,,,0,,,,,,,,,"We only saw a 2.1.2-rc4 tag in git, no official release. The releases web page shows 2.1.2 was released in October 9.",,dreamy_chillwave,holdenkarau,parente,xynny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 19:11:03 UTC 2017,,,,,,,,,,"0|i3lvsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/17 03:23;xynny;[~holdenk] is this just a new process? ;;;","31/Oct/17 08:23;srowen;Normally the release plugin would tag and push the tag. I think it's easy enough to push a tag manually (should be commit 2abaea9e40fce81cd4626498e0f5c28a70917499 at https://github.com/apache/spark/tree/2abaea9e40fce81cd4626498e0f5c28a70917499 ) but will wait to see if anyone knows the right-er way to push this to both Apache and github.;;;","02/Nov/17 19:11;holdenkarau;Pushed, looking at the scripts they are all for tagging the RCs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unresolved operator InsertIntoDir for Hive format when Hive Support is not enabled,SPARK-22396,13112949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,30/Oct/17 06:26,30/Oct/17 17:20,14/Jul/23 06:30,30/Oct/17 17:20,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When Hive support is not on, users can hit unresolved plan node when trying to call `INSERT OVERWRITE DIRECTORY` using Hive format. 

```
""unresolved operator 'InsertIntoDir true, Storage(Location: /private/var/folders/vx/j0ydl5rn0gd9mgrh1pljnw900000gn/T/spark-b4227606-9311-46a8-8c02-56355bf0e2bc, Serde Library: org.apache.hadoop.hive.ql.io.orc.OrcSerde, InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat), hive, true;;

```",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 30 06:29:04 UTC 2017,,,,,,,,,,"0|i3luqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/17 06:29;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19608;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the behavior of timestamp values for Pandas to respect session timezone,SPARK-22395,13112939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,30/Oct/17 04:07,28/Nov/17 13:47,14/Jul/23 06:30,28/Nov/17 08:46,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,release-notes,,,,,,,,"When converting Pandas DataFrame/Series from/to Spark DataFrame using {{toPandas()}} or pandas udfs, timestamp values behave to respect Python system timezone instead of session timezone.


For example, let's say we use {{""America/Los_Angeles""}} as session timezone and have a timestamp value {{""1970-01-01 00:00:01""}} in the timezone. Btw, I'm in Japan so Python timezone would be {{""Asia/Tokyo""}}.

The timestamp value from current {{toPandas()}} will be the following:

{noformat}
>>> spark.conf.set(""spark.sql.session.timeZone"", ""America/Los_Angeles"")
>>> df = spark.createDataFrame([28801], ""long"").selectExpr(""timestamp(value) as ts"")
>>> df.show()
+-------------------+
|                 ts|
+-------------------+
|1970-01-01 00:00:01|
+-------------------+

>>> df.toPandas()
                   ts
0 1970-01-01 17:00:01
{noformat}

As you can see, the value becomes {{""1970-01-01 17:00:01""}} because it respects Python timezone.


As we discussed in https://github.com/apache/spark/pull/18664, we consider this behavior is a bug and the value should be {{""1970-01-01 00:00:01""}}.
",,apachespark,cloud_fan,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22632,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 28 08:46:55 UTC 2017,,,,,,,,,,"0|i3luon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/17 05:33;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19607;;;","06/Nov/17 14:53;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19674;;;","28/Nov/17 08:46;cloud_fan;Issue resolved by pull request 19607
[https://github.com/apache/spark/pull/19607];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"spark-shell can't find imported types in class constructors, extends clause",SPARK-22393,13112883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mpetruska,rdub,rdub,29/Oct/17 14:42,03/Apr/18 05:33,14/Jul/23 06:30,01/Dec/17 11:14,2.0.2,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Shell,,,,,0,,,,,,,,,"{code}
$ spark-shell
…
scala> import org.apache.spark.Partition
import org.apache.spark.Partition

scala> class P(p: Partition)
<console>:11: error: not found: type Partition
       class P(p: Partition)
                  ^

scala> class P(val index: Int) extends Partition
<console>:11: error: not found: type Partition
       class P(val index: Int) extends Partition
                                       ^
{code}

Any class that I {{import}} gives ""not found: type ___"" when used as a parameter to a class, or in an extends clause; this applies to classes I import from JARs I provide via {{--jars}} as well as core Spark classes as above.

This worked in 1.6.3 but has been broken since 2.0.0.",,apachespark,devaraj,jerryshao,jmchung,jpallas,mpetruska,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 03 05:33:55 UTC 2018,,,,,,,,,,"0|i3luc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/17 15:29;srowen;That's a weird one. {{class P(p: org.apache.spark.Partition)}} works fine as does {{ {import org.apache.spark.Partition; class P(p: Partition)} }}. I think this is some subtlety of how the scala shell interpreter works.;;;","29/Oct/17 19:46;rdub;Everything works fine in a Scala shell ({{scala -cp $SPARK_HOME/jars/spark-core_2.11-2.2.0.jar}}) and via {{sbt console}} in a project that depends on Spark, so the problem seems specific to {{spark-shell}}.;;;","29/Oct/17 20:34;srowen;I'm guessing it's something to do with how it overrides the shell initialization or classloader. It could be worth trying the 2.12 build and shell as the shell integration is a little less hacky. But really no idea off the top of my head.;;;","19/Nov/17 10:58;mpetruska;Tested with spark-shell build 2.11:

{code}
import org.apache.spark.Partition
{code}


|| Code || Result ||
| {code} class P(p: Partition) {code}  | {color:red} not found: type Partition {color} |
| {code} class P(x: Int) extends Partition {code} | {color:red} not found: type Partition {color} |
| {code} var p: Partition = _ {code}   | {color:green} OK {color} |
| {code} def a(p: Partition): Int = 0 {code} | {color:green} OK {color} |
| {code} def a(): Partition = p {code} | {color:green} OK {color} |
| {code}
object P {
  class P(p: Partition)
} {code} | {color:green} OK {color} |
| {code}
:paste
import org.apache.spark.Partition
class P(p: Partition)
{code} | {color:green} OK {color} |
| {code}
type Partition = org.apache.spark.Partition
class P(p: Partition)
{code} | {color:green} OK {color} |


To start spark shell and the underlying scala repl with _trace_, _info_ and _debug_ settings:
{code}
$JAVA_HOME/bin/java -cp $SPARK_HOME/conf/:$SPARK_HOME/assembly/target/scala-2.11/jars/* -Dscala.usejavacp=true -Dscala.repl.trace=true -Dscala.repl.info=true -Dscala.repl.debug=true -Dscala.repl.prompt=""spark>>>  "" -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name ""Spark shell"" spark-shell
{code};;;","19/Nov/17 12:28;mpetruska;With the 2.12 build:

{code}
import org.apache.spark.Partition
{code}


|| Code || Result ||
| {code} class P(p: Partition) {code}  | {color:green} OK {color} |
| {code} class P(val index: Int) extends Partition {code} | {color:green} OK {color} |
| {code} var p: Partition = _ {code}   | {color:green} OK {color} |
| {code} def a(p: Partition): Int = 0 {code} | {color:green} OK {color} |
| {code} def a(): Partition = p {code} | {color:green} OK {color} |
| {code}
object P {
  class P(p: Partition)
} {code} | {color:green} OK {color} |
| {code}
:paste
import org.apache.spark.Partition
class P(p: Partition)
{code} | {color:green} OK {color} |
| {code}
type Partition = org.apache.spark.Partition
class P(p: Partition)
{code} | {color:green} OK {color} |


To start spark shell 2.12 and the underlying scala repl with _trace_, _info_ and _debug_ settings:
{code}
$JAVA_HOME/bin/java -cp $SPARK_HOME/conf/:$SPARK_HOME/assembly/target/scala-2.11/jars/* -Dscala.usejavacp=true -Dscala.repl.trace=true -Dscala.repl.info=true -Dscala.repl.debug=true -Dscala.repl.prompt=""spark>>>  "" -Xmx1g org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --name ""Spark shell"" spark-shell
{code};;;","19/Nov/17 12:35;mpetruska;The difference between scala repls 2.11 and 2.12 is seen with _trace_ options enabled.
2.12:

{code}
...
parse(""

sealed class $read extends _root_.java.io.Serializable { 
  ;

sealed class $iw extends _root_.java.io.Serializable { 
// $line14.$read.INSTANCE definedNames List(), curImps Set()
import org.apache.spark.Partition
sealed class $iw extends _root_.java.io.Serializable { 

class P(p: Partition)


}
...
{code}

Whereas in 2.11, scala repl does not insert `import org.apache.spark.Partition`, but only for `ClassDef`s. The underlying reason is unknown, will need more investigation.

Until then the workaround `type Partition = org.apache.spark.Partition` seems feasible.;;;","19/Nov/17 12:54;mpetruska;Trace of the 2.11 version:

{code}
...
parse(""

class $read extends Serializable { 
  ;

class $iw extends Serializable { 
class $iw extends Serializable { 

          class P(p: Partition) 



}
...
{code}

Strangely the missing import and others are there if the class is wrapped inside an object:

{code}
...
parse(""

class $read extends Serializable { 
  ;

class $iw extends Serializable { 
val $line3$read = $line3.$read.INSTANCE
import $line3$read.$iw.$iw.`spark`;
class $iw extends Serializable { 
import org.apache.spark.SparkContext._
class $iw extends Serializable { 
class $iw extends Serializable { 
import spark.implicits._
class $iw extends Serializable { 
import spark.sql
class $iw extends Serializable { 
import org.apache.spark.sql.functions._
class $iw extends Serializable { 
import org.apache.spark.Partition
class $iw extends Serializable { 

          object P {
            class P(p: Partition)
          } 



}
...
{code};;;","23/Nov/17 10:13;mpetruska;The problem is also reproducible with non-spark related classes.
Example:

{code}
import java.util.UUID
class U(u: UUID)
...
error: not found: type UUID
...
{code};;;","24/Nov/17 16:02;mpetruska;After digging in deeper, I found that there is a high chance that this is the underlying problem: https://issues.scala-lang.org/browse/SI-9881;;;","24/Nov/17 16:14;mpetruska;Proof for the above statement:
Tracing the import handler generated by `import java.util.UUID` yields

{code}
MemberHandlers$ImportHandler (refs: java)
expr: java.util
selectors: List(ImportSelector(UUID,17,UUID,17))
targetType: <noprefix>
importsWildcard: false
implicitSymbols: List()
importedSymbols: List()
importString: import java.util.UUID
{code}

(also in `scala.tools.nsc.interpreter.MemberHandlers.ImportHandler` (excerpt):
{code}
def importedSymbols = individualSymbols ++ wildcardSymbols

...

/** Complete list of names imported by a wildcard */
lazy val wildcardNames: List[Name]   = wildcardSymbols map (_.name)
lazy val individualNames: List[Name] = individualSymbols map (_.name)

/** The names imported by this statement */
override lazy val importedNames: List[Name] = wildcardNames ++ individualNames
{code}
)

and the source code of `scala.tools.nsc.interpreter.Imports.importsCode` (excerpt):

{code}
// Single symbol imports might be implicits! See bug #1752.  Rather than
// try to finesse this, we will mimic all imports for now.
def keepHandler(handler: MemberHandler) = handler match {
  // While defining classes in class based mode - implicits are not needed.
  case h: ImportHandler if isClassBased && definesClass => h.importedNames.exists(x => wanted.contains(x))
  case _: ImportHandler     => true
  case x if generousImports => x.definesImplicit || (x.definedNames exists (d => wanted.exists(w => d.startsWith(w))))
  case x                    => x.definesImplicit || (x.definedNames exists wanted)
}
{code}
;;;","24/Nov/17 16:17;mpetruska;https://github.com/scala/bug/issues/9881;;;","24/Nov/17 16:23;mpetruska;The fix for this went into the 2.12 branch of scala: https://github.com/scala/scala/pull/5640;;;","24/Nov/17 17:13;mpetruska;Created a PR to retrofit the fix to scala repl 2.11: https://github.com/scala/scala/pull/6195;;;","24/Nov/17 20:36;srowen;That's great detective work. You may be able to tell better than I then: does this entail changing the way in which spark-shell overrides stuff in the Scala shell? or really just something that has to be fixed and picked up from Scala?

2.12 support is mostly there in Spark, and will require 2.12.4, which has this fix, so OK there. 
For 2.11, I think Spark is mostly stuck on 2.11.8 because the Scala shell changed between 2.11.8 and 2.11.11 and it was very hard to make one set of code that worked on both. So I'm not sure if it will help in the end for Spark if it's back ported for Scala 2.11.;;;","24/Nov/17 22:02;rdub;Just confirming: it's expected that the Scala 2.11 repl works fine, but spark-shell does not? Is spark-shell using repl code paths that have this bug that the Scala repl avoids?;;;","25/Nov/17 10:38;mpetruska;I believe the scala repl fix would also fix this. Forcing the interpreter to a non-class based behaviour should also fix this issue (`isClassBased`) in my opinion. I think the problem is not caused by code and overrides added to `SparkILoop`. (Sorry about being this vague, but I have some evidence strengthening the above statements, but nothing that would prove it completely.)
[~rdub] Will do additional tests/experimentation with repl 2.11.8 to confirm; maybe find a way to force the non-class based behaviour...;;;","29/Nov/17 11:16;mpetruska;Based on the comments above and the comments in the Scala PR (https://github.com/scala/scala/pull/6195): it is not practical to push a fix to the Scala repo for this.
It seems that we should also avoid changing the ""class based"" implementation to fix this issue.
Adding a workaround for the ""scala-2.11"" code.;;;","29/Nov/17 14:59;srowen;OK, so this is basically ""fixed for Scala 2.12 only""?;;;","29/Nov/17 15:05;apachespark;User 'mpetruska' has created a pull request for this issue:
https://github.com/apache/spark/pull/19846;;;","29/Nov/17 15:11;mpetruska;[~srowen], [~rdub], I can now confirm that the original bug fix that was pushed to Scala 2.12 fixes this issue. Succeeded in retrofitting the same changes into Spark-shell, see: https://github.com/apache/spark/pull/19846.
The original fix for Scala 2.12 can be found at: https://github.com/scala/scala/pull/5640
The downside is that the code/fix is not the most approachable, could not refactor for better readability (and also making sure it compiles :) ).;;;","01/Dec/17 06:06;jerryshao;Shall we wait for this before 2.2.1 is out? ;;;","01/Dec/17 11:14;srowen;Issue resolved by pull request 19846
[https://github.com/apache/spark/pull/19846];;;","01/Apr/18 01:26;jpallas;The changes that were imported in [https://github.com/apache/spark/pull/19846] don't seem to cover all the cases that the Scala 2.12 changes covered.  To be specific, this sequence:
{code}
import scala.reflect.runtime.{universe => ru}
import ru.TypeTag
class C[T: TypeTag](value: T)
{code}
works correctly in Scala 2.12 with -Yrepl-class-based, but does not work in spark-shell 2.3.0.

I don't understand the import-handling code enough to understand the problem, however.  It figures out that it needs the import for TypeTag but it doesn't recognize that the import depends on the previous import:
{noformat}
<console>:9: error: not found: value ru
import ru.TypeTag
       ^
{noformat}

 ;;;","03/Apr/18 05:33;jpallas;Following up to my previous comment: [~mpetruska], It looks like the Spark retrofit is missing a change that should appear in {{SparkILoopInterpreter.SparkImportHandler}}. I believe it should include
{code:java}
override lazy val individualNames: List[Name] = importableSymbolsWithRenames map (_._2)
{code}
The corresponding change is in the [2.11 retrofit of SI-9881| https://github.com/scala/scala/pull/6195].  Does that look right?;;;",,,,,,,,,,,,,,,
Refine partition pruning when attribute is wrapped in Cast,SPARK-22384,13112866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,29/Oct/17 06:52,04/Jul/18 07:53,14/Jul/23 06:30,05/Jun/18 18:33,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,SQL,,,,,0,,,,,,,,,"Sql below will get all partitions from metastore, which put much burden on metastore;
{{CREATE TABLE test (value INT) PARTITIONED BY (dt STRING)}}
{{SELECT * from test where dt=2017}}

The reason is that the the analyzed attribute {{dt}} is wrapped in {{Cast}} and {{HiveShim}} fails to generate a proper partition filter.

Could we fix this? Sql like {{SELECT * from test where dt=2017}} is common in my warehouse.",,apachespark,cloud_fan,jinxing6042@126.com,mgaido,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 04 07:53:05 UTC 2018,,,,,,,,,,"0|i3lu8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/17 06:56;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/19602;;;","05/Jun/18 18:33;cloud_fan;Issue resolved by pull request 19602
[https://github.com/apache/spark/pull/19602];;;","04/Jul/18 07:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21712;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven nightly snapshot jenkins jobs are broken on multiple workers due to lsof,SPARK-22377,13112757,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,xynny,xynny,28/Oct/17 05:26,12/Dec/22 18:11,14/Jul/23 06:30,13/Nov/17 23:29,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,Build,,,,,0,,,,,,,,,"It looks like multiple workers in the amplab jenkins cannot execute lsof.  Example log below:

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20Packaging/job/spark-branch-2.1-maven-snapshots/182/console

spark-build/dev/create-release/release-build.sh: line 344: lsof: command not found
usage: kill [ -s signal | -p ] [ -a ] pid ...
       kill -l [ signal ]

I looked at the jobs and it looks like only  amp-jenkins-worker-01 works so you are getting a successful build every week or so.  Unclear if the snapshot is actually released.  



",,apachespark,xynny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 16 02:30:04 UTC 2017,,,,,,,,,,"0|i3ltkf:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"08/Nov/17 12:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19695;;;","13/Nov/17 23:29;gurwls223;Issue resolved by pull request 19695
[https://github.com/apache/spark/pull/19695];;;","16/Dec/17 02:30;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19998;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-tests.py fails at exec-sbt if run with Python 3,SPARK-22376,13112747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,TV4Fun,TV4Fun,28/Oct/17 03:06,12/Dec/22 18:10,14/Jul/23 06:30,07/Nov/17 10:45,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Running ./dev/run-tests with python3 as the default gives this error at the Building Spark stage:
{noformat}
========================================================================
Building Spark
========================================================================
[info] Building Spark (w/Hive 1.2.1) using SBT with these arguments:  -Phadoop-2.6 -Phive-thriftserver -Phive -Pkafka-0-8 -Pflume -Pyarn -Pmesos -Pkinesis-asl test:package streaming-kafka-0-8-assembly/assembly streaming-flume-assembly/assembly streaming-kinesis-asl-assembly/assembly
Traceback (most recent call last):
  File ""./dev/run-tests.py"", line 622, in <module>
    main()
  File ""./dev/run-tests.py"", line 593, in main
    build_apache_spark(build_tool, hadoop_version)
  File ""./dev/run-tests.py"", line 391, in build_apache_spark
    build_spark_sbt(hadoop_version)
  File ""./dev/run-tests.py"", line 344, in build_spark_sbt
    exec_sbt(profiles_and_goals)
  File ""./dev/run-tests.py"", line 293, in exec_sbt
    if not sbt_output_filter.match(line):
TypeError: cannot use a string pattern on a bytes-like object
{noformat}
This is because in Python 3, the stdout member of a POpen object defaults to returning a byte stream, and exec_sbt tries to read it as a text screen. This can be fixed by specifying universal_newlines=True when creating the POpen object. I notice that the hashbang at the start of run-tests.py says to run with python2, so I am not sure how much of the rest of it is Python 3 compatible, but this is the first error I've run into. It ran with python 3 because run-tests runs run-tests.py using the default Python, which on my system is Python 3. Not sure whether the better solution here is to try and fix Python 3 compatibility in run-tests.py or set run-tests to use Python 2.",OSX 10.12.6 Python 3.6.0 Anaconda 4.4,apachespark,TV4Fun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 07 10:45:58 UTC 2017,,,,,,,,,,"0|i3lti7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/17 09:06;srowen;If the change is still compatible with Python 2, yes go ahead. ;;;","06/Nov/17 09:40;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19665;;;","07/Nov/17 10:45;gurwls223;Issue resolved by pull request 19665
[https://github.com/apache/spark/pull/19665];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test script can fail if eggs are installed by setup.py during test process,SPARK-22375,13112745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,TV4Fun,TV4Fun,28/Oct/17 02:20,12/Dec/22 18:10,14/Jul/23 06:30,29/Oct/17 08:10,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Running ./dev/run-tests may install missing Python packages as part of it's setup process. setup.py can cache these in python/.eggs, and since the lint-python script checks any file with the .py extension anywhere in the Spark project, it will check files in .eggs and will fail if any of these do not meet style criteria, even though these are not part of the project lint-spark should exclude python/.eggs from its search directories.",OSX 10.12.6,apachespark,TV4Fun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/pypa/setuptools/issues/391,,,,,,,,,,9223372036854775807,,,Sun Oct 29 06:31:40 UTC 2017,,,,,,,,,,"0|i3lthr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/17 08:59;srowen;Certainly, please open a pull request.;;;","28/Oct/17 16:27;apachespark;User 'xynny' has created a pull request for this issue:
https://github.com/apache/spark/pull/19597;;;","29/Oct/17 06:31;gurwls223;Fixed in https://github.com/apache/spark/pull/19597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent NullPointerException in org.codehaus.janino.IClass.isAssignableFrom,SPARK-22373,13112714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,danmeany,danmeany,27/Oct/17 22:17,02/Dec/17 12:01,14/Jul/23 06:30,01/Dec/17 01:25,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.2,2.3.0,,,Spark Core,,,,,1,,,,,,,,,"Very occasional and retry works.

Full stack:
17/10/27 21:06:15 ERROR Executor: Exception in task 29.0 in stage 12.0 (TID 758)
java.lang.NullPointerException
	at org.codehaus.janino.IClass.isAssignableFrom(IClass.java:569)
	at org.codehaus.janino.UnitCompiler.isWideningReferenceConvertible(UnitCompiler.java:10347)
	at org.codehaus.janino.UnitCompiler.isMethodInvocationConvertible(UnitCompiler.java:8636)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8427)
	at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:8285)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8169)
	at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:8071)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4421)
	at org.codehaus.janino.UnitCompiler.access$7500(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3774)
	at org.codehaus.janino.UnitCompiler$12.visitMethodInvocation(UnitCompiler.java:3762)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4328)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3762)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4933)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3180)
	at org.codehaus.janino.UnitCompiler.access$5000(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3151)
	at org.codehaus.janino.UnitCompiler$9.visitMethodInvocation(UnitCompiler.java:3139)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:4328)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3139)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2112)
	at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1377)
	at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1370)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2558)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1370)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1450)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2811)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:550)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:890)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:894)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:377)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:369)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1128)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1209)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:564)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:890)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:894)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:377)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:369)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1128)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1209)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:564)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:420)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:206)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:374)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:369)
	at org.codehaus.janino.Java$AbstractPackageMemberClassDeclaration.accept(Java.java:1309)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:345)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:396)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:311)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:229)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:196)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:91)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:959)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1026)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1023)
	at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:908)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(WholeStageCodegenExec.scala:372)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(WholeStageCodegenExec.scala:371)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/10/27 21:06:15 INFO CodeGenerator: Code generated in 8.896831 ms

Intermittent nature of problem makes me suspect the cache or a thread-related issue.
Some the SQL that appears in the area of the code line reported in Spark UI:
     dense_rank() over (partition by itemid, type order by sum(col_a)+(sum(col_b)/1000000000000000.0) desc) as rank, 
             ...where cast(mytimestampfield as String) >= '$mydate'
           
","Hortonworks distribution: HDP 2.6.2.0-205 , /usr/hdp/current/spark2-client/jars/spark-core_2.11-2.1.1.2.6.2.0-205.jar
",apachespark,charleso,Daimon,danmeany,kiszk,leighklotz,mgaido,mshen,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/17 02:59;mshen;CodeGeneratorTester.scala;https://issues.apache.org/jira/secure/attachment/12899743/CodeGeneratorTester.scala","29/Nov/17 02:59;mshen;generated.java;https://issues.apache.org/jira/secure/attachment/12899744/generated.java",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 02 12:01:04 UTC 2017,,,,,,,,,,"0|i3ltav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/17 09:00;srowen;It looks like a Janino problem, so not sure if it can be fixed in Spark, or worked around if it can't be reproduced.;;;","07/Nov/17 04:35;kiszk;If you can submit a program that can reproduce this, I could investigate which software component can cause an issue.;;;","27/Nov/17 22:40;mshen;Also facing this issue.
From our experience, it is more likely to happen when handling data with very complicated schema.
The code to reproduce this issue can be as simple as reading the data followed by immediately writing it out.;;;","28/Nov/17 01:03;kiszk;[~mshen] Thank you for reporting this issue. We would appreciate it if you can post the code to reproduce.;;;","28/Nov/17 01:47;mshen;The code that would throw this NPE looks like the following:

{noformat}
import com.databricks.spark.avro._

val df = spark.read.avro(""/path/to/data/with/complicated/schema"")
df.write.mode(""overwrite"").avro(""/path/to/output"")
{noformat}

It seems to me that the trigger for getting this NPE is the data instead of the code.
Not sure how much this code would help.

I do have the generated java code that causes Janino to throw this NPE
I converted the [relevant code | https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala#L1215] in org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator into a standalone application, and running that application against the generated java code finished successfully.
However, when using multiple threads to run this application, I started seeing this NPE.
It appears to me that this issue is indeed related to Janino's thread safety.

[~kiszk], to further investigate, would it be helpful to provide that generated java code?;;;","28/Nov/17 17:41;mshen;Tried running the test application using 10 concurrent threads to compile the generated code I have.
With the current version of Janino 3.0.0 used by Spark, if I run the test application 100 times, I'm always able to see this NPE happening once or twice.
Switched to latest version of Janino 3.0.7 and tried multiple times, haven't seen this NPE happening yet.
Seems that this might be fixed by bumping up Janino dependency version.;;;","28/Nov/17 19:50;leighklotz;This happens to me regularly enough using 2.1.1.20, Avro, and more than one executor core that I have abandoned use of multiple cores with Avro.

I've attempted to make a 100% reproducible test case but failed, so I'm reporting these factors here.  

1. set --conf spark.executor.cores=2 (or any higher number)
2. reading in a certain large Avro file
3. spark 2.1.1.20
4. spark.read.avro(fn).cache.count or other action involving writes; just count doesn't do it.
5. The Avro file contains a key of type Map[String->Array[Byte]], though the values can all be empty arrays. The cardinality of they keyspace is high and the number of keys per map is tens to hundreds.
6. Multiple partitions are necessary to trigger the error.
7. Before the stack trace reported above, I see ""ERROR CodeGenerator: failed to compile: java.lang.NullPointerException"" followed by a dump of generated Java code.  

The nodes that fail have ""INFO CodeGenerator: Code generated in ##.##### ms"" messages and my theory is that the code generator being used here has a thread safety issue.  

;;;","29/Nov/17 01:54;mshen;[~leigjklotz],

I think bumping up Janino version to 3.0.7 definitely helps to resolve this issue.
I have tried multiple times since yesterday.
For both the standalone application and my Spark application dealing with data that almost always generate this issue, I no longer see the NPE issue after bumping Janino to 3.0.7.
Looking at Janino's release note, I haven't figured out which patch would fix this issue though.;;;","29/Nov/17 03:05;mshen;Attach the standalone testing application as well as the generated Java code that triggers this NPE so other people can also verify.
The application needs to be compiled pulling spark-catalyst, spark-core, and spark-sql as dependencies.
Easiest way is to put it inside spark-catalyst module and compile spark-catalyst itself.

With the application compiled, you can launch it taking the dependency JARs as classpath.
Again, an easy way is to take Spark distribution's jars directory as classpath.

Launch the application like the following:
{noformat}
for i in `seq 1 100`; do
  java -cp ""./*"" org.apache.spark.sql.catalyst.expressions.codegen.CodeGenTester 20 /path/to/generated.java;
done > output 2>&1 &
{noformat}

This will run the application 100 times, each attempting to compile the java code using 20 concurrent threads.
Using Janino 3.0.0, I can always reproduce this issue.
Using Janino 3.0.7, this issue is gone.;;;","29/Nov/17 03:21;apachespark;User 'Victsm' has created a pull request for this issue:
https://github.com/apache/spark/pull/19839;;;","29/Nov/17 03:21;mshen;Created PR https://github.com/apache/spark/pull/19839

[~sowen] [~kiszk],
Could you help to take a look?;;;","30/Nov/17 01:23;leighklotz;[~mshen] Thank you.  I've hand-upgraded janino and commons-compiler to 3.0.7, and did no other dependencies.  The NPE has not occurred, and I'm running further tests to make sure there are no other ill effects.
;;;","01/Dec/17 01:25;srowen;Issue resolved by pull request 19839
[https://github.com/apache/spark/pull/19839];;;","02/Dec/17 12:01;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19866;;;",,,,,,,,,,,,,,,,,,,,,,,,,
dag-scheduler-event-loop thread stopped with error  Attempted to access garbage collected accumulator 5605982,SPARK-22371,13112560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Rudoy,mayank.agarwal2305,mayank.agarwal2305,27/Oct/17 10:35,04/Oct/18 12:00,14/Jul/23 06:30,17/May/18 10:50,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.1,2.4.0,,,,Spark Core,,,,,7,,,,,,,,,"Our Spark Jobs are getting stuck on DagScheduler.runJob as dagscheduler thread is stopped because of *Attempted to access garbage collected accumulator 5605982*.

from our investigation it look like accumulator is cleaned by GC first and same accumulator is used for merging the results from executor on task completion event.


As the error java.lang.IllegalAccessError is LinkageError which is treated as FatalError so dag-scheduler loop is finished with below exception.

---ERROR stack trace --
Exception in thread ""dag-scheduler-event-loop"" java.lang.IllegalAccessError: Attempted to access garbage collected accumulator 5605982
	at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:253)
	at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:249)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.util.AccumulatorContext$.get(AccumulatorV2.scala:249)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1083)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1080)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1080)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1183)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1647)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

I am attaching the thread dump of driver as well ",,aclegg,apachespark,bysza,cloud_fan,craftsman,davide.mandrini,devaraj,dicee,dmcwhorter,eyalfa,kailashgupta1012,mayank.agarwal2305,mgaido,Rudoy,wabu,xuefuz,zwu.net@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/17 07:40;mayank.agarwal2305;Helper.scala;https://issues.apache.org/jira/secure/attachment/12902231/Helper.scala","15/Dec/17 08:49;mayank.agarwal2305;ShuffleIssue.java;https://issues.apache.org/jira/secure/attachment/12902246/ShuffleIssue.java","27/Oct/17 10:37;mayank.agarwal2305;driver-thread-dump-spark2.1.txt;https://issues.apache.org/jira/secure/attachment/12894331/driver-thread-dump-spark2.1.txt","15/Dec/17 08:54;mayank.agarwal2305;sampledata;https://issues.apache.org/jira/secure/attachment/12902247/sampledata",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 04 12:00:16 UTC 2018,,,,,,,,,,"0|i3lscv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/17 14:13;mgaido;Could you please provide an easy way to reproduce the issue?;;;","15/Dec/17 07:13;mayank.agarwal2305;Hi, Sorry for late reply.

From our analysis this error seems to come when there are jobs running parallel on datasets and union of all those datasets and Full GC triggered at same time which clears the accumulate of children dataset of union.

I am attaching a small program from which this error comes frequently.

[^ShuffleIssue.java]
[^Helper.scala]
[^sampledata]

Steps for generating sample data
{code:title=UNIX COMMAND |borderStyle=solid}
for((i=1472428800;i<=1472947200;i=i+86400));do mkdir -p output/testdata/eventtime=$i;cp sampledata output/testdata/eventtime=$i/000001_0;cp sampledata output/testdata/eventtime=$i/000001_1;cp sampledata output/testdata/eventtime=$i/000001_2;cp sampledata output/testdata/eventtime=$i/000001_3;done
{code};;;","13/Mar/18 07:25;mayank.agarwal2305;Hi, Any update on the above issue ;;;","16/Mar/18 18:01;craftsman;We've seen this for the first time on 2.3.0. 

The scenario that [~mayank.agarwal2305] described of jobs running in parallel on datasets and union of all datasets and full gc triggered"" sounds exactly like our scenario. We've been unable to upgrade because of this issue.;;;","30/Mar/18 11:18;bysza;We were hit by this issue on 2.3.0 too while running just ""show tables"".

Stack trace:

java.lang.IllegalStateException: Attempted to access garbage collected accumulator 793
at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:265)
at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:261)
at scala.Option.map(Option.scala:146)
at org.apache.spark.util.AccumulatorContext$.get(AccumulatorV2.scala:261)
at org.apache.spark.util.AccumulatorV2$$anonfun$name$1.apply(AccumulatorV2.scala:87)
at org.apache.spark.util.AccumulatorV2$$anonfun$name$1.apply(AccumulatorV2.scala:87)
at scala.Option.orElse(Option.scala:289)
at org.apache.spark.util.AccumulatorV2.name(AccumulatorV2.scala:87)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(TaskResultGetter.scala:103)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(TaskResultGetter.scala:102)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:102)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)
at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748);;;","17/Apr/18 09:53;aclegg;Another data point -- I've seen this happen (in 2.3.0) during cleanup after a task failure:

{code:none}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: java.lang.IllegalStateException: Attempted to access garbage collected accumulator 365
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)
        ... 31 more
{code};;;","20/Apr/18 16:36;Rudoy;Do we really need to throw an exception from AccumulatorContext.get() when an accumulator is garbage collected? There's a period of time when an accumulator has been garbage collected, but hasn't been removed from AccumulatorContext.originals by ContextCleaner. When an update is received for such accumulator it will throw an exception and kill the whole job. This can happen when a stage completed, but there're still running tasks from other attempts, speculation etc. Since AccumulatorContext.get() returns an Option we could just return None in such case. Before SPARK-20940 this method threw IllegalAccessError which is not a NonFatal, was caught at a lower level and didn't cause job failure.;;;","20/Apr/18 17:46;apachespark;User 'artemrd' has created a pull request for this issue:
https://github.com/apache/spark/pull/21114;;;","07/May/18 16:24;zwu.net@gmail.com;Got the same problem with 2.3 and also the program stalled:

{{ Uncaught exception in thread heartbeat-receiver-event-loop-thread}}
{{java.lang.IllegalStateException: Attempted to access garbage collected accumulator 8825}}
{{        at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:265)}}
{{        at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:261)}}
{{        at scala.Option.map(Option.scala:146)}}
{{        at org.apache.spark.util.AccumulatorContext$.get(AccumulatorV2.scala:261)}}
{{        at org.apache.spark.util.AccumulatorV2$$anonfun$name$1.apply(AccumulatorV2.scala:87)}}
{{        at org.apache.spark.util.AccumulatorV2$$anonfun$name$1.apply(AccumulatorV2.scala:87)}}
{{        at scala.Option.orElse(Option.scala:289)}}
{{        at org.apache.spark.util.AccumulatorV2.name(AccumulatorV2.scala:87)}}
{{        at org.apache.spark.util.AccumulatorV2.toInfo(AccumulatorV2.scala:108)}};;;","17/May/18 10:50;cloud_fan;Issue resolved by pull request 21114
[https://github.com/apache/spark/pull/21114];;;","04/Oct/18 11:46;davide.mandrini;Hello,

is this fix included in version 2.3.2?

From here, I would say yes: [https://github.com/apache/spark/blob/v2.3.2/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala]

In case it is, can you add 2.3.2 in the ""Fix version"" field of this Jira ticket?;;;","04/Oct/18 12:00;cloud_fan;if it's fixed in 2.3.1, it goes without saying that it's fixed in 2.3.2 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Config values should be captured in Driver.,SPARK-22370,13112529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,27/Oct/17 08:48,29/Dec/17 09:19,14/Jul/23 06:30,28/Oct/17 17:34,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"{{ArrowEvalPythonExec}} and {{FlatMapGroupsInPandasExec}} are refering config values of {{SQLConf}} in function for {{mapPartitions}}/{{mapPartitionsInternal}}, but we should capture them in Driver.",,apachespark,cloud_fan,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 29 09:19:04 UTC 2017,,,,,,,,,,"0|i3ls5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/17 08:51;srowen;What does this mean?;;;","27/Oct/17 08:53;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19587;;;","27/Oct/17 09:00;ueshin;[~srowen] I updated the description. Thanks.;;;","28/Oct/17 17:34;cloud_fan;Issue resolved by pull request 19587
[https://github.com/apache/spark/pull/19587];;;","29/Dec/17 09:19;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20115;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.binaryFiles ignore minPartitions parameter,SPARK-22357,13112210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bomeng,weichenxu123,weichenxu123,26/Oct/17 08:25,12/Dec/22 18:10,14/Jul/23 06:30,29/Aug/18 00:39,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Spark Core,,,,,0,behavior-changes,,,,,,,,"this is a bug in binaryFiles - even though we give it the partitions, binaryFiles ignores it.
This is a bug introduced in spark 2.1 from spark 2.0, in file PortableDataStream.scala the argument “minPartitions” is no longer used (with the push to master on 11/7/6):

{code}
/**
Allow minPartitions set by end-user in order to keep compatibility with old Hadoop API
which is set through setMaxSplitSize
*/
def setMinPartitions(sc: SparkContext, context: JobContext, minPartitions: Int) {
    val defaultMaxSplitBytes = sc.getConf.get(config.FILES_MAX_PARTITION_BYTES)
    val openCostInBytes = sc.getConf.get(config.FILES_OPEN_COST_IN_BYTES)
    val defaultParallelism = sc.defaultParallelism
    val files = listStatus(context).asScala
    val totalBytes = files.filterNot(.isDirectory).map(.getLen + openCostInBytes).sum
    val bytesPerCore = totalBytes / defaultParallelism
    val maxSplitSize = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))
    super.setMaxSplitSize(maxSplitSize)
}
{code}

The code previously, in version 2.0, was:
{code}
def setMinPartitions(context: JobContext, minPartitions: Int) {
    val totalLen = listStatus(context).asScala.filterNot(.isDirectory).map(.getLen).sum
    val maxSplitSize = math.ceil(totalLen / math.max(minPartitions, 1.0)).toLong
    super.setMaxSplitSize(maxSplitSize)
}
{code}

The new code is very smart, but it ignores what the user passes in and uses the data size, which is kind of a breaking change in some sense
In our specific case this was a problem, because we initially read in just the files names and only after that the dataframe becomes very large, when reading in the images themselves – and in this case the new code does not handle the partitioning very well.
I’m not sure if it can be easily fixed because I don’t understand the full context of the change in spark (but at the very least the unused parameter should be removed to avoid confusion).

",,apachespark,bomeng,imatiach,jbrock,jerryshao,maropu,Tagar,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 06 18:23:38 UTC 2018,,,,,,,,,,"0|i3lq73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/17 08:27;weichenxu123;cc [~imatiach] [~liancheng];;;","26/Oct/17 08:29;weichenxu123;cc [~jerryshao];;;","26/Oct/17 08:34;jerryshao;[~WeichenXu123] would you please format the code in JIRA description to make it easy to read :).;;;","26/Oct/17 08:42;weichenxu123;[~jerryshao] code formatted. This bug is reported from here https://github.com/apache/spark/pull/19439
You can ask [~imatiach] for more context.;;;","26/Oct/17 12:58;jerryshao;bq. In our specific case this was a problem, because we initially read in just the files names and only after that the dataframe becomes very large, when reading in the images themselves – and in this case the new code does not handle the partitioning very well.

Would you please explain more about this?;;;","26/Oct/17 13:21;weichenxu123;[~jerryshao] I checked the code, it ignore the `minPartitions` parameter indeed. About these text, [~imatiach] Can you help explain more ? ;;;","26/Oct/17 13:32;jerryshao;Yes, I know this parameter is ignored, but I'm not sure is it intended or not. If it breaks your case I think we should fix it anyway.;;;","26/Oct/17 19:30;bomeng;a quick fix could be as follows, correct me if i am wrong.
val defaultParallelism = Math.max(sc.defaultParallelism, minPartitions);;;","26/Oct/17 22:51;imatiach;binaryFiles ignores the number of partitions I want to have, even if I specify the value.  I have to repartition the returned DataFrame.  In my specific case, the number of partitions was very small, which caused performance issues.  I needed to increase the number of partitions by repartitioning the DataFrame after it was constructed, but this can be expensive - it would be better to create the DataFrame with the user-specified number of partitions.;;;","13/Jun/18 01:41;jbrock;What are people's opinions of [~bomeng]'s fix? This bug just bit me, so I'd like to see this fixed.;;;","22/Jun/18 04:27;gurwls223;Please open a PR and go ahead if you think it's right. I can take a look [~bomeng];;;","25/Jun/18 20:09;apachespark;User 'bomeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/21638;;;","29/Aug/18 00:39;srowen;Issue resolved by pull request 21638
[https://github.com/apache/spark/pull/21638];;;","06/Sep/18 18:22;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22356;;;","06/Sep/18 18:23;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22356;;;",,,,,,,,,,,,,,,,,,,,,,,,
data source table should support overlapped columns between data and partition schema,SPARK-22356,13112167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Oct/17 02:08,14/Feb/18 08:17,14/Jul/23 06:30,27/Oct/17 01:03,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 08:17:11 UTC 2018,,,,,,,,,,"0|i3lpxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/17 03:11;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19579;;;","14/Feb/18 08:17;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.collect is not threadsafe,SPARK-22355,13112141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Oct/17 23:03,02/Mar/20 01:28,14/Jul/23 06:30,27/Oct/17 01:04,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29419,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 25 23:10:04 UTC 2017,,,,,,,,,,"0|i3lprz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/17 23:10;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19577;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In on-heap mode, when allocating memory from pool,we should fill memory  with `MEMORY_DEBUG_FILL_CLEAN_VALUE`",SPARK-22349,13111885,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,25/Oct/17 06:53,25/Oct/17 16:10,14/Jul/23 06:30,25/Oct/17 16:10,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"In on-heap mode, when allocating memory from pool,we should fill memory  with `MEMORY_DEBUG_FILL_CLEAN_VALUE`",,10110346,apachespark,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 25 16:10:13 UTC 2017,,,,,,,,,,"0|i3lo7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/17 06:55;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/19572;;;","25/Oct/17 16:10;sameerag;Issue resolved by pull request 19572
https://github.com/apache/spark/pull/19572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[2.3.0] cannot run Spark on Yarn when Yarn impersonation is turned off,SPARK-22341,13111680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,maver1ck,maver1ck,24/Oct/17 12:58,12/Feb/19 21:22,14/Jul/23 06:30,12/Feb/19 21:22,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Spark Core,YARN,,,,0,,,,,,,,,"I'm trying to run 2.3.0 (from master) on my yarn cluster.
The result is:
{code}
Exception in thread ""main"" org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=EXECUTE, inode=""/user/bi/.sparkStaging/application_1508815646088_0164"":bi:hdfs:drwx------
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:208)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:171)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6795)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4387)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:855)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:835)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1990)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)
	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$7.apply(ApplicationMaster.scala:219)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$7.apply(ApplicationMaster.scala:216)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.deploy.yarn.ApplicationMaster.<init>(ApplicationMaster.scala:216)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:821)
	at org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:842)
	at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)
{code}

I think the problem exists, because I'm not using yarn impersonation which mean that all jobs on cluster are runned from user yarn.",,apachespark,dongjoon,maver1ck,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22290,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 12 21:22:47 UTC 2019,,,,,,,,,,"0|i3lmy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/17 13:09;srowen;Yes, but why is that a Spark problem?;;;","24/Oct/17 13:11;maver1ck;Because Spark 2.2.0 is working perfectly fine with such a configuration.

The problem is that Spark 2.3.0 is using wrong user on AM when trying to read files from hdfs.;;;","24/Oct/17 16:47;vanzin;I was messing with this area recently, so let me take a look...;;;","24/Oct/17 17:58;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19566;;;","12/Feb/19 21:22;vanzin;This was fixed, forgot to close.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark setJobGroup doesn't match java threads,SPARK-22340,13111578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,leif,leif,24/Oct/17 00:23,12/Dec/22 18:11,14/Jul/23 06:30,07/Nov/19 21:47,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,PySpark,,,,,0,,,,,,,,,"With pyspark, {{sc.setJobGroup}}'s documentation says

{quote}
Assigns a group ID to all the jobs started by this thread until the group ID is set to a different value or cleared.
{quote}

However, this doesn't appear to be associated with Python threads, only with Java threads.  As such, a Python thread which calls this and then submits multiple jobs doesn't necessarily get its jobs associated with any particular spark job group.  For example:

{code}
def run_jobs():
    sc.setJobGroup('hello', 'hello jobs')
    x = sc.range(100).sum()
    y = sc.range(1000).sum()
    return x, y

import concurrent.futures
with concurrent.futures.ThreadPoolExecutor() as executor:
    future = executor.submit(run_jobs)
    sc.cancelJobGroup('hello')
    future.result()
{code}

In this example, depending how the action calls on the Python side are allocated to Java threads, the jobs for {{x}} and {{y}} won't necessarily be assigned the job group {{hello}}.

First, we should clarify the docs if this truly is the case.

Second, it would be really helpful if we could make the job group assignment reliable for a Python thread, though I’m not sure the best way to do this.  As it stands, job groups are pretty useless from the pyspark side, if we can't rely on this fact.

My only idea so far is to mimic the TLS behavior on the Python side and then patch every point where job submission may take place to pass that in, but this feels pretty brittle. In my experience with py4j, controlling threading there is a challenge. ",,bryanc,leif,Tagar,tknaup,ueshin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29017,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 11 16:46:02 UTC 2019,,,,,,,,,,"0|i3lmbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/17 19:37;leif;This is less spooky than I initially thought, I will explain later tonight.;;;","24/Oct/17 22:46;leif;Ok, this is fairly straightforward.  The problem is that from the Python side, {{setJobGroup}} isn't thread-local, it's global.  Here is a tight reproducer:

{noformat}
import concurrent.futures
import threading
import time
executor = concurrent.futures.ThreadPoolExecutor()

latch_1 = threading.Event()
latch_2 = threading.Event()

def wait(x):
    time.sleep(x)
    return x

def multiple_job_groups():
    sc.setJobGroup('imajobgroup', 'helloitme')
    groups = []
    groups.append(get_job_group())
    sc.parallelize([1, 1]).map(wait).collect()
    latch_2.set()
    latch_1.wait()
    groups.append(get_job_group())
    sc.parallelize([1, 1]).map(wait).collect()
    groups.append(get_job_group())
    return groups

def another_job_group():
    latch_2.wait()
    sc.setJobGroup('another', 'itnotme')
    sc.parallelize([1, 1]).map(wait).collect()
    latch_1.set()

future_1 = executor.submit(multiple_job_groups)
future_2 = executor.submit(another_job_group)
future_1.result()
{noformat}

The result is that {{another_job_group}} modifies the local property in between the first and second executions of {{multiple_job_groups}}'s jobs, and we get this result:

{noformat}
['imajobgroup', 'another', 'another']
{noformat}

I think I can ""solve"" this by wrapping {{SparkContext}} with a lock (to sequence the execution of {{setJobGroup}} and something in py4j that will release the lock during JVM execution, which feels Very Dangerous.

Would greatly appreciate it if we could do something to really solve this inside pyspark, but will attempt the Dangerous on my side for now.;;;","26/Oct/17 01:28;leif;By monkey-patching {{SCCallSiteSync}}, I'm able to inject a call to {{setJobGroup}} based on a python thread-local variable.  This is vulnerable to a race condition where another thread can call {{setJobGroup}} in between my call to {{setJobGroup}} and the subsequent action, which is not great, but I think low-ish risk and probably lower risk than trying to introduce sufficient locking around this.;;;","26/Jun/19 01:26;viirya;[~hyukjin.kwon] Should we reopen this as you are open a PR for it now?;;;","07/Nov/19 21:47;gurwls223;Issue resolved by pull request 24898
[https://github.com/apache/spark/pull/24898];;;","11/Nov/19 16:46;Tagar;Glad to see this is solved. 

A nice side-effect should be somewhat better performance on some cases involving heavy python-java communication
on multi-numa/ multi-socket configurations. With static threads, Linux kernel will actually have a chance to 
schedule threads on processors/cores that are more local to data's numa placement. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ColumnReference should get higher priority than timeFunctionCall(CURRENT_DATE, CURRENT_TIMESTAMP)",SPARK-22333,13111378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,donnyzone,donnyzone,donnyzone,23/Oct/17 11:38,03/May/18 04:40,14/Jul/23 06:30,28/Oct/17 06:42,2.1.0,2.1.1,2.1.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"In our cluster, there is a table ""T"" with column named as ""current_date"". When we select data from this column with SQL:

{code:sql}
select current_date from T
{code}

We get the wrong answer, as the column is translated as CURRENT_DATE() function.
",,apachespark,donnyzone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24151,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 30 03:17:04 UTC 2017,,,,,,,,,,"0|i3ll3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/17 11:51;apachespark;User 'DonnyZone' has created a pull request for this issue:
https://github.com/apache/spark/pull/19559;;;","30/Oct/17 03:17;apachespark;User 'DonnyZone' has created a pull request for this issue:
https://github.com/apache/spark/pull/19606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NaiveBayes unit test occasionly fail,SPARK-22332,13111363,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weichenxu123,weichenxu123,weichenxu123,23/Oct/17 10:47,25/Oct/17 21:32,14/Jul/23 06:30,25/Oct/17 21:32,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,ML,,,,,0,test,,,,,,,,"NaiveBayes unit test occasionly fail:

{code}
Stacktrace

sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: Expected 
0.692522561237645    0.10378169316716802  0.10485603781693176  0.09883970777825533  
0.12590579710144928  0.6467391304347826   0.11594202898550725  0.11141304347826085  
0.1003761165961448   0.10413728255759282  0.7005171603196991   0.09496944052656324  
 and 
0.7                  0.10000000000000002  0.10000000000000002  0.10000000000000002  
0.10000000000000002  0.7                  0.10000000000000002  0.10000000000000002  
0.10000000000000002  0.10000000000000002  0.7                  0.10000000000000002  
 to be within 0.05 using absolute tolerance for all elements.
	at org.apache.spark.ml.util.TestingUtils$MatrixWithAlmostEquals.$tilde$eq$eq(TestingUtils.scala:205)
	at org.apache.spark.ml.classification.NaiveBayesSuite.validateModelFit(NaiveBayesSuite.scala:74)
	at org.apache.spark.ml.classification.NaiveBayesSuite$$anonfun$13.apply$mcV$sp(NaiveBayesSuite.scala:150)
	at org.apache.spark.ml.classification.NaiveBayesSuite$$anonfun$13.apply(NaiveBayesSuite.scala:134)
	at org.apache.spark.ml.classification.NaiveBayesSuite$$anonfun$13.apply(NaiveBayesSuite.scala:134)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

",,apachespark,josephkb,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 25 21:32:09 UTC 2017,,,,,,,,,,"0|i3ll07:",9223372036854775807,,,,,josephkb,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"23/Oct/17 10:54;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/19558;;;","25/Oct/17 21:32;josephkb;Issue resolved by pull request 19558
[https://github.com/apache/spark/pull/19558];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Linear containsKey operation for serialized maps.,SPARK-22330,13111260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,whoosh,whoosh,whoosh,22/Oct/17 21:44,06/Nov/17 23:50,14/Jul/23 06:30,06/Nov/17 23:47,1.2.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,performance,,,,,,,,"One of our production application which aggressively uses cached spark RDDs degraded after increasing volumes of data though it shouldn't. Fast profiling session showed that the slowest part was SerializableMapWrapper#containsKey: it delegates get and remove to actual implementation, but containsKey is inherited from AbstractMap which is implemented in linear time via iteration over whole keySet. A workaround was simple: replacing all containsKey with get(key) != null solved the issue.

Nevertheless, it would be much simpler for everyone if the issue will be fixed once and for all.
A fix is straightforward, delegate containsKey to actual implementation.",,apachespark,cloud_fan,Tagar,whoosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,SPARK-21657,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 06 23:47:48 UTC 2017,,,,,,,,,,"0|i3lkdb:",9223372036854775807,,,,,whoosh,,,,,,,,,,,,,,,,,,,"22/Oct/17 22:06;apachespark;User 'Whoosh' has created a pull request for this issue:
https://github.com/apache/spark/pull/19553;;;","06/Nov/17 23:47;cloud_fan;Issue resolved by pull request 19553
[https://github.com/apache/spark/pull/19553];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ClosureCleaner misses referenced superclass fields, gives them null values",SPARK-22328,13111177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,rdub,rdub,21/Oct/17 17:04,06/Sep/18 09:43,14/Jul/23 06:30,26/Oct/17 20:45,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"[Runnable repro here|https://github.com/ryan-williams/spark-bugs/tree/closure]:

Superclass with some fields:
{code}
abstract class App extends Serializable {
  // SparkContext stub
  @transient lazy val sc = new SparkContext(new SparkConf().setAppName(""test"").setMaster(""local[4]"").set(""spark.ui.showConsoleProgress"", ""false""))

  // These fields get missed by the ClosureCleaner in some situations
  val n1 = 111
  val s1 = ""aaa""

  // Simple scaffolding to exercise passing a closure to RDD.foreach in subclasses
  def rdd = sc.parallelize(1 to 1)
  def run(name: String): Unit = {
    print(s""$name:\t"")
    body()
    sc.stop()
  }
  def body(): Unit
}
{code}

Running a simple Spark job with various instantiations of this class:

{code}
object Main {
  /** [[App]]s generated this way will not correctly detect references to [[App.n1]] in Spark closures */
  val fn = () ⇒ new App {
    val n2 = 222
    val s2 = ""bbb""
    def body(): Unit = rdd.foreach { _ ⇒ println(s""$n1, $n2, $s1, $s2"") }
  }

  /** Doesn't serialize closures correctly */
  val app1 = fn()

  /** Works fine */
  val app2 =
    new App {
      val n2 = 222
      val s2 = ""bbb""
      def body(): Unit = rdd.foreach { _ ⇒ println(s""$n1, $n2, $s1, $s2"") }
    }

  /** [[App]]s created this way also work fine */
  def makeApp(): App =
    new App {
      val n2 = 222
      val s2 = ""bbb""
      def body(): Unit = rdd.foreach { _ ⇒ println(s""$n1, $n2, $s1, $s2"") }
    }

  val app3 = makeApp()  // ok

  val fn2 = () ⇒ makeApp()  // ok

  def main(args: Array[String]): Unit = {
    fn().run(""fn"")    // bad: n1 → 0, s1 → null
    app1.run(""app1"")  // bad: n1 → 0, s1 → null
    app2.run(""app2"")  // ok
    app3.run(""app3"")  // ok
    fn2().run(""fn2"")  // ok
  }
}
{code}

Build + Run:

{code}
$ sbt run
…
fn:	0, 222, null, bbb
app1:	0, 222, null, bbb
app2:	111, 222, aaa, bbb
app3:	111, 222, aaa, bbb
fn2:	111, 222, aaa, bbb
{code}

The first two versions have {{0}} and {{null}}, resp., for the {{A.n1}} and {{A.s1}} fields.

Something about this syntax causes the problem:

{code}
() => new App { … }
{code}",,apachespark,cloud_fan,kiszk,maropu,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 26 20:45:17 UTC 2017,,,,,,,,,,"0|i3ljuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/17 07:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19556;;;","26/Oct/17 20:45;cloud_fan;Issue resolved by pull request 19556
[https://github.com/apache/spark/pull/19556];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R CRAN check fails on non-latest branches,SPARK-22327,13111155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,21/Oct/17 07:15,08/Nov/17 05:05,14/Jul/23 06:30,08/Nov/17 05:05,1.6.4,2.0.3,2.1.3,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.3,2.2.1,2.3.0,,SparkR,,,,,0,,,,,,,,,"with warning
* checking CRAN incoming feasibility ... WARNING
Maintainer: 'Shivaram Venkataraman <shivaram@cs.berkeley.edu>'
Insufficient package version (submitted: 2.0.3, existing: 2.1.2)
Unknown, possibly mis-spelled, fields in DESCRIPTION:
  'RoxygenNote'
WARNING: There was 1 warning.
NOTE: There were 2 notes.

We have seen this in branch-1.6, branch-2.0, and this would be a problem for branch-2.1 after we ship 2.2.1.

The root cause of the issue is in the package version check in CRAN check. After the SparkR package version 2.1.2 (is first) published, any older version is failing the version check. As far as we know, there is no way to skip this version check.

Also, there is previously a NOTE on new maintainer. ",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 31 04:52:03 UTC 2017,,,,,,,,,,"0|i3ljpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/17 07:18;felixcheung;https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/3956/consoleFull;;;","21/Oct/17 07:45;felixcheung;in contrast, this is from master https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/82919/consoleFull

* checking CRAN incoming feasibility ... NOTE
Maintainer: 'Shivaram Venkataraman <shivaram@cs.berkeley.edu>'
Unknown, possibly mis-spelled, fields in DESCRIPTION:
  'RoxygenNote'
* checking package dependencies ... NOTE
  No repository set, so cyclic dependency check skipped
* checking R code for possible problems ... NOTE
Found the following calls to attach():
File 'SparkR/R/DataFrame.R':
  attach(newEnv, pos = pos, name = name, warn.conflicts = warn.conflicts)
See section 'Good practice' in '?attach'.
NOTE: There were 3 notes.

So it should have 3 notes, or (2 notes +  1warning) as the one note turns into a warning for Insufficient package version;;;","22/Oct/17 04:06;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19549;;;","22/Oct/17 04:10;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19550;;;","31/Oct/17 04:51;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19619;;;","31/Oct/17 04:52;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19620;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unnecessary hashCode and equals methods,SPARK-22326,13111143,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,21/Oct/17 02:04,21/Oct/17 04:00,14/Jul/23 06:30,21/Oct/17 04:00,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Plan equality should be computed by canonicalized, so we can remove unnecessary hashCode and equals methods.",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 21 02:06:04 UTC 2017,,,,,,,,,,"0|i3ljnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/17 02:06;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/19539;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmit calls getFileStatus before calling loginUserFromKeytab,SPARK-22319,13110877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Steven Rand,Steven Rand,Steven Rand,20/Oct/17 04:44,23/Oct/17 06:32,14/Jul/23 06:30,23/Oct/17 01:49,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Deploy,Spark Core,,,,0,,,,,,,,,"In the SparkSubmit code, we call {{resolveGlobPaths}}, which eventually calls {{getFileStatus}}, which for HDFS is an RPC call to the NameNode: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L346.

We do this before we call {{loginUserFromKeytab}}, which is further down in the same method: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L655.

The result is that the call to {{resolveGlobPaths}} fails in secure clusters with:

{code}
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
{code}

A workaround is to {{kinit}} on the host before using spark-submit. However, it's better if this workaround isn't necessary. A simple fix is to call loginUserFromKeytab before attempting to interact with HDFS.

At least for cluster mode, this would appear to be a regression caused by SPARK-21012.",,apachespark,Steven Rand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 02:43:03 UTC 2017,,,,,,,,,,"0|i3li07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/17 04:53;apachespark;User 'sjrand' has created a pull request for this issue:
https://github.com/apache/spark/pull/19540;;;","23/Oct/17 02:43;apachespark;User 'sjrand' has created a pull request for this issue:
https://github.com/apache/spark/pull/19554;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INFER_AND_SAVE overwrites important metadata in Parquet Metastore table,SPARK-22306,13110299,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,WhoisDavid,WhoisDavid,18/Oct/17 14:18,01/Jun/19 02:14,14/Jul/23 06:30,02/Nov/17 11:38,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"I noticed some critical changes on my hive tables and realized that they were caused by a simple select on SparkSQL. Looking at the logs, I found out that this select was actually performing an update on the database ""Saving case-sensitive schema for table"". 
I then found out that Spark 2.2.0 introduces a new default value for spark.sql.hive.caseSensitiveInferenceMode (see SPARK-20888): INFER_AND_SAVE

The issue is that this update changes critical metadata of the table, in particular:
- changes the owner to the current user
- removes bucketing metadata (BUCKETING_COLS, SDS)
- removes sorting metadata (SORT_COLS)

Switching the property to: NEVER_INFER prevents the issue.

Also, note that the damage can be fix manually in Hive with e.g.:
{code:sql}
alter table [table_name] 
clustered by ([col1], [col2]) 
sorted by ([colA], [colB])
into [n] buckets
{code}

*REPRODUCE (branch-2.2)*
In Spark 2.1.x (branch-2.1), NEVER_INFER is used. Spark 2.3 (master) branch is good due to SPARK-17729. This is a regression on Spark 2.2 only. By default, Parquet Hive table is affected and only Hive may suffer from this.
{code}
hive> CREATE TABLE t(a string, b string) CLUSTERED BY (a, b) SORTED BY (a, b) INTO 10 BUCKETS STORED AS PARQUET;
hive> INSERT INTO t VALUES('a','b');
hive> DESC FORMATTED t;
...
Num Buckets:        	10
Bucket Columns:     	[a, b]
Sort Columns:       	[Order(col:a, order:1), Order(col:b, order:1)]

scala> sql(""SELECT * FROM t"").show(false)

hive> DESC FORMATTED t;
Num Buckets:        	-1
Bucket Columns:     	[]
Sort Columns:       	[]
{code}","Hive 2.3.0 (PostgresQL metastore, stored as Parquet)
Spark 2.2.0",apachespark,budde,cloud_fan,dongjoon,WhoisDavid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22329,,,SPARK-20888,SPARK-19611,,,,,,,,,,,SPARK-22329,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 13:32:04 UTC 2017,,,,,,,,,,"0|i3letz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/17 21:50;dongjoon;Hi, [~cloud_fan] and [~smilegator].
Should we change the default value to prevent this regression?;;;","18/Oct/17 22:07;dongjoon;[~WhoisDavid].
Your table is 'STORED AS PARQUET', right?
Could you try `spark.sql.hive.convertMetastoreParquet.mergeSchema=false`, too?
;;;","19/Oct/17 14:32;WhoisDavid;[~dongjoon]
Seems to be set to false by default (or at least in my current setup);;;","19/Oct/17 16:03;dongjoon;Thank you for confirming.
Sorry for asking again, could you turn off `spark.sql.hive.convertMetastoreParquet=false`?;;;","19/Oct/17 17:16;dongjoon;Hi. [~WhoisDavid].
Spark 2.3 seems to be okay due to SPARK-17729.
I can check the bucket spec is preserved like `Some(10 buckets, bucket columns: [a, b], sort columns: [a, b])`.
You can download the snapshot and try that.;;;","19/Oct/17 17:51;WhoisDavid;With:
spark.sql.hive.caseSensitiveInferenceMode=INFER_AND_SAVE
spark.sql.hive.convertMetastoreParquet=false

The issue does not happen. Leaves the bucketing/sorting/owner as is.;;;","19/Oct/17 18:37;dongjoon;When we release 2.2.1, we had better give a warning about this regression.;;;","19/Oct/17 19:54;dongjoon;Hi, [~budde].
Could you take a look this issue in Spark 2.2.0?;;;","19/Oct/17 20:00;budde;I'll take a look and potentially start on a patch as soon as I get the time. Should be the next day or two.;;;","19/Oct/17 20:20;dongjoon;Thank you, [~budde]!;;;","25/Oct/17 15:34;cloud_fan;Seems like the `alterTable` removes some table metadata unexpectedly. I'm checking it.;;;","31/Oct/17 11:22;cloud_fan;This is a known issue before Spark 2.3: ALTER TABLE at Spark side erases the bucketing information of a hive table. However, for this certain case, the ALTER TABLE is triggered automatically, which it's pretty bad for users because of this bug.

I'm going to handle this case specially and suggest users to upgrade to Spark 2.3.;;;","31/Oct/17 15:04;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19622;;;","02/Nov/17 11:38;cloud_fan;Issue resolved by pull request 19622
[https://github.com/apache/spark/pull/19622];;;","02/Nov/17 13:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19644;;;",,,,,,,,,,,,,,,,,,,,,,,,
HDFSBackedStateStoreProvider fails with StackOverflowException when attempting to recover state,SPARK-22305,13110241,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,Yuval.Itzchakov,Yuval.Itzchakov,18/Oct/17 09:20,31/Oct/17 18:54,14/Jul/23 06:30,31/Oct/17 18:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Environment:

Spark: 2.2.0
Java version: 1.8.0_112
spark.sql.streaming.minBatchesToRetain: 100

After an application failure due to OOM exceptions, restarting the application with the existing state produces the following OOM:

{code:java}
java.io.IOException: com.google.protobuf.ServiceException: java.lang.StackOverflowError
	at org.apache.hadoop.ipc.ProtobufHelper.getRemoteException(ProtobufHelper.java:47)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:260)
	at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy18.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1240)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1227)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1215)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:303)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:269)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:261)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1540)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(HDFSBackedStateStoreProvider.scala:405)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:296)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:295)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:295)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$4.apply(HDFSBackedStateStoreProvider.scala:297)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$4.apply(HDFSBackedStateStoreProvider.scala:296)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:296)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:295)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:295)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$4.apply(HDFSBackedStateStoreProvider.scala:297)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$4.apply(HDFSBackedStateStoreProvider.scala:296)
	at scala.Option.getOrElse(Option.scala:121)
{code}

There were 2 snapshot files in the state directory and 230 delta files.",,apachespark,codingcat,Yuval.Itzchakov,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 30 16:37:07 UTC 2017,,,,,,,,,,"0|i3leh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/17 12:57;Yuval.Itzchakov;[~tdas] What do you think?;;;","26/Oct/17 23:20;zsxwing;[~Yuval.Itzchakov] how many batches per 1 minute in your query? If there are a lot of batches, you can try to run your application with `--conf spark.sql.streaming.stateStore.maintenanceInterval=10s` to set a small interval as a workaround.

However, we definitely should fix this by rewriting these codes in a non-recursive way. 
;;;","27/Oct/17 09:02;Yuval.Itzchakov;[~zsxwing] A typical query takes ~ 0.5 seconds to execute, so about 120 batches. 

I discovered something else which relates to the StackOverflow. It seems the way that `HDFSBackedStateStoreProvider` is implemented is that there is a correlation between the file name of the latest offset and the state store version Spark looks for.

But assume the following situation:

1. The state was modified in such a way that it is not backwards compatible with the state in the store
2. Due to 1, we delete the state in the store but keep the offset files
3. Spark starts, trying to recover the previous state, and takes the latest state version from the latest offset file
4. The state store recursively starts searching for the version of the state (which no longer exists), ending on a StackOverflowException.

Would it be possible to separate the two (state and offsets) such that it would be possible to automatically start with the latest stored offsets but without the state?;;;","27/Oct/17 22:53;zsxwing;Why not just delete the whole checkpoint dir? Dropping state store will make Spark return wrong answers.;;;","30/Oct/17 16:37;apachespark;User 'joseph-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/19611;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Getting java.sql.SQLException: Unsupported type 101 for BINARY_DOUBLE,SPARK-22303,13110199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,taroplus,taroplus,taroplus,18/Oct/17 04:54,31/Jan/18 17:08,14/Jul/23 06:30,23/Oct/17 16:57,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When a table contains columns such as BINARY_DOUBLE or BINARY_FLOAT, this JDBC connector throws SQL exception

{code}
java.sql.SQLException: Unsupported type 101
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$getCatalystType(JdbcUtils.scala:235)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$8.apply(JdbcUtils.scala:292)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$8.apply(JdbcUtils.scala:292)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:291)
  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:64)
  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:113)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)
{code}

these types are Oracle specific ones, described here
https://docs.oracle.com/cd/E11882_01/timesten.112/e21642/types.htm#TTSQL148",,apachespark,taroplus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 22 03:04:04 UTC 2017,,,,,,,,,,"0|i3le7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/17 06:37;srowen;If it's Oracle specific I don't think it's reasonable to expect support. ;;;","22/Oct/17 00:22;taroplus;Spark supports Oracle specific types, I'm working on a PR, please keep this open;;;","22/Oct/17 03:03;taroplus;https://github.com/apache/spark/pull/19548;;;","22/Oct/17 03:04;apachespark;User 'taroplus' has created a pull request for this issue:
https://github.com/apache/spark/pull/19548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update ORC to 1.4.1,SPARK-22300,13110093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Oct/17 19:04,19/Oct/17 05:37,14/Jul/23 06:30,19/Oct/17 05:37,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,"Apache ORC 1.4.1 is released yesterday.
- https://orc.apache.org/news/2017/10/16/ORC-1.4.1/

Like ORC-233 (Allow `orc.include.columns` to be empty), there are several important fixes.

We had better use the latest one.",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 19 05:37:23 UTC 2017,,,,,,,,,,"0|i3ldkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/17 19:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19521;;;","19/Oct/17 05:37;cloud_fan;Issue resolved by pull request 19521
[https://github.com/apache/spark/pull/19521];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: BlockManagerSuite ""Shuffle registration timeout and maxAttempts conf""",SPARK-22297,13110085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mpetruska,vanzin,vanzin,17/Oct/17 18:41,24/Jan/18 18:26,14/Jul/23 06:30,24/Jan/18 18:26,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Spark Core,Tests,,,,0,,,,,,,,,"Ran into this locally; the test code seems to use timeouts which generally end up in flakiness like this.

{noformat}
[info] - SPARK-20640: Shuffle registration timeout and maxAttempts conf are working *** FAILED *** (1 second, 203 milliseconds)
[info]   ""Unable to register with external shuffle server due to : java.util.concurrent.TimeoutException: Timeout waiting for task."" did not contain ""test_spark_20640_try_again"" (BlockManagerSuite.scala:1370)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)
[info]   at org.apache.spark.storage.BlockManagerSuite$$anonfun$14.apply$mcV$sp(BlockManagerSuite.scala:1370)
[info]   at org.apache.spark.storage.BlockManagerSuite$$anonfun$14.apply(BlockManagerSuite.scala:1323)
[info]   at org.apache.spark.storage.BlockManagerSuite$$anonfun$14.apply(BlockManagerSuite.scala:1323)
{noformat}
",,apachespark,jiangxb1987,mpetruska,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 18:26:10 UTC 2018,,,,,,,,,,"0|i3ldiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/17 12:49;apachespark;User 'mpetruska' has created a pull request for this issue:
https://github.com/apache/spark/pull/19671;;;","01/Jan/18 17:18;jiangxb1987;How often do we run into this? Personally I can't repro this test failure on my local environment.;;;","24/Jan/18 18:26;vanzin;Issue resolved by pull request 19671
[https://github.com/apache/spark/pull/19671];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Postgresql UUID[] to Cassandra: Conversion Error,SPARK-22291,13109872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jmchung,fabiojwalter,fabiojwalter,17/Oct/17 03:31,12/Dec/22 18:10,14/Jul/23 06:30,29/Oct/17 17:31,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,patch,postgresql,sql,,,,,,"My job reads data from a PostgreSQL table that contains columns of user_ids uuid[] type, so that I'm getting the error above when I'm trying to save data on Cassandra.

However, the creation of this same table on Cassandra works fine!  user_ids list<text>.

I can't change the type on the source table, because I'm reading data from a legacy system.

I've been looking at point printed on log, on class org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils.scala


Stacktrace on Spark:
{noformat}
Caused by: java.lang.ClassCastException: [Ljava.util.UUID; cannot be cast to [Ljava.lang.String;
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$14.apply(JdbcUtils.scala:443)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$14.apply(JdbcUtils.scala:442)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13$$anonfun$18.apply(JdbcUtils.scala:472)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13$$anonfun$18.apply(JdbcUtils.scala:472)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$nullSafeConvert(JdbcUtils.scala:482)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13.apply(JdbcUtils.scala:470)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13.apply(JdbcUtils.scala:469)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:330)
at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:312)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)
at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)
at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)
at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)
at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)
at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)
at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
at org.apache.spark.scheduler.Task.run(Task.scala:108)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
{noformat}


Proposed solution:

At this specific point spark-sql_2.11-2.2.0-sources.jar!/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala:443

{code:scala}
//My suggestion is change the line 443 from

```array.asInstanceOf[Array[java.lang.String]]
              .map(UTF8String.fromString)```

//to 
```array.map(UTF8String.fromString(_.toString))```
{code}

","Debian Linux, Scala 2.11, Spark 2.2.0, PostgreSQL 9.6, Cassandra 3",apachespark,fabiojwalter,jmchung,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/17 03:35;fabiojwalter;org_apache_spark_sql_execution_datasources_jdbc_JdbcUtil.png;https://issues.apache.org/jira/secure/attachment/12892516/org_apache_spark_sql_execution_datasources_jdbc_JdbcUtil.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Mon Oct 30 01:29:17 UTC 2017,,,,,,,,,,"0|i3lc7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/17 04:47;viirya;Could you send a PR for this?;;;","24/Oct/17 18:58;apachespark;User 'jmchung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19567;;;","29/Oct/17 23:31;viirya;[~cloud_fan] The Assignee should be [~jmchung]. Thanks.;;;","29/Oct/17 23:42;gurwls223;I happened to see this comment first and just updated.;;;","29/Oct/17 23:44;viirya;Thanks [~hyukjin.kwon].;;;","29/Oct/17 23:46;jmchung;Thank you all :);;;","30/Oct/17 01:29;apachespark;User 'jmchung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting second context in same JVM fails to get new Hive delegation token,SPARK-22290,13109831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,16/Oct/17 23:32,24/Oct/17 18:01,14/Jul/23 06:30,19/Oct/17 06:57,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Consider the following pyspark script:

{code}
sc = SparkContext()
// do stuff
sc.stop()

// do some other stuff

sc = SparkContext()
{code}

That code didn't use to work at all in 2.2 (failure to create the second context), but makes more progress in 2.3. But it fails to create new Hive delegation tokens; you see this error in the output:

{noformat}
17/10/16 16:26:50 INFO security.HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1714191595_19, ugi=blah(auth:KERBEROS)]]
17/10/16 16:26:50 INFO hive.metastore: Trying to connect to metastore with URI blah
17/10/16 16:26:50 INFO hive.metastore: Connected to metastore.
17/10/16 16:26:50 ERROR metadata.Hive: MetaException(message:Delegation Token can be issued only with kerberos authentication. Current AuthenticationMethod: TOKEN)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_delegation_token_result$get_delegation_token_resultStandardScheme.read(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_delegation_token_result$get_delegation_token_resultStandardScheme.read(ThriftHiveMetastore.java)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_delegation_token_result.read(ThriftHiveMetastore
{noformat}

The error is printed in the logs but it doesn't cause the app to fail (which might be considered wrong).

The effect is that when that old delegation token expires the new app will fail.

But the real issue here is that Spark shouldn't be mixing delegation tokens from different apps. It should try harder to isolate a set of delegation tokens to a single app submission.

And, in the case of Hive, there are many situations where a delegation token isn't needed at all.",,apachespark,jerryshao,stakiar,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11035,,,,,,,,,,,,,SPARK-22341,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 19 06:57:20 UTC 2017,,,,,,,,,,"0|i3lbyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/17 00:05;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19509;;;","19/Oct/17 06:57;jerryshao;Issue resolved by pull request 19509
[https://github.com/apache/spark/pull/19509];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot save LogisticRegressionModel with bounds on coefficients,SPARK-22289,13109795,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuhaoyan,nseggert,nseggert,16/Oct/17 20:46,08/Feb/18 01:32,14/Jul/23 06:30,12/Dec/17 19:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,ML,,,,,0,,,,,,,,,"I think this was introduced in SPARK-20047.

Trying to call save on a logistic regression model trained with bounds on its parameters throws an error. This seems to be because Spark doesn't know how to serialize the Matrix parameter.

Model is set up like this:
{code}
    val calibrator = new LogisticRegression()
      .setFeaturesCol(""uncalibrated_probability"")
      .setLabelCol(""label"")
      .setWeightCol(""weight"")
      .setStandardization(false)
      .setLowerBoundsOnCoefficients(new DenseMatrix(1, 1, Array(0.0)))
      .setFamily(""binomial"")
      .setProbabilityCol(""probability"")
      .setPredictionCol(""logistic_prediction"")
      .setRawPredictionCol(""logistic_raw_prediction"")
{code}

{code}
17/10/16 15:36:59 ERROR ApplicationMaster: User class threw exception: scala.NotImplementedError: The default jsonEncode only supports string and vector. org.apache.spark.ml.param.Param must override jsonEncode for org.apache.spark.ml.linalg.DenseMatrix.
scala.NotImplementedError: The default jsonEncode only supports string and vector. org.apache.spark.ml.param.Param must override jsonEncode for org.apache.spark.ml.linalg.DenseMatrix.
	at org.apache.spark.ml.param.Param.jsonEncode(params.scala:98)
	at org.apache.spark.ml.util.DefaultParamsWriter$$anonfun$1$$anonfun$2.apply(ReadWrite.scala:296)
	at org.apache.spark.ml.util.DefaultParamsWriter$$anonfun$1$$anonfun$2.apply(ReadWrite.scala:295)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.ml.util.DefaultParamsWriter$$anonfun$1.apply(ReadWrite.scala:295)
	at org.apache.spark.ml.util.DefaultParamsWriter$$anonfun$1.apply(ReadWrite.scala:295)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.ml.util.DefaultParamsWriter$.getMetadataToSave(ReadWrite.scala:295)
	at org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:277)
	at org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelWriter.saveImpl(LogisticRegression.scala:1182)
	at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:114)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:254)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$saveImpl$1.apply(Pipeline.scala:253)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.saveImpl(Pipeline.scala:253)
	at org.apache.spark.ml.PipelineModel$PipelineModelWriter.saveImpl(Pipeline.scala:337)
	at org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:114)
	-snip-
{code}",,apachespark,mlnick,nseggert,peng.meng@intel.com,yanboliang,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 18 07:01:03 UTC 2017,,,,,,,,,,"0|i3lbqn:",9223372036854775807,,,,,yanboliang,,,,,,,,,,,,,,,,,,,"17/Oct/17 06:05;yuhaoyan;Thanks for reporting the issue. Should be a straight-forward fix. Yet maybe we should cover it better in release QA.

There're two ways to support this as I see:
1. Support save/load on LogisticRegressionParams, and also adjust the save/load in LogisticRegression and LogisticRegressionModel.

2. Directly support Matrix in Param.jsonEncode, similar to what we have done for Vector.

IMO we need to collect opinions before sending a fix. Welcome to send other options.

I'm leaning towards 2, for simplicity and convenience for other classes. ;;;","17/Oct/17 07:27;yuhaoyan;cc [~yanboliang] [~dbtsai];;;","17/Oct/17 07:45;mlnick;I think option (2) is the more general fix here.;;;","17/Oct/17 23:44;yanboliang;+1 for option 2. Please feel free to send a PR. Thanks.;;;","17/Oct/17 23:58;yuhaoyan;Thanks for the reply. I'll start compose a PR.;;;","18/Oct/17 07:01;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/19525;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK_DAEMON_MEMORY not honored by MesosClusterDispatcher,SPARK-22287,13109737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adobe_pmackles,adobe_pmackles,adobe_pmackles,16/Oct/17 16:15,10/Nov/17 00:43,14/Jul/23 06:30,10/Nov/17 00:43,2.1.1,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Mesos,,,,,0,,,,,,,,,There does not appear to be a way to control the heap size used by MesosClusterDispatcher as the SPARK_DAEMON_MEMORY environment variable is not honored for that particular daemon.,,adobe_pmackles,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 12:03:03 UTC 2017,,,,,,,,,,"0|i3lbdr:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"17/Oct/17 12:03;apachespark;User 'pmackles' has created a pull request for this issue:
https://github.com/apache/spark/pull/19515;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Code of class \""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection\"" grows beyond 64 KB",SPARK-22284,13109649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,someonehere15,someonehere15,16/Oct/17 08:40,12/Dec/22 18:10,14/Jul/23 06:30,10/Nov/17 20:18,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Optimizer,PySpark,SQL,,,0,,,,,,,,,"I am using pySpark 2.1.0 in a production environment, and trying to join two DataFrames, one of which is very large and has complex nested structures.

Basically, I load both DataFrames and cache them.
Then, in the large DataFrame, I extract 3 nested values and save them as direct columns.
Finally, I join on these three columns with the smaller DataFrame.
This would be a short code for this:

{code}
dataFrame.read......cache()
dataFrameSmall.read.......cache()
dataFrame = dataFrame.selectExpr(['*','nested.Value1 AS Value1','nested.Value2 AS Value2','nested.Value3 AS Value3'])
dataFrame = dataFrame.dropDuplicates().join(dataFrameSmall, ['Value1','Value2',Value3'])
dataFrame.count()
{code}

And this is the error I get when it gets to the count():

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 7.0 failed 4 times, most recent failure: Lost task 11.3 in stage 7.0 (TID 11234, somehost.com, executor 10): java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.janino.JaninoRuntimeException: Code of method \""apply_1$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificUnsafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)V\"" of class \""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection\"" grows beyond 64 KB
{code}

I have seen many tickets with similar issues here, but no proper solution. Most of the fixes are until Spark 2.1.0 so I don't know if running it on Spark 2.2.0 would fix it. In any case I cannot change the version of Spark since it is in production.
I have also tried setting 
{code:java}
spark.sql.codegen.wholeStage=false
{code}
 but still the same error.

The job worked well up to now, also with large datasets, but apparently this batch got larger, and that is the only thing that changed. Is there any workaround for this?",,apachespark,cloud_fan,dongjoon,kiszk,maropu,someonehere15,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23156,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/17 14:56;someonehere15;64KB Error.log;https://issues.apache.org/jira/secure/attachment/12893056/64KB+Error.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 20:18:43 UTC 2017,,,,,,,,,,"0|i3lau7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/17 09:04;srowen;Without more details, this could be a duplicate of many existing issues, yes. Generally your ability to upgrade wouldn't dictate when changes are backported, and you should try 2.2. however if you find the fix that addresses this we can try to backport. But if you can't upgrade then it won't matter. ;;;","16/Oct/17 09:09;someonehere15;If I upgrade to 2.2.0 then I, for myself at least, would not need a backport anymore, although I would be happy to let you know if it works in 2.2.0 IF I could try it.
But the problem is exactly the fact that I cannot simply upgrade, at least for now, and wanted to ask if there is any general recommendation or workaround that I could try first, a configuration parameter or else.
I don't know what more details I can provide here to investigate. If there is, let me know.;;;","16/Oct/17 09:13;srowen;What you've described is a symptom with many causes. Have a look through JIRA. It would be useful to know if it happens in 2.2 to narrow it down. You could try disabling code gen but that has performance consequences. ;;;","16/Oct/17 09:15;someonehere15;I did try adding

{code:java}
--conf ""spark.sql.codegen.wholeStage=false""
{code}

in *spark-submit*, but the same error occurred.;;;","17/Oct/17 04:47;gurwls223;[~someonehere15], would you maybe able to provide a self-contained reproducer? I am willing to check it.;;;","17/Oct/17 08:53;viirya;Btw, we have used {{UnsafeProjection}} in many places for expression codegen. Disabling wholestage codegen doesn't help it. Even non-wholestage code path uses expression codegen.;;;","18/Oct/17 21:34;someonehere15;[~hyukjin.kwon], thank you very much for your assistance, but I don't think I can reproduce it easily.
Everything has been working fine until now, but it only got stuck in the last batch.

If 'm correct, you are also the developer of spark-xml, which is exactly what I use for reading the source files.
I read more than a million XML files per batch, and these files have complex nested structures, so a lot of data is parsed into a DataFrame.
There are cases that the XML files may have very deep levels of nested tags, so I'm not sure if that may be the cause, linking this issue to spark-xml and not the actual join step when the error happens.

Furthermore, I tried splitting the batch where it got stuck into smaller batches to process. The first half of around 700 thousand files worked, but the second half had the same error. Then I split the second half in 100 thousand files per batch, and then it worked. I cannot make the XML files available, which means I can't give you a way to reproduce it. I can send you the generated code that is included in the error, but I'm guessing that also doesn't help. However, if you think the deep nested levels may be a cause, maybe this can provide some insight.;;;","19/Oct/17 12:37;kiszk;Can you attach the generated code? It may help which `SpecificUnsafeProjection` may cause this exception.;;;","19/Oct/17 14:57;someonehere15;Sure, I just added it as an attachment.;;;","19/Oct/17 16:33;kiszk;Thank you for uploading the generated code. This is very helpful. I understood that the very huge method is generated for calculating hash value from many many String columns in struct or array.
If I correctly remember it, this problem is not be fixed in 2.2 or master.

I will try to create a repro and submit a PR to hopefully fix this.;;;","19/Oct/17 16:57;kiszk;I found [this JIRA|https://issues.apache.org/jira/browse/SPARK-18207] and realized that I created the PR.
According to the attached code, I imagine that this case uses more complicated columns in a row.;;;","19/Oct/17 23:17;dongjoon;Ping, [~mgaido].;;;","20/Oct/17 11:39;someonehere15;[~kiszk], happy to help, and thanks for your assistance.

So, if I understand correctly, there was a similar ticket and it was solved, but my case is more complicated, right?
There are some cases in the files where the structures may indeed get very complex.
Would this case also be solvable?;;;","22/Oct/17 14:48;kiszk;Yes, there was a similar ticket that was solved. Your case is more complicated case that includes `struct`. I created a repro in my environment. I think that it could be solvable. I will create a PR this week.

To fix a problem, we usually submit a PR to the master branch. Then, we consider whether the PR could be applicable to previous versions based on risk assessments by committers.;;;","24/Oct/17 09:36;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19563;;;","10/Nov/17 20:18;cloud_fan;Issue resolved by pull request 19563
[https://github.com/apache/spark/pull/19563];;;",,,,,,,,,,,,,,,,,,,,,,,
Handle R method breaking signature changes,SPARK-22281,13109555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,15/Oct/17 21:36,08/Nov/17 05:06,14/Jul/23 06:30,08/Nov/17 05:06,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SparkR,,,,,0,,,,,,,,,"cAs discussed here
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Spark-2-1-2-RC2-tt22540.html#a22555

this WARNING on R-devel

* checking for code/documentation mismatches ... WARNING
Codoc mismatches from documentation object 'attach':
attach
  Code: function(what, pos = 2L, name = deparse(substitute(what),
                 backtick = FALSE), warn.conflicts = TRUE)
  Docs: function(what, pos = 2L, name = deparse(substitute(what)),
                 warn.conflicts = TRUE)
  Mismatches in argument default values:
    Name: 'name' Code: deparse(substitute(what), backtick = FALSE) Docs: deparse(substitute(what))

Codoc mismatches from documentation object 'glm':
glm
  Code: function(formula, family = gaussian, data, weights, subset,
                 na.action, start = NULL, etastart, mustart, offset,
                 control = list(...), model = TRUE, method = ""glm.fit"",
                 x = FALSE, y = TRUE, singular.ok = TRUE, contrasts =
                 NULL, ...)
  Docs: function(formula, family = gaussian, data, weights, subset,
                 na.action, start = NULL, etastart, mustart, offset,
                 control = list(...), model = TRUE, method = ""glm.fit"",
                 x = FALSE, y = TRUE, contrasts = NULL, ...)
  Argument names in code not in docs:
    singular.ok
  Mismatches in argument names:
    Position: 16 Code: singular.ok Docs: contrasts
    Position: 17 Code: contrasts Docs: ...

Checked the latest release R 3.4.1 and the signature change wasn't there. This likely indicated an upcoming change in the next R release that could incur this new warning when we attempt to publish the package.

Not sure what we can do now since we work with multiple versions of R and they will have different signatures then.
",,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 07:22:04 UTC 2017,,,,,,,,,,"0|i3la9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/17 21:39;felixcheung;And here for all r-devel WARN
https://cran.r-project.org/web/checks/check_results_SparkR.html
;;;","22/Oct/17 20:08;felixcheung;tried a few things. If we remove the 
{code}
@param
{code}

then cran checks fail with
{code}
* checking Rd \usage sections ... WARNING
Undocumented arguments in documentation object 'attach'
  ‘what’ ‘pos’ ‘name’ ‘warn.conflicts’

Functions with \usage entries need to have the appropriate \alias
entries, and all their arguments documented.
The \usage entries must correspond to syntactically valid R code.
See chapter ‘Writing R documentation files’ in the ‘Writing R
Extensions’ manual.
{code}

if we change the method signature to

{code}
setMethod(""attach"",
          signature(what = ""SparkDataFrame""),
          function(what, ...) {
{code}

Then it fails to install
{code}
Error in rematchDefinition(definition, fdef, mnames, fnames, signature) :
  methods can add arguments to the generic ‘attach’ only if '...' is an argument to the generic
Error : unable to load R code in package ‘SparkR’
ERROR: lazy loading failed for package ‘SparkR’
{code}
;;;","23/Oct/17 05:34;shivaram;Thanks for looking into this [~felixcheung] Is there anyway we can remove the `usage` entry as well from the Rdoc ? This might also be something to raise with the roxygen project for a more long term solution ;;;","23/Oct/17 07:08;felixcheung;ok, I have a solution for each of both. it turns out the fix for glm is quite a bit different from attach, so I added the error above.

With attach, we need to match the signature of base::attach, since it changes we are going to generate the signature at runtime by pulling from base::attach directly.

in short, with glm it's pulling in the function definition (ie. ""usage"") from the stats::glm function. Since this is ""compiled in"" when we build the source package into the .Rd, when/if it changes at runtime or in CRAN check it won't match the latest signature.;;;","23/Oct/17 07:22;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19557;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve StatisticsSuite to test `convertMetastore` properly,SPARK-22280,13109532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,15/Oct/17 03:35,16/Oct/17 23:17,14/Jul/23 06:30,16/Oct/17 23:17,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,Tests,,,,0,,,,,,,,,"This issue aims to improve StatisticsSuite to test `convertMetastore` configuration properly. Currently, some test logic in ""test statistics of LogicalRelation converted from Hive serde tables"" depends on the default configuration. New test case is shorter and covers both(true/false) cases explicitly.

This test case was modified by SPARK-17410 and SPARK-17284 in Spark 2.3.0.
- https://github.com/apache/spark/commit/a2460be9c30b67b9159fe339d115b84d53cc288a#diff-1c464c86b68c2d0b07e73b7354e74ce7R443",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 15 03:44:04 UTC 2017,,,,,,,,,,"0|i3la47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/17 03:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19500;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn on spark.sql.hive.convertMetastoreOrc by default,SPARK-22279,13109460,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Oct/17 18:48,10/May/18 05:38,14/Jul/23 06:30,10/May/18 05:38,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,SQL,,,,,0,,,,,,,,,"Like Parquet, this issue aims to turn on `spark.sql.hive.convertMetastoreOrc` by default.",,apachespark,cloud_fan,dongjoon,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,SPARK-24112,,,,,,,,,,,,,,,SPARK-19109,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 05:38:48 UTC 2018,,,,,,,,,,"0|i3la0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/17 18:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19499;;;","08/Dec/17 22:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19930;;;","07/Feb/18 19:39;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/20536;;;","07/Feb/18 20:51;dongjoon;This will be reverted.;;;","27/Apr/18 21:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21186;;;","10/May/18 05:38;cloud_fan;Issue resolved by pull request 21186
[https://github.com/apache/spark/pull/21186];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix key/value schema field names in HashMapGenerators.,SPARK-22273,13109200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,13/Oct/17 09:03,14/Oct/17 06:26,14/Jul/23 06:30,14/Oct/17 06:26,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"When fixing schema field names using escape characters with {{addReferenceMinorObj()}} at SPARK-18952, double-quotes around the names were remained and the names become something like {{""((java.lang.String) references\[1\])""}}.

{code}
/* 055 */     private int maxSteps = 2;
/* 056 */     private int numRows = 0;
/* 057 */     private org.apache.spark.sql.types.StructType keySchema = new org.apache.spark.sql.types.StructType().add(""((java.lang.String) references[1])"", org.apache.spark.sql.types.DataTypes.StringType);
/* 058 */     private org.apache.spark.sql.types.StructType valueSchema = new org.apache.spark.sql.types.StructType().add(""((java.lang.String) references[2])"", org.apache.spark.sql.types.DataTypes.LongType);
/* 059 */     private Object emptyVBase;
{code}

We should remove the double-quotes to refer the values in {{references}} properly:

{code}
/* 055 */     private int maxSteps = 2;
/* 056 */     private int numRows = 0;
/* 057 */     private org.apache.spark.sql.types.StructType keySchema = new org.apache.spark.sql.types.StructType().add(((java.lang.String) references[1]), org.apache.spark.sql.types.DataTypes.StringType);
/* 058 */     private org.apache.spark.sql.types.StructType valueSchema = new org.apache.spark.sql.types.StructType().add(((java.lang.String) references[2]), org.apache.spark.sql.types.DataTypes.LongType);
/* 059 */     private Object emptyVBase;
{code}
",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 13 09:14:03 UTC 2017,,,,,,,,,,"0|i3l8f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/17 09:14;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19491;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Describe results in ""null"" for the value of ""mean"" of a numeric variable",SPARK-22271,13109148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxing,shafiquejamal,shafiquejamal,13/Oct/17 05:15,17/Oct/17 19:54,14/Jul/23 06:30,17/Oct/17 19:54,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"Please excuse me if this issue was addressed already - I was unable to find it.

Calling .describe().show() on my dataframe results in a value of null for the row ""mean"":


{noformat}
val foo = spark.read.parquet(""decimalNumbers.parquet"")        
foo.select(col(""numericvariable"")).describe().show()

foo: org.apache.spark.sql.DataFrame = [numericvariable: decimal(38,32)]
+-------+--------------------+
|summary|     numericvariable|
+-------+--------------------+
|  count|                 299|
|   mean|                null|
| stddev|  0.2376438793946738|
|    min|0.037815489727642...|
|    max|2.138189366554511...|
{noformat}


But all of the rows for this seem ok (I can attache a parquet file). When I round the column, however, all is fine:


{noformat}
foo.select(bround(col(""numericvariable""), 31)).describe().show()


+-------+---------------------------+
|summary|bround(numericvariable, 31)|
+-------+---------------------------+
|  count|                        299|
|   mean|       0.139522503183236...|
| stddev|         0.2376438793946738|
|    min|       0.037815489727642...|
|    max|       2.138189366554511...|
+-------+---------------------------+

{noformat}

Rounding using 32 gives null also though.",,apachespark,huaxingao,kiszk,maropu,shafiquejamal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/17 06:01;shafiquejamal;decimalNumbers.zip;https://issues.apache.org/jira/secure/attachment/12891973/decimalNumbers.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 14 05:13:03 UTC 2017,,,,,,,,,,"0|i3l83j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/17 05:37;maropu;You need to give us the data and the schema, too?;;;","13/Oct/17 05:55;shafiquejamal;I'm happy to share the parquet file - how can I upload it? I don't see an option to upload a file (only to upload an image). Thanks!;;;","13/Oct/17 05:58;maropu;More->Attach Files? btw, text file (csv or something) is better, I think.;;;","13/Oct/17 06:03;shafiquejamal;Found it - thanks! I uploaded a zip of the parquet folder. I'm not sure whether I would be able to reproduce the problem with csv file. I'll try later though.;;;","13/Oct/17 22:31;huaxingao;I looked the code, in Average.scala, it has
{code}
  override lazy val evaluateExpression = child.dataType match {
    case DecimalType.Fixed(p, s) =>
      // increase the precision and scale to prevent precision loss
      val dt = DecimalType.bounded(p + 14, s + 4)
      Cast(Cast(sum, dt) / Cast(count, dt), resultType)
    ......
  }
{code}
When using Shafique's test data, dt has precision 38 and scale 36. count is 299. Cast(count, dt) will set the scale to 36 and precision to 39, this will cause overflow. 
I have a fix and will submit a PR soon. ;;;","14/Oct/17 05:13;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19496;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL incorrectly reads ORC file when column order is different,SPARK-22267,13109016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Oct/17 18:45,11/Dec/17 13:55,14/Jul/23 06:30,11/Dec/17 13:55,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"For a long time, Apache Spark SQL returns incorrect results when ORC file schema is different from metastore schema order.

{code}
scala> Seq(1 -> 2).toDF(""c1"", ""c2"").write.format(""parquet"").mode(""overwrite"").save(""/tmp/p"")
scala> Seq(1 -> 2).toDF(""c1"", ""c2"").write.format(""orc"").mode(""overwrite"").save(""/tmp/o"")
scala> sql(""CREATE EXTERNAL TABLE p(c2 INT, c1 INT) STORED AS parquet LOCATION '/tmp/p'"")
scala> sql(""CREATE EXTERNAL TABLE o(c2 INT, c1 INT) STORED AS orc LOCATION '/tmp/o'"")
scala> spark.table(""p"").show  // Parquet is good.
+---+---+
| c2| c1|
+---+---+
|  2|  1|
+---+---+
scala> spark.table(""o"").show    // This is wrong.
+---+---+
| c2| c1|
+---+---+
|  1|  2|
+---+---+
scala> spark.read.orc(""/tmp/o"").show  // This is correct.
+---+---+
| c1| c2|
+---+---+
|  1|  2|
+---+---+
{code}

*TESTCASE*
{code}
  test(""SPARK-22267 Spark SQL incorrectly reads ORC files when column order is different"") {
    withTempDir { dir =>
      val path = dir.getCanonicalPath

      Seq(1 -> 2).toDF(""c1"", ""c2"").write.format(""orc"").mode(""overwrite"").save(path)
      checkAnswer(spark.read.orc(path), Row(1, 2))

      Seq(""true"", ""false"").foreach { value =>
        withTable(""t"") {
          withSQLConf(HiveUtils.CONVERT_METASTORE_ORC.key -> value) {
            sql(s""CREATE EXTERNAL TABLE t(c2 INT, c1 INT) STORED AS ORC LOCATION '$path'"")
            checkAnswer(spark.table(""t""), Row(2, 1))
          }
        }
      }
    }
  }
{code}",,apachespark,cloud_fan,dongjoon,kiszk,mpetruska,original-brownbear,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 13:55:27 UTC 2017,,,,,,,,,,"0|i3l7fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/17 11:16;apachespark;User 'mpetruska' has created a pull request for this issue:
https://github.com/apache/spark/pull/19744;;;","14/Nov/17 12:21;cloud_fan;[~dongjoon] will this be fixed by the new orc reader?;;;","14/Nov/17 17:30;dongjoon;[~cloud_fan]. This issue comes from old Hive reader path.
I'll check the PR and email.;;;","15/Nov/17 10:58;mpetruska;Investigated the issue; unfortunately did not find any means to fix the issue without forcing CONVERT_METASTORE_ORC or changing the ORC reader implementation.
Maybe https://github.com/apache/spark/pull/19470 fixes the issue? [~dongjoon];;;","15/Nov/17 19:43;dongjoon;[~mpetruska]. I made this issue during that PR.
Please see my comment at that time, https://github.com/apache/spark/pull/19470#issuecomment-336230552 .
We need more effort on this issue later. For Spark 2.3, convertMetastoreOrc with new OrcFileFormat will be the best option for us.;;;","16/Nov/17 10:46;mpetruska;Hi [~dongjoon], I can confirm that reading from an ORC file works correctly with ""convertMetastoreOrc"" set (it causes to use the new OrcFileFormat) in 2.3.0-SNAPSHOT.;;;","16/Nov/17 17:29;dongjoon;Yep. It does, but there exist the following issues still.
- SPARK-15474: ORC data source fails to write and read back empty dataframe
- SPARK-21791: ORC should support column names with dot

Those will be resolved in the following PR by using new ORC 1.4 (SPARK-20682).
- https://github.com/apache/spark/pull/19651;;;","22/Nov/17 16:10;mpetruska;Hi [~dongjoon], I see that both issues mentioned are in progress.
Do you need some help or should I work on using `OrcFileFormat` even when ""convertMetastoreOrc"" is not set?;;;","22/Nov/17 23:26;dongjoon;[~mpetruska]. If you have a patch, please feel free to proceed `convertMetastoreOrc=false` case. Thanks.;;;","01/Dec/17 14:12;mpetruska;Hi [~dongjoon], sorry for the late reply. I do not have a patch for that case; my first idea would be to force the `convertMetastoreOrc=true` behaviour even when it is set to false. Is it viable? Is there a better solution?;;;","01/Dec/17 15:39;cloud_fan;just to confirm, is it a hive bug or a Spark only bug?;;;","01/Dec/17 16:28;mpetruska;All evidence suggests that this is a hive bug.
In https://github.com/apache/spark/pull/19744 I tried a couple of configurations/properties for the hive `OrcInputFormat` and `OrcSerde`; none of them had any effect, the data was always read in the order as written (and not in the order requested).;;;","08/Dec/17 11:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19928;;;","08/Dec/17 11:27;dongjoon;Hi, [~mpetruska] and [~cloud_fan].
After SPARK-22279, the default configuration doesn't have this bug. Although Hive 1.2.1 library code path still has the problem, we had better have a test coverage on what we have now in order to prevent future regression on it. So, I create a test case PR.
I believe most of users are not going to hit this issue since 2.3.;;;","11/Dec/17 13:55;cloud_fan;Issue resolved by pull request 19928
[https://github.com/apache/spark/pull/19928];;;",,,,,,,,,,,,,,,,,,,,,,,,
Reserve all non-deterministic expressions in ExpressionSet.,SPARK-22257,13108741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,11/Oct/17 22:27,13/Oct/17 05:47,14/Jul/23 06:30,13/Oct/17 05:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"For non-deterministic expressions, they should be considered as not contained in the [[ExpressionSet]].
 
This is consistent with how we define `semanticEquals` between two expressions.

Otherwise, combining expressions will remove non-deterministic expressions which should be reserved.
E.g
Combine filters of 
```
      testRelation.where(Rand(0) > 0.1).where(Rand(0) > 0.1)
```
should result in
```
      testRelation.where(Rand(0) > 0.1 && Rand(0) > 0.1)
```",,apachespark,Gengliang.Wang,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 11 22:33:03 UTC 2017,,,,,,,,,,"0|i3l5qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 22:33;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19475;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up the implementation of `growToSize` in CompactBuffer,SPARK-22254,13108678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,liufeng.ee@gmail.com,liufeng.ee@gmail.com,11/Oct/17 17:29,04/Nov/17 06:38,14/Jul/23 06:30,04/Nov/17 06:38,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"two issues:

1. the arrayMax should be `ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH `
2. I believe some `-2` were introduced because `Integer.Max_Value` was used previously. We should make the calculation of newArrayLen concise. ",,apachespark,kiszk,liufeng.ee@gmail.com,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 03 03:24:04 UTC 2017,,,,,,,,,,"0|i3l5cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/17 12:47;kiszk;If noone has started working for this, I will work for this.;;;","02/Nov/17 16:49;kiszk;I started working for this, and will submit a PR within a few days.;;;","03/Nov/17 03:24;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileFormatWriter should respect the input query schema,SPARK-22252,13108633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Oct/17 14:31,13/Oct/17 15:08,14/Jul/23 06:30,12/Oct/17 12:21,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,roczei,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 13 15:08:03 UTC 2017,,,,,,,,,,"0|i3l52n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 14:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19474;;;","12/Oct/17 12:21;cloud_fan;Issue resolved by pull request 19474
[https://github.com/apache/spark/pull/19474];;;","12/Oct/17 14:24;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19484;;;","13/Oct/17 15:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19493;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Metric ""aggregate time"" is incorrect when codegen is off",SPARK-22251,13108619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,11/Oct/17 14:08,12/Oct/17 15:07,14/Jul/23 06:30,12/Oct/17 15:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When whole-stage codegen is off, metric ""aggregate time"" is not set correctly.

Repro:
# spark.conf.set(""spark.sql.codegen.wholeStage"", false)
# spark.range(5).crossJoin(spark.range(5)).toDF(""a"",""b"").groupBy(""a"").agg(sum(""b"")).show
# In Spark UI > SQL you can see that ""aggregate time total"" is 0 in ""HashAggregate"" boxes",,ala.luszczak,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 11 14:24:04 UTC 2017,,,,,,,,,,"0|i3l4zr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 14:10;srowen;Did you try setting this before the app starts? I'm not actually sure it would take effect if you set it after SparkSession has initialized.;;;","11/Oct/17 14:16;ala.luszczak;I checked that if you type spark.conf.set(""spark.sql.codegen.wholeStage"", false) in Spark shell, it does take effect immediately.;;;","11/Oct/17 14:24;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/19473;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException: empty.reduceLeft when caching a dataframe,SPARK-22249,13108572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,asmaier,asmaier,11/Oct/17 11:59,12/Dec/22 18:10,14/Jul/23 06:30,17/Oct/17 07:44,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"It seems that the {{isin()}} method with an empty list as argument only works, if the dataframe is not cached. If it is cached, it results in an exception. To reproduce

{code:java}
$ pyspark
>>> df = spark.createDataFrame([pyspark.Row(KEY=""value"")])
>>> df.where(df[""KEY""].isin([])).show()
+---+
|KEY|
+---+
+---+

>>> df.cache()
DataFrame[KEY: string]
>>> df.where(df[""KEY""].isin([])).show()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/anaconda3/envs/<myenv>/lib/python3.6/site-packages/pyspark/sql/dataframe.py"", line 336, in show
    print(self._jdf.showString(n, 20))
  File ""/usr/local/anaconda3/envs/<myenv>/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
  File ""/usr/local/anaconda3/envs/<myenv>/lib/python3.6/site-packages/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/usr/local/anaconda3/envs/<myenv>/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o302.showString.
: java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:180)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:74)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1.applyOrElse(InMemoryTableScanExec.scala:107)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1.applyOrElse(InMemoryTableScanExec.scala:71)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2.apply(InMemoryTableScanExec.scala:112)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2.apply(InMemoryTableScanExec.scala:111)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.<init>(InMemoryTableScanExec.scala:111)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$$anonfun$3.apply(SparkStrategies.scala:307)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$$anonfun$3.apply(SparkStrategies.scala:307)
	at org.apache.spark.sql.execution.SparkPlanner.pruneFilterProject(SparkPlanner.scala:99)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$.apply(SparkStrategies.scala:303)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2832)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
{code}
","$ uname -a
Darwin MAC-UM-024.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64
$ pyspark --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.0
      /_/
                        
Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_92
Branch 
Compiled by user jenkins on 2017-06-30T22:58:04Z
Revision 
Url ",apachespark,asmaier,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19317,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 24 10:00:00 UTC 2017,,,,,,,,,,"0|i3l4pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/17 16:01;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19494;;;","16/Oct/17 05:04;gurwls223;Looks a duplicate of SPARK-19317 BTW. Both look failed by the same reason:

https://github.com/apache/spark/blob/v2.1.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L90 (2.1.0)

https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L107 (2.2.0)



;;;","17/Oct/17 07:44;srowen;Resolved by https://github.com/apache/spark/pull/19494;;;","17/Oct/17 21:48;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19522;;;","24/Oct/17 10:00;asmaier;Thank you for solving this issue so quickly. Not every open source project is reacting so fast. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming job failed to restart from checkpoint,SPARK-22243,13108488,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,desmoon,desmoon,desmoon,11/Oct/17 03:38,10/Nov/17 19:01,14/Jul/23 06:30,02/Nov/17 18:07,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,DStreams,,,,,0,,,,,,,,,"My spark-defaults.conf has an item related to the issue, I upload all jars in spark's jars folder to the hdfs path:
spark.yarn.jars  hdfs:///spark/cache/spark2.2/* 

Streaming job failed to restart from checkpoint, ApplicationMaster throws  ""Error: Could not find or load main class org.apache.spark.deploy.yarn.ExecutorLauncher"".  The problem is always reproducible.

I examine the sparkconf object recovered from checkpoint, and find spark.yarn.jars are set empty, which let all jars not exist in AM side. The solution is spark.yarn.jars should be reload from properties files when recovering from checkpoint. 

attach is a demo to reproduce the issue.",,apachespark,desmoon,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22242,,,,,,,,,,,,,,,,,,,,"11/Oct/17 03:40;desmoon;CheckpointTest.scala;https://issues.apache.org/jira/secure/attachment/12891416/CheckpointTest.scala",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 00:46:04 UTC 2017,,,,,,,,,,"0|i3l46v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 03:39;apachespark;User 'ChenjunZou' has created a pull request for this issue:
https://github.com/apache/spark/pull/19469;;;","11/Oct/17 03:41;desmoon;the reproducible demo;;;","11/Oct/17 11:45;srowen;I think this might be the same issue as https://issues.apache.org/jira/browse/SPARK-19688 -- [~jerryshao]?;;;","11/Oct/17 14:33;jerryshao;Yes, it is a related issue regarding to Spark Streaming checkpoint recovery, the configurations should be updated, rather than keeping the old reference.;;;","01/Nov/17 00:34;desmoon;reopen
this issue need to merge.;;;","02/Nov/17 00:46;apachespark;User 'ChenjunZou' has created a pull request for this issue:
https://github.com/apache/spark/pull/19637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EnsureStatefulOpPartitioning shouldn't ask for the child RDD before planning is completed,SPARK-22238,13108416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,10/Oct/17 22:06,16/Sep/18 01:56,14/Jul/23 06:30,15/Oct/17 00:39,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"In EnsureStatefulOpPartitioning, we check that the inputRDD to a SparkPlan has the expected partitioning for Streaming Stateful Operators. The problem is that we are not allowed to access this information during planning.

The reason we added that check was because CoalesceExec could actually create RDDs with 0 partitions. We should fix it such that when CoalesceExec says that there is a SinglePartition, there is in fact an inputRDD of 1 partition instead of 0 partitions.",,apachespark,brkyvz,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 15 00:39:50 UTC 2017,,,,,,,,,,"0|i3l3r3:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"10/Oct/17 22:08;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/19467;;;","15/Oct/17 00:39;tdas;Issue resolved by pull request 19467
[https://github.com/apache/spark/pull/19467];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
agg(last('attr)) gives weird results for streaming,SPARK-22230,13108084,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joseph.torres,joseph.torres,joseph.torres,09/Oct/17 20:54,10/Oct/17 00:05,14/Jul/23 06:30,10/Oct/17 00:04,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"In stream aggregation, last('attr) yields the last value from the first microbatch forever. I'm not sure if it's fair to call this a correctness bug, since last doesn't have strong correctness semantics, but ignoring all rows past the first microbatch is at least weird.

Simple repro in StreamingAggregationSuite:

    val input = MemoryStream[Int]

    val aggregated = input.toDF().agg(last('value))
    testStream(aggregated, OutputMode.Complete())(
      AddData(input, 1, 2, 3),
      CheckLastBatch(3),
      AddData(input, 4, 5, 6),
      CheckLastBatch(6) // actually yields 3 again",,apachespark,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 09 20:58:04 UTC 2017,,,,,,,,,,"0|i3l1q7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/17 20:58;apachespark;User 'joseph-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/19461;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DiskBlockManager.getAllBlocks could fail if called during shuffle,SPARK-22227,13108025,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lebedev,lebedev,lebedev,09/Oct/17 16:42,17/May/20 18:21,14/Jul/23 06:30,25/Oct/17 21:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Block Manager,Spark Core,,,,0,,,,,,,,,"{{DiskBlockManager.getAllBlocks}} assumes that the directories managed by the block manager only contains files corresponding to ""valid"" block IDs, i.e. those parsable via {{BlockId.apply}}. This is not always the case as demonstrated by the following snippet

{code}
object GetAllBlocksFailure {
  def main(args: Array[String]): Unit = {
    val sc = new SparkContext(new SparkConf()
        .setMaster(""local[*]"")
        .setAppName(""demo""))

    new Thread {
      override def run(): Unit = {
        while (true) {
          println(SparkEnv.get.blockManager.diskBlockManager.getAllBlocks().length)
          Thread.sleep(10)
        }
      }
    }.start()

    val rdd = sc.range(1, 65536, numSlices = 10)
        .map(x => (x % 4096, x))
        .persist(StorageLevel.DISK_ONLY)
        .reduceByKey { _ + _ }
        .collect()
  }
}
{code}

We have a thread computing the number of bytes occupied by the block manager on-disk and it frequently crashes due to this assumption being violated. Relevant part of the stacktrace

{code}
2017-10-06 11:20:14,287 ERROR  org.apache.spark.util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[CoarseGrainedExecutorBackend-stop-executor,5,main]
java.lang.IllegalStateException: Unrecognized BlockId: shuffle_1_2466_0.data.5684dd9e-9fa2-42f5-9dd2-051474e372be
        at org.apache.spark.storage.BlockId$.apply(BlockId.scala:133)
        at org.apache.spark.storage.DiskBlockManager$$anonfun$getAllBlocks$1.apply(DiskBlockManager.scala:103)
        at org.apache.spark.storage.DiskBlockManager$$anonfun$getAllBlocks$1.apply(DiskBlockManager.scala:103)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:73)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.storage.DiskBlockManager.getAllBlocks(DiskBlockManager.scala:103)
{code}",,apachespark,lebedev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 09 16:57:53 UTC 2017,,,,,,,,,,"0|i3l1db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/17 16:47;lebedev;Sidenote: the trace above is caused by the temporary file created by [SortShuffleWriter|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala#L69]. Sometimes we also saw failures containing {{TempShuffleBlockId}} names.;;;","09/Oct/17 16:57;apachespark;User 'superbobry' has created a pull request for this issue:
https://github.com/apache/spark/pull/19458;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Override toString of KeyValueGroupedDataset & RelationalGroupedDataset ,SPARK-22224,13107925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,09/Oct/17 08:56,17/Oct/17 10:04,14/Jul/23 06:30,17/Oct/17 10:00,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"
scala> val words = spark.read.textFile(""README.md"").flatMap(_.split("" ""))
words: org.apache.spark.sql.Dataset[String] = [value: string]

scala> val grouped = words.groupByKey(identity)
grouped: org.apache.spark.sql.KeyValueGroupedDataset[String,String] = org.apache.spark.sql.KeyValueGroupedDataset@65214862
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 10:00:07 UTC 2017,,,,,,,,,,"0|i3l0r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/17 08:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19363;;;","17/Oct/17 10:00;cloud_fan;Issue resolved by pull request 19363
[https://github.com/apache/spark/pull/19363];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectHashAggregate introduces unnecessary shuffle,SPARK-22223,13107896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mcs,mcs,09/Oct/17 06:13,17/May/20 17:58,14/Jul/23 06:30,16/Oct/17 05:48,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Optimizer,SQL,,,,0,,,,,,,,,"Since Spark 2.2 the {{groupBy}} plus {{collect_list}} makes use of unnecessary shuffle when the partitions at previous step are based on looser criteria than the current {{groupBy}}.

For example:

{code:java}
//sample data from https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data

//Read the data and repartitions by ""Country""
val retailDF = spark.sql(""Select * from online_retail"")
    .repartition(col(""Country""))

//Group the data and collect.
val aggregatedDF = retailDF
  .withColumn(""Good"", expr(""(StockCode, UnitPrice, Quantity, Description)""))
  .groupBy(""Country"", ""CustomerID"", ""InvoiceNo"", ""InvoiceDate"")
  .agg(collect_list(""Good"").as(""Goods""))
  .withColumn(""Invoice"", expr(""(InvoiceNo, InvoiceDate, Goods)""))
  .groupBy(""Country"", ""CustomerID"")
  .agg(collect_list(""Invoice"").as(""Invoices""))
  .withColumn(""Customer"", expr(""(CustomerID, Invoices)""))
  .groupBy(""Country"")
  .agg(collect_list(""Customer"").as(""Customers""))
{code}

Without disabling the {{ObjectHashAggregate}} one gets the following physical plan:

{noformat}
== Physical Plan ==
ObjectHashAggregate(keys=[Country#14], functions=[finalmerge_collect_list(merge buf#317) AS collect_list(Customer#299, 0, 0)#310])
+- Exchange hashpartitioning(Country#14, 200)
   +- ObjectHashAggregate(keys=[Country#14], functions=[partial_collect_list(Customer#299, 0, 0) AS buf#317])
      +- *Project [Country#14, named_struct(CustomerID, CustomerID#13, Invoices, Invoices#294) AS Customer#299]
         +- ObjectHashAggregate(keys=[Country#14, CustomerID#13], functions=[finalmerge_collect_list(merge buf#319) AS collect_list(Invoice#278, 0, 0)#293])
            +- Exchange hashpartitioning(Country#14, CustomerID#13, 200)
               +- ObjectHashAggregate(keys=[Country#14, CustomerID#13], functions=[partial_collect_list(Invoice#278, 0, 0) AS buf#319])
                  +- *Project [Country#14, CustomerID#13, named_struct(InvoiceNo, InvoiceNo#7, InvoiceDate, InvoiceDate#11, Goods, Goods#271) AS Invoice#278]
                     +- ObjectHashAggregate(keys=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[finalmerge_collect_list(merge buf#321) AS collect_list(Good#249, 0, 0)#270])
                        +- Exchange hashpartitioning(Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11, 200)
                           +- ObjectHashAggregate(keys=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[partial_collect_list(Good#249, 0, 0) AS buf#321])
                              +- *Project [InvoiceNo#7, InvoiceDate#11, CustomerID#13, Country#14, named_struct(StockCode, StockCode#8, UnitPrice, UnitPrice#12, Quantity, Quantity#10, Description, Description#9) AS Good#249]
                                 +- Exchange hashpartitioning(Country#14, 200)
                                    +- *FileScan csv default.online_retail[InvoiceNo#7,StockCode#8,Description#9,Quantity#10,InvoiceDate#11,UnitPrice#12,CustomerID#13,Country#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/scgc0grb1506404260438], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:string,InvoiceDate:string,Un...
{noformat}


With Spark 2.1.0 or when {{ObjectHashAggregate}} is disabled, one gets a more efficient:

{noformat}
== Physical Plan ==
SortAggregate(key=[Country#14], functions=[finalmerge_collect_list(merge buf#3834) AS collect_list(Customer#299, 0, 0)#310])
+- SortAggregate(key=[Country#14], functions=[partial_collect_list(Customer#299, 0, 0) AS buf#3834])
   +- *Project [Country#14, named_struct(CustomerID, CustomerID#13, Invoices, Invoices#294) AS Customer#299]
      +- SortAggregate(key=[Country#14, CustomerID#13], functions=[finalmerge_collect_list(merge buf#319) AS collect_list(Invoice#278, 0, 0)#293])
         +- SortAggregate(key=[Country#14, CustomerID#13], functions=[partial_collect_list(Invoice#278, 0, 0) AS buf#319])
            +- *Project [Country#14, CustomerID#13, named_struct(InvoiceNo, InvoiceNo#7, InvoiceDate, InvoiceDate#11, Goods, Goods#271) AS Invoice#278]
               +- SortAggregate(key=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[finalmerge_collect_list(merge buf#321) AS collect_list(Good#249, 0, 0)#270])
                  +- SortAggregate(key=[Country#14, CustomerID#13, InvoiceNo#7, InvoiceDate#11], functions=[partial_collect_list(Good#249, 0, 0) AS buf#321])
                     +- *Sort [Country#14 ASC NULLS FIRST, CustomerID#13 ASC NULLS FIRST, InvoiceNo#7 ASC NULLS FIRST, InvoiceDate#11 ASC NULLS FIRST], false, 0
                        +- *Project [InvoiceNo#7, InvoiceDate#11, CustomerID#13, Country#14, named_struct(StockCode, StockCode#8, UnitPrice, UnitPrice#12, Quantity, Quantity#10, Description, Description#9) AS Good#249]
                           +- Exchange hashpartitioning(Country#14, 200)
                              +- *FileScan csv default.online_retail[InvoiceNo#7,StockCode#8,Description#9,Quantity#10,InvoiceDate#11,UnitPrice#12,CustomerID#13,Country#14] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/scgc0grb1506404260438], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:string,StockCode:string,Description:string,Quantity:string,InvoiceDate:string,Un...
{noformat}

In this example, a quick run on DataBricks Notebook showed that by manually disabling the {{ObjectHashAggregate}} one gets around 16s execution time versus the 25s needed when {{ObjectHashAggregate}} is enabled.

The use of the {{ObjectHashAggregate}} in the {{groupBy}} was introduced with SPARK-17949.","Spark 2.2.0 and following.
{{spark.sql.execution.useObjectHashAggregateExec = true}}",apachespark,cloud_fan,kiszk,maropu,mcs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22276,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 16 05:48:56 UTC 2017,,,,,,,,,,"0|i3l0kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/17 00:05;maropu;The hash-based aggregate implementation requires the partitioning that you select in group-by clauses. In the looser case you suggested, it seems we need shuffles. Do I misunderstand your suggestion?;;;","11/Oct/17 05:52;mcs;[~maropu] Not sure I understand your observation.

In the hash-based aggregate implementation Spark does a repartitioning and a shuffle at each {{groupBy}}.

In the sort-based aggregate one, Spark does no repartition and no shuffle.

I would like to ignore whether Spark prefers sort-based or hash-based aggregation, but I do not want Spark to shuffle and repartition when there is no need for it.;;;","11/Oct/17 06:00;maropu;Probably, this ticket is related to https://issues.apache.org/jira/browse/SPARK-16026; the better strategy of aggregates depends on costs.;;;","15/Oct/17 06:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19501;;;","16/Oct/17 05:48;cloud_fan;Issue resolved by pull request 19501
[https://github.com/apache/spark/pull/19501];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the ARRAY_MAX in BufferHolder and add a test,SPARK-22222,13107891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fengliu@databricks.com,liufeng.ee@gmail.com,liufeng.ee@gmail.com,09/Oct/17 05:28,09/Apr/18 20:49,14/Jul/23 06:30,10/Oct/17 04:42,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"This is actually a followup for SPARK-22033, which set the `ARRAY_MAX` to `Int.MaxValue - 8`. It is not a valid number because it will cause the following line fail when such a large byte array is allocated: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/BufferHolder.java#L86 We need to make sure the new length is a multiple of 8.

We need to add one test for the fix. Note that the test should work independently with the heap size of the test JVM. ",,apachespark,kiszk,liufeng.ee@gmail.com,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22033,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 08 18:11:08 UTC 2017,,,,,,,,,,"0|i3l0jj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/17 06:56;srowen;Ah, I see. I think we'd have to round this down to Int.MaxValue - 15 in this case then, and any others introduced in that change where the array length would matter.;;;","09/Oct/17 20:05;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/19460;;;","08/Nov/17 18:11;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19697;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark shuffle services fails to update secret on application re-attempts,SPARK-22218,13107553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tgraves,tgraves,tgraves,06/Oct/17 16:26,17/May/20 18:13,14/Jul/23 06:30,09/Oct/17 19:57,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Shuffle,Spark Core,YARN,,,0,,,,,,,,,"Running on yarn, If you have any application re-attempts using the spark 2.2 shuffle service, the external shuffle service does not update the credentials properly and the application re-attempts fail with javax.security.sasl.SaslException. 

A bug was fixed in 2.2 (SPARK-21494) where it changed the ShuffleSecretManager to use containsKey (https://git.corp.yahoo.com/hadoop/spark/blob/yspark_2_2_0/common/network-shuffle/src/main/java/org/apache/spark/network/sasl/ShuffleSecretManager.java#L50) , which is the proper behavior, the problem is that between application re-attempts it never removes the key. So when the second attempt starts, the code says it already contains the key (since the application id is the same) and it doesn't update the secret properly.

to reproduce this you can run something like a word count and have the directory already existing.  The first attempt will fail because the output directory exists, the subsequent attempts will fail with max number of executor failures.   Note that this is assuming the second and third attempts run on the same node as the first attempt.",,apachespark,glenn.strycker@gmail.com,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 06 18:59:04 UTC 2017,,,,,,,,,,"0|i3kyyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/17 16:27;tgraves;Working on a patch for this.;;;","06/Oct/17 18:59;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/19450;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LimitPushDown optimization for FullOuterJoin generates wrong results,SPARK-22211,13107418,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,henryr,bewang.tech,bewang.tech,06/Oct/17 03:09,09/Nov/17 01:17,14/Jul/23 06:30,05/Nov/17 05:56,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"LimitPushDown pushes LocalLimit to one side for FullOuterJoin, but this may generate a wrong result:

Assume we use limit(1) and LocalLimit will be pushed to left side, and id=999 is selected, but at right side we have 100K rows including 999, the result will be
- one row is (999, 999)
- the rest rows are (null, xxx)

Once you call show(), the row (999,999) has only 1/100000th chance to be selected by CollectLimit.

The actual optimization might be, 
- push down limit
- but convert the join to Broadcast LeftOuterJoin or RightOuterJoin.

Here is my notebook:
https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/349451637617406/2750346983121008/6888856075277290/latest.html
{code:java}
import scala.util.Random._

val dl = shuffle(1 to 100000).toDF(""id"")
val dr = shuffle(1 to 100000).toDF(""id"")

println(""data frame dl:"")
dl.explain

println(""data frame dr:"")
dr.explain

val j = dl.join(dr, dl(""id"") === dr(""id""), ""outer"").limit(1)

j.explain

j.show(false)
{code}

{code}
data frame dl:
== Physical Plan ==
LocalTableScan [id#10]
data frame dr:
== Physical Plan ==
LocalTableScan [id#16]
== Physical Plan ==
CollectLimit 1
+- SortMergeJoin [id#10], [id#16], FullOuter
   :- *Sort [id#10 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(id#10, 200)
   :     +- *LocalLimit 1
   :        +- LocalTableScan [id#10]
   +- *Sort [id#16 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(id#16, 200)
         +- LocalTableScan [id#16]
import scala.util.Random._
dl: org.apache.spark.sql.DataFrame = [id: int]
dr: org.apache.spark.sql.DataFrame = [id: int]
j: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, id: int]

+----+---+
|id  |id |
+----+---+
|null|148|
+----+---+
{code}","on community.cloude.databrick.com 
Runtime Version 3.2 (includes Apache Spark 2.2.0, Scala 2.11)",apachespark,bewang.tech,dongjoon,henryr,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 09 01:17:04 UTC 2017,,,,,,,,,,"0|i3ky4v:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"06/Oct/17 05:36;maropu;Probably, the suggested solution does not work when both-side tables are small and a limit value is high. IMHO one option to solve this is just to stop the pushdown for full-outer joins (because I couldn't find better solutions to solve this smartly...).;;;","12/Oct/17 17:02;bewang.tech;I think my suggestion solution is correct.

|| Case || Left join key || Right join key || Full outer join ||
| 1 | Y | N | {{(left(\*), null)}} |
| 2 | Y | Y | {{(left(\*), right(\*))}} |
| 3 | N | Y | {{(null, right(\*))}} |
| 4 | N | N | Not applied |

If LimitPushDown pushes limit to the left side, whatever a limit value is and how big of left side table, you will always select some rows, in other words, the join keys are always exists, and only case 1 and 2 will happen, so it is actually a Left-join instead. It is equivalent to right-join when pushing down to  the right side.

The only problem of this method is: case 3 has no chance to be shown while pushing down to the left side, and case 1 for the right side. I would say this is not a big issue because we just want to see some samples of the join result, but the benefit is huge. If we want to see left-only or right-only, we might add where clause.  ;;;","13/Oct/17 00:26;maropu;Aha, I misunderstood. But, I think the case 3 is not acceptable (I know we can get big performance gains though...) because the transformation of relational expressions must not change results. IIUC, in the case 3, the push-down changes the result, right?;;;","28/Oct/17 00:08;henryr;I think the optimization proposed works only for a self-join. If you transform a FOJ into a one-sided outer join in general, you might omit rows from the result that should be there. 

Assume that the idea is to transform the full outer-join into a right outer-join, with the limit pushed to either the LHS or RHS. One problem comes if the limit is pushed to the RHS, but is larger than the number of rows in the RHS. For example, if you have tables T1 = (1,2) and T2 = (1), consider the following query:

{{SELECT * FROM T1 a FULL OUTER JOIN T2 b on a.col = b.col LIMIT 2}}.

If the limit is pushed into T2's scan (and the FOJ is changed to a ROJ), the query would emit only the tuple (1,1) - and omit (2,null) which should be included.

(If the limit is pushed into T1's scan there are other bugs if the limit is _less_ than the number of rows, by a similar argument).

I checked and I don't think Postgres tries to push limits into a FOJ either. Impala doesn't. 
;;;","30/Oct/17 21:59;henryr;Thinking about it a more, I think the optimization that's currently implemented works as long as a) the limit is pushed to the streaming side of the join and b) the physical join implementation guarantees that it will emit rows that have non-null RHSs from the streaming side before any that have a null RHS.

That is: say we've got a build-side of one row, (A,C), and a streaming-side of (A,B). If we do a full outer-join of these two inputs, the result should be some ordering of (A, A), (null, B), (C, null). If we do a FOJ with LIMIT 1 pushed to the streaming side, imagine it returns (B). It's an error if the join operator sees that A on the build-side has no match, and emits that (A, null) before it sees that B has no match and emits (null, B). But if it emits (null, B) first, the limit above it should kick in and no further rows will be emitted. 

It seems a bit fragile to rely on this behaviour from all join implementations, and it has some implications for other transformations (e.g. it would not be safe to flip the join order for a FOJ with a pushed-down limit - but would be ok for a non-pushed-down one). However, it's also a bit concerning to remove an optimization that's probably a big win for some queries, even if it's incorrect. There are rewrites that would work, e.g.:

{code}x.join(y, x('bar') === y('bar'), ""outer"").limit(10) ==> x.sort.limit(10).join(y.sort.limit(10), x('bar') === y('bar'), ""outer"").sort.limit(10) {code}

seems like it would be correct. But for now, how about we disable the push-down optimization in {{LimitPushDown}} and see if there's a need to investigate more complicated optimizations after that?;;;","03/Nov/17 00:09;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/19647;;;","03/Nov/17 06:05;smilegator;Will submit a PR based on my previous PR https://github.com/apache/spark/pull/10454;;;","03/Nov/17 15:35;henryr;[~smilegator] - sounds good! What will your approach be? I wasn't able to see a safe way to push the limit through the join without either a more invasive rewrite or restricting the set of join operators for FOJ. ;;;","03/Nov/17 15:54;smilegator;The Join operator should be limit aware. Anyway, we can do it later.;;;","03/Nov/17 16:05;srowen;In the name of correctness, still worth disabling for now, and then fixing later right?;;;","03/Nov/17 16:06;smilegator;We should merge it to the master and the previous releases at first.;;;","03/Nov/17 20:03;henryr;Sounds good, thanks both.;;;","09/Nov/17 00:58;dongjoon;Hi, All.
This breaks `branch-2.2`.
- SBT: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.2-test-sbt-hadoop-2.7/408/
- MAVEN: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-2.2-test-maven-hadoop-2.7/423;;;","09/Nov/17 01:17;apachespark;User 'henryr' has created a pull request for this issue:
https://github.com/apache/spark/pull/19701;;;",,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not recognize imports from submodules,SPARK-22209,13107346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,TV4Fun,TV4Fun,05/Oct/17 20:25,08/Nov/17 00:02,14/Jul/23 06:30,08/Nov/17 00:02,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"Using submodule syntax inside a PySpark job seems to create issues. For example, the following:

{code:python}
import scipy.sparse
from pyspark import SparkContext, SparkConf


def do_stuff(x):
    y = scipy.sparse.dok_matrix((1, 1))
    y[0, 0] = x
    return y[0, 0]


def init_context():
    conf = SparkConf().setAppName(""Spark Test"")
    sc = SparkContext(conf=conf)
    return sc


def main():
    sc = init_context()
    data = sc.parallelize([1, 2, 3, 4])
    output_data = data.map(do_stuff)
    print(output_data.collect())


__name__ == '__main__' and main()
{code}
produces this error:
{noformat}
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
        at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)
        at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:280)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:214)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/home/matt/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 177, in main
    process()
  File ""/home/matt/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py"", line 172, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/home/matt/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py"", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""/home/jcroteau/is/pel_selection/test_sparse.py"", line 6, in dostuff
    y = scipy.sparse.dok_matrix((1, 1))
AttributeError: module 'scipy' has no attribute 'sparse'

        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
        at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
        at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{noformat}
But if this is changed to:
{code:python}
import scipy.sparse as sp
from pyspark import SparkContext, SparkConf


def do_stuff(x):
    y = sp.dok_matrix((1, 1))
    y[0, 0] = x
    return y[0, 0]


def init_context():
    conf = SparkConf().setAppName(""Spark Test"")
    sc = SparkContext(conf=conf)
    return sc


def main():
    sc = init_context()
    data = sc.parallelize([1, 2, 3, 4])
    output_data = data.map(do_stuff)
    print(output_data.collect())


__name__ == '__main__' and main()
{code}
It works fine. At the very least, this should be documented. I've looked through the documentation, but I haven't found a mention of this anywhere.","Anaconda 4.4.0, Python 3.6, Hadoop 2.7, CDH 5.3.3, JDK 1.8, Centos 6",bryanc,TV4Fun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21753,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 08 00:02:27 UTC 2017,,,,,,,,,,"0|i3kxov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/17 21:30;bryanc;As a workaround, you could probably do the following

{code}
from scipy import sparse

def do_stuff(x):
    y = sparse.dok_matrix((1, 1))
    y[0, 0] = x
    return y[0, 0]
{code};;;","20/Oct/17 01:33;TV4Fun;Yes [~bryanc], I know I can do that. It's just annoying that a long workflow failed after running for quite a while because of that. At the very least, it should be documented.;;;","20/Oct/17 17:01;bryanc;It does seem like a bug to me so it should be fixed, I suspect it is in Sparks cloudpickle module.  Would you be able to submit a patch for this?  If not, I can try to take a look.  cc [~holden.karau@gmail.com];;;","23/Oct/17 21:57;TV4Fun;I don't really have time to look for the exact cause, so if you could take a look, that would be great. Thanks.;;;","25/Oct/17 19:17;bryanc;I tried the example with the latest master and did not get the error. There was a recent update to cloud pickle [here|https://github.com/apache/spark/commit/751f513367ae776c6d6815e1ce138078924872eb] from SPARK-21753 that I believe fixes this.  [~TV4Fun] could you try again with this patch and see if it solves your issue?;;;","08/Nov/17 00:02;bryanc;Resolving this as fixed upstream by SPARK-21753, feel free to re-open if issue persists.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gapply in R can't work on empty grouping columns,SPARK-22206,13107135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,05/Oct/17 05:13,12/Dec/22 18:10,14/Jul/23 06:30,05/Oct/17 14:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,SparkR,SQL,,,,0,,,,,,,,,"{{gapply}} in R invokes {{FlatMapGroupsInRExec}} in runtime, but {{FlatMapGroupsInRExec.requiredChildDistribution}} didn't consider empty grouping attributes. So {{gapply}} can't work on empty grouping columns.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 05 14:37:59 UTC 2017,,,,,,,,,,"0|i3kwen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/17 05:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19436;;;","05/Oct/17 14:37;gurwls223;Issue resolved by pull request 19436
[https://github.com/apache/spark/pull/19436];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refresh Table does not refresh the underlying tables of the persistent view,SPARK-22178,13106270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,01/Oct/17 03:21,08/Oct/17 19:12,14/Jul/23 06:30,08/Oct/17 19:12,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,{{Refresh Table}} command does not refresh the underlying tables for persistent views (but we do it for temp views),,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 08 19:12:21 UTC 2017,,,,,,,,,,"0|i3kr47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/17 03:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19405;;;","08/Oct/17 19:12;dongjoon;This is resolved via https://github.com/apache/spark/pull/19405 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.show(Int.MaxValue) hits integer overflows,SPARK-22176,13106224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,30/Sep/17 09:25,03/Oct/17 07:08,14/Jul/23 06:30,03/Oct/17 07:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"scala> Seq((1, 2), (3, 4)).toDF(""a"", ""b"").show(Int.MaxValue)
org.apache.spark.sql.AnalysisException: The limit expression must be equal to or greater than 0, but got -2147483648;;
GlobalLimit -2147483648
+- LocalLimit -2147483648
   +- Project [_1#27218 AS a#27221, _2#27219 AS b#27222]
      +- LocalRelation [_1#27218, _2#27219]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:41)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:89)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$checkLimitClause(CheckAnalysis.scala:70)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:234)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)

",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 04:58:47 UTC 2017,,,,,,,,,,"0|i3kqtz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/17 09:29;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/19401;;;","03/Oct/17 04:58;maropu;[~smilegator] close this? thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker hangs when the external shuffle service port is already in use,SPARK-22172,13106183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,devaraj,devaraj,devaraj,30/Sep/17 01:20,01/Nov/17 10:08,14/Jul/23 06:30,01/Nov/17 10:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"When the external shuffle service port is already in use, Worker throws the below BindException and hangs forever, I think the exception should be handled gracefully. 

{code:xml}
17/09/29 11:16:30 INFO ExternalShuffleService: Starting shuffle service on port 7337 (auth enabled = false)
17/09/29 11:16:30 ERROR Inbox: Ignoring error
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind0(Native Method)
        at sun.nio.ch.Net.bind(Net.java:433)
        at sun.nio.ch.Net.bind(Net.java:425)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
        at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:500)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1218)
        at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:495)
        at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:480)
        at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:965)
        at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:209)
        at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:355)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399)

{code}",,apachespark,devaraj,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 01 10:08:17 UTC 2017,,,,,,,,,,"0|i3kqkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/17 01:25;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/19396;;;","01/Nov/17 10:08;jerryshao;Issue resolved by pull request 19396
[https://github.com/apache/spark/pull/19396];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe Table Extended Failed when Table Owner is Empty,SPARK-22171,13106178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,30/Sep/17 00:14,04/Oct/17 04:31,14/Jul/23 06:30,04/Oct/17 04:31,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Users could hit `java.lang.NullPointerException` when the tables were created by Hive and the table's owner is `null` that are got from Hive metastore. `DESC EXTENDED` failed with the error:
{noformat}
SQLExecutionException: java.lang.NullPointerException at scala.collection.immutable.StringOps$.length$extension(StringOps.scala:47) at scala.collection.immutable.StringOps.length(StringOps.scala:47) at scala.collection.IndexedSeqOptimized$class.isEmpty(IndexedSeqOptimized.scala:27) at scala.collection.immutable.StringOps.isEmpty(StringOps.scala:29) at scala.collection.TraversableOnce$class.nonEmpty(TraversableOnce.scala:111) at scala.collection.immutable.StringOps.nonEmpty(StringOps.scala:29) at org.apache.spark.sql.catalyst.catalog.CatalogTable.toLinkedHashMap(interface.scala:300) at org.apache.spark.sql.execution.command.DescribeTableCommand.describeFormattedTableInfo(tables.scala:565) at org.apache.spark.sql.execution.command.DescribeTableCommand.run(tables.scala:543) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:66) at 
{noformat}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 30 00:16:04 UTC 2017,,,,,,,,,,"0|i3kqjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/17 00:16;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support byte length literal as identifier,SPARK-22169,13106079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,29/Sep/17 16:16,08/Oct/17 19:17,14/Jul/23 06:30,08/Oct/17 19:15,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 08 19:17:16 UTC 2017,,,,,,,,,,"0|i3kpxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/17 16:24;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19392;;;","08/Oct/17 19:15;dongjoon;This is resolved via https://github.com/apache/spark/pull/19392 .;;;","08/Oct/17 19:17;dongjoon;Hi, @cloud-fan and [~smilegator].
I saw the discussion on the PR about backporting, but I updated this with the status of as of today.
Please update the fix version after you guys backport it. Thanks!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Packaging w/R distro issues,SPARK-22167,13106068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,holden,holden,holden,29/Sep/17 15:09,03/Oct/17 16:49,14/Jul/23 06:30,02/Oct/17 18:57,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Build,SparkR,,,,0,,,,,,,,,"The Spark packaging for Spark R in 2.1.2 did not work as expected, namely the R directory was missing from the hadoop-2.7 bin distro. This is the version we build the PySpark package for so it's possible this is related.",,apachespark,felixcheung,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 16:49:08 UTC 2017,,,,,,,,,,"0|i3kpvb:",9223372036854775807,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,,,,,,,"29/Sep/17 15:13;holden;Here is the build log https://gist.github.com/holdenk/8d8bf00a0fc2186bdcf46e2c8748d365;;;","29/Sep/17 15:43;holden;So for some reason the R directory in the hadoop 2.7 build looks like:
holden@holden:~/repos/spark/spark-2.1.2-bin-hadoop2.7$ ls R
check-cran.sh  CRAN_RELEASE.md  create-docs.sh  DOCUMENTATION.md  install-dev.bat  install-dev.sh  log4j.properties  pkg  README.md  run-tests.sh  WINDOWS.md
holden@holden:~/repos/spark/spark-2.1.2-bin-hadoop2.7$ 

I think there is a race condition which only shows up on my laptop where the Spark directory is modified in one of the previous build steps before copying into the hadoop-2.7 version.
;;;","30/Sep/17 15:11;holden;So some more debugging, it does not appear to be a race condition. Rather during the normal build ../R/install-dev.sh is called from an execute step inside of core/pom.xml, however since we're in release mode we're running with zinc in the pwd of the spark release dir, not under core/, so the execute step triggers a level up.

cc [~felixcheung];;;","30/Sep/17 15:17;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19402;;;","30/Sep/17 17:26;felixcheung;I think I'd propose a change on this part of the release build to depends on (a subset of) check-cran output instead, but thinking more about this the -Psparkr is more for developer and should be left as-is.

The issue is the output of install-dev is not really a release format, and I guess this has been the de facto release form we have for a very long time.

But this could be a separate follow up for 2.2.1/2.3.

;;;","03/Oct/17 06:15;holden;I agree we could improve this, I think though that swapping install-dev for check-cran subset probably belongs in 2.3 instead of a minor patch release (and also if that's going to be the case check-cran should probably be renamed).;;;","03/Oct/17 16:49;felixcheung;There are likely 2 stages to this.
More pressing might be the fact that hadoop-2.6 and hadoop-2.7 release tgz have fairly different content because of how the make-release script is structured.
I will open a new JIRA on this.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Type conflicts between dates, timestamps and date in partition column",SPARK-22165,13105961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,29/Sep/17 05:27,12/Dec/22 18:10,14/Jul/23 06:30,21/Nov/17 19:54,2.1.1,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,release-notes,,,,,,,,"It looks we have some bugs when resolving type conflicts in partition column. I found few corner cases as below:

Case 1: timestamp should be inferred but date type is inferred.

{code}
val df = Seq((1, ""2015-01-01""), (2, ""2016-01-01 00:00:00"")).toDF(""i"", ""ts"")
df.write.format(""parquet"").partitionBy(""ts"").save(""/tmp/foo"")
spark.read.load(""/tmp/foo"").printSchema()
{code}

{code}
root
 |-- i: integer (nullable = true)
 |-- ts: date (nullable = true)
{code}

Case 2: decimal should be inferred but integer is inferred.

{code}
val df = Seq((1, ""1""), (2, ""1"" * 30)).toDF(""i"", ""decimal"")
df.write.format(""parquet"").partitionBy(""decimal"").save(""/tmp/bar"")
spark.read.load(""/tmp/bar"").printSchema()
{code}

{code}
root
 |-- i: integer (nullable = true)
 |-- decimal: integer (nullable = true)
{code}

Looks we should de-duplicate type resolution logic if possible rather than separate numeric precedence-like comparison alone.",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 23 02:14:32 UTC 2017,,,,,,,,,,"0|i3kp7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/17 05:49;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19389;;;","21/Nov/17 19:54;cloud_fan;Issue resolved by pull request 19389
[https://github.com/apache/spark/pull/19389];;;","23/Nov/17 02:14;gurwls223;[~cloud_fan], BTW, should we maybe leave a release-notes tag too?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors and the driver use inconsistent Job IDs during the new RDD commit protocol,SPARK-22162,13105892,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rezasafi,rezasafi,rezasafi,28/Sep/17 22:41,05/Dec/17 17:16,14/Jul/23 06:30,04/Dec/17 17:25,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.2,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"After SPARK-18191 commit in pull request 15769, using the new commit protocol it is possible that driver and executors uses different jobIds during a rdd commit.
In the old code, the variable stageId is part of the closure used to define the task as you can see here:
 [https://github.com/apache/spark/blob/9c8deef64efee20a0ddc9b612f90e77c80aede60/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L1098]
As a result, a TaskAttemptId is constructed in executors using the same ""stageId"" as the driver, since it is a value that is serialized in the driver. Also the value of stageID is actually the rdd.id which is assigned here: [https://github.com/apache/spark/blob/9c8deef64efee20a0ddc9b612f90e77c80aede60/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L1084]
However, after the change in pull request 15769, the value is no longer part of the task closure, which gets serialized by the driver. Instead, it is pulled from the taskContext as you can see here:[https://github.com/apache/spark/pull/15769/files#diff-dff185cb90c666bce445e3212a21d765R103]
and then that value is used to construct the TaskAttemptId on the executors: [https://github.com/apache/spark/pull/15769/files#diff-dff185cb90c666bce445e3212a21d765R134]
taskContext has a stageID value which will be set in DAGScheduler. So after the change unlike the old code which a rdd.id was used, an actual stage.id is used which can be different between executors and the driver since it is no longer serialized.
In summary, the old code consistently used rddId, and just incorrectly named it ""stageId"".
The new code uses a mix of rddId and stageId. There should be a consistent ID between executors and the drivers.",,apachespark,rezasafi,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 00:09:03 UTC 2017,,,,,,,,,,"0|i3kosf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/17 22:41;rezasafi;I will send a pull request shortly for this issue.;;;","28/Sep/17 23:26;apachespark;User 'rezasafi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19388;;;","29/Nov/17 17:21;apachespark;User 'rezasafi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19848;;;","05/Dec/17 00:09;apachespark;User 'rezasafi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19886;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.sql.execution.arrow.enable and spark.sql.codegen.aggregate.map.twolevel.enable -> enabled,SPARK-22159,13105829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,28/Sep/17 18:42,10/Oct/17 02:37,14/Jul/23 06:30,06/Oct/17 03:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"We should make the config names consistent. They are supposed to end with "".enabled"", rather than ""enable"".
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 10 02:37:04 UTC 2017,,,,,,,,,,"0|i1u552:z",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"28/Sep/17 18:44;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19384;;;","10/Oct/17 02:37;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19462;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convertMetastore should not ignore storage properties,SPARK-22158,13105807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,28/Sep/17 17:23,09/Jul/19 22:45,14/Jul/23 06:30,03/Oct/17 18:43,2.0.0,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"From the beginning, convertMetastoreOrc ignores table properties and use an emtpy map instead. It's the same with convertMetastoreParquet.

{code}
val options = Map[String, String]()
{code}

- SPARK-14070: https://github.com/apache/spark/pull/11891/files#diff-ee66e11b56c21364760a5ed2b783f863R650
- master: https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala#L197",,apachespark,dongjoon,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,SPARK-22926,SPARK-23355,,,,,,,,,,,SPARK-17166,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 09 22:45:58 UTC 2019,,,,,,,,,,"0|i3ko9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/17 17:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19382;;;","03/Oct/17 03:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19417;;;","06/Feb/18 20:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/20522;;;","09/Jul/19 17:51;Tagar;[~dongjoon] can you please check if PR-20522 causes SPARK-28266 data correctness regression?

Thank you.;;;","09/Jul/19 22:16;dongjoon;[~Tagar]. This is not related to that because that is reported at 2.2.0 and this is merged to 2.2.1. :);;;","09/Jul/19 22:45;Tagar;[~dongjoon] I may have misreported it - sorry. 

[~waleedfateem] made some tests, I thought 2.2.0 is affected as well, but you're probably right that 2.2.1 is the first one affected.
Cloudera has pointed to this Jira.

Thank you. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PYTHONPATH not picked up from the spark.yarn.appMasterEnv properly,SPARK-22151,13105585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pgandhi,tgraves,tgraves,27/Sep/17 22:02,17/May/20 18:13,14/Jul/23 06:30,19/Jul/18 14:30,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"Running in yarn cluster mode and trying to set pythonpath via spark.yarn.appMasterEnv.PYTHONPATH doesn't work.

the yarn Client code looks at the env variables:
val pythonPathStr = (sys.env.get(""PYTHONPATH"") ++ pythonPath)
But when you set spark.yarn.appMasterEnv it puts it into the local env. 

So the python path set in spark.yarn.appMasterEnv isn't properly set.

You can work around if you are running in cluster mode by setting it on the client like:

PYTHONPATH=./addon/python/ spark-submit",,apachespark,bryanc,jerryshao,Tagar,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 19 14:40:28 UTC 2018,,,,,,,,,,"0|i3kmwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/17 02:27;jerryshao;Checking the Yarn client code, looks like there's specific code to handle {{PYTHONPATH}}, but not for {{spark.yarn.appMasterEnv.PYTHONPATH}}.;;;","31/May/18 14:42;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/21468;;;","19/Jul/18 14:31;tgraves;[~srowen] why did you blank out fixed version and transition back to in progress?;;;","19/Jul/18 14:37;srowen;It wasn't marked as Resolved, but had a Fix version. I think you just Resolved it. My Jira client reports just flag stuff like this. Usually it's because a reporter added Fix version improperly but here looks like the PR had been resolved.;;;","19/Jul/18 14:40;tgraves;ok thanks, must have missed that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskSetManager.abortIfCompletelyBlacklisted should not abort when all current executors are blacklisted but dynamic allocation is enabled,SPARK-22148,13105515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Dhruve Ashar,juanrh,juanrh,27/Sep/17 17:28,13/Apr/20 23:22,14/Jul/23 06:30,06/Nov/18 14:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.1,3.0.0,,,,Scheduler,Spark Core,,,,1,,,,,,,,,"Currently TaskSetManager.abortIfCompletelyBlacklisted aborts the TaskSet and the whole Spark job with `task X (partition Y) cannot run anywhere due to node and executor blacklist. Blacklisting behavior can be configured via spark.blacklist.*.` when all the available executors are blacklisted for a pending Task or TaskSet. This makes sense for static allocation, where the set of executors is fixed for the duration of the application, but this might lead to unnecessary job failures when dynamic allocation is enabled. For example, in a Spark application with a single job at a time, when a node fails at the end of a stage attempt, all other executors will complete their tasks, but the tasks running in the executors of the failing node will be pending. Spark will keep waiting for those tasks for 2 minutes by default (spark.network.timeout) until the heartbeat timeout is triggered, and then it will blacklist those executors for that stage. At that point in time, other executors would had been released after being idle for 1 minute by default (spark.dynamicAllocation.executorIdleTimeout), because the next stage hasn't started yet and so there are no more tasks available (assuming the default of spark.speculation = false). So Spark will fail because the only executors available are blacklisted for that stage. 

An alternative is requesting more executors to the cluster manager in this situation. This could be retried a configurable number of times after a configurable wait time between request attempts, so if the cluster manager fails to provide a suitable executor then the job is aborted like in the previous case. ",,apachespark,beettlle,devaraj,Dhruve Ashar,glenn.strycker@gmail.com,irashid,jerryshao,tgraves,uditme,vsowrirajan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15815,,,SPARK-31418,,,SPARK-15815,SPARK-24413,,,,,,,,,,,,,"24/Oct/17 17:16;juanrh;SPARK-22148_WIP.diff;https://issues.apache.org/jira/secure/attachment/12893769/SPARK-22148_WIP.diff",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 23:22:39 UTC 2020,,,,,,,,,,"0|i3kmhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/17 17:16;juanrh;Hi, 

I've been working on this issue, and I would like to get your feedback on the following approach. The idea is that instead of failing in `TaskSetManager.abortIfCompletelyBlacklisted`, when a task cannot be scheduled in any executor but dynamic allocation is enabled, we will register this task with `ExecutorAllocationManager`. Then `ExecutorAllocationManager` will request additional executors for these ""unscheduleable tasks"" by increasing the value returned in `ExecutorAllocationManager.maxNumExecutorsNeeded`. This way we are counting these tasks twice, but this makes sense because the current executors don't have any slot for these tasks, so we actually want to get new executors that are able to run these tasks. To avoid a deadlock due to tasks being unscheduleable forever, we store the timestamp when a task was registered as unscheduleable, and in `ExecutorAllocationManager.schedule` we abort the application if there is some task that has been unscheduleable for a configurable age threshold. This way we give an opportunity to dynamic allocation to get more executors that are able to run the tasks, but we don't make the application wait forever. 

Attached is a patch with a draft for this approach. Looking forward to your feedback on this. ;;;","27/Oct/17 22:04;apachespark;User 'juanrh' has created a pull request for this issue:
https://github.com/apache/spark/pull/19590;;;","27/Oct/17 22:32;irashid;Hi [~juanrh], thanks for filing this and the PR.  is this SPARK-15815?? though the initial summary & description aren't as succinct as your, if you follow the discussion its certainly related.  Perhaps its distinct because I think you are talking about something more particular to dynamic allocation.

anyway I haven't taken a more detailed look yet, just wanted to you point to the other issue.
;;;","30/Oct/17 17:29;juanrh;Hi [~irashid]. This looks like a different problem, because this issue is about a crash due to job aborted because there is no place to schedule a task, and SPARK-15815 is about a hang. But I have seen hangs similar to the one described in SPARK-15815 in the past, also related to dynamic allocation, so it looks like the root cause could be related. \

My proposal is similar to some of the ideas you outline in SPARK-15815. The main difference is that I don't suggest killing an executor, but requesting more executors to the resource manager. The result is similar, but your approach would work even if no more capacity is available. On the other hand my approach won't kill an executor that is progressing in other tasks. However my approach won't work if 1) there are no more executors available in the cluster, and 2) the executor timeout if very long, or executors are caching RDDs and the default timeout of infinite, as I was expecting to cover the case of no more capacity available by assuming an executor will eventually become idle. Killing an executor has no terrible consequences because with dynamic allocation we probably have external shuffle, so I think the approach you propose in SPARK-15815 is a better alternative. ;;;","14/Jun/18 14:10;irashid;[~tgraves] we might be able to work on this soon -- a week or two out at least, though.  I know you mentioned some interest in looking at this too, so please let us know if you want to take it up.;;;","14/Jun/18 16:13;tgraves;ok, just update if you start working on it. thanks.;;;","30/Aug/18 18:24;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/22288;;;","07/Apr/20 21:59;vsowrirajan;[~irashid][~Dhruve Ashar] Recently we have enabled blacklisting in our platform and it works nicely mostly. We also have this fix where there are no executors to retry due to blacklisting (mainly with dynamic allocation enabled and happens during the tail end of the stage). 

I also went through the fix and in general blacklisting code. Although it still happens, where all the other executors are busy and no idle blacklisted executor left to kill and request a new executor which causes the stage and eventually the job to be aborted before all the retries. 

Do you guys also see this behavior or have this issue? Do you think requesting a new executor in general would help rather than trying to kill a blacklisted idle executor?;;;","07/Apr/20 22:24;tgraves;I'm not sure I follow what you are saying.  Are you just saying even with this change, you still see the behavior that your job is aborted?  This PR is a heuristic which makes it better in some cases but it still might hit that condition.

You say "" where all the other executors are busy and no idle blacklisted executor left to kill"".   I'm not sure what that means.I assume it already killed some and if there aren't any left to kill, is it just taking a long time to acquire more from yarn?  If not please give more detail;;;","07/Apr/20 23:57;vsowrirajan;Thanks for responding [~tgraves]. Thats right. 

Lets say all the executors are busy with some task and one of the task fails, then we are aborting the stage as there is no idle blacklisted executor available to kill and replace. But with dynamic allocation enabled, we could have requested for more executors and retried the task.

Infact, I can reproduce this with min executors set to 1 and max to some number. In this case, it wouldn't scale up immediately and the first task fails the whole stage because the only executor available is blacklisted for the task and also busy running other task at that time.

// Though this example would fail as casting an int to string is not valid. Just for example purposes.

{code:java}
def test(a: Int) = { a.asInstanceOf[String] }
sc.parallelize(1 to 10, 10).map(x => test(x)).collect 
{code}


Although if there are more executors, then its retried. Similar other cases are possible ;;;","08/Apr/20 13:29;tgraves;so off the top of my head, I think the main issue with just requesting more is that the dynamic allocation manager isn't tied very tightly to the scheduler or the blacklist tracker, so getting the information required to properly track why we have more executors then needed took quite a bit more work and code refactoring.  If you are still seeing issues regularly though we could revisit to see if we could either request more or perhaps kill executors that are blacklisted that aren't completely idle.  But I would have to re-read through these and think about it more.  If you have ideas feel free to propose, though we should do it under a new Jira and link them ;;;","08/Apr/20 17:35;vsowrirajan;Thanks for your comments [~tgraves] Makes sense, I will think about it more, create a new JIRA and share a new proposal based on how we think about it internally.;;;","13/Apr/20 23:22;xkrogen;For future folks: the JIRA created for the issue is SPARK-31418 and discussion is continuing there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
FileNotFoundException while reading ORC files containing '%',SPARK-22146,13105427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,27/Sep/17 13:10,02/Dec/17 10:49,14/Jul/23 06:30,29/Sep/17 06:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"Reading ORC files containing ""strange"" characters like '%' fails with a FileNotFoundException.

For instance, if you have:

{noformat}
/tmp/orc_test/folder %3Aa/orc1.orc
/tmp/orc_test/folder %3Ab/orc2.orc
{noformat}

and you try to read the ORC files with:


{noformat}
spark.read.format(""orc"").load(""/tmp/orc_test/*/*"").show
{noformat}

you will get a:

{noformat}
java.io.FileNotFoundException: File file:/tmp/orc_test/folder%20%253Aa/orc1.orc does not exist
  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
  at org.apache.spark.deploy.SparkHadoopUtil.listLeafStatuses(SparkHadoopUtil.scala:194)
  at org.apache.spark.sql.hive.orc.OrcFileOperator$.listOrcFiles(OrcFileOperator.scala:94)
  at org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:67)
  at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)
  at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
  at org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:77)
  at org.apache.spark.sql.hive.orc.OrcFileFormat.inferSchema(OrcFileFormat.scala:60)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:197)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:197)
  at scala.Option.orElse(Option.scala:289)
  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:196)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:387)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:168)
  ... 48 elided
{noformat}

Note that the same code works for Parquet and text files.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 15:39:04 UTC 2017,,,,,,,,,,"0|i3klyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/17 13:16;srowen;Does the file actually exist locally, or is it meant to be on HDFS?;;;","27/Sep/17 14:29;mgaido;If you look carefully at the file which Spark is looking for, you'll see that it doesn't exist because it is the result of a improper encoding.
So, yes, the right file exists. But Spark is looking for a wrong one.
We tried both on HDFS and on the local filesystem, the error is the same, and it is due to the encoding of the path in the inferSchema process. I am preparing a PR to fix it. I will post it as soon as it is ready.;;;","27/Sep/17 14:42;srowen;No, ""file:/tmp/orc_test/folder%20%253Aa/orc1.orc"" is the correct URI for the file you mentioned -- if it's a local file.
If this file exists locally on the driver machine, but you're trying to find it from a distributed operation, it won't work. I'm trying to figure out whether this is all there is to the issue.;;;","27/Sep/17 14:58;mgaido;Yes, that is a local file and I am running `spark-shell` locally on my machine from the current master.;;;","27/Sep/17 15:22;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19368;;;","29/Sep/17 15:39;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/19391;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues with driver re-starting on mesos (supervise),SPARK-22145,13105386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,27/Sep/17 11:12,02/Nov/17 13:26,14/Jul/23 06:30,02/Nov/17 13:26,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Mesos,,,,,0,,,,,,,,,"There are two issues with driver re-starting on mesos using the supervise flag:
- We need to add spark.mesos.driver.frameworkId to the reloaded properties for checkpointing, otherwise the new frameworkId propagated by the dispatcher will be overwritten by the checkpointed data.
- Unique driver task ids are not used by the dispatcher:
https://issues.apache.org/jira/browse/MESOS-4737
https://issues.apache.org/jira/browse/MESOS-3070
This issue is the same in principle as in the case with standalone mode where the master needs to re-launch drivers with a new appId (driverId) to deal with net partitions.

",,apachespark,devaraj,skonto,susanxhuynh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 02 13:26:02 UTC 2017,,,,,,,,,,"0|i3klpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/17 11:16;skonto;[~susanxhuynh][~arand] pls take a look. I will make a PR shortly.;;;","27/Sep/17 22:15;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/19374;;;","02/Nov/17 13:26;srowen;Issue resolved by pull request 19374
[https://github.com/apache/spark/pull/19374];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffHeapColumnVector may leak memory,SPARK-22143,13105339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,27/Sep/17 08:47,28/Sep/17 15:53,14/Jul/23 06:30,27/Sep/17 21:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,ColumnVector does not clean-up its children on close. This means that we are sure to leak memory when we are using OffHeapColumnVectors.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 28 08:50:04 UTC 2017,,,,,,,,,,"0|i3klev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/17 14:03;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/19367;;;","28/Sep/17 08:50;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/19378;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate empty relation before checking Cartesian products,SPARK-22141,13105330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,27/Sep/17 08:00,28/Sep/17 09:29,14/Jul/23 06:30,27/Sep/17 10:45,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"When inferring constraints from children, Join's condition can be simplified as None.
For example,
    val testRelation = LocalRelation('a.int)
    val x = testRelation.as(""x"")
    val y = testRelation.where($""a"" === 2 && !($""a"" === 2)).as(""y"")
    x.join(y).where($""x.a"" === $""y.a"")

The plan will become

Join Inner
:- LocalRelation <empty>, [a#23]
+- LocalRelation <empty>, [a#224]

And the Cartesian products check will throw exception.
Propagate empty relation before checking Cartesian products, and the issue is resolved.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 28 09:29:04 UTC 2017,,,,,,,,,,"0|i3kld3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/17 08:06;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19362;;;","27/Sep/17 11:53;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19366;;;","28/Sep/17 09:29;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
metrics in spark-dispatcher not being registered properly,SPARK-22135,13105254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adobe_pmackles,adobe_pmackles,adobe_pmackles,26/Sep/17 22:11,28/Sep/17 06:54,14/Jul/23 06:30,28/Sep/17 06:52,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Deploy,Mesos,,,,0,,,,,,,,,"There is a bug in the way that the metrics in org.apache.spark.scheduler.cluster.mesos.MesosClusterSchedulerSource are initialized such that they are never registered with the underlying registry. Basically, each call to the overridden ""metricRegistry"" function results in the creation of a new registry. PR is forthcoming.",,adobe_pmackles,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 28 06:52:25 UTC 2017,,,,,,,,,,"0|i3kkwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/17 00:51;adobe_pmackles;here is the PR: https://github.com/apache/spark/pull/19358;;;","27/Sep/17 00:52;apachespark;User 'pmackles' has created a pull request for this issue:
https://github.com/apache/spark/pull/19358;;;","28/Sep/17 06:52;jerryshao;Issue resolved by pull request 19358
[https://github.com/apache/spark/pull/19358];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark release scripts ignore the GPG_KEY and always sign with your default key,SPARK-22129,13105124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,holden,holden,holden,26/Sep/17 15:03,29/Sep/17 15:05,14/Jul/23 06:30,29/Sep/17 15:05,2.2.1,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Build,,,,,0,,,,,,,,,Currently the release scripts require GPG_KEY be specified but the param is ignored and instead the default GPG key is used. Change this to sign with the specified key.,,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 15:05:01 UTC 2017,,,,,,,,,,"0|i3kk3j:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"27/Sep/17 03:02;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19359;;;","29/Sep/17 15:05;holden;Issue resolved by pull request 19359
[https://github.com/apache/spark/pull/19359];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading tables partitioned by columns that look like timestamps has inconsistent schema inference,SPARK-22109,13104416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,irashid,irashid,22/Sep/17 21:40,12/Dec/22 17:35,14/Jul/23 06:30,23/Sep/17 15:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,1,,,,,,,,,"If you try to read a partitioned json table, spark automatically tries to read figure out if the partition column is a timestamp based on the first value it sees.  So if you really partitioned by a string, and the first value happens to look like a timestamp, then you'll run into errors.  Even if you specify a schema, the schema is ignored, and spark still tries to infer a timestamp type for the partition column.

This is particularly weird because schema-inference does *not* work for regular timestamp columns in a flat table.  You have to manually specify the schema to get the column interpreted as a timestamp.

This problem does not appear to be present for other types.  Eg., if I partition by a string column, and the first value happens to look like an int, schema inference is still fine.

Here's a small example:

{noformat}
val df = Seq(
  (1, ""2015-01-01 00:00:00"", Timestamp.valueOf(""2015-01-01 00:00:00"")),
  (2, ""2014-01-01 00:00:00"", Timestamp.valueOf(""2014-01-01 00:00:00"")),
  (3, ""blah"", Timestamp.valueOf(""2016-01-01 00:00:00""))).toDF(""i"", ""str"", ""t"")


df.write.partitionBy(""str"").json(""partition_by_str"")
df.write.partitionBy(""t"").json(""partition_by_t"")
df.write.json(""flat"")

val readStr = spark.read.json(""partition_by_str"")/*
java.util.NoSuchElementException: None.get
  at scala.None$.get(Option.scala:347)
  at scala.None$.get(Option.scala:345)
  at org.apache.spark.sql.catalyst.expressions.TimeZoneAwareExpression$class.timeZone(datetimeExpressions.scala:46)
  at org.apache.spark.sql.catalyst.expressions.Cast.timeZone$lzycompute(Cast.scala:172)
  at org.apache.spark.sql.catalyst.expressions.Cast.timeZone(Cast.scala:172)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$3$$anonfun$apply$16.apply(Cast.scala:208)  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$3$$anonfun$apply$16.apply(Cast.scala:208)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$buildCast(Cast.scala:201)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToString$3.apply(Cast.scala:207)
  at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:533)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:327)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningUtils$$resolveTypeConflicts$1.apply(PartitioningUtils.scala:485)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$org$apache$spark$sql$execution$datasources$PartitioningUtils$$resolveTypeConflicts$1.apply(PartitioningUtils.scala:484)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$.org$apache$spark$sql$execution$datasources$PartitioningUtils$$resolveTypeConflicts(PartitioningUtils.scala:484)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$15.apply(PartitioningUtils.scala:340)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$15.apply(PartitioningUtils.scala:339)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.Range.foreach(Range.scala:160)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:339)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:141)
  at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:97)
  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:153)
  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:70)
  at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)
  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:133)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:333)
  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:279)
  ... 48 elided
*/

val readStr = spark.read.schema(df.schema).json(""partition_by_str"")
/*
same exception
*/

val readT = spark.read.json(""partition_by_t"") // OK
val readT = spark.read.schema(df.schema).json(""partition_by_t"") // OK

val readFlat = spark.read.json(""flat"") // NO error, by timestamp column is read a String
val readFlat = spark.read.schema(df.schema).json(""flat"") // OK
{noformat}",,apachespark,dongjoon,irashid,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 23 15:32:03 UTC 2017,,,,,,,,,,"0|i3kfqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/17 10:32;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19331;;;","23/Sep/17 15:12;ueshin;Issue resolved by pull request 19331
https://github.com/apache/spark/pull/19331;;;","23/Sep/17 15:32;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""as"" should be ""alias"" in python quick start documentation",SPARK-22107,13104383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,epugachev,epugachev,epugachev,22/Sep/17 20:00,12/Dec/22 18:10,14/Jul/23 06:30,25/Sep/17 00:18,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,,,,,0,,,,,,,,,"One piece of example python code in quick-start.md reads

{code}>>> wordCounts = textFile.select(explode(split(textFile.value, ""\s+"")).as(""word"")).groupBy(""word"").count(){code}

This does not execute and presumably should be {code}alias(""word""){code} instead: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode",,apachespark,epugachev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 25 00:18:19 UTC 2017,,,,,,,,,,"0|i3kfjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 20:47;apachespark;User 'jgoleary' has created a pull request for this issue:
https://github.com/apache/spark/pull/19326;;;","25/Sep/17 00:18;gurwls223;Fixed in https://github.com/apache/spark/pull/19326;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Request an accurate memory after we unrolled the block,SPARK-22097,13104153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,coneyliu,coneyliu,22/Sep/17 03:52,12/Oct/17 12:28,14/Jul/23 06:30,12/Oct/17 12:27,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,We only need request bbos.size - unrollMemoryUsedByThisBlock after unrolled the block.,,apachespark,cloud_fan,coneyliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 12 12:27:55 UTC 2017,,,,,,,,,,"0|i3ke4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 03:55;apachespark;User 'ConeyLiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/19316;;;","12/Oct/17 12:27;cloud_fan;Issue resolved by pull request 19316
[https://github.com/apache/spark/pull/19316];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
processAllAvailable should not block forever when a query is stopped,SPARK-22094,13104122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,22/Sep/17 00:01,22/Sep/17 05:09,14/Jul/23 06:30,22/Sep/17 05:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"When a query is stopped, `processAllAvailable` may just block forever.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 00:04:03 UTC 2017,,,,,,,,,,"0|i3kdxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 00:04;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19314;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"UtilsSuite ""resolveURIs with multiple paths"" test always cancelled",SPARK-22093,13104016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,vanzin,vanzin,21/Sep/17 16:57,12/Dec/22 18:10,14/Jul/23 06:30,24/Sep/17 08:13,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"There's a call to {{assume}} in that test that is triggering, and causing the test to print an exception and report itself as ""cancelled"". It only happens in the last unit test (so coverage is fine, I guess), but still, that seems wrong.

{noformat}
[info] - resolveURIs with multiple paths !!! CANCELED !!! (15 milliseconds)
[info]   1 was not greater than 1 (UtilsSuite.scala:491)
[info]   org.scalatest.exceptions.TestCanceledException:
[info]   at org.scalatest.Assertions$class.newTestCanceledException(Assertions.scala:531)
[info]   at org.scalatest.FunSuite.newTestCanceledException(FunSuite.scala:1560)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssume(Assertions.scala:516)
[info]   at org.apache.spark.util.UtilsSuite$$anonfun$6.assertResolves$2(UtilsSuite.scala:491)
[info]   at org.apache.spark.util.UtilsSuite$$anonfun$6.apply$mcV$sp(UtilsSuite.scala:512)
[info]   at org.apache.spark.util.UtilsSuite$$anonfun$6.apply(UtilsSuite.scala:489)
[info]   at org.apache.spark.util.UtilsSuite$$anonfun$6.apply(UtilsSuite.scala:489)
{noformat}
",,apachespark,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 24 08:13:05 UTC 2017,,,,,,,,,,"0|i3kda7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 01:46;gurwls223;Would it make sense to just remove that {{assume}}? It looks that {{assume}} is not quite meaningful as that {{Utils.resolveURIs}} seems also supporting a single URI as the input too and I wonder why we should assume the input to contains {{,}} and multiple URIs.;;;","23/Sep/17 10:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19332;;;","24/Sep/17 08:13;gurwls223;Fixed in https://github.com/apache/spark/pull/19332;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reallocation in OffHeapColumnVector.reserveInternal corrupts array data,SPARK-22092,13104012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,21/Sep/17 16:40,23/Sep/17 14:12,14/Jul/23 06:30,22/Sep/17 13:35,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"OffHeapColumnVector.reserveInternal() will only copy already inserted values during reallocation if this.data != null. However, for vectors containing arrays, field data is disused and always equals null. Hence, the reallocation of array vector always causes data loss.",,ala.luszczak,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 14:46:04 UTC 2017,,,,,,,,,,"0|i3kd9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/17 16:51;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/19308;;;","22/Sep/17 14:46;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/19323;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect scalastyle comment causes wrong styles in stringExpressions,SPARK-22088,13103880,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,21/Sep/17 10:22,23/Sep/17 00:13,14/Jul/23 06:30,21/Sep/17 18:51,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,There is an incorrect {{scalastyle:on}} comment in `stringExpressions.scala` and causes the line size limit check ineffective in the file.,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 23 00:13:04 UTC 2017,,,,,,,,,,"0|i3kcfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/17 10:25;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19305;;;","23/Sep/17 00:13;apachespark;User 'kevinyu98' has created a pull request for this issue:
https://github.com/apache/spark/pull/19328;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When dropping multiple blocks to disk, Spark should release all locks on a failure",SPARK-22083,13103808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,21/Sep/17 04:09,03/Oct/17 06:10,14/Jul/23 06:30,25/Sep/17 19:09,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.1.3,2.2.1,2.3.0,,Block Manager,Spark Core,,,,0,,,,,,,,,"{{MemoryStore.evictBlocksToFreeSpace}} first [acquires writer locks on all the blocks it intends to evict | https://github.com/apache/spark/blob/55d5fa79db883e4d93a9c102a94713c9d2d1fb55/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala#L520].  However, if there is an exception while dropping blocks, there is no {{finally}} block to release all the locks.

If there is only one block being dropped, this isn't a problem (probably).  Usually the call stack goes from {{MemoryStore.evictBlocksToFreeSpace --> dropBlocks --> BlockManager.dropFromMemory --> DiskStore.put}}.  And {{DiskStore.put}} does do a [{{removeBlock()}} in a {{finally}} block|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/DiskStore.scala#L83], which cleans up the locks.

I ran into this from the serialization issue in SPARK-21928.  In that, a netty thread ends up trying to evict some blocks from memory to disk, and fails.  When there is only one block that needs to be evicted, and the error occurs, there isn't any real problem; I assume that netty thread is dead, but the executor threads seem fine.  However, in the cases where two blocks get dropped, one task gets completely stuck.  Unfortunately I don't have a stack trace from the stuck executor, but I assume it just waits forever on this lock that never gets released.",,apachespark,bryanc,irashid,jbrock,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21928,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 18:34:51 UTC 2017,,,,,,,,,,"0|i3kc07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/17 19:18;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/19311;;;","22/Sep/17 18:34;irashid;After another look at this, I'm actually not sure why we didn't see problems when we were only dropping one block.  Though there is a finally block in {{DiskStore.put()}}, that just calls {{DiskStore.remove()}}, not {{BlockInfoManager.removeBlock() / unlock()}}, so the lock should still be held.  I guess just luck that an executor task thread wasn't stuck trying to get the lock.

I feel like the lock management needs another review, there seems to be an implicit assumption that block management is always done by task threads, but its also done by netty threads as blocks get promoted to or evicted from memory.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expand.projections should not be a Stream,SPARK-22076,13103516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Sep/17 06:48,21/Sep/17 00:16,14/Jul/23 06:30,20/Sep/17 16:01,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 00:16:04 UTC 2017,,,,,,,,,,"0|i3ka73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/17 06:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19289;;;","21/Sep/17 00:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19298;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task killed by other attempt task should not be resubmitted,SPARK-22074,13103504,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,20/Sep/17 05:27,09/Oct/17 06:19,14/Jul/23 06:30,09/Oct/17 06:17,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,speculation,,,,,,,,"When a task killed by other task attempt, the task still resubmitted while its executor lost. There is a certain probability caused the stage hanging forever because of the unnecessary resubmit(see the scenario description below). Although the patch https://issues.apache.org/jira/browse/SPARK-13931 can resolve the hanging problem(thx [~GavinGavinNo1] :) ), but the unnecessary resubmit should abandon.


Detail scenario description:
1. A ShuffleMapStage has many tasks, some of them finished successfully
2. An Executor Lost happened, this will trigger a new TaskSet resubmitted, includes all missing partitions.
3. Before the resubmitted TaskSet completed, another executor which only include the task killed by other attempt lost, trigger the Resubmitted Event, current stage's pendingPartitions is not empty.
4. Resubmitted TaskSet end, shuffleMapStage.isAvailable == true, but pendingPartitions is not empty, never step into submitWaitingChildStages.

Leave the key logs of this scenario below:
{noformat}
393332:17/09/11 13:45:24 [dag-scheduler-event-loop] INFO DAGScheduler: Submitting 120 missing tasks from ShuffleMapStage 1046 (MapPartitionsRDD[5321] at rdd at AFDEntry.scala:116)
393333:17/09/11 13:45:24 [dag-scheduler-event-loop] INFO YarnClusterScheduler: Adding task set 1046.0 with 120 tasks
408766:17/09/11 13:46:25 [dispatcher-event-loop-5] INFO TaskSetManager: Starting task 66.0 in stage 1046.0 (TID 110761, hidden-baidu-host.baidu.com, executor 15, partition 66, PROCESS_LOCAL, 6237 bytes)

[1] Executor 15 lost, task 66.0 and 90.0 on it

410532:17/09/11 13:46:32 [dispatcher-event-loop-47] INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 15.
410900:17/09/11 13:46:33 [dispatcher-event-loop-34] INFO TaskSetManager: Starting task 66.1 in stage 1046.0 (TID 111400, hidden-baidu-host.baidu.com, executor 70, partition 66, PROCESS_LOCAL, 6237 bytes)

[2] Task 66.0 killed by 66.1

411315:17/09/11 13:46:37 [task-result-getter-2] INFO TaskSetManager: Killing attempt 0 for task 66.0 in stage 1046.0 (TID 110761) on hidden-baidu-host.baidu.com as the attempt 1 succeeded on hidden-baidu-host.baidu.com
411316:17/09/11 13:46:37 [task-result-getter-2] INFO TaskSetManager: Finished task 66.1 in stage 1046.0 (TID 111400) in 3545 ms on hidden-baidu-host.baidu.com (executor 70) (115/120)

[3] Executor 7 lost, task 0.0 72.0 7.0 on it

411390:17/09/11 13:46:37 [dispatcher-event-loop-24] INFO YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 7.
416014:17/09/11 13:46:59 [dag-scheduler-event-loop] INFO DAGScheduler: ShuffleMapStage 1046 (rdd at AFDEntry.scala:116) finished in 94.577 s

[4] ShuffleMapStage 1046.0 finished, missing partition trigger resubmitted 1046.1

416019:17/09/1 13:46:59 [dag-scheduler-event- oop] INFO DAGScheduler: Resubmitting ShuffleMapStage 1046 (rdd at AFDEntry.scala:116) because some of its tasks had failed: 0, 72, 79
416020:17/09/11 13:46:59 [dag-scheduler-event-loop] INFO DAGScheduler: Submitting ShuffleMapStage 1046 (MapPartitionsRDD[5321] at rdd at AFDEntry.scala:116), which has no missing parents
416030:17/09/11 13:46:59 [dag-scheduler-event-loop] INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 1046 (MapPartitionsRDD[5321] at rdd at AFDEntry.scala:116)
416032:17/09/11 13:46:59 [dag-scheduler-event-loop] INFO YarnClusterScheduler: Adding task set 1046.1 with 3 tasks
416034:17/09/11 13:46:59 [dispatcher-event-loop-21] INFO TaskSetManager: Starting task 0.0 in stage 1046.1 (TID 112788, hidden-baidu-host.baidu.com, executor 37, partition 0, PROCESS_LOCAL, 6237 bytes)
416037:17/09/11 13:46:59 [dispatcher-event-loop-23] INFO TaskSetManager: Starting task 1.0 in stage 1046.1 (TID 112789, yq01-inf-nmg01-spark03-20160817113538.yq01.baidu.com, executor 69, partition 72, PROCESS_LOCAL, 6237 bytes)
416039:17/09/11 13:46:59 [dispatcher-event-loop-23] INFO TaskSetManager: Starting task 2.0 in stage 1046.1 (TID 112790, hidden-baidu-host.baidu.com, executor 26, partition 79, PROCESS_LOCAL, 6237 bytes)

[5] ShuffleMapStage 1046.1 still running, the attempted task killed by other trigger the Resubmitted event

416646:17/09/11 13:47:01 [dispatcher-event-loop-26] WARN TaskSetManager: Lost task 66.0 in stage 1046.0 (TID 110761, hidden-baidu-host.baidu.com, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_1502719603300_158941_01_104857616 on host: hidden-baidu-host.baidu.com. Exit status: -100. Diagnostics: Container released on a *lost* node
416647:17/09/11 13:47:01 [dag-scheduler-event-loop] INFO DAGScheduler: Resubmitted ShuffleMapTask(1046, 66), so marking it as still running
416648:17/09/11 13:47:01 [dispatcher-event-loop-26] WARN TaskSetManager: Lost task 90.0 in stage 1046.0 (TID 110788, hidden-baidu-host.baidu.com, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_1502719603300_158941_01_104857616 on host: hidden-baidu-host.baidu.com. Exit status: -100. Diagnostics: Container released on a *lost* node
416649:17/09/11 13:47:01 [dag-scheduler-event-loop] INFO DAGScheduler: Resubmitted ShuffleMapTask(1046, 90), so marking it as still running
417197:17/09/11 13:47:02 [task-result-getter-0] INFO TaskSetManager: Finished task 0.0 in stage 1046.1 (TID 112788) in 3027 ms on hidden-baidu-host.baidu.com (executor 37) (1/3)
417206:17/09/11 13:47:02 [task-result-getter-1] INFO TaskSetManager: Finished task 1.0 in stage 1046.1 (TID 112789) in 3106 ms on yq01-inf-nmg01-spark03-20160817113538.yq01.baidu.com (executor 69) (2/3)
417383:17/09/11 13:47:03 [task-result-getter-0] INFO TaskSetManager: Finished task 2.0 in stage 1046.1 (TID 112790) in 3634 ms on hidden-baidu-host.baidu.com (executor 26) (3/3)
417384:17/09/11 13:47:03 [task-result-getter-0] INFO YarnClusterScheduler: Removed TaskSet 1046.1, whose tasks have all completed, from pool

[6] Task 1046.1 success, but 1046 stage forever in running queue

417817:17/09/11 13:47:06 [task-result-getter-2] INFO YarnClusterScheduler: Removed TaskSet 1046.0, whose tasks have all completed, from pool
417872:17/09/11 13:47:06 [dag-scheduler-event-loop] INFO DAGScheduler: running: Set(ShuffleMapStage 1090, ResultStage 1069, ShuffleMapStage 1070, ShuffleMapStage 1113, ShuffleMapStage 1092, ShuffleMapStage 1063, ShuffleMapStage 1086, ShuffleMapStage 1065, ShuffleMapStage 1109, ShuffleMapStage 1088, ShuffleMapStage 1111, ShuffleMapStage 1082, ResultStage 1104, ShuffleMapStage 1105, ShuffleMapStage 1084, ShuffleMapStage 1076, ShuffleMapStage 1107, ShuffleMapStage 1099, ShuffleMapStage 1078, ShuffleMapStage 1101, ShuffleMapStage 1080, ShuffleMapStage 1072, ShuffleMapStage 1094, ResultStage 1103, ShuffleMapStage 1074, ResultStage 1096, ShuffleMapStage 1067, ShuffleMapStage 1046, ShuffleMapStage 1097)
......
598792:17/09/11 13:58:55 [dag-scheduler-event-loop] INFO DAGScheduler: running: Set(ShuffleMapStage 1577, ShuffleMapStage 1548, ShuffleMapStage 1571, ShuffleMapStage 1550, ShuffleMapStage 1543, ShuffleMapStage 1573, ShuffleMapStage 1065, ShuffleMapStage 1565, ResultStage 1545, ShuffleMapStage 1567, ShuffleMapStage 1546, ShuffleMapStage 1539, ShuffleMapStage 1569, ShuffleMapStage 1561, ShuffleMapStage 1541, ShuffleMapStage 1533, ShuffleMapStage 1563, ResultStage 1556, ShuffleMapStage 1535, ShuffleMapStage 1557, ShuffleMapStage 1579, ShuffleMapStage 1537, ShuffleMapStage 1559, ShuffleMapStage 1581, ShuffleMapStage 1552, ShuffleMapStage 1531, ShuffleMapStage 1575, ShuffleMapStage 1554, ShuffleMapStage 1046)
{noformat}",,apachespark,devaraj,jerryshao,KSLaskfla,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 09 06:17:33 UTC 2017,,,,,,,,,,"0|i3ka4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/17 05:48;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/19287;;;","26/Sep/17 08:59;jerryshao;Hi [~XuanYuan], can you please help me to understand your scenario, is it happened only when task attempt (66.0) is lost (which will be adding to pending list), at this time another attempt (66.1) is finished, it will try to kill 66.0, but because 66.0 is pending for resubmitting, so it is not truly killed,  so attempt 66.0 is lingering in the stage 1046.0, which makes 1046 fail to finish, do I understand right?

Can you please explain more if my assumption is wrong.;;;","26/Sep/17 12:06;XuanYuan;Hi [~jerryshao], thanks for you comment. 
In my scenario, the 66.0 is truly killed by 66.1, the root case cause 1046.0 fail to finish is that the resubmitted event of task 66.0(killed by 66.1before) reached while 1046.1 running.;;;","27/Sep/17 03:19;jerryshao;Hey [~XuanYuan], I'm a little confused why there will be a resubmit event after 66.0 is killed, since this killing action is expected and Spark should not launch another attempt.;;;","27/Sep/17 04:01;XuanYuan;Hi [~jerryshao] saisai, the 66.0 resubmitted because of its executor lost during 1046.1 running. I also reproduce this in the UT(https://github.com/apache/spark/pull/19287/files#diff-8425e96a6c100b5f368b8e520ad80068R748) added in my patch and add detailed scenario description in comment, it will fail without the changes in this PR and will pass conversely. Could you help me check the UT recreate the scenario right? Thanks a lot. :);;;","27/Sep/17 06:00;jerryshao;So if I understand correctly, this happens when speculation is happened, if one task attempt is finished (66.1), it will try to kill all other attempts (66.0), but before this attempt (66.0) is fully killed, the executor who run this attempt is lost, so scheduler will resubmit this attempt because of executor lost, and neglect other successful attempt, Am I right?

;;;","27/Sep/17 06:46;XuanYuan;Yes, that's right.;;;","09/Oct/17 06:17;jerryshao;Issue resolved by pull request 19287
[https://github.com/apache/spark/pull/19287];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve release build scripts to check correct JAVA version is being used for build,SPARK-22071,13103501,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,holden,holden,20/Sep/17 05:09,22/Sep/17 07:16,14/Jul/23 06:30,22/Sep/17 07:16,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Build,,,,,0,,,,,,,,,The current release scripts assume the correct JAVA_HOME is set for the release. While this isn't an issue when they are called in Jenkins it would be reasonable to check this for release managers who may decide to build outside of Jenkins (or as we migrate our release build environment into docker).,,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22055,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 07:16:31 UTC 2017,,,,,,,,,,"0|i3ka3r:",9223372036854775807,,,,,,,,,,,,,2.1.2,2.3.0,,,,,,,,,,"21/Sep/17 19:55;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19312;;;","22/Sep/17 07:16;holden;Issue resolved by pull request 19312
[https://github.com/apache/spark/pull/19312];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrowWriter StringWriter not using position of ByteBuffer holding data,SPARK-22067,13103399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,19/Sep/17 21:53,20/Sep/17 01:51,14/Jul/23 06:30,20/Sep/17 01:51,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When ArrowWriter is copying a StringType column to ArrowData, then StringWriter gets data as a ByteBuffer and sets the ArrowBuf using a position of 0.  This currently works because of a bug in Arrow ARROW-1443 that was not making use of the position.  This has been fixed as of Arrow 0.7.0 and this surfaced when testing out this version.  To fix this, the position of the ByteBuffer should be passed into {{setSafe}} instead of 0.",,apachespark,bryanc,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 20 01:51:39 UTC 2017,,,,,,,,,,"0|i3k9h3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/17 21:54;bryanc;I'll submit a PR for this;;;","19/Sep/17 22:27;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/19284;;;","20/Sep/17 01:51;ueshin;Issue resolved by pull request 19284
[https://github.com/apache/spark/pull/19284];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockManager does not account for memory consumed by remote fetches,SPARK-22062,13103266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,lebedev,lebedev,19/Sep/17 15:21,17/May/20 18:21,14/Jul/23 06:30,17/Oct/17 14:55,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Block Manager,Spark Core,,,,0,,,,,,,,,"We use Spark exclusively with {{StorageLevel.DiskOnly}} as our workloads are very sensitive to memory usage. Recently, we've spotted that the jobs sometimes OOM leaving lots of byte[] arrays on the heap. Upon further investigation, we've found that the arrays come from {{BlockManager.getRemoteBytes}}, which [calls|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L638] {{BlockTransferService.fetchBlockSync}}, which in its turn would [allocate|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala#L99] an on-heap {{ByteBuffer}} of the same size as the block (e.g. full partition), if the block was successfully retrieved over the network.

This memory is not accounted towards Spark storage/execution memory and could potentially lead to OOM if {{BlockManager}} fetches too many partitions in parallel. I wonder if this is intentional behaviour, or in fact a bug?",,apachespark,cloud_fan,devaraj,erofeev,jerryshao,jinxing6042@126.com,lebedev,liufeng.ee@gmail.com,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6235,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 14:55:19 UTC 2017,,,,,,,,,,"0|i3k8nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/17 01:20;jerryshao;Yes, there potentially has OOM problem, but I think this kind of temporarily allocated {{ByteBuffer}} is difficult to be defined as whether it should be accounted into storage memory or execution memory. Furthermore, how to deal with remote fetching if memory is not enough, shall we fail the task or can we stream the remote fetches?

What I can think of is to leverage the current implementation of shuffle to spill the large blocks to local disk during fetching, and tasks can read the data from local temporary files, this could avoid OOM.;;;","12/Oct/17 02:09;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19476;;;","17/Oct/17 14:55;cloud_fan;Issue resolved by pull request 19476
[https://github.com/apache/spark/pull/19476];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CrossValidator/TrainValidationSplit parallelism param persist/load bug,SPARK-22060,13103240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,19/Sep/17 13:46,23/Sep/17 01:15,14/Jul/23 06:30,23/Sep/17 01:15,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,,,,,0,,,,,,,,,"CrossValidator/TrainValidationSplit `parallelism` param cannot be saved, when we save the CrossValidator/TrainValidationSplit object to disk.",,apachespark,josephkb,mlnick,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 23 01:15:18 UTC 2017,,,,,,,,,,"0|i3k8i7:",9223372036854775807,,,,,josephkb,,,,,,,,2.3.0,,,,,,,,,,,"19/Sep/17 13:48;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/19278;;;","23/Sep/17 01:15;josephkb;Issue resolved by pull request 19278
[https://github.com/apache/spark/pull/19278];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Metric assigned in MetricsReporter.scala,SPARK-22052,13103008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Taaffy,Taaffy,Taaffy,18/Sep/17 17:29,19/Sep/17 09:21,14/Jul/23 06:30,19/Sep/17 09:20,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Input/Output,Structured Streaming,,,,0,,,,,,,,,"The wrong metric is being sent in MetricsReporter.scala

The current implementation for processingRate-total is assigned the wrong metric:
Look at the first and second registerGauge. The second one mistakenly uses inputRowsPerSecond instead of processedRowsPerSecond.

{code:java}
class MetricsReporter(
    stream: StreamExecution,
    override val sourceName: String) extends CodahaleSource with Logging {

  override val metricRegistry: MetricRegistry = new MetricRegistry

  // Metric names should not have . in them, so that all the metrics of a query are identified
  // together in Ganglia as a single metric group
  registerGauge(""inputRate-total"", () => stream.lastProgress.inputRowsPerSecond)
  registerGauge(""processingRate-total"", () => stream.lastProgress.inputRowsPerSecond)
  registerGauge(""latency"", () => stream.lastProgress.durationMs.get(""triggerExecution"").longValue())

  private def registerGauge[T](name: String, f: () => T)(implicit num: Numeric[T]): Unit = {
    synchronized {
      metricRegistry.register(name, new Gauge[T] {
        override def getValue: T = f()
      })
    }
  }
}
{code}

[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MetricsReporter.scala]

After adjusting the line and rebuilding from source I tested the change by checking the csv files produced via the metrics properties file. Previously inputRate-total and processingRate-total were identical due to the same metric being used. After the change the processingRate-total file held the right value. 

Please check the attached file ""Processed Rows Per Second"".
After altering the code the correct values are displayed in column B. 
They match the data from the INFO StreamExecution displayed during streaming

","Spark 2.2 
MetricsReporter.scala",apachespark,lwlin,pgaref,Taaffy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/17 17:30;Taaffy;Processed Rows Per Second.png;https://issues.apache.org/jira/secure/attachment/12887702/Processed+Rows+Per+Second.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/pull/19268,,,,,,,,,,9223372036854775807,,,Tue Sep 19 09:20:47 UTC 2017,,,,,,,,,,"0|i3k733:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/17 17:59;apachespark;User 'Taaffy' has created a pull request for this issue:
https://github.com/apache/spark/pull/19268;;;","19/Sep/17 09:20;srowen;Issue resolved by pull request 19268
[https://github.com/apache/spark/pull/19268];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveExternalCatalogVersionsSuite is Flaky on Jenkins,SPARK-22047,13102888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,original-brownbear,original-brownbear,18/Sep/17 07:34,15/Dec/17 01:28,14/Jul/23 06:30,15/Dec/17 01:28,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,flaky-test,,,,,,,,"HiveExternalCatalogVersionsSuite fails quite a bit lately e.g.

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.7/3490/testReport/junit/org.apache.spark.sql.hive/HiveExternalCatalogVersionsSuite/_It_is_not_a_test_it_is_a_sbt_testing_SuiteSelector_/

{code}
Error Message

org.scalatest.exceptions.TestFailedException: spark-submit returned with exit code 1. Command line: './bin/spark-submit' '--name' 'prepare testing tables' '--master' 'local[2]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.warehouse.dir=/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/warehouse-b266cb0e-5180-4ba8-80a3-b790b3be3aa0' '--conf' 'spark.sql.test.version.index=0' '--driver-java-options' '-Dderby.system.home=/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/warehouse-b266cb0e-5180-4ba8-80a3-b790b3be3aa0' '/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/test120059455549609580.py'  2017-09-17 04:26:11.641 - stderr> Error: Could not find or load main class org.apache.spark.launcher.Main            
Stacktrace

sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: spark-submit returned with exit code 1.
Command line: './bin/spark-submit' '--name' 'prepare testing tables' '--master' 'local[2]' '--conf' 'spark.ui.enabled=false' '--conf' 'spark.master.rest.enabled=false' '--conf' 'spark.sql.warehouse.dir=/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/warehouse-b266cb0e-5180-4ba8-80a3-b790b3be3aa0' '--conf' 'spark.sql.test.version.index=0' '--driver-java-options' '-Dderby.system.home=/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/warehouse-b266cb0e-5180-4ba8-80a3-b790b3be3aa0' '/home/jenkins/workspace/spark-master-test-sbt-hadoop-2.7/target/tmp/test120059455549609580.py'

2017-09-17 04:26:11.641 - stderr> Error: Could not find or load main class org.apache.spark.launcher.Main
           
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)
	at org.scalatest.Assertions$class.fail(Assertions.scala:1089)
	at org.scalatest.FunSuite.fail(FunSuite.scala:1560)
	at org.apache.spark.sql.hive.SparkSubmitTestUtils$class.runSparkSubmit(SparkSubmitTestUtils.scala:81)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite.runSparkSubmit(HiveExternalCatalogVersionsSuite.scala:38)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite$$anonfun$beforeAll$1.apply(HiveExternalCatalogVersionsSuite.scala:120)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite$$anonfun$beforeAll$1.apply(HiveExternalCatalogVersionsSuite.scala:105)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.hive.HiveExternalCatalogVersionsSuite.beforeAll(HiveExternalCatalogVersionsSuite.scala:105)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:480)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,cloud_fan,irashid,original-brownbear,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21936,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 14 19:00:47 UTC 2017,,,,,,,,,,"0|i3k6cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/17 08:05;srowen;[~cloud_fan] I think this is failing builds on master / 2.2. Should we revert temporarily?;;;","18/Sep/17 08:33;cloud_fan;let me send a PR to ignore this test suite temporarily, I'm looking into it;;;","18/Sep/17 08:42;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19264;;;","18/Sep/17 12:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19265;;;","19/Sep/17 03:56;cloud_fan;Let's wait for a few days to see if my fix works well, before resolving this ticket.;;;","14/Dec/17 19:00;irashid;[~cloud_fan] think we can close this now?  I don't recall seeing this fail recently, though I also haven't looked very extensively.  (seems spark-tests.appspot.com has stopped updating ...);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReorderJoinPredicates can break when child's partitioning is not decided,SPARK-22042,13102800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejasp,tejasp,tejasp,17/Sep/17 00:49,21/Dec/17 00:42,14/Jul/23 06:30,13/Dec/17 07:30,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When `ReorderJoinPredicates` tries to get the `outputPartitioning` of its children, the children may not be properly constructed as the child-subtree has to still go through other planner rules.

In this particular case, the child is `SortMergeJoinExec`. Since the required `Exchange` operators are not in place (because `EnsureRequirements` runs _after_ `ReorderJoinPredicates`), the join's children would not have partitioning defined. This breaks while creation the `PartitioningCollection` here : https://github.com/apache/spark/blob/94439997d57875838a8283c543f9b44705d3a503/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L69

Small repro:

{noformat}
context.sql(""SET spark.sql.autoBroadcastJoinThreshold=0"")

val df = (0 until 50).map(i => (i % 5, i % 13, i.toString)).toDF(""i"", ""j"", ""k"")
df.write.format(""parquet"").saveAsTable(""table1"")
df.write.format(""parquet"").saveAsTable(""table2"")
df.write.format(""parquet"").bucketBy(8, ""j"", ""k"").saveAsTable(""bucketed_table"")

sql(""""""
  SELECT *
  FROM (
    SELECT a.i, a.j, a.k
    FROM bucketed_table a
    JOIN table1 b
    ON a.i = b.i
  ) c
  JOIN table2
  ON c.i = table2.i
"""""").explain
{noformat}

This fails with :

{noformat}
java.lang.IllegalArgumentException: requirement failed: PartitioningCollection requires all of its partitionings have the same numPartitions.
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.sql.catalyst.plans.physical.PartitioningCollection.<init>(partitioning.scala:324)
  at org.apache.spark.sql.execution.joins.SortMergeJoinExec.outputPartitioning(SortMergeJoinExec.scala:69)
  at org.apache.spark.sql.execution.ProjectExec.outputPartitioning(basicPhysicalOperators.scala:82)
  at org.apache.spark.sql.execution.joins.ReorderJoinPredicates$$anonfun$apply$1.applyOrElse(ReorderJoinPredicates.scala:91)
  at org.apache.spark.sql.execution.joins.ReorderJoinPredicates$$anonfun$apply$1.applyOrElse(ReorderJoinPredicates.scala:76)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
  at org.apache.spark.sql.execution.joins.ReorderJoinPredicates.apply(ReorderJoinPredicates.scala:76)
  at org.apache.spark.sql.execution.joins.ReorderJoinPredicates.apply(ReorderJoinPredicates.scala:34)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:100)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:100)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:100)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:90)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:90)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:201)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$simpleString$1.apply(QueryExecution.scala:201)
  at org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:201)
  at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:147)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:78)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:75)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)
  at org.apache.spark.sql.Dataset.explain(Dataset.scala:464)
  at org.apache.spark.sql.Dataset.explain(Dataset.scala:477)
  ... 60 elided
{noformat}",,aash,apachespark,dongjoon,felixcheung,maropu,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 00:42:04 UTC 2017,,,,,,,,,,"0|i3k5tr:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"17/Sep/17 01:03;tejasp;At the core, the problem is `ReorderJoinPredicates` expects the input tree to be resolved (esp. when it comes to ensuring partitioning requirements). 

Possible fixes:
- Option 1: Have `EnsureRequirements` be done before `ReorderJoinPredicates`. Since `EnsureRequirements` would have inserted sort and shuffle nodes, `ReorderJoinPredicates` would have to remove those out. This would definitely produce sub-optimal plans as `EnsureRequirements` would have done some decision making (eg. how many shuffle partitions to generate ?) and at that point `ReorderJoinPredicates` cannot correct that easily.
- Option 2: In `EnsureRequirements`, apply `ReorderJoinPredicates` over the input tree before doing its core logic. Since the tree is transformed bottom-up, we can assure that the children are resolved before doing `ReorderJoinPredicates`. This will be simple change but would look weird given that we dont call rules from other rules (not to my knowledge).

I will create a PR for option #2 but will be happy to discuss if there are better solutions.;;;","17/Sep/17 01:16;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/19257;;;","26/Oct/17 16:37;aash;Hi I'm seeing this problem as well, thanks for investigating and putting up a PR [~tejasp]!  Have you been running any of your clusters with a patched version of Spark including that change, and has it been behaving as expected?

The repro one of my users independently provided was this:
{noformat}
val rows = List(1, 2, 3, 4, 5, 6);
 
val df1 = sc.parallelize(rows).toDF(""col"").repartition(1);
val df2 = sc.parallelize(rows).toDF(""col"").repartition(2);
val df3 = sc.parallelize(rows).toDF(""col"").repartition(2);
 
val dd1 = df1.join(df2, df1.col(""col"").equalTo(df2.col(""col""))).join(df3, df2.col(""col"").equalTo(df3.col(""col"")));
 
dd1.show;
{noformat};;;","10/Nov/17 07:19;dongjoon;Since this raises a runtime exception to the query, I raised this to Major. It would be great if 2.2.1 has this.
How do you think about this issue, [~smilegator] and [~cloud_fan]?;;;","10/Nov/17 07:19;dongjoon;Also, cc [~felixcheung].;;;","11/Nov/17 06:59;felixcheung;any taker? is this a blocker for 2.2.1?;;;","12/Nov/17 00:31;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/19725;;;","12/Nov/17 00:33;tejasp;Am trying out the suggestion discussed in https://github.com/apache/spark/pull/19257#issuecomment-331069250 
Here is an unpolished but working PR : https://github.com/apache/spark/pull/19725 (will polish it after observing the test case results).;;;","12/Nov/17 22:52;felixcheung;since this is in 2.1.0 and 2.2.0, technically this isn't a regression. I will watch this fix but currently not a blocker for 2.2.1;;;","21/Dec/17 00:42;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/20041;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigDecimal multiplication sometimes returns null,SPARK-22036,13102759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,OlivierBlanvillain,OlivierBlanvillain,16/Sep/17 11:12,20/Sep/18 23:40,14/Jul/23 06:30,18/Jan/18 13:30,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,1,,,,,,,,,"The multiplication of two BigDecimal numbers sometimes returns null. Here is a minimal reproduction:

{code:java}
object Main extends App {
  import org.apache.spark.{SparkConf, SparkContext}
  import org.apache.spark.sql.SparkSession
  import spark.implicits._
  val conf = new SparkConf().setMaster(""local[*]"").setAppName(""REPL"").set(""spark.ui.enabled"", ""false"")
  val spark = SparkSession.builder().config(conf).appName(""REPL"").getOrCreate()
  implicit val sqlContext = spark.sqlContext

  case class X2(a: BigDecimal, b: BigDecimal)
  val ds = sqlContext.createDataset(List(X2(BigDecimal(-0.1267333984375), BigDecimal(-1000.1))))
  val result = ds.select(ds(""a"") * ds(""b"")).collect.head
  println(result) // [null]
}
{code}
",,akhilnaidu,annunarcist,apachespark,bersprockets,cloud_fan,hvivani,juanrh,kiszk,ksunitha,maropu,mgaido,OlivierBlanvillain,taklwu,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24606,,,,,,SPARK-23179,,,HIVE-15331,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 20 23:40:29 UTC 2018,,,,,,,,,,"0|i3k5kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/17 16:39;mgaido;This happens because there is an overflow in the operation. I am not sure of what should be done in this case. The current implementation returns null when an operation cause a loss of precision.;;;","16/Sep/17 17:19;OlivierBlanvillain;It's surprising because in this case the resulting value seems to fit within the range of representable values:

{code:java}
scala> val result = BigDecimal(-0.1267333984375) * BigDecimal(-1000.1)
result: scala.math.BigDecimal = 126.74607177734375

scala> sqlContenxt.createDataset(List(result)).head == result
res10: Boolean = true
{code}

Also Spark will silently loses BigDecimal precision in other circumstances:

{code:java}
scala> val tooPrecise = BigDecimal(""126.74607177734375111111111"")
tooPrecise: scala.math.BigDecimal = 126.74607177734375111111111

scala> val ds = sqlContenxt.createDataset(List(tooPrecise))
ds: org.apache.spark.sql.Dataset[scala.math.BigDecimal] = [value: decimal(38,18)]

scala> ds.head
res14: scala.math.BigDecimal = 126.746071777343751111

scala> ds.select(ds(""value"") * BigDecimal(1)).head
res15: org.apache.spark.sql.Row = [126.746071777343751111]
{code}

> I am not sure of what should be done in this case

Given that Sparks' BigDecimal have bounded precision I would consider following what is done for other numeric representations and return the closest representable value in case of overflow.;;;","16/Sep/17 17:36;OlivierBlanvillain;This seems to be a multiplication only thing, for instance Spark silently loses precision with addition:

{code:java}
scala> val a = BigDecimal(""43.65"")
a: scala.math.BigDecimal = 43.65

scala> val b = BigDecimal(""61.11"")
b: scala.math.BigDecimal = 61.11

scala> case class X2(a: BigDecimal, b: BigDecimal)
defined class X2

scala> val ds = sqlContenxt.createDataset(List(X2(a, b)))
ds: org.apache.spark.sql.Dataset[X2] = [a: decimal(38,18), b: decimal(38,18)]

scala> val res = ds.select(ds(""a"") + ds(""b"")).head.get(0).asInstanceOf[java.math.BigDecimal]
res: java.math.BigDecimal = 104.760000000000000000

scala> res.subtract((a + b).underlying)
res3: java.math.BigDecimal = 0E-18
{code}
;;;","16/Sep/17 19:08;mgaido;Yes, it is only for multiplications. The reason is that for the multiplication it expects the result to have a scale which is the sum of the two scales of the operands. When there is an overflow in the result of the operations, the result is rounded up and the scale is one less than the expected. In this situation, the result is set to null.;;;","16/Sep/17 20:45;OlivierBlanvillain;I understand. Is this working as intended / fulfils a specification or is it just an artifact of the current implementation?;;;","16/Sep/17 20:59;mgaido;Honestly I don't know, that is why I said that I don't know what should be done.;;;","16/Sep/17 22:14;mgaido;Maybe the ""bad"" part is that by default spark creates the columns as {{Decimal(38, 18)}}. This is the problem. With a multiplication this leads to a {{Decimal(38, 36)}}, which as you can easily understand is the root of the problem of your operation. If you cast the two columns before the multiplication, like {{ds(""a"").cast(DecimalType(20,14))}}, you won't have any problem anymore.
Currently you should suggest Spark which are the right values to use.;;;","16/Sep/17 23:05;OlivierBlanvillain;Adding a cast indeed prevents getting null values. However this solution is less than satisfactory as the resulting multiplications are _less_ precise on {{BigDecimal}} than on {{Double}}.

Here is an example. We compute the product of two numbers: {{0.0000199735164642333984375}} and {{-0.000010430812835693359375}}. Below are the result of this multiplication by the JVM as {{java.lang.Double}} (double), by Spark using {{.cast(DecimalType(20,14))}} (casted), and finally using unlimited precision arithmetic (actual):

{{double: -2.083400119090584E-10}}
{{casted: -2.083400119509193464E-10}}
{{actual: -2.083400119090583757497370243072509765625E-10}};;;","12/Dec/17 03:02;taklwu;+1, we met the similar issue when multiplying 3+ BigDecimal, although we have a workaound to cast them into a smaller number, it's not easy to tell when it will fail especially we have more complicated query.;;;","14/Dec/17 23:49;annunarcist;+1 Issue reproduced on spark-2.2.0 : 

Data at s3 location - s3://bucket/spark-sql-jira/ :
---------------------------------------------
100|99999

drop table if exists test;
CREATE EXTERNAL TABLE `test` (
a    decimal(38,10),
b    decimal(38,10)
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE
LOCATION 's3://bucket/spark-sql-jira/';

spark-sql> select a,(a*b*0.98765432100) from test;
100	9876444.4445679
Time taken: 11.033 seconds, Fetched 1 row(s)

spark-sql> select a,(a*b*0.987654321000) from test;
100	NULL
Time taken: 0.523 seconds, Fetched 1 row(s)

Changing a column's scale from decimal(38,10) to decimal(38,9) also helped but we would loose precision. ;;;","19/Dec/17 16:42;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/20023;;;","18/Jan/18 13:30;cloud_fan;Issue resolved by pull request 20023
[https://github.com/apache/spark/pull/20023];;;","03/Feb/18 11:24;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/20498;;;","17/Sep/18 20:18;bersprockets;[~mgaido] In this change, you modified how precision and scale are determined when literals are promoted to decimal. For example, before the change, an integer literal's precision and scale would be hardcoded to DecimalType(10, 0). After the change, it's based on the number of digits in the literal.

However, that new behavior for literals is not toggled by {{spark.sql.decimalOperations.allowPrecisionLoss}} like the other changes in behavior introduced by the PR.

As a result, there are cases where we see truncation and rounding in 2.3/2.4 that we don't see in 2.2, and this change in behavior is not controllable via the configuration setting. E.g,:

In 2.2:
{noformat}
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").printSchema
root
 |-- c1: decimal(27,13) (nullable = true) <== 13 decimal digits
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").show
+----------------+
|              c1|
+----------------+
|26.3934994510000|
+----------------+
{noformat}
In 2.3 and up:
{noformat}
scala> sql(""set spark.sql.decimalOperations.allowPrecisionLoss"").show
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.decimal...| true|
+--------------------+-----+
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").printSchema
root
 |-- c1: decimal(12,7) (nullable = true)
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").show
+----------+
|        c1|
+----------+
|26.3934995| <== result is truncated and rounded up.
+----------+
scala> sql(""set spark.sql.decimalOperations.allowPrecisionLoss=false"").show
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.decimal...|false|
+--------------------+-----+
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").printSchema
root
 |-- c1: decimal(12,7) (nullable = true)
scala> sql(""select 26393499451/(1e6 * 1000) as c1"").show
+----------+
|        c1|
+----------+
|26.3934995| <== result is still truncated and rounded up.
+----------+
scala> 
{noformat}
I can force it to behave the old way, at least for this case, by explicitly casting the literal:
{noformat}
scala> sql(""select 26393499451/(1e6 * cast(1000 as decimal(10, 0))) as c1"").show
+----------------+
|              c1|
+----------------+
|26.3934994510000|
+----------------+
{noformat}
Do you think it makes sense for {{spark.sql.decimalOperations.allowPrecisionLoss}} to also toggle how literal promotion happens (the old way vs. the new way)?;;;","18/Sep/18 10:34;mgaido;[~bersprockets] first of all thank you for reporting this and sorry for my mistake on this.

I think the solution you are suggesting isn't the right one. Also the result in the case allowPrecisionLoss=true should not have any truncation here. The problem is the way we handle negative scale. So this issue I think is related to SPARK-24468. The problem is that Hive and MSSQL we are taking our rules from are not allowing negative scale, while we do. So this has to be revisited. May you please submit a new JIRA for this? Meanwhile I am starting working on it and I'll submit a fix ASAP. Sorry for the trouble. Thanks.;;;","18/Sep/18 13:45;mgaido;[~bersprockets] I created SPARK-25454 for tracking since I have a path for this and it might be considered as a blocker for 2.4, so I wanted to expedite it. I am submitting a patch for this soon. Sorry for the problem again. Thanks.;;;","20/Sep/18 23:40;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/22494;;;",,,,,,,,,,,,,,,,,,,,,,
"BufferHolder, other size checks should account for the specific VM array size limitations",SPARK-22033,13102696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,buryat,buryat,15/Sep/17 22:06,09/Apr/18 20:49,14/Jul/23 06:30,23/Sep/17 14:41,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"User may get the following OOM Error while running a job with heavy aggregations

```
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:73)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:235)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:228)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$2.apply(AggregationIterator.scala:254)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$2.apply(AggregationIterator.scala:247)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:88)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:167)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```

The [`BufferHolder.grow` tries to create a byte array of `Integer.MAX_VALUE` here](https://github.com/apache/spark/blob/v2.2.0/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/BufferHolder.java#L72) but the maximum size of an array depends on specifics of a VM.

The safest value seems to be `Integer.MAX_VALUE - 8` 
http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/tip/src/share/classes/java/util/ArrayList.java#l229

In my JVM:
```
java -version
openjdk version ""1.8.0_141""
OpenJDK Runtime Environment (build 1.8.0_141-b16)
OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode)
```

the max is `new Array[Byte](Integer.MAX_VALUE - 2)`",,apachespark,buryat,cloud_fan,danospv,huaxingao,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22222,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 23 14:41:13 UTC 2017,,,,,,,,,,"0|i3k56n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/17 22:15;buryat;Leaving traces for others if they happen to hit the same issue.

The issue isn't exclusive to the ObjectAggregator, it can happen in the SortBasedAggregator too

```
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:73)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:235)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:228)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$2.apply(AggregationIterator.scala:254)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$2.apply(AggregationIterator.scala:247)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:159)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:29)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.initialize(SortBasedAggregationIterator.scala:103)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:113)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```;;;","15/Sep/17 22:59;srowen;Hm, good point. There may be other similar issues throughout the code.
[~cloud_fan] does that sound right to you?;;;","16/Sep/17 14:04;cloud_fan;yea sounds good to me, though I can't think of other places that have the similar issue.;;;","17/Sep/17 14:42;kiszk;I think {{ColumnVector}} and {{HashMapGrowthStrategy}} may have possibility of the similar issue.
What do you think?;;;","18/Sep/17 14:54;maropu;just a heads-up: this is probably related to the 2G limit issue: SPARK-6235 (I think you all have known though...).;;;","18/Sep/17 15:01;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19266;;;","18/Sep/17 15:01;srowen;I tried to change all the locations where an array size was checked vs Int.MaxValue. Modifying the checks in MLlib might be going overboard as a failure there isn't avoidable if the array is too big, whereas in other cases, it's actually avoiding an avoidable error.

Thoughts on the PR?;;;","23/Sep/17 14:41;srowen;Issue resolved by pull request 19266
[https://github.com/apache/spark/pull/19266];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphiteSink fails to re-connect to Graphite instances behind an ELB or any other auto-scaled LB,SPARK-22030,13102602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,alexmnyc,alexmnyc,alexmnyc,15/Sep/17 15:31,19/Sep/17 02:08,14/Jul/23 06:30,19/Sep/17 02:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Upgrade codahale metrics library so that Graphite constructor can re-resolve hosts behind a CNAME with re-tried DNS lookups. When Graphite is deployed behind an ELB, ELB may change IP addresses based on auto-scaling needs. Using current approach yields Graphite usage impossible, fixing for that use case

Upgrade to codahale 3.1.5
Use new Graphite(host, port) constructor instead of new Graphite(new InetSocketAddress(host, port)) constructor

This are proposed changes for codahale lib - dropwizard/metrics@v3.1.2...v3.1.5#diff-6916c85d2dd08d89fe771c952e3b8512R120. Specifically, https://github.com/dropwizard/metrics/blob/b4d246d34e8a059b047567848b3522567cbe6108/metrics-graphite/src/main/java/com/codahale/metrics/graphite/Graphite.java#L120


",,alexmnyc,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 19 02:06:36 UTC 2017,,,,,,,,,,"0|i3k4m7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/17 15:35;apachespark;User 'alexmnyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19210;;;","19/Sep/17 02:06;jerryshao;Issue resolved by pull request 19210
[https://github.com/apache/spark/pull/19210];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalyst Optimizer does not preserve top-level metadata while collapsing projects,SPARK-22018,13102437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,15/Sep/17 00:29,17/May/20 17:58,14/Jul/23 06:30,15/Sep/17 05:32,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Optimizer,SQL,Structured Streaming,,,0,,,,,,,,,"If there are two projects like as follows.
{code}
Project [a_with_metadata#27 AS b#26]
+- Project [a#0 AS a_with_metadata#27]
   +- LocalRelation <empty>, [a#0, b#1]
{code}

Child Project has an output column with a metadata in it, and the parent Project has an alias that implicitly forwards the metadata. So this metadata is visible for higher operators. Upon applying CollapseProject optimizer rule, the metadata is not preserved.

{code}
Project [a#0 AS b#26]
+- LocalRelation <empty>, [a#0, b#1]
{code}

This is incorrect, as downstream operators that expect certain metadata (e.g. watermark in structured streaming) to identify certain fields will fail to do so.
",,apachespark,maropu,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 15 05:32:46 UTC 2017,,,,,,,,,,"0|i3k3lj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/17 00:35;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/19240;;;","15/Sep/17 05:32;tdas;Issue resolved by pull request 19240
[https://github.com/apache/spark/pull/19240];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark evaluation with multi-input stream operators is unspecified,SPARK-22017,13102397,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joseph.torres,joseph.torres,14/Sep/17 21:52,16/Sep/18 01:51,14/Jul/23 06:30,16/Sep/17 04:11,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Watermarks are stored as a single value in StreamExecution. If a query has multiple watermark nodes (which can generally only happen with multi input operators like union), a headOption call will arbitrarily pick one to use as the real one. This will happen independently in each batch, possibly leading to strange and undefined behavior.

We should instead choose the minimum from all watermark exec nodes as the query-wide watermark.",,apachespark,joseph.torres,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 16 04:11:42 UTC 2017,,,,,,,,,,"0|i3k3cn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/17 21:58;apachespark;User 'joseph-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/19239;;;","16/Sep/17 04:11;tdas;Issue resolved by pull request 19239
[https://github.com/apache/spark/pull/19239];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.codehaus.commons.compiler.CompileException: toString method is not declared,SPARK-22000,13102124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,hearded,hearded,14/Sep/17 02:29,28/Oct/19 15:38,14/Jul/23 06:30,27/Feb/19 05:53,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,SQL,,,,,1,,,,,,,,,"the error message say that toString is not declared on ""value13"" which is ""long"" type in generated code.
i think value13 should be Long type.

==error message
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 70, Column 32: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 70, Column 32: A method named ""toString"" is not declared in any enclosing class nor any supertype, nor through a static import


/* 033 */   private void apply1_2(InternalRow i) {
/* 034 */
/* 035 */
/* 036 */     boolean isNull11 = i.isNullAt(1);
/* 037 */     UTF8String value11 = isNull11 ? null : (i.getUTF8String(1));
/* 038 */     boolean isNull10 = true;
/* 039 */     java.lang.String value10 = null;
/* 040 */     if (!isNull11) {
/* 041 */
/* 042 */       isNull10 = false;
/* 043 */       if (!isNull10) {
/* 044 */
/* 045 */         Object funcResult4 = null;
/* 046 */         funcResult4 = value11.toString();
/* 047 */
/* 048 */         if (funcResult4 != null) {
/* 049 */           value10 = (java.lang.String) funcResult4;
/* 050 */         } else {
/* 051 */           isNull10 = true;
/* 052 */         }
/* 053 */
/* 054 */
/* 055 */       }
/* 056 */     }
/* 057 */     javaBean.setApp(value10);
/* 058 */
/* 059 */
/* 060 */     boolean isNull13 = i.isNullAt(12);
/* 061 */     long value13 = isNull13 ? -1L : (i.getLong(12));
/* 062 */     boolean isNull12 = true;
/* 063 */     java.lang.String value12 = null;
/* 064 */     if (!isNull13) {
/* 065 */
/* 066 */       isNull12 = false;
/* 067 */       if (!isNull12) {
/* 068 */
/* 069 */         Object funcResult5 = null;
/* 070 */         funcResult5 = value13.toString();
/* 071 */
/* 072 */         if (funcResult5 != null) {
/* 073 */           value12 = (java.lang.String) funcResult5;
/* 074 */         } else {
/* 075 */           isNull12 = true;
/* 076 */         }
/* 077 */
/* 078 */
/* 079 */       }
/* 080 */     }
/* 081 */     javaBean.setReasonCode(value12);
/* 082 */
/* 083 */   }",,BdLearner,hearded,jmchung,kabhwan,kiszk,maropu,mgaido,praetp,xsergey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/19 14:33;xsergey;testcase.zip;https://issues.apache.org/jira/secure/attachment/12959446/testcase.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 28 15:38:09 UTC 2019,,,,,,,,,,"0|i3k1nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/17 03:31;maropu;What's the query?;;;","14/Sep/17 07:26;kiszk;It would be good to generate {{((Long)value13).toString()}} to reduce # of boxing/unboxing.
Anyway, as @maropu pointed out, could you please put the query? Then, I will create a PR.;;;","14/Sep/17 07:34;srowen;That causes a boxing; better still is String.valueOf;;;","14/Sep/17 07:43;kiszk;Thank you for good suggestion. I will try to use {{String.valueOf}}.;;;","15/Sep/17 02:32;hearded;@members, my code generates this error message but i cannot make sample code to reproduce this issue. ;;;","18/Sep/17 17:10;kiszk;If there is no sample code, it may take a long time to fix this.
Is it possible to attach all code or to put code to create all of Dataset or DataFrame?;;;","20/Feb/19 14:33;xsergey;I've got similar issue. My sample code is attached.;;;","21/Feb/19 09:15;kabhwan;Thanks [~xsergey] I can easily reproduce it in master branch and have a fix. Will raise a PR shortly.
Credit to [~srowen] given I'm leveraging String.valueOf(). Thanks!;;;","28/Oct/19 15:25;BdLearner;[~srowen] I am using spark.2.4.1 version getting same error ... for more details please check this , [https://stackoverflow.com/questions/58593215/inserting-into-cassandra-table-from-spark-dataframe-results-in-org-codehaus-comm]     how to fix this ?;;;","28/Oct/19 15:38;srowen;As indicated, it's fixed in 3.0, not 2.4.x. The change is substantial so is hard to back-port, but I'd review a change that includes the three linked PRs above in 2.4, if it can be made to work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeJoinExec did not calculate its outputOrdering correctly during physical planning,SPARK-21998,13102039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,13/Sep/17 19:17,22/Sep/17 06:58,14/Jul/23 06:30,22/Sep/17 06:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Right now the calculation of SortMergeJoinExec's outputOrdering relies on the fact that its children have already been sorted on the join keys, while this is often not true until EnsureRequirements has been applied.
{code}
  /**
   * For SMJ, child's output must have been sorted on key or expressions with the same order as
   * key, so we can get ordering for key from child's output ordering.
   */
  private def getKeyOrdering(keys: Seq[Expression], childOutputOrdering: Seq[SortOrder])
    : Seq[SortOrder] = {
    keys.zip(childOutputOrdering).map { case (key, childOrder) =>
      SortOrder(key, Ascending, childOrder.sameOrderExpressions + childOrder.child - key)
    }
  }
{code}
Thus SortMergeJoinExec's outputOrdering is most likely not correct during the physical planning stage, and as a result, potential physical optimizations that rely on the required/output orderings, like SPARK-18591, will not work for SortMergeJoinExec.
The right behavior of {{getKeyOrdering(keys, childOutputOrdering)}} should be:
1. If the childOutputOrdering satisfies (is a superset of) the required child ordering => childOutputOrdering
2. Otherwise => required child ordering",,apachespark,maropu,maryannxue,ram_krish,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 19 18:55:22 UTC 2017,,,,,,,,,,"0|i3k17b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/17 22:35;maropu;I think the orders depend on their children, e.g. https://github.com/apache/spark/commit/e9c91badce64731ffd3e53cbcd9f044a7593e6b8. If you have other cases the Spark planner adds unnecessary sorts, could you put an concrete example? Thanks! ;;;","14/Sep/17 05:20;maryannxue;Thank you for pointing this out, [~maropu]! Yes, you are right.
It would be more accurate to say that the outputOrdering of SortMergeJoinExec should be a superset of (i.e., satisfy) the join key ordering.
Let's look at {{SELECT * FROM t1 JOIN t2 ON t1.a = t2.z}} for example and focus on {{t1}} for simplicity,
1) If t1 is not sorted on a, the SMJ outputOrdering should be (a, ASC)
2) If t1 is sorted on a, the SMJ outputOrdering should be (a, ASC)
3) If t1 is sorted both on a and b, the SMJ outputOrdering should at least include (a, ASC), and whether the SMJ output is still sorted on b remains implementation specific, and in current Spark SQL implementation it surely is. But we can imagine a less smart SMJ implementation which cannot avoid extra sorting on t1 and meanwhile uses an unstable sorting can destroy the sortedness of column b.

Anyway, my point is the current SMJ implementation will return NOTHING as outputOrdering when the child ordering is not immediately available and can only return the right value at a later stage after EnsureRequirements happens. You can try my query example in https://issues.apache.org/jira/browse/SPARK-18591?focusedCommentId=16165148&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16165148 with your proposed change for SPARK-18591 and see what's going on.;;;","19/Sep/17 18:54;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/19281;;;","19/Sep/17 18:55;maryannxue;Thanks again for your comment, [~maropu]! I changed the title and description of this JIRA accordingly and created a PR as https://github.com/apache/spark/pull/19281. Could you please take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming ignores files with spaces in the file names,SPARK-21996,13101975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xiayunsun,isharamet,isharamet,13/Sep/17 14:11,18/Jan/18 00:45,14/Jul/23 06:30,18/Jan/18 00:43,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,3,,,,,,,,,"I tried to stream text files from folder and noticed that files inside this folder with spaces in their names ignored and there are some warnings in the log:

{code}
17/09/13 16:15:14 WARN InMemoryFileIndex: The directory file:/in/two%20two.txt was not found. Was it deleted very recently?
{code}

I found that this happens due to duplicate file path URI encoding (I suppose) and the actual URI inside path objects looks like this {{file:/in/two%2520two.txt}}.

To reproduce this issue just place some text files with spaces in their names and execute some simple streaming code:

{code:java}
/in
    /one.txt
    /two two.txt
    /three.txt
{code}

{code}
sparkSession.readStream.textFile(""/in"")
      .writeStream
      .option(""checkpointLocation"", ""/checkpoint"")
      .format(""text"")
      .start(""/out"")
      .awaitTermination()
{code}

The result will contain only content of files {{one.txt}} and {{three.txt}}.
","openjdk version ""1.8.0_131""
OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-2ubuntu1.17.04.3-b11)
OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)
",apachespark,isharamet,kirills2006@gmail.com,mgaido,xiayunsun,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/17 14:58;isharamet;spark-streaming.zip;https://issues.apache.org/jira/secure/attachment/12886885/spark-streaming.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 00:43:11 UTC 2018,,,,,,,,,,"0|i3k0t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/17 14:16;srowen;Your code doesn't show a path with a space. Are you sure the problem isn't that you're referring to a local dir in a distirbuted job?;;;","13/Sep/17 14:24;isharamet;Hi Sean,

As I wrote in the description this happens while reading from folder, which contains files with spaces in their names.

Thanks,
Ivan
;;;","13/Sep/17 14:26;srowen;/dir does not occur in your code snippet. I'm questioning whether you're sure about the example. You are also apparently running a distributed job looking for a local driver file. That won't work.;;;","13/Sep/17 14:38;isharamet;Sure, it's generic example: the folder names and locations don't matter, just provide your own.;;;","13/Sep/17 14:59;isharamet;[~sowen], I attached sample app to reproduce this issue. Sorry I didn't do it earlier.;;;","15/Sep/17 14:22;apachespark;User 'xysun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19247;;;","15/Sep/17 14:29;xiayunsun;I can reproduce this issue for master branch, and found out it was from using {{.toString}} instead of {{.getPath}} to get the file path in string format. 

I pushed a PR to make it work for streaming. But there're other places in spark where we're using `.toString`. Not sure if it's a general concern. ;;;","18/Jan/18 00:43;zsxwing;Issue resolved by pull request 19247
[https://github.com/apache/spark/pull/19247];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[LAUNCHER] LauncherServer acceptConnections thread sometime dies if machine has very high load,SPARK-21991,13101894,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nivox,nivox,nivox,13/Sep/17 09:02,25/Oct/17 18:04,14/Jul/23 06:30,25/Oct/17 17:12,2.0.2,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.3,2.2.1,2.3.0,,Spark Submit,,,,,1,,,,,,,,,"The way the _LauncherServer_ _acceptConnections_ thread schedules client timeouts causes (non-deterministically) the thread to die with the following exception if the machine is under very high load:

{noformat}
Exception in thread ""LauncherServer-1"" java.lang.IllegalStateException: Task already scheduled or cancelled
        at java.util.Timer.sched(Timer.java:401)
        at java.util.Timer.schedule(Timer.java:193)
        at org.apache.spark.launcher.LauncherServer.acceptConnections(LauncherServer.java:249)
        at org.apache.spark.launcher.LauncherServer.access$000(LauncherServer.java:80)
        at org.apache.spark.launcher.LauncherServer$1.run(LauncherServer.java:143)
{noformat}

The issue is related to the ordering of actions that the _acceptConnections_ thread uses to handle a client connection:

# create timeout action
# create client thread
# start client thread
# schedule timeout action

Under normal conditions the scheduling of the timeout action happen before the client thread has a chance to start, however if the machine is under very high load the client thread can receive CPU time before the timeout action gets scheduled.

If this condition happen, the client thread cancel the timeout action (which is not yet been scheduled) and goes on, but as soon as the _acceptConnections_ thread gets the CPU back, it will try to schedule the timeout action (which has already been canceled) thus raising the exception.

Changing the order in which the client thread gets started and the timeout gets scheduled seems to be sufficient to fix this issue.

As stated above the issue is non-deterministic, I faced the issue multiple times on a single-node machine submitting a high number of short jobs sequentially, but I couldn't easily create a test reproducing the issue. ","Single node machine running Ubuntu 16.04.2 LTS (4.4.0-79-generic)
YARN 2.7.2
Spark 2.0.2",aash,apachespark,mynos_main@yahoo.it,nivox,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 25 18:04:04 UTC 2017,,,,,,,,,,"0|i3k0b3:",9223372036854775807,,,,,vanzin,,,,,,,,,,,,,,,,,,,"13/Sep/17 09:17;apachespark;User 'nivox' has created a pull request for this issue:
https://github.com/apache/spark/pull/19217;;;","24/Oct/17 08:37;aash;Thanks for debugging and diagnosing this [~nivox]! I'm seeing the same issue right now on one of my Spark clusters so am interested in getting your fix in to mainline Spark for my users.

Have you deployed the change from your linked PR in a live setting, and has it fixed the issue for you?;;;","24/Oct/17 08:58;nivox;[~aash] I've tested it on a staging environment and it seemed to resolve the issue. However I didn't deploy it to the affected system since I didn't want to run a patched Spark distribution. For the time being I've setup a script to detect when the issue happen and restart the component. 
This is obviously an ugly workaround and I would much prefer to have Spark handle it.;;;","25/Oct/17 17:29;aash;Thanks for the contribution to Spark [~nivox]!  I'll be testing this on some clusters of mine and will echo back here if I see any problems.  Cheers!;;;","25/Oct/17 18:04;apachespark;User 'ash211' has created a pull request for this issue:
https://github.com/apache/spark/pull/19574;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.3 cannot read 2.2 event logs,SPARK-21987,13101764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,vanzin,vanzin,12/Sep/17 20:12,15/Sep/17 07:48,14/Jul/23 06:30,15/Sep/17 07:48,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Reported by [~jincheng] in a comment in SPARK-18085:

{noformat}
com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""metadata"" (class org.apache.spark.sql.execution.SparkPlanInfo), not marked as ignorable (4 known properties: ""simpleString"", ""nodeName"", ""children"", ""metrics""])
 at [Source: {""Event"":""org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart"",""executionId"":0,""description"":""json at NativeMethodAccessorImpl.java:0"",""details"":""org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:487)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:280)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.GatewayConnection.run(GatewayConnection.java:214)\njava.lang.Thread.run(Thread.java:748)"",""physicalPlanDescription"":""== Parsed Logical Plan ==\nRepartition 200, true\n+- LogicalRDD [uid#327L, gids#328]\n\n== Analyzed Logical Plan ==\nuid: bigint, gids: array<bigint>\nRepartition 200, true\n+- LogicalRDD [uid#327L, gids#328]\n\n== Optimized Logical Plan ==\nRepartition 200, true\n+- LogicalRDD [uid#327L, gids#328]\n\n== Physical Plan ==\nExchange RoundRobinPartitioning(200)\n+- Scan ExistingRDD[uid#327L,gids#328]"",""sparkPlanInfo"":{""nodeName"":""Exchange"",""simpleString"":""Exchange RoundRobinPartitioning(200)"",""children"":[{""nodeName"":""ExistingRDD"",""simpleString"":""Scan ExistingRDD[uid#327L,gids#328]"",""children"":[],""metadata"":{},""metrics"":[{""name"":""number of output rows"",""accumulatorId"":140,""metricType"":""sum""}]}],""metadata"":{},""metrics"":[{""name"":""data size total (min, med, max)"",""accumulatorId"":139,""metricType"":""size""}]},""time"":1504837052948}; line: 1, column: 1622] (through reference chain: org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart[""sparkPlanInfo""]->org.apache.spark.sql.execution.SparkPlanInfo[""children""]->com.fasterxml.jackson.module.scala.deser.BuilderWrapper[0]->org.apache.spark.sql.execution.SparkPlanInfo[""metadata""])
	at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:51)
{noformat}

This was caused by SPARK-17701 (which at this moment is still open even though the patch has been committed).",,apachespark,cloud_fan,jincheng,smilegator,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 14 17:21:05 UTC 2017,,,,,,,,,,"0|i3jziv:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"12/Sep/17 20:38;smilegator;Thanks for reporting this! We need to ensure Spark 2.3 still can process 2.2 event logs and revert the changes in SparkPlanGraph;;;","13/Sep/17 06:01;jincheng;In fact , It only occurs when using SQL;;;","13/Sep/17 07:44;cloud_fan;Adding back the `SparkPlanGraph.metadata` is one solution, and we can also annotate `SparkPlanGraph` with {{@JsonIgnoreProperties(ignoreUnknown = true)}}. IMO keeping the {{metadata}} field is a little ugly, since `SparkPlanGraph` is a developer API, and we should be able to remove some fields if necessary.;;;","14/Sep/17 17:21;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19237;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark PairDeserializer is broken for double-zipped RDDs,SPARK-21985,13101697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,stuarteberg,stuarteberg,12/Sep/17 16:05,12/Dec/22 18:11,14/Jul/23 06:30,17/Sep/17 17:47,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,PySpark,,,,,0,bug,,,,,,,,"PySpark fails to deserialize double-zipped RDDs.  For example, the following example used to work in Spark 2.0.2:


{code:}
>>> a = sc.parallelize('aaa')
>>> b = sc.parallelize('bbb')
>>> c = sc.parallelize('ccc')
>>> a_bc = a.zip( b.zip(c) )
>>> a_bc.collect()
[('a', ('b', 'c')), ('a', ('b', 'c')), ('a', ('b', 'c'))]
{code}

But in Spark >=2.1.0, it fails (regardless of Python 2 vs 3):

{code:}
>>> a_bc.collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/workspace/spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py"", line 810, in collect
    return list(_load_from_socket(port, self._jrdd_deserializer))
  File ""/workspace/spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py"", line 329, in _load_stream_without_unbatching
    if len(key_batch) != len(val_batch):
TypeError: object of type 'itertools.izip' has no len()
{code}

As you can see, the error seems to be caused by [a check in the PairDeserializer class|https://github.com/apache/spark/blob/d03aebbe6508ba441dc87f9546f27aeb27553d77/python/pyspark/serializers.py#L346-L348]:

{code:}
if len(key_batch) != len(val_batch):
    raise ValueError(""Can not deserialize PairRDD with different number of items""
                     "" in batches: (%d, %d)"" % (len(key_batch), len(val_batch)))
{code}

If that check is removed, then the example above works without error.  Can the check simply be removed?",,apachespark,holden,stuarteberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 17 17:47:45 UTC 2017,,,,,,,,,,"0|i3jz47:",9223372036854775807,,,,,,,,,,,,,2.1.2,,,,,,,,,,,"13/Sep/17 22:40;holden;CC [~a1ray] moving the discussion back here from github, I'm looking to try and cut 2.1.2 RC1 today/tonight if it's a simple enough fix (and it sounds like we could skip this check) would be nice to have this in release. Let me know if that sounds reasonable.;;;","14/Sep/17 01:34;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/19226;;;","14/Sep/17 03:34;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19228;;;","14/Sep/17 03:35;gurwls223;Oh, I am sorry, [~holdenk] and [~a1ray]. I just submitted a PR and I did not notice the comment here (I made this PR yesterday in my local and submitted now without seeing the comments here ..). Could you guys please review my approach please?;;;","14/Sep/17 03:37;gurwls223;Doh, sorry, the PR comment {{User 'aray' has created a pull request for this issue:}} didn't show up my browser when I write comment ! ;;;","17/Sep/17 17:47;gurwls223;Issue resolved by pull request 19226
[https://github.com/apache/spark/pull/19226];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
References in grouping functions should be indexed with resolver,SPARK-21980,13101579,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,donnyzone,donnyzone,donnyzone,12/Sep/17 09:24,13/Sep/17 17:11,14/Jul/23 06:30,13/Sep/17 17:10,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"In our spark-2.1 cluster, when users sumbit queries like

{code:sql}
select a, grouping(b), sum(c) from table group by a, b with cube
{code}

It works well. However, when the query is 

{code:sql}
select a, grouping(B), sum(c) from table group by a, b with cube
{code}

We will get the exception:

{code:java}
org.apache.spark.sql.AnalysisException: Column of grouping (B#11) can't be found in grouping columns a#10,b#11
{code}

The root cause is the replaceGroupingFunc's incorrect logic in ResolveGroupingAnalytics
 rule. It indexes the column without resolver.",,apachespark,donnyzone,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 09:37:03 UTC 2017,,,,,,,,,,"0|i3jye7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/17 09:37;apachespark;User 'DonnyZone' has created a pull request for this issue:
https://github.com/apache/spark/pull/19202;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve QueryPlanConstraints framework,SPARK-21979,13101574,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,12/Sep/17 09:10,09/Sep/18 22:49,14/Jul/23 06:30,12/Sep/17 20:03,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Improve QueryPlanConstraints framework, make it robust and simple.
In apache/spark#15319, constraints for expressions like a = f(b, c) is resolved.
However, for expressions like

a = f(b, c) && c = g(a, b)
The current QueryPlanConstraints framework will produce non-converging constraints.
Essentially, the problem is caused by having both the name and child of aliases in the same constraint set. We infer constraints, and push down constraints as predicates in filters, later on these predicates are propagated as constraints, etc..
Simply using the alias names only can resolve these problems. The size of constraints is reduced without losing any information. We can always get these inferred constraints on child of aliases when pushing down filters.

Also, the EqualNullSafe between name and child in propagating alias is meaningless

allConstraints += EqualNullSafe(e, a.toAttribute)
It just produce redundant constraints.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 09:14:06 UTC 2017,,,,,,,,,,"0|i3jyd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/17 09:14;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/19201;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SinglePartition optimizations break certain Streaming Stateful Aggregation requirements,SPARK-21977,13101529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,12/Sep/17 03:38,14/Dec/17 10:14,14/Jul/23 06:30,20/Sep/17 07:02,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"This is a bit hard to explain as there are several issues here, I'll try my best. Here are the requirements:

  1. A StructuredStreaming Source that can generate empty RDDs with 0 partitions
  2. A StructuredStreaming query that uses the above source, performs a stateful aggregation (mapGroupsWithState, groupBy.count, ...), and coalesce's by 1

The crux of the problem is that when a dataset has a `coalesce(1)` call, it receives a `SinglePartition` partitioning scheme. This scheme satisfies most required distributions used for aggregations such as HashAggregateExec. This causes a world of problems:

 Symptom 1. If the input RDD has 0 partitions, the whole lineage will receive 0 partitions, nothing will be executed, the state store will not create any delta files. When this happens, the next trigger fails, because the StateStore fails to load the delta file for the previous trigger

 Symptom 2. Let's say that there was data. Then in this case, if you stop your stream, and change `coalesce(1)` with `coalesce(2)`, then restart your stream, your stream will fail, because `spark.sql.shuffle.partitions - 1` number of StateStores will fail to find its delta files.

To fix the issues above, we must check that the partitioning of the child of a `StatefulOperator` satisfies:
  If the grouping expressions are empty:
    a) AllTuple distribution
    b) Single physical partition
  If the grouping expressions are non empty:
    a) Clustered distribution
    b) spark.sql.shuffle.partition # of partitions
whether or not coalesce(1) exists in the plan, and whether or not the input RDD for the trigger has any data.

Once you fix the above problem by adding an Exchange to the plan, you come across the following bug:

If you call `coalesce(1).groupBy().count()` on a Streaming DataFrame, and if you have a trigger with no data, `StateStoreRestoreExec` doesn't return the prior state. However, for this specific aggregation, `HashAggregateExec` after the restore returns a (0, 0) row, since we're performing a count, and there is no data. Then this data gets stored in `StateStoreSaveExec` causing the previous counts to be overwritten and lost.
  ",,apachespark,brkyvz,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22752,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 10:55:04 UTC 2017,,,,,,,,,,"0|i3jy33:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"12/Sep/17 04:36;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/19196;;;","21/Sep/17 10:55;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommandUtils.updateTableStats should call refreshTable,SPARK-21969,13101101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,bograd,bograd,10/Sep/17 19:49,19/Sep/17 21:19,14/Jul/23 06:30,19/Sep/17 21:19,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"The table is cached so even though statistics are removed, they will still be used by the existing sessions.


{code}
spark.range(100).write.saveAsTable(""tab1"")
sql(""analyze table tab1 compute statistics"")
sql(""explain cost select distinct * from tab1"").show(false)
{code}

Produces:
{code}
Relation[id#103L] parquet, Statistics(sizeInBytes=784.0 B, rowCount=100, hints=none)
{code}


{code}
spark.range(100).write.mode(""append"").saveAsTable(""tab1"")
sql(""explain cost select distinct * from tab1"").show(false)
{code}

After append something, the same stats are used
{code}
Relation[id#135L] parquet, Statistics(sizeInBytes=784.0 B, rowCount=100, hints=none)
{code}

Manually refreshing the table removes the stats
{code}
spark.sessionState.catalog.refreshTable(TableIdentifier(""tab1""))
sql(""explain cost select distinct * from tab1"").show(false)
{code}

{code}
Relation[id#155L] parquet, Statistics(sizeInBytes=1568.0 B, hints=none)
{code}",,apachespark,bograd,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21237,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 16 12:19:04 UTC 2017,,,,,,,,,,"0|i3jvgn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"16/Sep/17 12:19;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19252;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attempting to save large Word2Vec model hangs driver in constant GC.,SPARK-21958,13100846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,travis.hegner,travis.hegner,travis.hegner,08/Sep/17 15:41,15/Sep/17 13:18,14/Jul/23 06:30,15/Sep/17 13:17,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,,,,,0,easyfix,patch,performance,,,,,,"In the new version of Word2Vec, the model saving was modified to estimate an appropriate number of partitions based on the kryo buffer size. This is a great improvement, but there is a caveat for very large models.

The {{(word, vector)}} tuple goes through a transformation to a local case class of {{Data(word, vector)}}... I can only assume this is for the kryo serialization process. The new version of the code iterates over the entire vocabulary to do this transformation (the old version wrapped the entire datum) in the driver's heap. Only to have the result then distributed to the cluster to be written into it's parquet files.

With extremely large vocabularies (~2 million docs, with uni-grams, bi-grams, and tri-grams), that local driver transformation is causing the driver to hang indefinitely in GC as I can only assume that it's generating millions of short lived objects which can't be evicted fast enough.

Perhaps I'm overlooking something, but it seems to me that since the result is distributed over the cluster to be saved _after_ the transformation anyway, we may as well distribute it _first_, allowing the cluster resources to do the transformation more efficiently, and then write the parquet file from there.

I have a patch implemented, and am in the process of testing it at scale. I will open a pull request when I feel that the patch is successfully resolving the issue, and after making sure that it passes unit tests.","Running spark on yarn, hadoop 2.7.2 provided by the cluster",apachespark,mgaido,mlnick,travis.hegner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Sep 15 13:17:56 UTC 2017,,,,,,,,,,"0|i3jtun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/17 07:51;mlnick;Seems like your proposal could improve things - but yeah let's see what your testing results are.;;;","11/Sep/17 15:36;travis.hegner;Running the patch applied to tag {{v2.2.0}} allows a 4.0G model to save and load as expected, where without the patch my driver would hang in GC forever, even with a 64G heap. I believe the patch is working successfully so I will re-base against master and open the pull request. Running {{mvn test}} for only the ML sub-component works, but I have failing tests in areas of code I haven't touched.;;;","11/Sep/17 15:54;apachespark;User 'travishegner' has created a pull request for this issue:
https://github.com/apache/spark/pull/19191;;;","15/Sep/17 13:17;mlnick;Issue resolved by pull request 19191
[https://github.com/apache/spark/pull/19191];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JacksonUtils should verify MapType's value type instead of key type,SPARK-21954,13100711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,08/Sep/17 08:26,12/Dec/22 18:10,14/Jul/23 06:30,09/Sep/17 10:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{{JacksonUtils.verifySchema}} verifies if a data type can be converted to JSON. For {{MapType}}, it now verifies the key type. However, in {{JacksonGenerator}}, when converting a map to JSON, we only care about its values and create a writer for the values. The keys in a map are treated as strings by calling {{toString}} on the keys.

Thus, we should change {{JacksonUtils.verifySchema}} to verify the value type of {{MapType}}.
",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 09 10:16:15 UTC 2017,,,,,,,,,,"0|i3jt0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/17 08:29;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19167;;;","09/Sep/17 10:16;gurwls223;Fixed in https://github.com/apache/spark/pull/19167;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show both memory and disk bytes spilled if either is present,SPARK-21953,13100706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aash,aash,aash,08/Sep/17 07:47,18/Sep/17 02:44,14/Jul/23 06:30,18/Sep/17 02:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Web UI,,,,,0,,,,,,,,,"https://github.com/apache/spark/commit/a1f0992faefbe042a9cb7a11842a817c958e4797#diff-fa4cfb2cce1b925f55f41f2dfa8c8501R61 should be {{||}} not {{&&}}

As written now, there must be both memory and disk bytes spilled to show either of them.  If there is only one of those types of spill recorded, it will be hidden.",,aash,ajbozarth,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 18 02:44:08 UTC 2017,,,,,,,,,,"0|i3jszj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/17 07:50;apachespark;User 'ash211' has created a pull request for this issue:
https://github.com/apache/spark/pull/19164;;;","18/Sep/17 02:44;cloud_fan;Issue resolved by pull request 19164
[https://github.com/apache/spark/pull/19164];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.sql.tests.SQLTests2 should stop SparkContext.,SPARK-21950,13100664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,08/Sep/17 02:34,08/Sep/17 05:28,14/Jul/23 06:30,08/Sep/17 05:28,2.0.3,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.1,2.3.0,,PySpark,SQL,Tests,,,0,,,,,,,,,{{pyspark.sql.tests.SQLTests2}} doesn't stop newly created spark context in the test and it might affect the following tests.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 05:28:11 UTC 2017,,,,,,,,,,"0|i3jsq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/17 02:43;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/19158;;;","08/Sep/17 05:28;ueshin;Issue resolved by pull request 19158
[https://github.com/apache/spark/pull/19158];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
monotonically_increasing_id doesn't work in Structured Streaming,SPARK-21947,13100633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,zsxwing,zsxwing,07/Sep/17 23:57,06/Oct/17 20:10,14/Jul/23 06:30,06/Oct/17 20:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Right now, it will return wrong answers. If we cannot support it, we should throw an exception if someone is using it.",,apachespark,lwlin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 25 04:47:04 UTC 2017,,,,,,,,,,"0|i3jsjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/17 04:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19336;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: InMemoryCatalogedDDLSuite.`alter table: rename cached table`,SPARK-21946,13100624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,dongjoon,dongjoon,07/Sep/17 23:09,08/Sep/17 16:40,14/Jul/23 06:30,08/Sep/17 16:40,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Tests,,,,,0,,,,,,,,,"According to the [Apache Spark Jenkins History|https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-2.7/lastCompletedBuild/testReport/org.apache.spark.sql.execution.command/InMemoryCatalogedDDLSuite/alter_table__rename_cached_table/history/]

InMemoryCatalogedDDLSuite.`alter table: rename cached table` is very flaky. We had better stablize this.

{code}
- alter table: rename cached table !!! CANCELED !!!
  Array([2,2], [1,1]) did not equal Array([1,1], [2,2]) bad test: wrong data (DDLSuite.scala:786)
{code}

",,apachespark,dongjoon,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 06:15:08 UTC 2017,,,,,,,,,,"0|i3jshb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/17 02:40;kiszk;If someone has not worked for this, I will create a PR.;;;","08/Sep/17 02:41;dongjoon;Thanks! Go for it!  :);;;","08/Sep/17 06:15;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/19159;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark --py-files doesn't work in yarn client mode,SPARK-21945,13100565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,tgraves,tgraves,07/Sep/17 20:01,12/Dec/22 18:11,14/Jul/23 06:30,17/May/18 04:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.1,2.4.0,,,,PySpark,,,,,0,,,,,,,,,"I tried running pyspark with --py-files pythonfiles.zip  but it doesn't properly add the zip file to the PYTHONPATH.

I can work around by exporting PYTHONPATH.

Looking in SparkSubmitCommandBuilder.buildPySparkShellCommand  I don't see this supported at all.   If that is the case perhaps it should be moved to improvement.

Note it works via spark-submit in both client and cluster mode to run python script.",,apachespark,bryanc,devaraj,dongjoon,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 24 17:02:14 UTC 2018,,,,,,,,,,"0|i3js4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/18 08:12;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/21267;;;","17/May/18 04:09;gurwls223;Fixed in https://github.com/apache/spark/pull/21267;;;","17/May/18 19:06;dongjoon;Hi, [~vanzin], [~tgraves], [~hyukjin.kwon].
I'm wondering if we can have this at 2.3.1 RC2, too? Or, at least, `branch-2.3` sometime?;;;","17/May/18 19:47;vanzin;Let me run some tests and if they're ok, I'll cherry pick.;;;","17/May/18 21:35;vanzin;This works for cluster mode and pyspark (shell), but does not for spark-submit in client mode. Still this is an improvement, so I'll cherry pick.

It would be nice to open a separate bug to close the remaining gap.;;;","18/May/18 05:53;dongjoon;Thank you for verifying and including this in 2.3.1 RC2, [~vanzin].;;;","23/May/18 02:24;gurwls223;[~vanzin], I tried spark-submit in client mode and cluster mode with both zip and .py files and look working fine. Mind if I how you did? I tried as written in the PR description FYI.;;;","23/May/18 17:27;vanzin;{noformat}
$ cat test.py 
from pyspark import SparkContext, SparkConf
import funcs

sc = SparkContext()
funcs.test()
sc.stop()

$ cat lib/funcs.py 

def test():
  print ""This is a test.""

$ spark-submit --master yarn --py-files lib/funcs.py test.py         
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/SPARK2-2.3.0.cloudera3-1.cdh5.13.3.p0.378171/lib/spark2/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.15.1-1.cdh5.15.1.p0.10/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Traceback (most recent call last):
  File ""/home/systest/test.py"", line 2, in <module>
    import funcs
ImportError: No module named funcs

$ pyspark --master yarn --py-files lib/funcs.py
[blah blah blah]
Using Python version 2.7.5 (default, Sep 15 2016 22:37:39)
SparkSession available as 'spark'.
>>> import funcs
>>> 
{noformat};;;","24/May/18 06:12;gurwls223;Right. This case looks weird again. In case the given Python file is .py file (zip file seems fine), seems the python path is dynamically added after the context is got initialized.

with this pyFile:

{code}
$ cat /home/spark/tmp.py
def testtest():
    return 1
{code}

This works:

{code}
$ cat app.py
import pyspark
pyspark.sql.SparkSession.builder.getOrCreate()
import tmp
print(""************************%s"" % tmp.testtest())

$ ./bin/spark-submit --master yarn --deploy-mode client --py-files /home/spark/tmp.py app.py
...
************************1
{code}

but this doesn't:

{code}
$ cat app.py
import pyspark
import tmp
pyspark.sql.SparkSession.builder.getOrCreate()
print(""************************%s"" % tmp.testtest())

$ ./bin/spark-submit --master yarn --deploy-mode client --py-files /home/spark/tmp.py app.py
Traceback (most recent call last):
  File ""/home/spark/spark/app.py"", line 2, in <module>
    import tmp
ImportError: No module named tmp
{code}

;;;","24/May/18 07:22;gurwls223;cc [~jerryshao] FYI.;;;","24/May/18 09:07;gurwls223;Same thing happens in standalone mode too.;;;","24/May/18 16:37;vanzin;It happens because the import happens before the context is initialized, and your fix only copies the files during initialization of the context.

To fix this case you'd have to add logic to perform the copy into the launcher library, which would be kinda weird...;;;","24/May/18 16:47;gurwls223;To be more correct, the paths are added as are given my investigation so far. It's fine for zip archive but for .py file the paths shouldn't be added as are (but its parent directory) ...
so for py files, yes, we should copy them too.

It's weird but I think this is all because we happened to support .py file in the same option whereas PYTHONPATH doesn't expect a .py file.;;;","24/May/18 16:48;gurwls223;Just for clarification, zip file works fine because the python paths are added as are to PythonRunner given my investigation so far.;;;","24/May/18 17:02;gurwls223;For another clarification, the launch execution codepath is diverted for shell (and also Yarn) specifically. So, I believe the merged PR itself is a-okay.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Stop storing unused attemptId in SQLTaskMetrics,SPARK-21941,13100382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aash,aash,aash,07/Sep/17 07:11,09/Sep/17 06:34,14/Jul/23 06:30,09/Sep/17 06:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,,"Currently SQLTaskMetrics has a long attemptId field on it that is unused, with a TODO saying to populate the value in the future.  We should save this memory by leaving the TODO but taking the unused field out of the class.

I have a driver that heap dumped on OOM and has 390,105 instances of SQLTaskMetric -- removing this 8 bytes field will save roughly 390k*8 = 3.1MB of heap space.  It's not going to fix my OOM, but there's no reason to put this pressure on the GC if we don't get anything by storing it.

https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/ui/SQLListener.scala#L485",,aash,ajbozarth,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 07 07:18:05 UTC 2017,,,,,,,,,,"0|i3jqzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/17 07:18;apachespark;User 'ash211' has created a pull request for this issue:
https://github.com/apache/spark/pull/19153;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support `ALTER TABLE table_name ADD COLUMNS(..)` for ORC data source,SPARK-21929,13099997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,06/Sep/17 00:43,24/Oct/17 00:22,14/Jul/23 06:30,24/Oct/17 00:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"SPARK-19261 implemented `ADD COLUMNS` at Spark 2.2, but ORC data source is not supported due to its limit.

{code}
scala> sql(""CREATE TABLE tab (c1 int, c2 int, c3 int) USING ORC PARTITIONED BY (c3)"")
scala> sql(""ALTER TABLE tab ADD COLUMNS (c4 int)"")
org.apache.spark.sql.AnalysisException:
ALTER ADD COLUMNS does not support datasource table with type ORC.
You must drop and re-create the table for adding the new columns. Tables: `tab`;
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,SPARK-19261,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 20 21:28:04 UTC 2017,,,,,,,,,,"0|i3jom7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/17 21:28;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19545;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException for custom Kryo registrator class during serde in netty threads,SPARK-21928,13099975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,jbrock,jbrock,05/Sep/17 22:37,25/Oct/18 14:14,14/Jul/23 06:30,21/Sep/17 17:21,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Spark Core,,,,,0,,,,,,,,,"From SPARK-13990 & SPARK-13926, Spark's SerializerManager has its own instance of a KryoSerializer which does not have the defaultClassLoader set on it. For normal task execution, that doesn't cause problems, because the serializer falls back to the current thread's task loader, which is set anyway.

however, netty maintains its own thread pool, and those threads don't change their classloader to include the extra use jars needed for the custom kryo registrator. That only matters when blocks are sent across the network which force serde in the netty thread. That won't happen often, because (a) spark tries to execute tasks where the RDDs are already cached and (b) broadcast blocks generally don't require any serde in the netty threads (that occurs in the task thread that is reading the broadcast value).  However it can come up with remote cache reads, or if fetching a broadcast block forces another block to disk, which requires serialization.

This doesn't effect the shuffle path, because the serde is never done in the threads created by netty.

I think a fix for this should be fairly straight-forward, we just need to set the classloader on that extra kryo instance.

 (original problem description below)

I unfortunately can't reliably reproduce this bug; it happens only occasionally, when training a logistic regression model with very large datasets. The training will often proceed through several {{treeAggregate}} calls without any problems, and then suddenly workers will start running into this {{java.lang.ClassNotFoundException}}.

After doing some debugging, it seems that whenever this error happens, Spark is trying to use the {{sun.misc.Launcher$AppClassLoader}} {{ClassLoader}} instance instead of the usual {{org.apache.spark.util.MutableURLClassLoader}}. {{MutableURLClassLoader}} can see my custom Kryo registrator, but the {{AppClassLoader}} instance can't.

When this error does pop up, it's usually accompanied by the task seeming to hang, and I need to kill Spark manually.

I'm running a Spark application in cluster mode via spark-submit, and I have a custom Kryo registrator. The JAR is built with {{sbt assembly}}.

Exception message:

{noformat}
17/08/29 22:39:04 ERROR TransportRequestHandler: Error opening block StreamChunkId{streamId=542074019336, chunkIndex=0} for request from /10.0.29.65:34332
org.apache.spark.SparkException: Failed to register classes with Kryo
    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:139)
    at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:292)
    at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:277)
    at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:186)
    at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:169)
    at org.apache.spark.storage.BlockManager$$anonfun$dropFromMemory$3.apply(BlockManager.scala:1382)
    at org.apache.spark.storage.BlockManager$$anonfun$dropFromMemory$3.apply(BlockManager.scala:1377)
    at org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)
    at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1377)
    at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:524)
    at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:545)
    at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:539)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:539)
    at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
    at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
    at org.apache.spark.memory.StaticMemoryManager.acquireStorageMemory(StaticMemoryManager.scala:72)
    at org.apache.spark.storage.memory.MemoryStore.putBytes(MemoryStore.scala:147)
    at org.apache.spark.storage.BlockManager.maybeCacheDiskBytesInMemory(BlockManager.scala:1143)
    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doGetLocalBytes(BlockManager.scala:594)
    at org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2.apply(BlockManager.scala:559)
    at org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2.apply(BlockManager.scala:559)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:559)
    at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:353)
    at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61)
    at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:60)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
    at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31)
    at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:89)
    at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.foo.bar.MyKryoRegistrator
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:134)
    at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:134)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:134)
    ... 60 more
{noformat}

My Spark session is created like so:

{code:java}
val spark = SparkSession.builder()
                .appName(""FooBar"")
                .config(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
                .config(""spark.kryoserializer.buffer.max"", ""2047m"")                                        
                .config(""spark.kryo.registrator"",""com.foo.bar.MyKryoRegistrator"")
                .config(""spark.kryo.registrationRequired"", ""true"")
                .config(""spark.network.timeout"", ""3600s"")
                .config(""spark.driver.maxResultSize"", ""0"")
                .config(""spark.rdd.compress"", ""true"")
                .config(""spark.shuffle.spill"", ""true"")
                .getOrCreate()
{code}


Here are the config options I'm passing to spark-submit:

{noformat}
--conf ""spark.executor.heartbeatInterval=400s""
--conf ""spark.speculation=true""
--conf ""spark.speculation.multiplier=30""
--conf ""spark.speculation.quantile=0.95""
--conf ""spark.memory.useLegacyMode=true""
--conf ""spark.shuffle.memoryFraction=0.8""
--conf ""spark.storage.memoryFraction=0.2""
--driver-java-options ""-XX:+UseG1GC""
{noformat}

I was able to find a workaround: copy your application JAR to each of the machines in your cluster, and pass the JAR's path to {{spark-submit}} with:

{noformat}
--conf ""spark.driver.extraClassPath=/path/to/sparklogisticregre‌​ssion.jar""
--conf ""spark.executor.extraClassPath=/path/to/sparklogisticreg‌​ression.jar""
{noformat}
",,apachespark,bryanc,irashid,jbrock,peng.meng@intel.com,soravgulati@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22083,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 25 14:14:53 UTC 2018,,,,,,,,,,"0|i3johb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/17 13:40;irashid;Hi [~jbrock],

thanks for reporting this.  I have another bug report which I think is similar, I don't think this actually has anything to do with ML, it can happen anytime cached RDDs get sent over the network (perhaps that is a bit more likely to happen with ML workloads).

I do have one question for you, though -- in the other bug report I have, the user says that when they hit this error, the executor gets stuck, which eventually leads to the application failing.  However, I haven't been able to reproduce that so far -- there are some errors, but from what I see, the executor just gives up fetching the remote data, and regenerates it locally.  What have you observed when this happens?

more details:

From SPARK-13990 & SPARK-13926, Spark's SerializerManager has [its own instance of a KryoSerializer|https://github.com/apache/spark/blob/581200af717bcefd11c9930ac063fe53c6fd2fde/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala#L42] which does not have the defaultClassLoader set on it. For normal task execution, that doesn't cause problems, because the serializer falls back to the current thread's task loader, which is set anyway.

however, netty maintains its own thread pool, and those threads don't change their classloader to include the extra use jars needed for the custom kryo registrator. That only matters when cached RDDs are sent across the network. That won't happen often, because spark tries to execute tasks where the RDDs are already cached.  (You'll notice your stack trace includes netty stuff underneath.)

This doesn't effect the shuffle path, because the serde is never done in the threads created by netty.

I think a fix for this should be fairly straight-forward, we just need to set the classloader on that extra kryo instance.;;;","19/Sep/17 18:16;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/19280;;;","19/Sep/17 18:24;jbrock;[~irashid], thanks for taking a look. I see the same thing as that user -- the executor gets stuck, causing the application to fail. Before I found the workaround I mentioned above with spark.driver.extraClassPath and spark.executor.extraClassPath, I was using speculation to kill off the hanging tasks, although this wasn't always enough (e.g., if a stage got stuck before reaching the speculation threshold), and sometimes caused long-running (but non-stuck) tasks to be killed.;;;","21/Sep/17 04:11;irashid;I believe I figured out why things get stuck sometimes, filed SPARK-22083.  [~jbrock], if you happen to still have logs from a case where you see this, can you check if the stuck executor shows something like ""INFO MemoryStore: 2 blocks selected for dropping"" (or maybe more than 2)?;;;","21/Sep/17 16:37;jbrock;It does! I see this in the log right before an executor got stuck:
{{17/08/31 19:56:25 INFO MemoryStore: 3 blocks selected for dropping (284.2 MB bytes)}};;;","21/Sep/17 20:12;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/19313;;;","21/Sep/17 20:29;irashid;thanks [~jbrock], thats great.  I think this is fully explained now.  I updated the title and description so folks know it is not related to ML, hope that is OK.;;;","21/Sep/17 20:59;jbrock;Excellent, thanks for looking into this.;;;","25/Oct/18 06:22;soravgulati@gmail.com;I am using Spark 2.30 version and I am still getting this Exception. Is it not fixed in Spark 2.3.0?;;;","25/Oct/18 14:14;irashid;[~soravgulati@gmail.com] this is believed to be fixed in 2.3.0. Can you share more details about what you see -- the full stack trace and what you were trying to do?  Its possible there is another cause of a similar exception.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Structured Streaming Documentation,SPARK-21924,13099839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,adamreith,adamreith,adamreith,05/Sep/17 13:32,06/Sep/17 07:24,14/Jul/23 06:30,06/Sep/17 07:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,,,,,0,,,,,,,,,"Under the structured streaming documentation page, more precisely in text immediately after the image ""Watermarking in Windowed Grouped Aggregation with Update Mode"" there's the following erroneous sentence: ""For example, the data (12:09, cat) is out of order and late, and it falls in windows 12:05 - 12:15 and 12:10 - 12:20."". It should be updated as following ""For example, the data (12:09, cat) is out of order and late, and it falls in windows 12:00 - 12:10 and 12:05 - 12:15."" because 12:09 cannot fall in the 12:10 - 12:20 window.",,adamreith,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 06 07:23:59 UTC 2017,,,,,,,,,,"0|i3jnnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 15:35;srowen;Agree, go ahead and make a pull request.;;;","05/Sep/17 15:39;apachespark;User 'riccardocorbella' has created a pull request for this issue:
https://github.com/apache/spark/pull/19137;;;","06/Sep/17 07:23;srowen;Issue resolved by pull request 19137
[https://github.com/apache/spark/pull/19137];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When executor failed and task metrics have not send to driver,the status will always be 'RUNNING' and the duration will be 'CurrentTime - launchTime'",SPARK-21922,13099762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cane,cane,cane,05/Sep/17 08:31,14/Sep/17 12:46,14/Jul/23 06:30,14/Sep/17 12:45,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,,"As title described,and below is an example:
!notfixed01.png|Before fixed!
!notfixed02.png|Before fixed!
We can fix the duration time by the modify time of event log:
!fixed01.png|After fixed!
!fixed02.png|After fixed!",,ajbozarth,apachespark,cane,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 10:32;cane;fixed01.png;https://issues.apache.org/jira/secure/attachment/12885356/fixed01.png","05/Sep/17 10:32;cane;fixed02.png;https://issues.apache.org/jira/secure/attachment/12885355/fixed02.png","05/Sep/17 10:32;cane;notfixed01.png;https://issues.apache.org/jira/secure/attachment/12885354/notfixed01.png","05/Sep/17 10:32;cane;notfixed02.png;https://issues.apache.org/jira/secure/attachment/12885353/notfixed02.png",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 10:46:03 UTC 2017,,,,,,,,,,"0|i3jn67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 10:46;apachespark;User 'caneGuy' has created a pull request for this issue:
https://github.com/apache/spark/pull/19132;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remote http(s) resources is not supported in YARN mode,SPARK-21917,13099720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,05/Sep/17 05:59,17/May/20 18:15,14/Jul/23 06:30,19/Sep/17 14:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,Spark Submit,YARN,,,0,,,,,,,,,"In the current Spark, when submitting application on YARN with remote resources {{./bin/spark-shell --jars http://central.maven.org/maven2/com/github/swagger-akka-http/swagger-akka-http_2.11/0.10.1/swagger-akka-http_2.11-0.10.1.jar --master yarn-client -v}}, Spark will be failed with:

{noformat}
java.io.IOException: No FileSystem for scheme: http
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2586)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2593)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2632)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2614)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:354)
	at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:478)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:600)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:599)
	at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:599)
	at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:598)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:598)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:848)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:173)
{noformat}

This is because {{YARN#client}} assumes resources must be on the Hadoop compatible FS, also in the NM (https://github.com/apache/hadoop/blob/99e558b13ba4d5832aea97374e1d07b4e78e5e39/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ContainerLocalizer.java#L245) it will only use Hadoop compatible FS to download resources. So this makes Spark on YARN fail to support remote http(s) resources.

To solve this problem, there might be several options:

* Download remote http(s) resources to local and add this local downloaded resources to dist cache. The downside of this option is that remote resources will be uploaded again unnecessarily.

* Filter remote http(s) resources and add them with spark.jars or spark.files, to leverage Spark's internal fileserver to distribute remote http(s) resources. The problem of this solution is: for some resources which require to be available before application start may not work.

* Leverage Hadoop's support http(s) file system (https://issues.apache.org/jira/browse/HADOOP-14383). This is only worked in Hadoop 2.9+, and I think even we implement a similar one in Spark will not be worked.",,apachespark,cloud_fan,jerryshao,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 06:51:05 UTC 2018,,,,,,,,,,"0|i3jmwv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 06:05;jerryshao;I'm inclining to choose option 1, the only overhead is resource re-uploading, the fix is restricted to SparkSubmit and other codes could be worked transparently.

What's your opinion [~tgraves] [~vanzin]?;;;","05/Sep/17 08:27;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19130;;;","05/Sep/17 18:21;vanzin;I think the optimal way would be to try to add things to the cache and, if that fails, fallback to download + re-upload.

The sucky part is that there's no reliable way to do it. The libraries available on the client side may support different file systems than the libraries available on the NM; so if you have the http fs in your classpath, but the NM does not, the container localizer would probably fail.

#1 works but it also penalizes those who are running YARN 2.9 or any other future version where that support exists.

So perhaps a compromise could be:

- by default, assume that client and NM libraries are ""in sync""; if {{FileSystem.get()}} does not complain, assume the NM can also download files from that scheme. If it does, then download the file and re-upload it to HDFS.
- add a config option where users can blacklist schemes and force those to download and re-upload.
;;;","06/Sep/17 03:09;jerryshao;Thanks [~vanzin], I think your suggestion is great, this will also handle Hadoop 2.9+ compatible issue, let me improve the current code.;;;","19/Sep/17 14:27;cloud_fan;Issue resolved by pull request 19130
[https://github.com/apache/spark/pull/19130];;;","25/Jun/18 06:51;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/21633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Model 1 and Model 2 ParamMaps Missing,SPARK-21915,13099704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,MarkTab,MarkTab,05/Sep/17 04:00,08/Sep/17 07:08,14/Jul/23 06:30,08/Sep/17 07:08,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,2.2.1,,,,,ML,PySpark,,,,0,easyfix,,,,,,,,"Error in PySpark example code
[https://github.com/apache/spark/blob/master/examples/src/main/python/ml/estimator_transformer_param_example.py]

The original Scala code says
println(""Model 2 was fit using parameters: "" + model2.parent.extractParamMap)

The parent is lr

There is no method for accessing parent as is done in Scala.

----

This code has been tested in Python, and returns values consistent with Scala


Proposing to call the lr variable instead of model1 or model2



----
This patch was tested with Spark 2.1.0 comparing the Scala and PySpark results. Pyspark returns nothing at present for those two print lines.

The output for model2 in PySpark should be

{Param(parent='LogisticRegression_4187be538f744d5a9090', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='predictionCol', doc='prediction column name.'): 'prediction',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='featuresCol', doc='features column name.'): 'features',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='labelCol', doc='label column name.'): 'label',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'myProbability',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',
Param(parent='LogisticRegression_4187be538f744d5a9090', name='fitIntercept', doc='whether to fit an intercept term.'): True,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='maxIter', doc='max number of iterations (>= 0).'): 30,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='regParam', doc='regularization parameter (>= 0).'): 0.1,
Param(parent='LogisticRegression_4187be538f744d5a9090', name='standardization', doc='whether to standardize the training features before fitting the model.'): True}",,apachespark,MarkTab,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Sep 08 07:08:25 UTC 2017,,,,,,,,,,"0|i3jmtb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/17 04:01;apachespark;User 'marktab' has created a pull request for this issue:
https://github.com/apache/spark/pull/19126;;;","07/Sep/17 02:24;apachespark;User 'marktab' has created a pull request for this issue:
https://github.com/apache/spark/pull/19152;;;","08/Sep/17 07:08;srowen;Issue resolved by pull request 19152
[https://github.com/apache/spark/pull/19152];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`withDatabase` should drop database with CASCADE,SPARK-21913,13099684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,04/Sep/17 23:23,05/Sep/17 07:21,14/Jul/23 06:30,05/Sep/17 07:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"Currently, it fails if the database is not empty. It would be great if we drop cleanly with CASCADE.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 04 23:26:03 UTC 2017,,,,,,,,,,"0|i3jmov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/17 23:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19125;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC/Parquet table should not create invalid column names,SPARK-21912,13099679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,04/Sep/17 20:44,17/Sep/20 21:55,14/Jul/23 06:30,07/Sep/17 05:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, users meet job abortions while creating ORC data source tables with invalid column names. We had better prevent this by raising AnalysisException like Paquet data source tables.

{code}
scala> sql(""CREATE TABLE orc1 USING ORC AS SELECT 1 `a b`"")
17/09/04 13:28:21 ERROR Utils: Aborting task
java.lang.IllegalArgumentException: Error: : expected at the position 8 of 'struct<a b:int>' but ' ' is found.
	at org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils$TypeInfoParser.expect(TypeInfoUtils.java:360)
...
17/09/04 13:28:21 WARN FileOutputCommitter: Could not delete file:/Users/dongjoon/spark-release/spark-master/spark-warehouse/orc1/_temporary/0/_temporary/attempt_20170904132821_0001_m_000000_0
17/09/04 13:28:21 ERROR FileFormatWriter: Job job_20170904132821_0001 aborted.
17/09/04 13:28:21 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.apache.spark.SparkException: Task failed while writing rows.
{code}",,apachespark,chakravarthi,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,SPARK-32889,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 21:41:04 UTC 2017,,,,,,,,,,"0|i3jmnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/17 20:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19124;;;","23/Oct/17 21:41;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in UnsafeExternalSorter.spill(),SPARK-21907,13099575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eyalfa,juliuszsompolski,juliuszsompolski,04/Sep/17 09:48,10/Aug/18 00:09,14/Jul/23 06:30,10/Oct/17 20:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"I see NPE during sorting with the following stacktrace:
{code}
java.lang.NullPointerException
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:383)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:63)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:43)
	at org.apache.spark.util.collection.TimSort.countRunAndMakeAscending(TimSort.java:270)
	at org.apache.spark.util.collection.TimSort.sort(TimSort.java:142)
	at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getSortedIterator(UnsafeInMemorySorter.java:345)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:206)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:203)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:281)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:90)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:173)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:221)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:203)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:281)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:90)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:349)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:400)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:109)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:778)
	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextInnerJoinRows(SortMergeJoinExec.scala:685)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1$$anon$2.advanceNext(SortMergeJoinExec.scala:259)
	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:346)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
{code}
",,apachespark,codingcat,devaraj,dongjoon,eyalfa,juliuszsompolski,kiszk,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22517,,,,,,,,,SPARK-13210,SPARK-25081,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 12 20:10:26 UTC 2017,,,,,,,,,,"0|i3jm0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/17 09:53;juliuszsompolski;Note that UnsafeExternalSorter.spill appears twice on the stack trace, so it's nested spilling: the first triggered spilling triggers another spilling through UnsafeInMemorySorter.reset.

Possibly it's messing up something by nested-spilling itself twice?
Or messing something with
{code:java}
    if (trigger != this) {
      if (readingIterator != null) {
        return readingIterator.spill();
      }
      return 0L; // this should throw exception
    }
{code}
in spill();;;","06/Sep/17 20:46;kiszk;Thank you for your report. Could you please attach a program that can reproduce this issue?;;;","06/Sep/17 21:08;juliuszsompolski;[~kiszk] unfortunately I don't have a small scale repro. I hit it several times when running sql queries operating on several TB of data on a cluster with ~20 nodes / ~300 cores.
Looking at the code, I'm quite sure it's caused by UnsafeInMemorySorter.reset:
{code}
      consumer.freeArray(array);
      array = consumer.allocateArray(initialSize);
{code}
where allocating this array just after it was freed fails with another OOM, causing a nested spill.
I think nested spilling is invalid, and thus acquiring memory in a way that can cause spill is invalid on code paths that are already during spilling.;;;","08/Sep/17 17:42;kiszk;If you cannot provide a repro, could you please run your program with the latest master branch?
SPARK-21319 may alleviate this issue.;;;","09/Sep/17 19:45;eyalfa;[~juliuszsompolski], I've followed the stack trace you've attached (as much as it's possible with master's code) and I tend to agree with your assumption about UnsafeInMemorySorter.reset.
it seems that allocateArray did fail on OOM and triggered a nested spill, the thing is that by this point array points to an already freed block, the first time array is actually accessed (TimSort invokes the comparator) it fails on an NPE (assuming asserts are really turned off on your env).

i think the way to solve this is by temporarily setting array (and the relevant pos, capacity, etc) to null/zero/some other value indicating a currently unreadable/empty buffer.

[~kiszk], what do you think?;;;","10/Sep/17 14:46;apachespark;User 'eyalfa' has created a pull request for this issue:
https://github.com/apache/spark/pull/19181;;;","10/Sep/17 14:46;eyalfa;opened PR: https://github.com/apache/spark/pull/19181;;;","12/Oct/17 09:36;apachespark;User 'eyalfa' has created a pull request for this issue:
https://github.com/apache/spark/pull/19481;;;","12/Oct/17 20:10;dongjoon;Hi, [~hvanhovell] and [~eyalfa].
I added `2.2.1` in fixed versions since it's merged into `branch-2.2` today.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename tempTables to tempViews in SessionCatalog,SPARK-21904,13099539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,04/Sep/17 06:13,30/Sep/17 02:36,14/Jul/23 06:30,30/Sep/17 02:36,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 04 06:16:06 UTC 2017,,,,,,,,,,"0|i3jlsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/17 06:16;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19117;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack Overflow when window function nested inside aggregate function,SPARK-21896,13099323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aokolnychyi,yluyao,yluyao,01/Sep/17 21:36,13/Oct/18 06:05,14/Jul/23 06:30,04/Jun/18 20:28,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"A minimal example: with the following simple test data

{noformat}
>>> df = spark.createDataFrame([(1, 2), (1, 3), (2, 4)], ['a', 'b'])
>>> df.show()
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  1|  3|
|  2|  4|
+---+---+
{noformat}

This works: 

{noformat}
>>> w = Window().orderBy('b')
>>> result = (df.select(F.rank().over(w).alias('rk'))
...            .groupby()
...            .agg(F.max('rk'))
...          )
>>> result.show()
+-------+
|max(rk)|
+-------+
|      3|
+-------+
{noformat}

But this equivalent gives an error. Note that the error is thrown right when the operation is defined, not when an action is called later:

{noformat}
>>> result = (df.groupby()
...            .agg(F.max(F.rank().over(w)))
...          )

Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2885, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-61-32e96531496f>"", line 2, in <module>
    .agg(F.max(F.rank().over(w)))
  File ""/usr/lib/spark/python/pyspark/sql/group.py"", line 91, in agg
    _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))
  File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File ""/usr/lib/spark/python/pyspark/sql/utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling o10789.agg.
: java.lang.StackOverflowError
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:55)
	at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:400)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:381)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$71.apply(Analyzer.scala:1688)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$71.apply(Analyzer.scala:1724)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$.org$apache$spark$sql$catalyst$analysis$Analyzer$ExtractWindowExpressions$$extract(Analyzer.scala:1687)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$apply$26.applyOrElse(Analyzer.scala:1825)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$apply$26.applyOrElse(Analyzer.scala:1800)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:295)

{noformat}

",,aokolnychyi,apachespark,cloud_fan,kiszk,toopt4,vincent-gg,yluyao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22804,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 13 06:05:17 UTC 2018,,,,,,,,,,"0|i3jkgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/17 17:46;aokolnychyi;The root cause of this issue is the inability of {{ExtractWindowExpressions}} to handle aggregations defined directly on top of window expressions (e.g., {{df.groupBy().agg(max(rank().over(window)))}}). 

The first place that has to be updated is {{ExtractWindowExpressions#extract}} and the second one is {{ExtractWindowExpressions#apply}} (only the aggregation case). The former does not extract present window expressions from aggregate functions (lines 1771-1774 in Analyzer) while the latter does not handle cases when aggregations should be computed on top of window computations (lines 1877-1888 in Analyzer).

I did not dive too much into the code, but it seems like {{ExtractWindowExpressions}} does not assume such definitions as {{df.groupBy().agg(max(rank().over(window)))}}. The question is if {{ExtractWindowExpressions}} should be extended/fixed to handle this case as well or it is by design and some analysis exception should be thrown.

As it is mentioned in the ticket description, {{df.select(rank().over(window).alias(""rank"")).agg(max(""rank""))}} works fine.;;;","11/Sep/17 16:40;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/19193;;;","31/May/18 20:52;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/21473;;;","04/Jun/18 20:28;cloud_fan;Issue resolved by pull request 21473
[https://github.com/apache/spark/pull/21473];;;","12/Oct/18 21:17;vincent-gg;Hello!
I see the fix is just to throw an exception, is there a plan to make this work (without requiring a sub-query) , in another issue maybe ?
thanks!;;;","13/Oct/18 06:05;cloud_fan;This patch just throws an exception explicitly, which means the error message is better, but the case is not supported yet. So I don't see a strong reason to backport it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObtainCredentials does not pass creds to addDelegationTokens,SPARK-21890,13099046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sanket991,sanket991,sanket991,31/Aug/17 20:14,07/Sep/17 17:21,14/Jul/23 06:30,07/Sep/17 16:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"I observed this while running a oozie job trying to connect to hbase via spark.
It look like the creds are not being passed in thehttps://github.com/apache/spark/blob/branch-2.2/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HadoopFSCredentialProvider.scala#L53 for 2.2 release.

More Info as to why it fails on secure grid:
Oozie client gets the necessary tokens the application needs before launching.  It passes those tokens along to the oozie launcher job (MR job) which will then actually call the Spark client to launch the spark app and pass the tokens along.
The oozie launcher job cannot get anymore tokens because all it has is tokens ( you can't get tokens with tokens, you need tgt or keytab).  
The error here is because the launcher job runs the Spark Client to submit the spark job but the spark client doesn't see that it already has the hdfs tokens so it tries to get more, which ends with the exception.
There was  a change with SPARK-19021 to generalize the hdfs credentials provider that changed it so we don't pass the existing credentials into the call to get tokens so it doesn't realize it already has the necessary tokens.


Stack trace:
Warning: Skip remote jar hdfs://axonitered-nn1.red.ygrid.yahoo.com:8020/user/schintap/spark_oozie/apps/lib/spark-starter-2.0-SNAPSHOT-jar-with-dependencies.jar.
Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.SparkMain], main() threw exception, Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:5858)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:687)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:1003)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:448)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:881)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:810)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1936)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2523)

org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:5858)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:687)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:1003)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:448)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:881)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:810)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1936)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2523)

	at org.apache.hadoop.ipc.Client.call(Client.java:1471)
	at org.apache.hadoop.ipc.Client.call(Client.java:1408)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
	at com.sun.proxy.$Proxy10.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:933)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:1038)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1543)
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:531)
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:509)
	at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2228)
	at org.apache.spark.deploy.yarn.security.HadoopFSCredentialProvider$$anonfun$obtainCredentials$1.apply(HadoopFSCredentialProvider.scala:53)
	at org.apache.spark.deploy.yarn.security.HadoopFSCredentialProvider$$anonfun$obtainCredentials$1.apply(HadoopFSCredentialProvider.scala:50)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:94)
	at org.apache.spark.deploy.yarn.security.HadoopFSCredentialProvider.obtainCredentials(HadoopFSCredentialProvider.scala:50)
	at org.apache.spark.deploy.yarn.security.ConfigurableCredentialManager$$anonfun$obtainCredentials$2.apply(ConfigurableCredentialManager.scala:82)
	at org.apache.spark.deploy.yarn.security.ConfigurableCredentialManager$$anonfun$obtainCredentials$2.apply(ConfigurableCredentialManager.scala:80)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.deploy.yarn.security.ConfigurableCredentialManager.obtainCredentials(ConfigurableCredentialManager.scala:80)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:393)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:896)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:173)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1190)
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1249)
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:776)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
	at org.apache.oozie.action.hadoop.SparkMain.runSpark(SparkMain.java:335)
	at org.apache.oozie.action.hadoop.SparkMain.run(SparkMain.java:265)
	at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:59)
	at org.apache.oozie.action.hadoop.SparkMain.main(SparkMain.java:80)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:235)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:380)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:301)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:187)
	at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:230)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

I have already done the tests will put up a PR shortly with the updates",,apachespark,sanket991,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 19:51:05 UTC 2017,,,,,,,,,,"0|i3jirb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/17 14:11;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/19103;;;","01/Sep/17 15:57;sanket991;Will put up a PR for master too thanks;;;","05/Sep/17 19:51;apachespark;User 'redsanket' has created a pull request for this issue:
https://github.com/apache/spark/pull/19140;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutputMetrics doesn't count written bytes correctly in the saveAsHadoopDataset function,SPARK-21882,13098786,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,linxiaojun,linxiaojunchina,linxiaojunchina,31/Aug/17 07:38,14/Jun/19 17:48,14/Jul/23 06:30,14/Jun/19 17:47,1.6.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.4,2.4.4,3.0.0,,,Spark Core,,,,,1,,,,,,,,,"The first job called from saveAsHadoopDataset, running in each executor, does not calculate the writtenBytes of OutputMetrics correctly (writtenBytes is 0). The reason is that we did not initialize the callback function called to find bytes written in the right way. As usual, statisticsTable which records statistics in a FileSystem must be initialized at the beginning (this will be triggered when open SparkHadoopWriter). The solution for this issue is to adjust the order of callback function initialization. ",,apachespark,jerryshao,linxiaojunchina,sukumaar,Victor3y,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/17 07:57;linxiaojunchina;SPARK-21882.patch;https://issues.apache.org/jira/secure/attachment/12884622/SPARK-21882.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Fri Jun 14 17:47:53 UTC 2019,,,,,,,,,,"0|i3jho7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/17 07:57;linxiaojunchina;SPARK-21882.patch for Spark-1.6.1;;;","01/Sep/17 09:13;jerryshao;Please submit the patch to Github Apache Spark repo.;;;","04/Sep/17 03:54;apachespark;User 'awarrior' has created a pull request for this issue:
https://github.com/apache/spark/pull/19115;;;","04/Sep/17 06:28;apachespark;User 'awarrior' has created a pull request for this issue:
https://github.com/apache/spark/pull/19118;;;","04/Sep/17 20:10;apachespark;User 'awarrior' has created a pull request for this issue:
https://github.com/apache/spark/pull/19115;;;","14/Jun/19 17:47;srowen;Resolved by https://github.com/apache/spark/pull/24863;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[spark UI]In the SQL table page, modify jobs trace information",SPARK-21880,13098740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,heqiao,heqiao,heqiao,31/Aug/17 06:16,01/Sep/17 17:47,14/Jul/23 06:30,01/Sep/17 17:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,," I think it makes sense for ""jobs"" to change to ""job id"" in the SQL table page. Because when job 5 fails, it's easy to misunderstand that five jobs have failed.
",,ajbozarth,apachespark,heqiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 06:39:03 UTC 2017,,,,,,,,,,"0|i3jhdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/17 06:39;apachespark;User 'Geek-He' has created a pull request for this issue:
https://github.com/apache/spark/pull/19093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows command script can not handle quotes in parameter,SPARK-21877,13098697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhaoxk,zhaoxk,zhaoxk,31/Aug/17 00:44,12/Dec/22 18:10,14/Jul/23 06:30,06/Oct/17 14:39,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Deploy,Windows,,,,0,,,,,,,,,"All the windows command scripts can not handle quotes in parameter.

Run a windows command shell with parameter which has quotes can reproduce the bug:

{quote}
C:\Users\meng\software\spark-2.2.0-bin-hadoop2.7> bin\spark-shell --driver-java-options "" -Dfile.encoding=utf-8 ""
'C:\Users\meng\software\spark-2.2.0-bin-hadoop2.7\bin\spark-shell2.cmd"" --driver-java-options ""' is not recognized as an internal or external command,
operable program or batch file.
{quote}

Windows recognize ""--driver-java-options"" as part of the command.

All the Windows command script has the following code have the bug.

{quote}
cmd /V /E /C ""<other command>"" %*
{quote}

","Spark version: spark-2.2.0-bin-hadoop2.7
Windows version: Windows 10",apachespark,zhaoxk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 06 14:39:11 UTC 2017,,,,,,,,,,"0|i3jh4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/17 00:52;apachespark;User 'minixalpha' has created a pull request for this issue:
https://github.com/apache/spark/pull/19090;;;","06/Oct/17 14:39;gurwls223;Issue resolved by pull request 19090
[https://github.com/apache/spark/pull/19090];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A cached Kafka producer should not be closed if any task is using it.,SPARK-21869,13098421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,zsxwing,zsxwing,29/Aug/17 23:40,13/Jan/20 07:50,14/Jul/23 06:30,23/Dec/19 22:20,2.4.4,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Structured Streaming,,,,,0,,,,,,,,,Right now a cached Kafka producer may be closed if a large task uses it for more than 10 minutes.,,apachespark,gsomogyi,Julescs0,prashant,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 23 22:20:09 UTC 2019,,,,,,,,,,"0|i3jffj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/17 23:41;zsxwing;[~scrapcodes@gmail.com] do you want to take this task?;;;","30/Aug/17 08:32;prashant;Yes, looking at it. ;;;","31/Aug/17 11:56;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/19096;;;","08/Nov/19 01:07;vanzin;Issue resolved by pull request 25853
[https://github.com/apache/spark/pull/25853];;;","10/Dec/19 21:42;zsxwing;Reopened this. https://github.com/apache/spark/pull/25853 has been reverted.;;;","23/Dec/19 22:20;vanzin;Issue resolved by pull request 26845
[https://github.com/apache/spark/pull/26845];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make codegen fallback of expressions configurable,SPARK-21845,13097794,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,26/Aug/17 22:13,04/Sep/17 06:42,14/Jul/23 06:30,30/Aug/17 03:59,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"We should make codegen fallback of expressions configurable. So far, it is always on. We might hide it when our codegen have compilation bugs. Thus, we should also disable the codegen fallback when running test cases.",,apachespark,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 04 06:42:04 UTC 2017,,,,,,,,,,"0|i3jblz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/17 22:22;smilegator;https://github.com/apache/spark/pull/19062
;;;","28/Aug/17 02:40;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19062;;;","28/Aug/17 02:40;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19062;;;","04/Sep/17 06:42;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19119;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UserDefinedTypeSuite local UDFs not actually testing what it intends,SPARK-21837,13097550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,25/Aug/17 10:23,25/Aug/17 20:36,14/Jul/23 06:30,25/Aug/17 20:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,Tests,,,,0,,,,,,,,,"Consider this test in {{UserDefinedTypeSuite}}:

{code}
  test(""Local UDTs"") {
    val df = Seq((1, new UDT.MyDenseVector(Array(0.1, 1.0)))).toDF(""int"", ""vec"")
    df.collect()(0).getAs[UDT.MyDenseVector](1)
    df.take(1)(0).getAs[UDT.MyDenseVector](1)
    df.limit(1).groupBy('int).agg(first('vec)).collect()(0).getAs[UDT.MyDenseVector](0)
    df.orderBy('int).limit(1).groupBy('int).agg(first('vec)).collect()(0)
      .getAs[UDT.MyDenseVector](0)
  }
{code}

I claim the last two lines can't be right, because they say that the first column in the aggregation is the vector, when it is the grouping key (int). But it passes! 

But it started failing when I made seemingly unrelated changes in https://github.com/apache/spark/pull/18645 like:

{code}
[info] - Local UDTs *** FAILED *** (144 milliseconds)
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.spark.sql.UDT$MyDenseVector
[info]   at org.apache.spark.sql.UserDefinedTypeSuite$$anonfun$10.apply(UserDefinedTypeSuite.scala:211)
[info]   at org.apache.spark.sql.UserDefinedTypeSuite$$anonfun$10.apply(UserDefinedTypeSuite.scala:205)
{code}

I modified the test to actually assert that the vector that results in each case is the expected one, and it began failing with the same error, in master. Therefore I am pretty sure the test is not quite doing what it seems to want to, and the result of these expressions just happened to not be fully evaluated or checked.

CC [~marmbrus] for the discussion at https://github.com/apache/spark/commit/3ae25f244bd471ef77002c703f2cc7ed6b524f11##commitcomment-23320234 and apologies if I'm still really missing something here. I'll open a PR to show you what I mean.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 25 20:23:59 UTC 2017,,,,,,,,,,"0|i1u552:r",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/17 20:23;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19053;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect executor request in case of dynamic allocation,SPARK-21834,13097475,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sitalkedia@gmail.com,sitalkedia@gmail.com,sitalkedia@gmail.com,25/Aug/17 01:01,17/May/20 17:48,14/Jul/23 06:30,30/Aug/17 21:20,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,Scheduler,Spark Core,,,,0,,,,,,,,,"killExecutor api currently does not allow killing an executor without updating the total number of executors needed. In case of dynamic allocation is turned on and the allocator tries to kill an executor, the scheduler reduces the total number of executors needed ( see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L635) which is incorrect because the allocator already takes care of setting the required number of executors itself. ",,apachespark,Dhruve Ashar,irashid,pclay,sitalkedia@gmail.com,xuefuz,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22598,,,,,,,,,SPARK-23365,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 18:55:32 UTC 2018,,,,,,,,,,"0|i3j9nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/17 00:02;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/19048;;;","29/Aug/17 22:14;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/19081;;;","23/Mar/18 18:55;irashid;SPARK-23365 is basically a duplicate of this, though they both have changes associated with them (though I didn't realize it at the time, SPARK-23365 is not strictly necessary on top of this, but does improve code clarity).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge SQLBuilderTest into ExpressionSQLBuilderSuite,SPARK-21832,13097383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,24/Aug/17 18:31,26/Aug/17 03:09,14/Jul/23 06:30,25/Aug/17 16:01,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"After SPARK-19025, there is no need to keep SQLBuilderTest. 
ExpressionSQLBuilderSuite is the only place to use it.
This issue aims to remove SQLBuilderTest.",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 26 03:09:40 UTC 2017,,,,,,,,,,"0|i3j933:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/17 16:01;smilegator;https://github.com/apache/spark/pull/19044;;;","26/Aug/17 03:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19044;;;","26/Aug/17 03:09;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19044;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove `spark.sql.hive.convertMetastoreOrc` config in HiveCompatibilitySuite,SPARK-21831,13097376,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,24/Aug/17 18:00,26/Aug/17 02:52,14/Jul/23 06:30,26/Aug/17 02:52,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Tests,,,,,0,,,,,,,,,"SPARK-19025 removes SQLBuilder, so we need to remove the following in HiveCompatibilitySuite.

{code}
// Ensures that the plans generation use metastore relation and not OrcRelation
// Was done because SqlBuilder does not work with plans having logical relation
TestHive.setConf(HiveUtils.CONVERT_METASTORE_ORC, false)
{code}",,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 18:13:31 UTC 2017,,,,,,,,,,"0|i3j91j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/17 18:13;dongjoon;Link to PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump the dependency of ANTLR to version 4.7,SPARK-21830,13097344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,24/Aug/17 15:56,24/Aug/17 23:34,14/Jul/23 06:30,24/Aug/17 23:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,There are a few minor issues with the current ANTLR version. Version 4.7 fixes most of these. I'd like to bump the version to that one.,,hvanhovell,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 23:34:58 UTC 2017,,,,,,,,,,"0|i3j8un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/17 23:34;smilegator;https://github.com/apache/spark/pull/19042
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
outer broadcast hash join should not throw NPE,SPARK-21826,13097257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,24/Aug/17 08:16,24/Aug/17 14:51,14/Jul/23 06:30,24/Aug/17 14:51,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,cloud_fan,hvanhovell,mgaido,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 14:51:21 UTC 2017,,,,,,,,,,"0|i3j8br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/17 14:51;hvanhovell;Fixed per https://github.com/apache/spark/pull/19036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultivariateOnlineSummarizer.variance generate negative result,SPARK-21818,13096988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,23/Aug/17 10:51,28/Aug/17 07:00,14/Jul/23 06:30,28/Aug/17 06:41,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,ML,MLlib,,,,0,,,,,,,,,"Because of numerical error, MultivariateOnlineSummarizer.variance is possible to generate negative variance.
This is a serious bug because many algos in MLLib use stddev computed from sqrt(variance),
it will generate NaN and crash the whole algorithm.

we can reproduce this bug use the following code:
{code}
    val summarizer1 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.7)
    val summarizer2 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)
    val summarizer3 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.5)
    val summarizer4 = (new MultivariateOnlineSummarizer)
      .add(Vectors.dense(3.0), 0.4)

    val summarizer = summarizer1
      .merge(summarizer2)
      .merge(summarizer3)
      .merge(summarizer4)

    println(summarizer.variance(0))
{code}",,facai,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 06:41:58 UTC 2017,,,,,,,,,,"0|i3j6o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/17 06:41;srowen;Issue resolved by pull request 19029
[https://github.com/apache/spark/pull/19029];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Inconsistency when finding the widest common type of a combination of DateType, StringType, and NumericType",SPARK-21811,13096869,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,ryanbald,ryanbald,22/Aug/17 23:10,12/Dec/22 18:10,14/Jul/23 06:30,19/Apr/18 13:23,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,SQL,,,,,0,,,,,,,,,"Finding the widest common type for the arguments of a variadic function (such as IN or COALESCE) when the types of the arguments are a combination of DateType/TimestampType, StringType, and NumericType fails with an AnalysisException for some orders of the arguments and succeeds with a common type of StringType for other orders of the arguments.

The below examples used to reproduce the error assume a schema of:
{{[c1: date, c2: string, c3: int]}}

The following succeeds:
{{SELECT coalesce(c1, c2, c3) FROM table}}

While the following produces an exception:
{{SELECT coalesce(c1, c3, c2) FROM table}}

The order of arguments affects the behavior because it looks to be the widest common type is found by repeatedly looking at two arguments at a time, the widest common type found thus far and the next argument. On initial thought of a fix, I think the way the widest common type is found would have to be changed and instead look at all arguments first before deciding what the widest common type should be.

As my boss is out of office for the rest of the day I will give a pull request a shot, but as I am not super familiar with Scala or Spark's coding style guidelines, a pull request is not promised. Going forward with my attempted pull request, I will assume having DateType/TimestampType, StringType, and NumericType arguments in an IN expression and COALESCE function (and any other function/expression where this combination of argument types can occur) is valid. I find it also quite reasonable to have this combination of argument types to be invalid, so if that's what is decided, then oh well.

If I were a betting man, I'd say the fix would be made in the following file: [TypeCoercion.scala|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala]",,apachespark,maropu,ryanbald,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 19 13:23:00 UTC 2018,,,,,,,,,,"0|i3j5xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/17 23:38;ryanbald;Just kidding, looking at every argument's type before deciding what the widest common type should be is overkill, will try to avoid.;;;","24/Aug/17 06:28;ryanbald;Pull request created.

Again, assuming that having a sequence of data types consisting of DateType/TimestampType, NumericType, and StringType is valid and the widest type of that sequence would be StringType.;;;","28/Aug/17 16:56;apachespark;User 'ryanbald' has created a pull request for this issue:
https://github.com/apache/spark/pull/19033;;;","15/Apr/18 15:28;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/21074;;;","19/Apr/18 13:23;gurwls223;Fixed in https://github.com/apache/spark/pull/21074;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json_tuple returns null values within repeated columns except the first one,SPARK-21804,13096649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cjm,cjm,cjm,22/Aug/17 06:25,12/Dec/22 18:10,14/Jul/23 06:30,24/Aug/17 10:24,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,starter,,,,,,,,"I was testing json_tuple in extracting values from JSON but I found it could actually returns null values within repeated columns except the first one as below:

{code:language=scala}
scala> spark.sql(""""""SELECT json_tuple('{""a"":1, ""b"":2}', 'a', 'b', 'a')"""""").show()
+---+---+----+
| c0| c1|  c2|
+---+---+----+
|  1|  2|null|
+---+---+----+
{code}

I think this should be consistent with Hive's implementation:
{code:language=scala}
hive> SELECT json_tuple('{""a"": 1, ""b"": 2}', 'a', 'a');
...
1    1
{code}
",,cjm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 10:24:28 UTC 2017,,,,,,,,,,"0|i3j4lr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/17 06:30;cjm;I’m working on this;;;","22/Aug/17 10:02;cjm;Submitted a PR at [https://github.com/apache/spark/pull/19017];;;","24/Aug/17 10:24;gurwls223;Issue resolved by pull request 19017
[https://github.com/apache/spark/pull/19017];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR unit test randomly fail on trees,SPARK-21801,13096631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,felixcheung,weichenxu123,weichenxu123,22/Aug/17 03:56,03/Sep/17 07:20,14/Jul/23 06:30,29/Aug/17 17:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,Tests,,,,0,,,,,,,,,"SparkR unit test sometimes will randomly occur such error:
```
1. Error: spark.randomForest (@test_mllib_tree.R#236) --------------------------
java.lang.IllegalArgumentException: requirement failed: The input column stridx_87ea3065aeb2 should have at least two distinct values.
```
or
```
1. Error: spark.decisionTree (@test_mllib_tree.R#353) --------------------------
java.lang.IllegalArgumentException: requirement failed: The input column stridx_d6a0b492cfa1 should have at least two distinct values.
```",,apachespark,felixcheung,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 03 07:20:04 UTC 2017,,,,,,,,,,"0|i3j4hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/17 03:57;weichenxu123;cc [~felixcheung] Can you help fix this ?;;;","22/Aug/17 06:16;felixcheung;I've seen it a couple of times. Looking at the test now i think this is because the tests are not run with a random seed.

let me submit a PR to see if it addresses the failure;;;","22/Aug/17 07:16;felixcheung;https://github.com/apache/spark/pull/19018;;;","28/Aug/17 10:44;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19018;;;","28/Aug/17 10:45;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19018;;;","03/Sep/17 07:20;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19111;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No config to replace deprecated SPARK_CLASSPATH config for launching daemons like History Server,SPARK-21798,13096437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pgandhi,sanket991,sanket991,21/Aug/17 14:35,28/Aug/17 13:52,14/Jul/23 06:30,28/Aug/17 13:52,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,History Server Launch uses SparkClassCommandBuilder for launching the server. It is observed that SPARK_CLASSPATH has been removed and deprecated. For spark-submit this takes a different route and spark.driver.extraClasspath takes care of specifying additional jars in the classpath that were previously specified in the SPARK_CLASSPATH. Right now the only way specify the additional jars for launching daemons such as history server is using SPARK_DIST_CLASSPATH (https://spark.apache.org/docs/latest/hadoop-provided.html) but this I presume is a distribution classpath. It would be nice to have a similar config like spark.driver.extraClasspath for launching daemons similar to history server. ,,apachespark,jerryshao,pgandhi,sanket991,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 25 22:33:38 UTC 2017,,,,,,,,,,"0|i3j3bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/17 03:02;jerryshao;I think this one could be used, looks like there's no other options in the current code.

BTW, why do you need to expand Spark history server's classpath, do you have some customized history provider?;;;","22/Aug/17 20:09;tgraves;You could need to add things to the classpath for custom UI filters, certain hadoop configurations may need extra things added, like paths to rack switch resolvers, etc.

I think it would make sense to add back in a SPARK_DAEMON_CLASSPATH.  I would think people could want something similar for the standalone mode as well as history server so a general daemon one would make sense.  But if people don't think anyone will use it in standalone mode we could just make a history server specific env for it as well.  ;;;","24/Aug/17 22:13;pgandhi;Filed a pull request for this ticket:
https://github.com/apache/spark/pull/19047;;;","25/Aug/17 22:33;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/19047;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct validateAndTransformSchema in GaussianMixture and AFTSurvivalRegression,SPARK-21793,13096276,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sharp-pixel,sharp-pixel,srowen,20/Aug/17 10:04,20/Aug/17 10:07,14/Jul/23 06:30,20/Aug/17 10:06,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,MLlib,,,,,0,,,,,,,,,"From user sharp-pixel:

The line SchemaUtils.appendColumn(schema, $(predictionCol), IntegerType) did not modify the variable schema, hence only the last line had any effect. A temporary variable is used to correctly append the two columns predictionCol and probabilityCol.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 20 10:06:35 UTC 2017,,,,,,,,,,"0|i1u552:i",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/17 10:06;srowen;Issue resolved by pull request 18980
[https://github.com/apache/spark/pull/18980];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle more exceptions when stopping a streaming query,SPARK-21788,13096138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,18/Aug/17 20:45,24/Aug/17 18:05,14/Jul/23 06:30,24/Aug/17 17:24,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,,,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 17:24:21 UTC 2017,,,,,,,,,,"0|i3j1hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/17 17:24;tdas;Issue resolved by pull request 18997
[https://github.com/apache/spark/pull/18997];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The 'spark.sql.parquet.compression.codec' configuration doesn't take effect on tables with partition field(s),SPARK-21786,13095617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jinhua Fu,Jinhua Fu,Jinhua Fu,18/Aug/17 11:45,31/Aug/18 09:49,14/Jul/23 06:30,06/Jan/18 10:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Since Hive 1.1, Hive allows users to set parquet compression codec via table-level properties parquet.compression. See the JIRA: https://issues.apache.org/jira/browse/HIVE-7858 . We do support orc.compression for ORC. Thus, for external users, it is more straightforward to support both. See the stackflow question: https://stackoverflow.com/questions/36941122/spark-sql-ignores-parquet-compression-propertie-specified-in-tblproperties
In Spark side, our table-level compression conf compression was added by #11464 since Spark 2.0.
We need to support both table-level conf. Users might also use session-level conf spark.sql.parquet.compression.codec. The priority rule will be like
If other compression codec configuration was found through hive or parquet, the precedence would be compression, parquet.compression, spark.sql.parquet.compression.codec. Acceptable values include: none, uncompressed, snappy, gzip, lzo.
The rule for Parquet is consistent with the ORC after the change.

Changes:
1.Increased acquiring 'compressionCodecClassName' from parquet.compression,and the precedence order is compression,parquet.compression,spark.sql.parquet.compression.codec, just like what we do in OrcOptions.

2.Change spark.sql.parquet.compression.codec to support ""none"".Actually in ParquetOptions,we do support ""none"" as equivalent to ""uncompressed"", but it does not allowed to configured to ""none"".",,apachespark,byakuinss,Jinhua Fu,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 31 09:49:06 UTC 2018,,,,,,,,,,"0|i3iybj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/17 08:52;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/19189;;;","13/Sep/17 09:31;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/19218;;;","25/Dec/17 07:44;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/20076;;;","27/Dec/17 03:32;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/20087;;;","31/Aug/18 08:41;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/22301;;;","31/Aug/18 09:49;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/22302;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repartition creates skews when numPartitions is a power of 2,SPARK-21782,13095558,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,megaserg,megaserg,megaserg,18/Aug/17 05:47,13/Mar/23 23:14,14/Jul/23 06:30,21/Aug/17 07:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,repartition,,,,,,,,"*Problem:*
When an RDD (particularly with a low item-per-partition ratio) is repartitioned to {{numPartitions}} = power of 2, the resulting partitions are very uneven-sized. This affects both {{repartition()}} and {{coalesce(shuffle=true)}}.

*Steps to reproduce:*

{code}
$ spark-shell

scala> sc.parallelize(0 until 1000, 250).repartition(64).glom().map(_.length).collect()
res0: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 144, 250, 250, 250, 106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
{code}

*Explanation:*
Currently, the [algorithm for repartition|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L450] (shuffle-enabled coalesce) is as follows:
- for each initial partition {{index}}, generate {{position}} as {{(new Random(index)).nextInt(numPartitions)}}
- then, for element number {{k}} in initial partition {{index}}, put it in the new partition {{position + k}} (modulo {{numPartitions}}).

So, essentially elements are smeared roughly equally over {{numPartitions}} buckets - starting from the one with number {{position+1}}.

Note that a new instance of {{Random}} is created for every initial partition {{index}}, with a fixed seed {{index}}, and then discarded. So the {{position}} is deterministic for every {{index}} for any RDD in the world. Also, [{{nextInt(bound)}} implementation|http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/8u40-b25/java/util/Random.java/#393] has a special case when {{bound}} is a power of 2, which is basically taking several highest bits from the initial seed, with only a minimal scrambling.

Due to deterministic seed, using the generator only once, and lack of scrambling, the {{position}} values for power-of-two {{numPartitions}} always end up being almost the same regardless of the {{index}}, causing some buckets to be much more popular than others. So, {{repartition}} will in fact intentionally produce skewed partitions even when before the partition were roughly equal in size.

The behavior seems to have been introduced in SPARK-1770 by https://github.com/apache/spark/pull/727/
{quote}
The load balancing is not perfect: a given output partition
can have up to N more elements than the average if there are N input
partitions. However, some randomization is used to minimize the
probabiliy that this happens.
{quote}

Another related ticket: SPARK-17817 - https://github.com/apache/spark/pull/15445",,apachespark,megaserg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/17 05:50;megaserg;Screen Shot 2017-08-16 at 3.40.01 PM.png;https://issues.apache.org/jira/secure/attachment/12882514/Screen+Shot+2017-08-16+at+3.40.01+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 23:14:59 UTC 2023,,,,,,,,,,"0|i3ixyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/17 05:52;megaserg;Distribution of partition sizes (in bytes) spotted in the wild. Horizontal axis: partition index ({{0..1023}}). Vertical axis: partition size in bytes.;;;","18/Aug/17 06:12;srowen;Is the problem summary just: with a power of 2 bound, similar seeds give similar output?
Isn't that solved with a better RNG or just simple scrambling of the seed bits?
The seed is actually essential.;;;","18/Aug/17 06:30;megaserg;Your understanding is correct. Either reusing the same {{Random}} instance multiple times (not really an option as shuffle is parallel), using a better RNG, or substantially scrambling the seed (hashing?) will help.
Changing the ""smearing"" algorithm would also work, e.g. to something like this:
{code}
      val distributePartition = (index: Int, items: Iterator[T]) => {
         val rng = new Random(index)
         items.map { t => (rng.nextInt(numPartitions), t) }
       } : Iterator[(Int, T)]
{code}

Please let me know which way you'd like to see it.;;;","18/Aug/17 06:59;srowen;I think scrambling the seed a bit is the smallest change that works, so would favor that.;;;","18/Aug/17 07:43;megaserg;I played with Scala's `hashing.byteswap32()` and it seems to be working pretty well. Not perfect, you can still see ""patterns"" in the size distribution, but it's much better than before.

{code}
scala> new ShuffledRDD[Int, Int, Int](sc.parallelize(0 until 1000, 250).mapPartitionsWithIndex(distributePartition), new HashPartitioner(64)).glom().map(_.length).collect()
res50: Array[Int] = Array(26, 25, 18, 15, 14, 11, 13, 16, 17, 19, 21, 19, 18, 18, 14, 13, 12, 8, 14, 15, 16, 17, 13, 13, 17, 22, 22, 20, 18, 14, 14, 15, 12, 14, 13, 14, 13, 10, 10, 10, 11, 12, 11, 10, 9, 13, 16, 19, 21, 19, 17, 14, 14, 13, 14, 16, 16, 15, 16, 16, 16, 20, 24, 25)
{code};;;","21/Aug/17 07:21;srowen;Issue resolved by pull request 18990
[https://github.com/apache/spark/pull/18990];;;","13/Mar/23 23:14;apachespark;User 'megaserg' has created a pull request for this issue:
https://github.com/apache/spark/pull/18990;;;","13/Mar/23 23:14;apachespark;User 'megaserg' has created a pull request for this issue:
https://github.com/apache/spark/pull/18990;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Decimal Test For Avro in VersionSuite,SPARK-21767,13095432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,17/Aug/17 19:07,17/Aug/17 23:35,14/Jul/23 06:30,17/Aug/17 23:35,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,Decimal is a logical type of AVRO. We need to ensure the support of Hive's AVRO serde works well in Spark,,richardatcloudera,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 19:13:48 UTC 2017,,,,,,,,,,"0|i3ix6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/17 19:13;smilegator;https://github.com/apache/spark/pull/18977
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame toPandas() raises ValueError with nullable int columns,SPARK-21766,13095428,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,bryanc,bryanc,17/Aug/17 18:50,12/Dec/22 18:10,14/Jul/23 06:30,22/Sep/17 13:40,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"When calling {{DataFrame.toPandas()}} (without Arrow enabled), if there is a IntegerType column that has null values the following exception is thrown:

{noformat}
ValueError: Cannot convert non-finite values (NA or inf) to integer
{noformat}

This is because the null values first get converted to float NaN during the construction of the Pandas DataFrame in {{from_records}}, and then it is attempted to be converted back to to an integer where it fails.",,apachespark,bryanc,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 22 13:40:38 UTC 2017,,,,,,,,,,"0|i3ix5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 06:35;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19319;;;","22/Sep/17 13:40;gurwls223;Issue resolved by pull request 19319
[https://github.com/apache/spark/pull/19319];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileFormatWriter/BasicWriteTaskStatsTracker metrics collection fails if a new file isn't yet visible,SPARK-21762,13095385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,17/Aug/17 15:58,14/Oct/17 06:08,14/Jul/23 06:30,14/Oct/17 06:08,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,1,,,,,,,,,"The metrics collection of SPARK-20703 can trigger premature failure if the newly written object isn't actually visible yet, that is if, after {{writer.close()}}, a {{getFileStatus(path)}} returns a {{FileNotFoundException}}.

Strictly speaking, not having a file immediately visible goes against the fundamental expectations of the Hadoop FS APIs, namely full consistent data & medata across all operations, with immediate global visibility of all changes. However, not all object stores make that guarantee, be it only newly created data or updated blobs. And so spurious FNFEs can get raised, ones which *should* have gone away by the time the actual task is committed. Or if they haven't, the job is in such deep trouble.

What to do?
# leave as is: fail fast & so catch blobstores/blobstore clients which don't behave as required. One issue here: will that trigger retries, what happens there, etc, etc.
# Swallow the FNFE and hope the file is observable later.
# Swallow all IOEs and hope that whatever problem the FS has is transient.

Options 2 & 3 aren't going to collect metrics in the event of a FNFE, or at least, not the counter of bytes written.",object stores without complete creation consistency (this includes AWS S3's caching of negative GET results),apachespark,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,SPARK-22258,,,,,,,,,SPARK-21669,SPARK-20703,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 12 04:20:41 UTC 2017,,,,,,,,,,"0|i3iww7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/17 19:41;stevel@apache.org;SPARK-21669 simplifies this, especially testing, as it's isolated from FileFormatWriter. Same problem exists though: if you are getting any Create inconsistency, metrics probes trigger failures which may not be present by the time task commit actually takes place;;;","28/Aug/17 16:56;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/18979;;;","12/Oct/17 04:20;dongjoon;Since this is a regression like SPARK-22258, I updated the priority.
FileNotFoundException occurs always on ORC file format.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
In.checkInputDataTypes should not wrongly report unresolved plans for IN correlated subquery,SPARK-21759,13095223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,17/Aug/17 04:09,24/Aug/17 13:47,14/Jul/23 06:30,24/Aug/17 13:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"With the check for structural integrity proposed in SPARK-21726, I found that an optimization rule {{PullupCorrelatedPredicates}} can produce unresolved plans.

For a correlated IN query like:
{code}
Project [a#0]
+- Filter a#0 IN (list#4 [b#1])
   :  +- Project [c#2]
   :     +- Filter (outer(b#1) < d#3)
   :        +- LocalRelation <empty>, [c#2, d#3]
   +- LocalRelation <empty>, [a#0, b#1]
{code}

After {{PullupCorrelatedPredicates}}, it produces query plan like:
{code}
'Project [a#0]
+- 'Filter a#0 IN (list#4 [(b#1 < d#3)])
   :  +- Project [c#2, d#3]
   :     +- LocalRelation <empty>, [c#2, d#3]
   +- LocalRelation <empty>, [a#0, b#1]
{code}

Because the correlated predicate involves another attribute {{d#3}} in subquery, it has been pulled out and added into the {{Project}} on the top of the subquery.

When {{list}} in {{In}} contains just one {{ListQuery}}, {{In.checkInputDataTypes}} checks if the size of {{value}} expressions matches the output size of subquery. In the above example, there is only {{value}} expression and the subquery output has two attributes {{c#2, d#3}}, so it fails the check and {{In.resolved}} returns {{false}}.

We should not let {{In.checkInputDataTypes}} wrongly report unresolved plans to fail the structural integrity check.

",,cloud_fan,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 13:47:55 UTC 2017,,,,,,,,,,"0|i3ivw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/17 23:46;viirya;Submitted PR at https://github.com/apache/spark/pull/18968;;;","24/Aug/17 13:47;cloud_fan;Issue resolved by pull request 18968
[https://github.com/apache/spark/pull/18968];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
running pi example with pypy on spark fails to serialize ,SPARK-21753,13095121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rgbkrk,tgraves,tgraves,16/Aug/17 19:49,12/Dec/22 18:11,14/Jul/23 06:30,22/Aug/17 02:34,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"I'm trying to run the pi example (https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py)  on pyspark using pypy 2.5.1 but everything I've tried results in a serialization error:

Traceback (most recent call last):
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 147, in dump
    return Pickler.dump(self, obj)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 224, in dump
    self.save(obj)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 562, in save_tuple
    save(element)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 254, in save_function
    self.save_function_tuple(obj)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 291, in save_function_tuple
    save((code, closure, base_globals))
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 548, in save_tuple
    save(element)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 633, in _batch_appends
    save(x)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 254, in save_function
    self.save_function_tuple(obj)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 291, in save_function_tuple
    save((code, closure, base_globals))
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 548, in save_tuple
    save(element)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 600, in save_list
    self._batch_appends(iter(obj))
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 636, in _batch_appends
    save(tmp[0])
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 248, in save_function
    self.save_function_tuple(obj)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 296, in save_function_tuple
    save(f_globals)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 653, in save_dict
    self._batch_setitems(obj.iteritems())
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 690, in _batch_setitems
    save(v)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 447, in save_instancemethod
    obj=obj)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 581, in save_reduce
    save(args)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 548, in save_tuple
    save(element)
  File ""//home/tgraves/pypy-my-own-package-name/lib-python/2.7/pickle.py"", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 246, in save_function
    if islambda(obj) or obj.__code__.co_filename == '<stdin>' or themodule is None:
AttributeError: 'builtin-code' object has no attribute 'co_filename'
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tgraves/y-spark-git/python/pyspark/rdd.py"", line 834, in reduce
    vals = self.mapPartitions(func).collect()
  File ""/home/tgraves/y-spark-git/python/pyspark/rdd.py"", line 808, in collect
    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File ""/home/tgraves/y-spark-git/python/pyspark/rdd.py"", line 2440, in _jrdd
    self._jrdd_deserializer, profiler)
  File ""/home/tgraves/y-spark-git/python/pyspark/rdd.py"", line 2373, in _wrap_function
    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
  File ""/home/tgraves/y-spark-git/python/pyspark/rdd.py"", line 2359, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
  File ""/home/tgraves/y-spark-git/python/pyspark/serializers.py"", line 460, in dumps
    return cloudpickle.dumps(obj, 2)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 703, in dumps
    cp.dump(obj)
  File ""/home/tgraves/y-spark-git/python/pyspark/cloudpickle.py"", line 160, in dump
    raise pickle.PicklingError(msg)

It looks like the issue is with serializing random().  If you remove random() from the function then everything works fine.

I'm just running PYSPARK_PYTHON=//home/tgraves/pypy-my-own-package-name/bin/pypy ./bin/pyspark

I've tried multiple versions of pypy from 2.5.1 to 5.8.0. I tried the portable version as well as built pypy from source.

If it works for others perhaps I have a setup issue, any hints on that would be appreciated.",,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22209,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 22 02:34:28 UTC 2017,,,,,,,,,,"0|i3iv9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/17 19:51;tgraves;[~holden.karau@gmail.com] would be curious if you have run the pi example?  I know there are python tests being run but I didn't see any testing the same functionality as pi here. ;;;","16/Aug/17 19:56;tgraves;I should also state that many other things work fine with pypy.  For instance just doing the things in the programming guide: http://spark.apache.org/docs/2.2.0/rdd-programming-guide.html

I tried a bunch of those operations and things like wordcount and they work fine.;;;","17/Aug/17 02:09;gurwls223;Hi [~tgraves] and [~holdenk], I had some time to reproduce and look into this. I could reproduce this in the current master.

Sounds related with https://github.com/cloudpipe/cloudpickle/pull/51 and [~holdenk] opened a PR to port many improvements - https://github.com/apache/spark/pull/18734 and it looks this fix is included (I left a side note about this in that PR). I double checked it works with the change in her PR.;;;","22/Aug/17 02:33;gurwls223;I merged her PR and double checked if it works:

Before:

{code}
PYSPARK_PYTHON=pypy ./bin/spark-submit examples/src/main/python/pi.py
...
PicklingError: Could not serialize object: AttributeError: 'builtin-code' object has no attribute 'co_filename'
{code}

After:

{code}
PYSPARK_PYTHON=pypy ./bin/spark-submit examples/src/main/python/pi.py
...
Pi is roughly 3.137520
{code}

;;;","22/Aug/17 02:34;gurwls223;Fixed in https://github.com/apache/spark/pull/18734;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
top-most limit should not cause memory leak,SPARK-21743,13094920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Aug/17 04:25,27/Sep/18 13:07,14/Jul/23 06:30,27/Sep/18 13:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,hvanhovell,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 09 16:30:05 UTC 2018,,,,,,,,,,"0|i3iu1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/17 13:31;cloud_fan;issue resolved by https://github.com/apache/spark/pull/18955;;;","15/Jun/18 01:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21573;;;","15/Jun/18 12:34;hvanhovell;Reopening issue, this is causing a regression in the CSV reader.;;;","09/Jul/18 16:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21738;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
timestamp partition would fail in v2.2.0,SPARK-21739,13094800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,donnyzone,zhihao,zhihao,15/Aug/17 16:04,18/Aug/17 05:43,14/Jul/23 06:30,18/Aug/17 05:43,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"The spark v2.2.0 introduce TimeZoneAwareExpression, which causes bugs if we select data from a table with timestamp partitions.
The steps to reproduce it:

{code:java}
spark.sql(""create table test (foo string) parititioned by (ts timestamp)"")
spark.sql(""insert into table test partition(ts = 1) values('hi')"")
spark.table(""test"").show()
{code}

The root cause is that TableReader.scala#230 try to cast the string to timestamp regardless if the timeZone exists.

Here is the error stack trace

{code}
java.util.NoSuchElementException: None.get
  at scala.None$.get(Option.scala:347)
  at scala.None$.get(Option.scala:345)
  at org.apache.spark.sql.catalyst.expressions.TimeZoneAwareExpression$class.timeZone(datetimeExpressions.scala:46)
  at org.apache.spark.sql.catalyst.expressions.Cast.timeZone$lzycompute(Cast.scala:172)                                                                                         at org.apache.spark.sql.catalyst.expressions.Cast.timeZone(Cast.scala:172)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToTimestamp$1$$anonfun$apply$24.apply(Cast.scala:253)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToTimestamp$1$$anonfun$apply$24.apply(Cast.scala:253)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$buildCast(Cast.scala:201)
  at org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToTimestamp$1.apply(Cast.scala:253)
  at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:533)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:327)
  at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5$$anonfun$fillPartitionKeys$1$1.apply(TableReader.scala:230)
  at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$5$$anonfun$fillPartitionKeys$1$1.apply(TableReader.scala:228)
{code}
",,donnyzone,ueshin,zhihao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 16 08:38:43 UTC 2017,,,,,,,,,,"0|i3itan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/17 03:45;donnyzone;In such case, the Cast expression is evaluated directly without resolving *timeZoneId* because it is in the execution phase.

{code:java}
Cast(Literal(rawPartValues(partOrdinal)), attr.dataType).eval(null)
{code}

I will check this kind of issues and fix them.

;;;","16/Aug/17 08:38;donnyzone;Submit a PR at: https://github.com/apache/spark/pull/18960;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thriftserver doesn't cancel jobs when session is closed,SPARK-21738,13094788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgaido,mgaido,mgaido,15/Aug/17 15:47,18/Nov/20 22:06,14/Jul/23 06:30,16/Aug/17 16:40,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When a session is closed, the jobs launched by that session should be killed in order to avoid waste of resources. Instead, this doesn't happen.

So at the moment, if a user launches a query and then closes his connection, the query goes on running until completion. This behavior should be changed.",,apachespark,maropu,mgaido,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 22:06:42 UTC 2020,,,,,,,,,,"0|i3it7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/17 17:08;smilegator;https://github.com/apache/spark/pull/18951;;;","18/Nov/20 22:06;apachespark;User 'mgaido91' has created a pull request for this issue:
https://github.com/apache/spark/pull/18951;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operating on an ArrayType in a SparkR DataFrame throws error,SPARK-21727,13094584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,neilalex,neilalex,neilalex,14/Aug/17 20:04,12/Dec/22 18:10,14/Jul/23 06:30,24/Jan/18 06:38,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,"Previously [posted|https://stackoverflow.com/questions/45056973/sparkr-dataframe-with-r-lists-as-elements] this as a stack overflow question but it seems to be a bug.

If I have an R data.frame where one of the column data types is an integer *list* – i.e., each of the elements in the column embeds an entire R list of integers – then it seems I can convert this data.frame to a SparkR DataFrame just fine... SparkR treats the column as ArrayType(Double).

However, any subsequent operation on this SparkR DataFrame appears to throw an error.

Create an example R data.frame:
{code:java}
indices <- 1:4
myDf <- data.frame(indices)
myDf$data <- list(rep(0, 20))
{code}
Examine it to make sure it looks okay:
{code:java}
> str(myDf) 
'data.frame':   4 obs. of  2 variables:  
 $ indices: int  1 2 3 4  
 $ data   :List of 4
   ..$ : num  0 0 0 0 0 0 0 0 0 0 ...
   ..$ : num  0 0 0 0 0 0 0 0 0 0 ...
   ..$ : num  0 0 0 0 0 0 0 0 0 0 ...
   ..$ : num  0 0 0 0 0 0 0 0 0 0 ...

> head(myDf)   
  indices                                                       data 
1       1 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 
2       2 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 
3       3 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 
4       4 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
{code}
Convert it to a SparkR DataFrame:
{code:java}
library(SparkR, lib.loc=paste0(Sys.getenv(""SPARK_HOME""),""/R/lib""))
sparkR.session(master = ""local[*]"")
mySparkDf <- as.DataFrame(myDf)
{code}
Examine the SparkR DataFrame schema; notice that the list column was successfully converted to ArrayType:
{code:java}
> schema(mySparkDf)
StructType
|-name = ""indices"", type = ""IntegerType"", nullable = TRUE
|-name = ""data"", type = ""ArrayType(DoubleType,true)"", nullable = TRUE
{code}
However, operating on the SparkR DataFrame throws an error:
{code:java}
> collect(mySparkDf)
17/07/13 17:23:00 ERROR executor.Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: 
java.lang.Double is not a valid external type for schema of array<double>
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null 
else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, indices), IntegerType) AS indices#0
... long stack trace ...
{code}
Using Spark 2.2.0, R 3.4.0, Java 1.8.0_131, Windows 10.",,apachespark,felixcheung,neilalex,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 28 07:19:27 UTC 2018,,,,,,,,,,"0|i3irzj:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"01/Sep/17 14:46;yanboliang;I can run successfully with minor change:
{code}
indices <- 1:4
myDf <- data.frame(indices)
myDf$data <- list(as.list(rep(0, 20)))
mySparkDf <- as.DataFrame(myDf)
collect(mySparkDf)
{code}
This is because rep(0, 20) is not type of list, we should convert it to list explicitly.
{code}
> class(rep(0, 20))
[1] ""numeric""
> class(as.list(rep(0, 20)))
[1] ""list""
{code};;;","02/Sep/17 07:54;felixcheung;hmm.. I think that's what the error message is saying
{code}
java.lang.Double is not a valid external type for schema of array<double>
{code}

it's finding a double and not an array of double;;;","02/Sep/17 14:42;neilalex;Ah yes and I realize I posted incorrectly -- the code embeds an integer vector, not integer list. (And converting it to a list first creates a workaround for me -- thank you.)

This said, according to Spark [documentation|https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#data-types], both R vectors and lists should be supported?;;;","02/Sep/17 20:48;felixcheung;That is true. I think the documentation is unclear in this case - it should say the vector should be converted to list or similar.

In fact the code explicitly does not support column with atomic vector values into array column type
https://github.com/apache/spark/blob/master/R/pkg/R/serialize.R#L54

But with that said, I think we could and should make a minor change to support that implicitly
https://github.com/apache/spark/blob/master/R/pkg/R/serialize.R#L39
;;;","02/Sep/17 20:48;felixcheung;any taker of this change?;;;","04/Sep/17 14:01;yanboliang;[~felixcheung] What do you mean for this comment?
{quote}
But with that said, I think we could and should make a minor change to support that implicitly
https://github.com/apache/spark/blob/master/R/pkg/R/serialize.R#L39
{quote}
How we can get the SerDe type of atomic vector? Just like I mentioned above,
{code}
> class(rep(0, 20))
[1] ""numeric""
> class(as.list(rep(0, 20)))
[1] ""list""
{code}
_class_ function can't return type _vector_, how we can determine the type of object is _vector_ or _numeric_ ? Thanks.
;;;","04/Sep/17 16:09;neilalex;Well, if class is ""numeric"" (or ""integer"", ""character"", etc.), then technically it is always a vector? (There are no distinct scalars in R?) We could look at length > 1?;;;","04/Sep/17 16:11;neilalex;Happy to take on the change this side... (unless [~yanboliang] you had intended to?);;;","04/Sep/17 23:07;felixcheung;precisely.
as far as I can tell, everything should ""just work"" if we return ""array"" from `getSerdeType()` for this case when length > 1.
;;;","05/Sep/17 07:13;yanboliang;[~neilalex] Please feel free to take this task. Thanks.;;;","22/Dec/17 05:10;felixcheung;Neil McQuarrie is going to work on this;;;","28/Dec/17 08:17;felixcheung;[~neilalex] How it is going?;;;","31/Dec/17 18:43;neilalex;[~felixcheung] Okay thanks -- sorry, need a few more days;;;","05/Jan/18 04:40;neilalex;I was able to get a version working by checking for length > 1, as well as whether the type is integer, character, logical, double, numeric, Date, POSIXlt, or POSIXct.

[https://github.com/neilalex/spark/blob/neilalex-sparkr-arraytype/R/pkg/R/serialize.R#L39]

The reason I'm checking the specific type is that it seems quite a few objects flow through getSerdeType() besides just the object we're converting -- for example, after calling 'as.DataFrame(myDf)' per above, I saw a jobj ""Java ref type org.apache.spark.sql.SparkSession id 1"" of length 2, as well as a raw object of length ~700, both pass through getSerdeType(). So, it seems we can't just check length?

Also, in general this makes me a little nervous -- will there be scenarios when a 2+ length vector of one of the above types should also not be converted to array? I'll familiarize myself further with how getSerdeType() is called, to see if I can think this through.;;;","07/Jan/18 19:18;felixcheung;I think we should use
is.atomic(object)

?;;;","10/Jan/18 03:35;neilalex;Oh -- interesting... thanks.

I think ""raw"" type is atomic though? 

Is 
{code}is.atomic(object) & !is.raw(object){code}
too clunky?;;;","10/Jan/18 07:52;felixcheung;good call...

if (
(is.atomic(object) && !is.raw(object)) &&
length(object) > 1
);;;","10/Jan/18 15:30;neilalex;Okay great. Implemented and tests are passing. I'll submit a PR shortly. Thanks for the help.;;;","21/Jan/18 23:05;felixcheung;how are we doing?;;;","22/Jan/18 21:02;apachespark;User 'neilalex' has created a pull request for this issue:
https://github.com/apache/spark/pull/20352;;;","28/Jan/18 07:19;gurwls223;(y);;;",,,,,,,,,,,,,,,,,,
Can't write LibSVM - key not found: numFeatures,SPARK-21723,13094423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vrs,vrs,vrs,14/Aug/17 09:44,16/Aug/17 07:25,14/Jul/23 06:30,16/Aug/17 07:24,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Input/Output,ML,,,,0,,,,,,,,,"Writing a dataset to LibSVM format raises an exception

{{java.util.NoSuchElementException: key not found: numFeatures}}

Happens only when the dataset was NOT read from a LibSVM format before (because otherwise numFeatures is in its metadata). Steps to reproduce:

{code}
import org.apache.spark.ml.linalg.Vectors
val rawData = Seq((1.0, Vectors.sparse(3, Seq((0, 2.0), (1, 3.0)))),
                  (4.0, Vectors.sparse(3, Seq((0, 5.0), (2, 6.0)))))
val dfTemp = spark.sparkContext.parallelize(rawData).toDF(""label"", ""features"")
dfTemp.coalesce(1).write.format(""libsvm"").save(""...filename..."")
{code}

PR with a fix and unit test is ready - see [https://github.com/apache/spark/pull/18872].",,mlnick,vrs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 16 07:24:46 UTC 2017,,,,,,,,,,"0|i3iqzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/17 08:19;mlnick;Yes, we should definitely be able to write LibSVM format regardless of whether the original data was read from that format, and whether we have ML metadata attached to the dataframe. We should be able to inspect the vectors to get the size in the absence of the metadata.

;;;","16/Aug/17 07:24;srowen;Issue resolved by pull request 18872
[https://github.com/apache/spark/pull/18872];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in org.apache.spark.sql.hive.execution.InsertIntoHiveTable,SPARK-21721,13094373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,yzheng616,yzheng616,14/Aug/17 02:53,15/Aug/17 15:49,14/Jul/23 06:30,15/Aug/17 15:49,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"The leak came from org.apache.spark.sql.hive.execution.InsertIntoHiveTable. At line 118, it put a staging path to FileSystem delete cache, and then remove the path from disk at line 385. It does not remove the path from FileSystem cache. If a streaming application keep persisting data to a partitioned hive table, the memory will keep increasing until JVM terminated.

Below is a simple code to reproduce it.
{code:java}
package test

import org.apache.spark.sql.SparkSession
import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.sql.SaveMode
import java.lang.reflect.Field



case class PathLeakTest(id: Int, gp: String)

object StagePathLeak {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder().master(""local[4]"").appName(""StagePathLeak"").enableHiveSupport().getOrCreate()
    spark.conf.set(""hive.exec.dynamic.partition.mode"", ""nonstrict"")
    //create a partitioned table
    spark.sql(""drop table if exists path_leak"");
    spark.sql(""create table if not exists path_leak(id int)"" +
        "" partitioned by (gp String)""+
      "" row format serde 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'""+
      "" stored as""+
        "" inputformat 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'""+
        "" outputformat 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'"")

    var seq = new scala.collection.mutable.ArrayBuffer[PathLeakTest]()
    // 2 partitions
    for (x <- 1 to 2) {
      seq += (new PathLeakTest(x, ""g"" + x))
    }
    val rdd = spark.sparkContext.makeRDD[PathLeakTest](seq)

    //insert 50 records to Hive table
    for (j <- 1 to 50) {
      val df = spark.createDataFrame(rdd)
      //#1 InsertIntoHiveTable line 118:  add stage path to FileSystem deleteOnExit cache
      //#2 InsertIntoHiveTable line 385:  delete the path from disk but not from the FileSystem cache, and it caused the leak
      df.write.mode(SaveMode.Overwrite).insertInto(""path_leak"")  
    }
    
    val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
    val deleteOnExit = getDeleteOnExit(fs.getClass)
    deleteOnExit.setAccessible(true)
    val caches = deleteOnExit.get(fs).asInstanceOf[java.util.TreeSet[Path]]
    //check FileSystem deleteOnExit cache size
    println(caches.size())
    val it = caches.iterator()
    //all starge pathes were still cached even they have already been deleted from the disk
    while(it.hasNext()){
      println(it.next());
    }
  }
  
  def getDeleteOnExit(cls: Class[_]) : Field = {
    try{
       return cls.getDeclaredField(""deleteOnExit"")
    }catch{
      case ex: NoSuchFieldException => return getDeleteOnExit(cls.getSuperclass)
    }
    return null
  }

}
{code}

 


 ",,kiszk,viirya,yzheng616,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 14 04:22:33 UTC 2017,,,,,,,,,,"0|i3iqon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/17 04:22;viirya;Submitted a PR at https://github.com/apache/spark/pull/18934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmit in Yarn Client mode downloads remote files and then reuploads them again,SPARK-21714,13094172,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jerryshao,tgraves,tgraves,11/Aug/17 18:54,30/Aug/17 19:33,14/Jul/23 06:30,25/Aug/17 16:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Submit,,,,,0,,,,,,,,,"SPARK-10643 added the ability for spark-submit to download remote file in client mode.

However in yarn mode this introduced a bug where it downloads them for the client but then yarn client just reuploads them to HDFS and uses them again. This should not happen when the remote file is HDFS.  This is wasting resources and its defeating the  distributed cache because if the original object was public it would have been shared by many users. By us downloading and reuploading, it becomes private.",,apachespark,jerryshao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21689,,,,SPARK-10643,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 06:00:03 UTC 2017,,,,,,,,,,"0|i3ipgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/17 01:11;jerryshao;I noticed this issue before and tried to fix it, but the solution made the SparkSubmit code a little complicated.;;;","15/Aug/17 13:13;jerryshao;Let me take a crack on this if no one is working on it.;;;","15/Aug/17 13:23;tgraves;I haven't had time to get to it, so it would be great if you can work on it;;;","29/Aug/17 06:00;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19074;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
State Store can't handle corrupted snapshots,SPARK-21696,13093879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,nonsleepr,nonsleepr,10/Aug/17 18:16,13/Jul/18 05:32,14/Jul/23 06:30,14/Aug/17 22:08,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"State store's asynchronous maintenance task (generation of Snapshot files) is not rescheduled if crashed which might lead to corrupted snapshots.

In our case, on multiple occasions, executors died during maintenance task with Out Of Memory error which led to following error on recovery:
{code:none}
17/08/07 20:12:24 WARN TaskSetManager: Lost task 3.1 in stage 102.0 (TID 3314, dnj2-bach-r2n10.bloomberg.com, executor 94): java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(HDFSBackedStateStoreProvider.scala:436)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:314)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:313)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:313)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:220)
        at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:186)
        at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,gaaldornick,nonsleepr,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 16:54:21 UTC 2017,,,,,,,,,,"0|i3ino7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/17 18:42;nonsleepr;{{HDFSBackedStateStoreProvider.doMaintenance()}} will supress any {{NonFatal}} exceptions. {{startMaintenanceIfNeeded.startMaintenanceIfNeeded()}} wouldn't restart maintenance if crashed. State Store still can function even when snapshot file is corrupted by simply falling back to deltas.;;;","14/Aug/17 22:08;tdas;Issue resolved by pull request 18928
[https://github.com/apache/spark/pull/18928];;;","28/Aug/17 16:54;apachespark;User 'nonsleepr' has created a pull request for this issue:
https://github.com/apache/spark/pull/18911;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Params isSet in scala Transformer triggered by _setDefault in pyspark,SPARK-21685,13093594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,ratan,ratan,09/Aug/17 20:30,23/Mar/18 18:43,14/Jul/23 06:30,23/Mar/18 18:43,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,PySpark,,,,,0,,,,,,,,,"I'm trying to write a PySpark wrapper for a Transformer whose transform method includes the line

{code:java}
require(!(isSet(outputNodeName) && isSet(outputNodeIndex)), ""Can't set both outputNodeName and outputNodeIndex"")
{code}

This should only throw an exception when both of these parameters are explicitly set.

In the PySpark wrapper for the Transformer, there is this line in ___init___

{code:java}
self._setDefault(outputNodeIndex=0)
{code}

Here is the line in the main python script showing how it is being configured
{code:java}
cntkModel = CNTKModel().setInputCol(""images"").setOutputCol(""output"").setModelLocation(spark, model.uri).setOutputNodeName(""z"")
{code}

As you can see, only setOutputNodeName is being explicitly set but the exception is still being thrown.

If you need more context, https://github.com/RatanRSur/mmlspark/tree/default-cntkmodel-output is the branch with the code, the files I'm referring to here that are tracked are the following:

src/cntk-model/src/main/scala/CNTKModel.scala
notebooks/tests/301 - CIFAR10 CNTK CNN Evaluation.ipynb

The pyspark wrapper code is autogenerated",,apachespark,bryanc,josephkb,ratan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23244,SPARK-23234,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 16:55:02 UTC 2017,,,,,,,,,,"0|i3im33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/17 23:01;josephkb;Could you please point to more info, such as the Python wrappers you are calling?  I don't see enough info here to identify the problem.;;;","11/Aug/17 17:54;ratan;The python wrapper is generated so I've pasted it here so you don't have to build it:


{code:java}
# Copyright (C) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root for information.


import sys
if sys.version >= '3':
    basestring = str

from pyspark.ml.param.shared import *
from pyspark import keyword_only
from pyspark.ml.util import JavaMLReadable, JavaMLWritable
from pyspark.ml.wrapper import JavaTransformer, JavaEstimator, JavaModel
from pyspark.ml.common import inherit_doc
from mmlspark.Utils import *

@inherit_doc
class _CNTKModel(ComplexParamsMixin, JavaMLReadable, JavaMLWritable, JavaTransformer):
    """"""
    The ``CNTKModel`` evaluates a pre-trained CNTK model in parallel.  The
    ``CNTKModel`` takes a path to a model and automatically loads and
    distributes the model to workers for parallel evaluation using CNTK's
    java bindings.
    
    The ``CNTKModel`` loads the pretrained model into the ``Function`` class
    of CNTK.  One can decide which node of the CNTK Function computation
    graph to evaluate by passing in the name of the output node with the
    output node parameter.  Currently the ``CNTKModel`` supports single
    input single output models.
    
    The ``CNTKModel`` takes an input column which should be a column of
    spark vectors and returns a column of spark vectors representing the
    activations of the selected node.  By default, the CNTK model defaults
    to using the model's first input and first output node.

    Args:
        inputCol (str): The name of the input column (undefined)
        inputNode (int): index of the input node (default: 0)
        miniBatchSize (int): size of minibatches (default: 10)
        model (object): Array of bytes containing the serialized CNTKModel (undefined)
        outputCol (str): The name of the output column (undefined)
        outputNodeIndex (int): index of the output node (default: 0)
        outputNodeName (str): name of the output node (undefined)
    """"""

    @keyword_only
    def __init__(self, inputCol=None, inputNode=0, miniBatchSize=10, model=None, outputCol=None, outputNodeIndex=0, outputNodeName=None):
        super(_CNTKModel, self).__init__()
        self._java_obj = self._new_java_obj(""com.microsoft.ml.spark.CNTKModel"")
        self.inputCol = Param(self, ""inputCol"", ""inputCol: The name of the input column (undefined)"")
        self.inputNode = Param(self, ""inputNode"", ""inputNode: index of the input node (default: 0)"")
        self._setDefault(inputNode=0)
        self.miniBatchSize = Param(self, ""miniBatchSize"", ""miniBatchSize: size of minibatches (default: 10)"")
        self._setDefault(miniBatchSize=10)
        self.model = Param(self, ""model"", ""model: Array of bytes containing the serialized CNTKModel (undefined)"")
        self.outputCol = Param(self, ""outputCol"", ""outputCol: The name of the output column (undefined)"")
        self.outputNodeIndex = Param(self, ""outputNodeIndex"", ""outputNodeIndex: index of the output node (default: 0)"")
        self._setDefault(outputNodeIndex=0)
        self.outputNodeName = Param(self, ""outputNodeName"", ""outputNodeName: name of the output node (undefined)"")
        if hasattr(self, ""_input_kwargs""):
            kwargs = self._input_kwargs
        else:
            kwargs = self.__init__._input_kwargs
        self.setParams(**kwargs)

    @keyword_only
    def setParams(self, inputCol=None, inputNode=0, miniBatchSize=10, model=None, outputCol=None, outputNodeIndex=0, outputNodeName=None):
        """"""
        Set the (keyword only) parameters

        Args:
            inputCol (str): The name of the input column (undefined)
            inputNode (int): index of the input node (default: 0)
            miniBatchSize (int): size of minibatches (default: 10)
            model (object): Array of bytes containing the serialized CNTKModel (undefined)
            outputCol (str): The name of the output column (undefined)
            outputNodeIndex (int): index of the output node (default: 0)
            outputNodeName (str): name of the output node (undefined)
        """"""
        if hasattr(self, ""_input_kwargs""):
            kwargs = self._input_kwargs
        else:
            kwargs = self.__init__._input_kwargs
        return self._set(**kwargs)

    def setInputCol(self, value):
        """"""

        Args:
            inputCol (str): The name of the input column (undefined)

        """"""
        self._set(inputCol=value)
        return self


    def getInputCol(self):
        """"""

        Returns:
            str: The name of the input column (undefined)
        """"""
        return self.getOrDefault(self.inputCol)


    def setInputNode(self, value):
        """"""

        Args:
            inputNode (int): index of the input node (default: 0)

        """"""
        self._set(inputNode=value)
        return self


    def getInputNode(self):
        """"""

        Returns:
            int: index of the input node (default: 0)
        """"""
        return self.getOrDefault(self.inputNode)


    def setMiniBatchSize(self, value):
        """"""

        Args:
            miniBatchSize (int): size of minibatches (default: 10)

        """"""
        self._set(miniBatchSize=value)
        return self


    def getMiniBatchSize(self):
        """"""

        Returns:
            int: size of minibatches (default: 10)
        """"""
        return self.getOrDefault(self.miniBatchSize)


    def setModel(self, value):
        """"""

        Args:
            model (object): Array of bytes containing the serialized CNTKModel (undefined)

        """"""
        self._set(model=value)
        return self


    def getModel(self):
        """"""

        Returns:
            object: Array of bytes containing the serialized CNTKModel (undefined)
        """"""
        return self.getOrDefault(self.model)


    def setOutputCol(self, value):
        """"""

        Args:
            outputCol (str): The name of the output column (undefined)

        """"""
        self._set(outputCol=value)
        return self


    def getOutputCol(self):
        """"""

        Returns:
            str: The name of the output column (undefined)
        """"""
        return self.getOrDefault(self.outputCol)


    def setOutputNodeIndex(self, value):
        """"""

        Args:
            outputNodeIndex (int): index of the output node (default: 0)

        """"""
        self._set(outputNodeIndex=value)
        return self


    def getOutputNodeIndex(self):
        """"""

        Returns:
            int: index of the output node (default: 0)
        """"""
        return self.getOrDefault(self.outputNodeIndex)


    def setOutputNodeName(self, value):
        """"""

        Args:
            outputNodeName (str): name of the output node (undefined)

        """"""
        self._set(outputNodeName=value)
        return self


    def getOutputNodeName(self):
        """"""

        Returns:
            str: name of the output node (undefined)
        """"""
        return self.getOrDefault(self.outputNodeName)



    @classmethod
    def read(cls):
        """""" Returns an MLReader instance for this class. """"""
        return JavaMMLReader(cls)

    @staticmethod
    def getJavaPackage():
        """""" Returns package name String. """"""
        return ""com.microsoft.ml.spark.CNTKModel""

    @staticmethod
    def _from_java(java_stage):
        module_name=_CNTKModel.__module__
        module_name=module_name.rsplit(""."", 1)[0] + "".CNTKModel""
        return from_java(java_stage, module_name)

{code}
;;;","18/Aug/17 00:11;bryanc;I believe the problem is during the call to transform, the PySpark model does not differentiate between set and default params and then sets them all in Java.  I have submitting a fix at https://github.com/apache/spark/pull/18982, could you try that and see if it works for you?;;;","28/Aug/17 16:55;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/18982;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLOR do not work correctly when featureStd contains zero,SPARK-21681,13093545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weichenxu123,weichenxu123,weichenxu123,09/Aug/17 17:17,24/Aug/17 17:21,14/Jul/23 06:30,24/Aug/17 17:20,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,ML,,,,,0,correctness,,,,,,,,"MLOR do not work correctly when featureStd contains zero.
We can reproduce the bug through such dataset (features including zero variance), will generate wrong result (all coefficients becomes 0)

{code}
    val multinomialDatasetWithZeroVar = {
      val nPoints = 100
      val coefficients = Array(
        -0.57997, 0.912083, -0.371077,
        -0.16624, -0.84355, -0.048509)

      val xMean = Array(5.843, 3.0)
      val xVariance = Array(0.6856, 0.0)  // including zero variance

      val testData = generateMultinomialLogisticInput(
        coefficients, xMean, xVariance, addIntercept = true, nPoints, seed)

      val df = sc.parallelize(testData, 4).toDF().withColumn(""weight"", lit(1.0))
      df.cache()
      df
    }
{code}

",,josephkb,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 17:20:13 UTC 2017,,,,,,,,,,"0|i3ils7:",9223372036854775807,,,,,josephkb,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"22/Aug/17 23:58;josephkb;I'll leave this open until it's been backported to 2.2;;;","24/Aug/17 17:20;josephkb;Issue resolved by pull request 19026
[https://github.com/apache/spark/pull/19026];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json_tuple throws NullPointException when column is null as string type.,SPARK-21677,13093474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cjm,gurwls223,,09/Aug/17 13:23,12/Dec/22 18:11,14/Jul/23 06:30,17/Aug/17 23:37,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,Starter,,,,,,,,"I was testing {{json_tuple}} before using this to extract values from JSONs in my testing cluster but I found it could actually throw  {{NullPointException}} as below sometimes:

{code}
scala> Seq((""""""{""Hyukjin"": 224, ""John"": 1225}"""""")).toDS.selectExpr(""json_tuple(value, trim(' Hyukjin    '))"").show()
+---+
| c0|
+---+
|224|
+---+

scala> Seq((""""""{""Hyukjin"": 224, ""John"": 1225}"""""")).toDS.selectExpr(""json_tuple(value, trim(' Jackson    '))"").show()
+----+
|  c0|
+----+
|null|
+----+

scala> Seq((""""""{""Hyukjin"": 224, ""John"": 1225}"""""")).toDS.selectExpr(""json_tuple(value, trim(null))"").show()
...
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.JsonTuple$$anonfun$foldableFieldNames$1.apply(jsonExpressions.scala:367)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple$$anonfun$foldableFieldNames$1.apply(jsonExpressions.scala:366)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.foldableFieldNames$lzycompute(jsonExpressions.scala:366)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.foldableFieldNames(jsonExpressions.scala:365)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.constantFields$lzycompute(jsonExpressions.scala:373)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.constantFields(jsonExpressions.scala:373)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.org$apache$spark$sql$catalyst$expressions$JsonTuple$$parseRow(jsonExpressions.scala:417)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple$$anonfun$eval$4.apply(jsonExpressions.scala:401)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple$$anonfun$eval$4.apply(jsonExpressions.scala:400)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2559)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.eval(jsonExpressions.scala:400)
{code}

It sounds we should show explicit error messages or return {{NULL}}.
",,cjm,panbingkun,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 15:06:49 UTC 2017,,,,,,,,,,"0|i3ilcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/17 13:24;gurwls223;cc [~viirya], I remember your mentee was checking through JSON related code paths. Does this make sense to you and would you be interested in this? I don't have time to work on this and am currently fighting with AppVeyor time limit issue. ;;;","09/Aug/17 13:53;viirya;[~hyukjin.kwon] Thanks! Definitely we are interested in this. We will work on this.;;;","10/Aug/17 13:56;cjm;to [~hyukjin.kwon], the return {{NULL}} you mentioned does it means all fields should be null in json_tuple, or just the non-existence field as shown in the following. Thanks!

{code:language=scala|borderStyle=solid}
e.g., spark.sql(""""""SELECT json_tuple('{""a"":1, ""b"":2}', 'a', 'b', 'not_exising_fields')"""""").show()

+---+---+----+
| c0| c1|  c2|
+---+---+----+
|  1|  2|null|
+---+---+----+
{code}

;;;","10/Aug/17 14:11;gurwls223;[~cjm], I was thinking like

{code}
spark.sql(""""""SELECT json_tuple('{""a"":1, ""b"":2}', cast(NULL AS STRING), 'b', cast(NULL AS STRING), 'a')"""""").show()

+----+---+----+---+
|  c0| c1|  c2| c3|
+----+---+----+---+
|null|  2|null|  1|
+----+---+----+---+
{code}

I think this could be at least consistent with Hive's implementation:

{code}
hive> SELECT json_tuple('{""a"":1, ""b"":2}', cast(NULL AS STRING), 'b', cast(NULL AS STRING), 'a');
...
NULL	2	NULL	1
{code};;;","10/Aug/17 14:38;viirya;As a given field name {{null}} can't be matched with any field names in json, we just output {{null}} as its column value. I think it's reasonable.;;;","18/Aug/17 15:06;gurwls223;Issue resolved by pull request 18930
https://github.com/apache/spark/pull/18930;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark local directory is not set correctly,SPARK-21673,13093328,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jach9009,jach9009,jach9009,08/Aug/17 22:53,17/May/20 18:21,14/Jul/23 06:30,22/May/18 13:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Block Manager,Mesos,Spark Core,,,0,,,,,,,,,Currently the way Spark discovers the Mesos sandbox is wrong. As seen here https://github.com/apache/spark/blob/e26dac5feb02033f980b1e69c9b0ff50869b6f9e/core/src/main/scala/org/apache/spark/util/Utils.scala#L806 it is checking the env variable called MESOS_DIRECTORY however this env variable was depricated (see https://www.mail-archive.com/dev@mesos.apache.org/msg36621.html) in favor of using MESOS_SANDBOX env variable. This should be updated in the spark code to reflect this change in mesos.,,apachespark,jach9009,susanxhuynh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 13:09:25 UTC 2018,,,,,,,,,,"0|i3ikgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/17 13:08;jach9009;https://github.com/apache/spark/pull/18894;;;","28/Aug/17 16:54;apachespark;User 'jakecharland' has created a pull request for this issue:
https://github.com/apache/spark/pull/18894;;;","22/May/18 13:09;srowen;Issue resolved by pull request 18894
[https://github.com/apache/spark/pull/18894];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark has exponential time complexity to explode(array of structs),SPARK-21657,13092970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,uzadude,Tagar,Tagar,07/Aug/17 18:21,31/Jan/18 18:57,14/Jul/23 06:30,29/Dec/17 13:09,2.0.0,2.1.0,2.1.1,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,4,cache,caching,collections,nested_types,performance,pyspark,sparksql,sql,"It can take up to half a day to explode a modest-sized nested collection (0.5m).
On a recent Xeon processors.

See attached pyspark script that reproduces this problem.

{code}
cached_df = sqlc.sql('select individ, hholdid, explode(amft) from ' + table_name).cache()
print sqlc.count()
{code}

This script generate a number of tables, with the same total number of records across all nested collection (see `scaling` variable in loops). `scaling` variable scales up how many nested elements in each record, but by the same factor scales down number of records in the table. So total number of records stays the same.

Time grows exponentially (notice log-10 vertical axis scale):
!ExponentialTimeGrowth.PNG!

At scaling of 50,000 (see attached pyspark script), it took 7 hours to explode the nested collections (\!) of 8k records.

After 1000 elements in nested collection, time grows exponentially.
",,apachespark,cloud_fan,cwsteinbach,dongjoon,ewanleith,jcdauchy,maropu,nadavw,rdsr,sayuan,simeons,szhemzhitsky,Tagar,uzadude,viirya,zkull,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4502,,,SPARK-15214,SPARK-16998,SPARK-22330,SPARK-22385,,,,,,,,,,,"07/Aug/17 18:21;Tagar;ExponentialTimeGrowth.PNG;https://issues.apache.org/jira/secure/attachment/12880674/ExponentialTimeGrowth.PNG","07/Aug/17 18:22;Tagar;nested-data-generator-and-test.py;https://issues.apache.org/jira/secure/attachment/12880673/nested-data-generator-and-test.py",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 29 16:39:34 UTC 2017,,,,,,,,,,"0|i3ii93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/17 18:38;srowen;(Not a bug)
I doubt this is meant to be efficient at the scale you're using it. Is this a real use case?
What change are you proposing?;;;","07/Aug/17 18:46;Tagar;Absolutely, this is a real use case. 
We have a lot of production data that rely on that kind of schema for BI reporting. 
Other Hadoop sql engines, including Hive and Impala scale its time to explode nested collections linearly. 
Spark has exponential complexity to explode nested collection.
There is definitely a room for improvement, as after ~40k+ records in a nested collection, most time of the job
is spent in exploding; after ~200k+ records in a nested collection, Spark is not usable.;;;","10/Aug/17 15:31;Tagar;[~bjornjons] confirms this problem pertains to Spark 2.2 too.
;;;","11/Aug/17 08:54;viirya;Maybe not very related to this issue. But I'm exploring Generate related code to get hint about this issue. I'm curious why we still don't enable codegen of Generate for now. [~hvanhovell] Maybe you know why it is disabled? Thanks.;;;","15/Aug/17 07:11;maropu;[~viirya] Probably, this is what you want? https://github.com/apache/spark/commit/b5c5bd98ea5e8dbfebcf86c5459bdf765f5ceb53;;;","15/Aug/17 16:49;Tagar;Thank you [~maropu] and [~viirya], that commit is for Spark 2.2 so this problem might be worse in 2.2, but I don't think it's a root cause.
As we see the same exponential time complexity to explode a nested array in Spark 2.0 and 2.1.;;;","16/Aug/17 03:16;viirya;[~maropu] I've noticed that change. There is a hotfix trying to revert that: https://github.com/apache/spark/pull/17425. But in the end the hotfix doesn't revert it back.

Actually I've tried to enable codegen for GenerateExec and ran those tests without failure in local. So I'm wondering why we still disable it.;;;","26/Oct/17 13:51;uzadude;Hi,
Wanted to add that we're facing exactly the same issue. 6 hours work for one row that contains 250k array (of struct of 4 strings).
Just wanted to state that if we explode only the array, e.g, in your example:
cached_df = sqlc.sql('select explode(amft) from ' + table_name)

it finishes in about 3 mins. 
it happens in Spark 2.1 and also 2.2, eventhough SPARK-16998 was resolved.;;;","26/Oct/17 14:06;srowen;I suspect that something somewhere is doing something that's linear-time that looks like it should be constant-time, like referencing a linked list by index. See https://issues.apache.org/jira/browse/SPARK-22330 for a similar type of thing (though don't think it's the same issue as this one);;;","27/Oct/17 12:52;uzadude;Hi,
Just ran a profiler for this code:
{code:java}
val BASE = 100000000
val N = 100000
val df = sc.parallelize(List((""1234567890"", (BASE to (BASE+N)).map(x => (x.toString, (x+1).toString, (x+2).toString, (x+3).toString)).toList ))).toDF(""c1"", ""c_arr"")
val df_exploded = df.select(expr(""c1""), explode($""c_arr"").as(""c2""))
df_exploded.write.mode(""overwrite"").format(""json"").save(""/tmp/blah_explode"")
{code}

and it looks like [~srowen] is right, most of the time is spent in scala.collection.immutable.List.apply()	 (72.1%). inside:
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext()  (100%)

I logged the generated code and found the problematic code:
{code:java}
 if (serializefromobject_funcResult1 != null) {
             serializefromobject_value5 = (scala.collection.immutable.List) serializefromobject_funcResult1;
           } else {
             serializefromobject_isNull5 = true;
         }
.
.
.
 while (serializefromobject_loopIndex < serializefromobject_dataLength) {
           MapObjects_loopValue0 = (scala.Tuple4) (serializefromobject_value5.apply(serializefromobject_loopIndex));
{code}

so that causes the quadratic time complexity.
However, I'm not sure where is the code that generates this list instead of array for the exploded array.;;;","27/Oct/17 13:21;srowen;What if you call toArray in your code, and explode that? if it's just assuming the column type is constant-time to access at an index, then that would work around it. 
Ideally it would generate code that traversed the collection if it doesn't support fast RandomAccess, or implements like this otherwise (instance of IndexedSeq or something). But that might help narrow down the issue.;;;","29/Oct/17 09:14;uzadude;I Switched to toArray instead of toList in the above code and I did get an improvement by factor of 2. but we still remain with the main bottleneck.
now the diff in the above example between:
{code:java}
val df_exploded = df.select(expr(""c1""), explode($""c_arr"").as(""c2""))
{code}
and:
{code:java}
val df_exploded = df.select(explode($""c_arr"").as(""c2""))
{code}
is 128 secs vs. 3 secs.

Again I profiled the former and saw that all the time got consumed in:
org.apache.spark.unsafe.Platform.copyMemory()	97.548096	23,991 ms (97.5%)	

the obvious diff between the execution plans is that the former has two WholeStageCodeGen plans and the later just one.
I didn't exactly understood the generated code but I would guess that what happens is that in the problematic case the generated explode code is actually multiplying the long array to all the exploded rows and only filters it in the end.
Please see if you can verify it or think on a workaround for it.

;;;","29/Oct/17 09:43;srowen;Can you paste the plans? this difference might be down to a different cause.

The linear-time-access List issue still look worth solving. [~hvanhovell] [~cloud_fan] are either of you familiar with how the explode code is generated? I also couldn't quite figure out what was generating access to a linked list (immutable.List) where a random-access collection looks more appropriate.;;;","29/Oct/17 10:46;uzadude;Sure,
the plan for
{code:java}
val df_exploded = df.select(expr(""c1""), explode($""c_arr"").as(""c2"")).selectExpr(""c1"" ,""c2.*"")
{code}
is 
{noformat}
== Parsed Logical Plan ==
'Project [unresolvedalias('c1, None), ArrayBuffer(c2).*]
+- Project [c1#6, c2#25]
   +- Generate explode(c_arr#7), true, false, [c2#25]
      +- Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#4]
            +- ExternalRDD [obj#2]

== Analyzed Logical Plan ==
c1: string, _1: string, _2: string, _3: string, _4: string
Project [c1#6, c2#25._1 AS _1#40, c2#25._2 AS _2#41, c2#25._3 AS _3#42, c2#25._4 AS _4#43]
+- Project [c1#6, c2#25]
   +- Generate explode(c_arr#7), true, false, [c2#25]
      +- Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#4]
            +- ExternalRDD [obj#2]

== Optimized Logical Plan ==
Project [c1#6, c2#25._1 AS _1#40, c2#25._2 AS _2#41, c2#25._3 AS _3#42, c2#25._4 AS _4#43]
+- Generate explode(c_arr#7), true, false, [c2#25]
   +- Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
      +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(input[0, scala.Tuple2, true])._2, None) AS _2#4]
         +- ExternalRDD [obj#2]

== Physical Plan ==
*Project [c1#6, c2#25._1 AS _1#40, c2#25._2 AS _2#41, c2#25._3 AS _3#42, c2#25._4 AS _4#43]
+- Generate explode(c_arr#7), true, false, [c2#25]
   +- *Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
      +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(input[0, scala.Tuple2, true])._2, None) AS _2#4]
         +- Scan ExternalRDDScan[obj#2]
{noformat}
and for:
{code:java}
val df_exploded = df.select(explode($""c_arr"").as(""c2"")).selectExpr(""c2.*"")
{code}
is 
{noformat}
== Parsed Logical Plan ==
'Project [ArrayBuffer(c2).*]
+- Project [c2#25]
   +- Generate explode(c_arr#7), false, false, [c2#25]
      +- Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#4]
            +- ExternalRDD [obj#2]

== Analyzed Logical Plan ==
_1: string, _2: string, _3: string, _4: string
Project [c2#25._1 AS _1#38, c2#25._2 AS _2#39, c2#25._3 AS _3#40, c2#25._4 AS _4#41]
+- Project [c2#25]
   +- Generate explode(c_arr#7), false, false, [c2#25]
      +- Project [_1#3 AS c1#6, _2#4 AS c_arr#7]
         +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, None) AS _2#4]
            +- ExternalRDD [obj#2]

== Optimized Logical Plan ==
Project [c2#25._1 AS _1#38, c2#25._2 AS _2#39, c2#25._3 AS _3#40, c2#25._4 AS _4#41]
+- Generate explode(c_arr#7), false, false, [c2#25]
   +- Project [_2#4 AS c_arr#7]
      +- SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(input[0, scala.Tuple2, true])._2, None) AS _2#4]
         +- ExternalRDD [obj#2]

== Physical Plan ==
*Project [c2#25._1 AS _1#38, c2#25._2 AS _2#39, c2#25._3 AS _3#40, c2#25._4 AS _4#41]
+- Generate explode(c_arr#7), false, false, [c2#25]
   +- *Project [_2#4 AS c_arr#7]
      +- *SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, scala.Tuple2, true])._1, true) AS _1#3, mapobjects(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), if (isnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))) null else named_struct(_1, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._1, true), _2, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._2, true), _3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._3, true), _4, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(lambdavariable(MapObjects_loopValue0, MapObjects_loopIsNull0, ObjectType(class scala.Tuple4), true))._4, true)), assertnotnull(input[0, scala.Tuple2, true])._2, None) AS _2#4]
         +- Scan ExternalRDDScan[obj#2]
{noformat};;;","29/Oct/17 12:39;srowen;Thanks [~cloud_fan] for the fast look. You're saying that https://issues.apache.org/jira/browse/SPARK-22385 is a superset of this issue?;;;","29/Oct/17 13:10;cloud_fan;I'd say they are different issues, and I haven't figured out the reason for this issue yet, and wanna fix that small issue first.;;;","30/Oct/17 04:18;uzadude;After futher investigating I believe that my assesment is correct, the former case creates a generator with join=true while the later with join=false, as you can see in plans above (I also debugged). this causes the very long array of size 100k to be duplicated 100k times and afterwards get pruned because its column is not in the final projection. 
I'm not sure what's the best way to address this issue - ammend the generate operator according to the projection.
in the meanwhile, in our case, I worked around that by manually adding the outer fields into each of structs of the array and then exploded only the array. it's an ugly solution but reduces our query time from 6 hours to about 2 mins.;;;","30/Oct/17 04:36;uzadude;ok i found the relevant rule:
{code:java|title=Optimizer.scala.java|borderStyle=solid}
    // Turn off `join` for Generate if no column from it's child is used
    case p @ Project(_, g: Generate)
        if g.join && !g.outer && p.references.subsetOf(g.generatedSet) =>
      p.copy(child = g.copy(join = false))
{code}
I'm not sure yet why it doesn't work.;;;","30/Oct/17 06:25;uzadude;After some debugging, I think I understand the tricky part here.
because there are outer fields in the query we set join=true for the Generate class, and because the Generator uses the array as child it can't be removed from the Generate output.
I that because omitting the original column is so common it would make sense to add another attribute to the Generate class, like {{omitChild: Boolean}} and let the Optimizer turn it on with appropriate Rule.
what do you think?;;;","07/Nov/17 11:54;apachespark;User 'uzadude' has created a pull request for this issue:
https://github.com/apache/spark/pull/19683;;;","07/Nov/17 11:54;uzadude;Hi,
I created a pull request: https://github.com/apache/spark/pull/19683
would appreciate if you could take a look.;;;","09/Nov/17 05:46;Tagar;Thank you [~uzadude] - great investigative work.
Would be great if this patch can make it to the 2.3 release.
;;;","29/Dec/17 13:09;cloud_fan;Issue resolved by pull request 19683
[https://github.com/apache/spark/pull/19683];;;","29/Dec/17 16:39;Tagar;Thank you everyone involved. It would be the most exciting fix in 2.3 release for us.;;;",,,,,,,,,,,,,,,
spark dynamic allocation should not idle timeout executors when there are enough tasks to run on them,SPARK-21656,13092964,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yoonlee95,yoonlee95,yoonlee95,07/Aug/17 18:12,18/Dec/17 17:21,14/Jul/23 06:30,16/Aug/17 14:45,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"Right now with dynamic allocation spark starts by getting the number of executors it needs to run all the tasks in parallel (or the configured maximum) for that stage.  After it gets that number it will never reacquire more unless either an executor dies, is explicitly killed by yarn or it goes to the next stage.  The dynamic allocation manager has the concept of idle timeout. Currently this says if a task hasn't been scheduled on that executor for a configurable amount of time (60 seconds by default), then let that executor go.  Note when it lets that executor go due to the idle timeout it never goes back to see if it should reacquire more.

This is a problem for multiple reasons:
1 . Things can happen in the system that are not expected that can cause delays. Spark should be resilient to these. If the driver is GC'ing, you have network delays, etc we could idle timeout executors even though there are tasks to run on them its just the scheduler hasn't had time to start those tasks.  Note that in the worst case this allows the number of executors to go to 0 and we have a deadlock.

2. Internal Spark components have opposing requirements. The scheduler has a requirement to try to get locality, the dynamic allocation doesn't know about this and if it lets the executors go it hurts the scheduler from doing what it was designed to do.  For example the scheduler first tries to schedule node local, during this time it can skip scheduling on some executors.  After a while though the scheduler falls back from node local to scheduler on rack local, and then eventually on any node.  So during when the scheduler is doing node local scheduling, the other executors can idle timeout.  This means that when the scheduler does fall back to rack or any locality where it would have used those executors, we have already let them go and it can't scheduler all the tasks it could which can have a huge negative impact on job run time.
 
In both of these cases when the executors idle timeout we never go back to check to see if we need more executors (until the next stage starts).  In the worst case you end up with 0 and deadlock, but generally this shows itself by just going down to very few executors when you could have 10's of thousands of tasks to run on them, which causes the job to take way more time (in my case I've seen it should take minutes and it takes hours due to only been left a few executors).  

We should handle these situations in Spark.   The most straight forward approach would be to not allow the executors to idle timeout when there are tasks that could run on those executors. This would allow the scheduler to do its job with locality scheduling.  In doing this it also fixes number 1 above because you never can go into a deadlock as it will keep enough executors to run all the tasks on. 

There are other approaches to fix this, like explicitly prevent it from going to 0 executors, that prevents a deadlock but can still cause the job to slowdown greatly.  We could also change it at some point to just re-check to see if we should get more executors, but this adds extra logic, we would have to decide when to check, its also just overhead in letting them go and then re-acquiring them again and this would cause some slowdown in the job as the executors aren't immediately there for the scheduler to place things on. ",,codingcat,csun,Dhruve Ashar,kellyzly,tgraves,yoonlee95,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 18 17:21:53 UTC 2017,,,,,,,,,,"0|i3ii7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/17 18:17;srowen;I don't see how an executor would be idle if there is a task to run, unless of course you changed the locality settings a lot. There's no real detail here that would establish a problem in Spark. ;;;","07/Aug/17 19:11;tgraves;The executor can be idle if the scheduler doesn't put any tasks on it. The scheduler can skip executors due to the locality settings (spark.locality.wait.node).  We have seen this many times now where it gets in this harmonic where some executors get node locality and other don't.  The scheduler skips many of the executors that don't get locality and eventually they idle timeout when there are 10's of thousands of tasks left. 
We generally see this with very large jobs that have like 1000 executors, 150000 map tasks.

We shouldn't allow them to idle timeout if we still need them. ;;;","07/Aug/17 19:34;srowen;Good point. In that case, what's wrong with killing the executor? if the scheduler is consistently preferring locality enough to let those executors go idle -- either those settings are wrong or those executors aren't needed. What's the argument that the app needs them if no tasks are scheduling?;;;","07/Aug/17 19:56;tgraves;If given more time the scheduler would have fallen back to use those for rack local or any locality.    Yes you can get around this by changing the locality settings (which is what the work around is) but I don't think that is what should happen.  Its 2 features that are conflicting with timeouts. And it is the defaults we ship with causing bad things to happen. I do think we should look at the locality logic in the scheduler more to see if there is anything to improve there but I haven't had time to do that.

The thing is that dynamic allocation never gets more executors for the same stage once its  acquired them and let them idle timeout. So if you get some weird situations you end up just having very few executors to run thousands of tasks.  In my opinion its better to hold those executors and let the normal scheduler logic work.  

We can add a config flag for this if needed if people would like this behavior but I think that conflict with the scheduler logic.;;;","07/Aug/17 20:02;srowen;If the issue is ""given more time"" then increase the idle timeout? or indeed the locality settings. Why does this need another configuration? It sounds like it's at best a change to defaults, but, how about start by having the app care less about locality? It doesn't make sense to say that executors that are by definition not needed according to a user's config should not be reclaimed because the config is wrong.;;;","07/Aug/17 20:22;tgraves;why not fix the bug in dynamic allocation?  changing configs is a work around.  like everything else what are the best configs for everyone's job.  

dynamic allocation is supposed to get you enough executors to run all your tasks in parallel (up to your config limits).  This is not allowing that and its code within SPARK that is doing it, not user code. Thus a bug in my opinion.

The documentation even hints at it. The problem is we just didn't catch this issue that in the initial code.

From:
http://spark.apache.org/docs/2.2.0/job-scheduling.html#remove-policy

""in that an executor should not be idle if there are still pending tasks to be scheduled""

One other option here would be to actually let them go and get new ones. This may or may not help depending on if it can get ones with better locality.  it might also just waste time releasing and reacquiring.

I personally would also be ok with changing the locality wait for node to 0 which generally works around the problem, but I think this could happen in other cases and we should fix this bug too.  For instance say your driver does a full GC and can't schedule things within 60 seconds, you lose those executors and we never get them back.   What if you have temporary network congestion and your network timeout is plenty big to allow for, you could idle timeout.  yes we could increase the idle timeout, but in the normal working case the idle timeout is meant to be cases where you don't have any tasks to run on this executor.  Your stage has completed enough you can release some. This is not that case.;;;","07/Aug/17 20:26;tgraves;Another option would be just to add logic for spark to look at some point to see if it should try reacquiring some. All of that though seems like more logic then just not letting them go.  To me Spark needs to be more resilient about this and should handle various possible conditions.  User shouldn't have to tune every single job to account for weird things happening.  Note that if dynamic allocation is off this doesn't happen. So why is user getting worse experience in this case.;;;","07/Aug/17 20:36;srowen;I do not understand what the bug is. Configuration says an executor should go away if idle for X seconds. Configuration leads tasks to schedule on other executors for X seconds. It is correct that it is removed. You are claiming that it would help the application, but, the application is not scheduling anything on the executor. It does not help the app to keep it alive. Right? this seems obvious, so we must be talking about something different. You're talking about a bunch of other logic but what would it be based on? all of the data it has says the executor will be unused, indefinitely.;;;","07/Aug/17 20:49;tgraves;As a said above it DOES help the application to keep them alive. the scheduler logic will fall back to them at some point when it goes to rack/any locality or when it finishes the tasks that are getting locality on those few nodes.  Thus why I'm saying its a conflict within spark. 

SPARK should be resilient to any weird things happening.  In the cases I have described we could actually release all of our executors and never ask for more within a stage, that is a BUG.   We can change the configs to make it so that doesn't normally happen but a user could change them back and when they do that it shouldn't result in a deadlock.

;;;","11/Aug/17 13:46;tgraves;example of test results with this.

We have production job running 21600 tasks.  With default locality the job takes 3.1 hours due to this issue. With the fix proposed in the pull request the job takes 17 minutes.  The resource utilization of the fix does use more resource but every executor eventually has multiple tasks run on it, demonstrating that if we hold on to them for a while the scheduler will fall back and use them. ;;;","11/Aug/17 13:55;srowen;Is this the 'busy driver' scenario that the PR contemplates? If not, then this may be true, but it's not the motivation of the PR, right? this is just a case where you need shorter locality timeout, or something. It's also not the 0-executor scenario that is the motivation of the PR either.

If this is the 'busy driver' scenario, then I also wonder what happens if you increase the locality timeout. That was one unfinished thread in the PR discussion; why do the other executors get tasks only so very eventually?

I want to stay clear on what we're helping here, and also what the cost is: see the flip-side to this situation described in the PR, which could get worse.;;;","11/Aug/17 14:12;tgraves;I don't know what you mean by busy driver.  The example of the tests results is showing this is fixing the issue.  The issue is as I've describe in the description of the jira.  In this case its due to the scheduler and the fact it doesn't immediately use the executors due to the locality settings, as long as you keep those executors around (don't idle timeout them) they do get used and it has a huge impact on the run time.  the executors only eventually get tasks because of the scheduler locality delay.  

I don't know what you mean by the flip-side of the situation and how this gets worse.

If you want something to compare to go see how other frameworks due this same thing. TEZ for instance. This fix is changing it so it acts very similar to those.

;;;","11/Aug/17 14:24;srowen;I'm referring to the same issue you cite repeatedly, including:
https://github.com/apache/spark/pull/18874#issuecomment-321313616
https://issues.apache.org/jira/browse/SPARK-21656?focusedCommentId=16117200&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16117200 

Something like a driver busy in long GC pauses doesn't keep up with the fact that executors are non-idle and removes them. Its conclusion is incorrect and that's what we're trying to fix. All the more because going to 0 executors stops the stage.

Right? I though we finally had it clear that this was the problem being fixed.

Now you're just describing a job that needs a lower locality timeout. (Or else, describing a different problem with different solution, as in https://github.com/apache/spark/pull/18874#issuecomment-321625808 -- why do they take so much longer than 3s to fall back to other executors?) That scenario is not a reason to make this change.

[~tgraves] please read https://github.com/apache/spark/pull/18874#issuecomment-321683515 . You're saying there's no counterpart scenario that is actually harmed by this change a bit, and I think there is. We need to get on the same page.

;;;","11/Aug/17 14:40;tgraves;Yes there is a trade off here, use some more resource or have your job run time be really really slow and possibly deadlock. I completely understand the scenario where some executors may stay up when they aren't being used, if you have a better solution to do both please state it.  As I've stated changing config to me is a work around and not a solution. This case is handled by many other big data frameworks (pig, tez, mapreduce) and I believe spark should handle it as well.   

I would much rather lean towards having as many jobs run as fast as possible without the user having to tune things even at the expense of possibly using more resources.  I've describe 2 scenarios in which this problem can occur, there is also the extreme case where it goes to 0 that you keep mentioning. The fix provided is to address both of them.



;;;","11/Aug/17 14:42;tgraves;Note, I've never said there is no counter part scenario and if you read what I said in the pr you will see that:

| It doesn't hurt the common case, the common case is all your executors have tasks on them as long as there are tasks to run. Normally scheduler can fill up the executors. It will use more resources if the scheduler takes time to put tasks on them, but that versus the time wasted in jobs that don't have enough executors to run on is hard to quantify because its going to be so application dependent. yes it is a behavior change but a behavior change that is fixing an issue.;;;","11/Aug/17 14:48;srowen;In the end I still don't quite agree with how you frame it here. It's making some jobs use more resource to let _other_ jobs move faster when bumping up against timeout limits. The downside of this change it no compelling just so that someone doesn't have to tune their job, so I'd discard that argument. It is compelling to solve the ""busy driver"" and ""0 executor"" problems. I'd have preferred to frame it that way from the get-go. This discussion isn't going to get farther, and agreeing on an outcome but disagreeing about why is close enough.;;;","18/Dec/17 17:21;codingcat;NOTE: the issue fixed by https://github.com/apache/spark/pull/18874;;;",,,,,,,,,,,,,,,,,,,,,,
Optimizer cannot reach a fixed point on certain queries,SPARK-21652,13092801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,aokolnychyi,aokolnychyi,07/Aug/17 07:21,25/Jan/19 02:14,14/Jul/23 06:30,19/Dec/17 17:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Optimizer,SQL,,,,0,,,,,,,,,"The optimizer cannot reach a fixed point on the following query:

{code}
Seq((1, 2)).toDF(""col1"", ""col2"").write.saveAsTable(""t1"")
Seq(1, 2).toDF(""col"").write.saveAsTable(""t2"")
spark.sql(""SELECT * FROM t1, t2 WHERE t1.col1 = 1 AND 1 = t1.col2 AND t1.col1 = t2.col AND t1.col2 = t2.col"").explain(true)
{code}

At some point during the optimization, InferFiltersFromConstraints infers a new constraint '(col2#33 = col1#32)' that is appended to the join condition, then PushPredicateThroughJoin pushes it down, ConstantPropagation replaces '(col2#33 = col1#32)' with '1 = 1' based on other propagated constraints, ConstantFolding replaces '1 = 1' with 'true and BooleanSimplification finally removes this predicate. However, InferFiltersFromConstraints will again infer '(col2#33 = col1#32)' on the next iteration and the process will continue until the limit of iterations is reached. 

See below for more details

{noformat}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
!Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                       Join Inner, ((col2#33 = col1#32) && ((col1#32 = col#34) && (col2#33 = col#34)))
 :- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))   :- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))
 :  +- Relation[col1#32,col2#33] parquet                                                      :  +- Relation[col1#32,col2#33] parquet
 +- Filter ((1 = col#34) && isnotnull(col#34))                                                +- Filter ((1 = col#34) && isnotnull(col#34))
    +- Relation[col#34] parquet                                                                  +- Relation[col#34] parquet
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushPredicateThroughJoin ===
!Join Inner, ((col2#33 = col1#32) && ((col1#32 = col#34) && (col2#33 = col#34)))              Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))
!:- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))   :- Filter (col2#33 = col1#32)
!:  +- Relation[col1#32,col2#33] parquet                                                      :  +- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))
!+- Filter ((1 = col#34) && isnotnull(col#34))                                                :     +- Relation[col1#32,col2#33] parquet
!   +- Relation[col#34] parquet                                                               +- Filter ((1 = col#34) && isnotnull(col#34))
!                                                                                                +- Relation[col#34] parquet
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CombineFilters ===
 Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                          Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))
!:- Filter (col2#33 = col1#32)                                                                   :- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && (col2#33 = col1#32))
!:  +- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))   :  +- Relation[col1#32,col2#33] parquet
!:     +- Relation[col1#32,col2#33] parquet                                                      +- Filter ((1 = col#34) && isnotnull(col#34))
!+- Filter ((1 = col#34) && isnotnull(col#34))                                                      +- Relation[col#34] parquet
!   +- Relation[col#34] parquet                                                                  
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantPropagation ===
 Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                                                Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))
!:- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && (col2#33 = col1#32))   :- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && (1 = 1))
 :  +- Relation[col1#32,col2#33] parquet                                                                               :  +- Relation[col1#32,col2#33] parquet
 +- Filter ((1 = col#34) && isnotnull(col#34))                                                                         +- Filter ((1 = col#34) && isnotnull(col#34))
    +- Relation[col#34] parquet                                                                                           +- Relation[col#34] parquet
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding ===
 Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                                    Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))
!:- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && (1 = 1))   :- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && true)
 :  +- Relation[col1#32,col2#33] parquet                                                                   :  +- Relation[col1#32,col2#33] parquet
 +- Filter ((1 = col#34) && isnotnull(col#34))                                                             +- Filter ((1 = col#34) && isnotnull(col#34))
    +- Relation[col#34] parquet                                                                               +- Relation[col#34] parquet
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.BooleanSimplification ===
 Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                                 Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))
!:- Filter (((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33))) && true)   :- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))
 :  +- Relation[col1#32,col2#33] parquet                                                                :  +- Relation[col1#32,col2#33] parquet
 +- Filter ((1 = col#34) && isnotnull(col#34))                                                          +- Filter ((1 = col#34) && isnotnull(col#34))
    +- Relation[col#34] parquet                                                                            +- Relation[col#34] parquet
                

=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
!Join Inner, ((col1#32 = col#34) && (col2#33 = col#34))                                       Join Inner, ((col2#33 = col1#32) && ((col1#32 = col#34) && (col2#33 = col#34)))
 :- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))   :- Filter ((isnotnull(col1#32) && isnotnull(col2#33)) && ((col1#32 = 1) && (1 = col2#33)))
 :  +- Relation[col1#32,col2#33] parquet                                                      :  +- Relation[col1#32,col2#33] parquet
 +- Filter ((1 = col#34) && isnotnull(col#34))                                                +- Filter ((1 = col#34) && isnotnull(col#34))
    +- Relation[col#34] parquet                                                                  +- Relation[col#34] parquet
{noformat}
",,aokolnychyi,apachespark,kiszk,maropu,pfchang,tejasp,vish741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26569,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 14 11:08:32 UTC 2018,,,,,,,,,,"0|i3ih7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/17 07:28;aokolnychyi;One option to fix this is NOT to apply ConstantPropagation to such predicates as '(col1 = col2)' if both sides can be replaced with a constant value.;;;","07/Aug/17 08:15;maropu;It seems the known issue; have you tried `spark.sql.constraintPropagation.enabled`?;;;","07/Aug/17 17:54;aokolnychyi;Yes, disabling the constraint propagation helps because `InferFiltersFromConstraints` will not apply. I found several known issues regarding the performance of `InferFiltersFromConstraints` but what about the logic of `ConstantPropagation` in the above example? Should it replace such predicates as `(a = b)` with `(1 = 1)` even if it is semantically correct?;;;","08/Aug/17 11:26;maropu;oh, yea. I see. Probably, I think `InferFiltersFromConstraints` should not infer `col1 = col2`.;;;","15/Aug/17 08:05;maropu;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18882;;;","31/Aug/17 21:58;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/19099;;;","06/Sep/17 18:12;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19149;;;","14/Sep/18 11:08;pfchang;Hi, after this change, there are some cases not supported, see follows:

select * from test1, test2, test3 where test1.a = test2.a and test1.a = test3.a and test3.a =1;

The filter a = 1 will only be pushed down to test1 and test3, not test2.

Is this the  expected behavior?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confusing assert failure in JDBC source when users misspell the option `partitionColumn`,SPARK-21648,13092782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Aug/17 05:25,07/Aug/17 20:05,14/Jul/23 06:30,07/Aug/17 20:05,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
CREATE TABLE mytesttable1 
USING org.apache.spark.sql.jdbc 
  OPTIONS ( 
  url 'jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername}&password=${jdbcPassword}', 
  dbtable 'mytesttable1', 
  paritionColumn 'state_id', 
  lowerBound '0', 
  upperBound '52', 
  numPartitions '53', 
  fetchSize '10000' 
)
{noformat}

The above option name `paritionColumn` is wrong. That mean, users did not provide the value for `partitionColumn`. In such case, users hit a confusing error
{noformat}

AssertionError: assertion failed
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:156)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:39)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:312)
{noformat}
",,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-08-07 05:25:29.0,,,,,,,,,,"0|i3ih33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeJoin failed when using CROSS,SPARK-21647,13092762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,07/Aug/17 01:00,07/Aug/17 16:01,14/Jul/23 06:30,07/Aug/17 16:01,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
val df = Seq((1, 1)).toDF(""i"", ""j"")
df.createOrReplaceTempView(""T"")
withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> ""-1"") {
  sql(""select * from (select a.i from T a cross join T t where t.i = a.i) as t1 "" +
    ""cross join T t2 where t2.i = t1.i"").explain(true)
}
{noformat}
The above code could cause the following exception:
{noformat}
SortMergeJoinExec should not take Cross as the JoinType
java.lang.IllegalArgumentException: SortMergeJoinExec should not take Cross as the JoinType
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.outputOrdering(SortMergeJoinExec.scala:100)
{noformat}",,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 07 01:02:41 UTC 2017,,,,,,,,,,"0|i3igz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/17 01:02;smilegator;https://github.com/apache/spark/pull/18863;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalLimit.maxRows is defined incorrectly,SPARK-21644,13092647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,05/Aug/17 01:19,03/Oct/17 19:39,14/Jul/23 06:30,03/Oct/17 19:39,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"{code}
case class LocalLimit(limitExpr: Expression, child: LogicalPlan) extends UnaryNode {
  override def output: Seq[Attribute] = child.output
  override def maxRows: Option[Long] = {
    limitExpr match {
      case IntegerLiteral(limit) => Some(limit)
      case _ => None
    }
  }
}
{code}

This is simply wrong, since LocalLimit is only about partition level limits.
",,apachespark,rxin,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 29 21:38:09 UTC 2017,,,,,,,,,,"0|i3ig9r:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"10/Aug/17 17:25;smilegator;https://github.com/apache/spark/pull/18851;;;","28/Aug/17 16:53;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18851;;;","29/Sep/17 21:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19393;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FQDN for DRIVER_HOST_ADDRESS instead of ip address,SPARK-21642,13092499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanakahda,tanakahda,tanakahda,04/Aug/17 15:00,14/Dec/20 08:18,14/Jul/23 06:30,17/Aug/17 14:02,2.0.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"In current implementation, ip address of a driver host is set to DRIVER_HOST_ADDRESS [1]. This becomes a problem when we enable SSL using ""spark.ssl.enabled"", ""spark.ssl.trustStore"" and ""spark.ssl.keyStore"" properties. When we configure these properties, spark web ui is launched with SSL enabled and the HTTPS server is configured with the custom SSL certificate you configured in these properties.

In this case, client gets javax.net.ssl.SSLPeerUnverifiedException exception when the client accesses the spark web ui because the client fails to verify the SSL certificate (Common Name of the SSL cert does not match with DRIVER_HOST_ADDRESS).
To avoid the exception, we should use FQDN of the driver host for DRIVER_HOST_ADDRESS.

[1]  https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/package.scala#L222
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L942

Error message that client gets when the client accesses spark web ui:
javax.net.ssl.SSLPeerUnverifiedException: Certificate for <10.102.138.239> doesn't match any of the subject alternative names: []


{code:java}
$ spark-submit /path/to/jar
..
17/08/04 14:48:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/08/04 14:48:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.43.3.8:4040

$ curl -I http://10.43.3.8:4040
HTTP/1.1 302 Found
Date: Fri, 04 Aug 2017 14:48:20 GMT
Location: https://10.43.3.8:4440/
Content-Length: 0
Server: Jetty(9.2.z-SNAPSHOT)

$ curl -v https://10.43.3.8:4440
* Rebuilt URL to: https://10.43.3.8:4440/
*   Trying 10.43.3.8...
* TCP_NODELAY set
* Connected to 10.43.3.8 (10.43.3.8) port 4440 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* Server certificate:
* 	subject: CN=*.example.com,OU=MyDept,O=MyOrg,L=Area,C=US
* 	start date: Jun 12 00:05:02 2017 GMT
* 	expire date: Jun 12 00:05:02 2018 GMT
* 	common name: *.example.com
* 	issuer: CN=*.example.com,OU=MyDept,O=MyOrg,L=Area,C=US
{code}

",,cloud_fan,jniebuhr,jzhuge,tanakahda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33774,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 16 16:01:59 UTC 2018,,,,,,,,,,"0|i3ifen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/17 14:02;cloud_fan;Issue resolved by pull request 18846
[https://github.com/apache/spark/pull/18846];;;","14/Mar/18 20:47;jzhuge;Thanks [~tanakahda] and [~cloud_fan] for adding this feature to support SSL. Unfortunately it breaks my environment where SSL is not used and FQDN can not be set up easily. What do you recommend? Is it possible to switch to the old behavior when SSL is not enabled? If this is feasible, I can create an PR.;;;","16/May/18 12:14;jniebuhr;[~jzhuge] I ran into the same problem. I simply put
{code:java}
export SPARK_LOCAL_HOSTNAME=$(ip route get 1 | awk '{print $NF;exit}'){code}
 into my launch script to solve the issue. This forces spark to use your IP as hostname. It should work on any debian based system.;;;","16/May/18 16:01;jzhuge;Thanks [~jniebuhr] for the tip! Let me try it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warning message of RF is not accurate,SPARK-21638,13092412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,peng.meng@intel.com,peng.meng@intel.com,peng.meng@intel.com,04/Aug/17 08:43,11/Dec/17 13:57,14/Jul/23 06:30,10/Aug/17 20:38,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,,,,,0,,,,,,,,,"When train RF model, there is many warning message like this:
{quote}WARN RandomForest: Tree learning is using approximately 268492800 bytes per iteration, which exceeds requested limit maxMemoryUsage=268435456. This allows splitting 2622 nodes in this iteration.{quote}
This warning message is unnecessary and the data is not accurate.

Actually, if all the nodes cannot split in one iteration, it will show this warning. For most of the case, all the nodes cannot split just in one iteration, so for most of the case, it will show this warning for each iteration.

This is because:
{code:java}
while (nodeStack.nonEmpty && (memUsage < maxMemoryUsage || memUsage == 0)) {
      val (treeIndex, node) = nodeStack.top
      // Choose subset of features for node (if subsampling).
      val featureSubset: Option[Array[Int]] = if (metadata.subsamplingFeatures) {
        Some(SamplingUtils.reservoirSampleAndCount(Range(0,
          metadata.numFeatures).iterator, metadata.numFeaturesPerNode, rng.nextLong())._1)
      } else {
        None
      }
      // Check if enough memory remains to add this node to the group.
      val nodeMemUsage = RandomForest.aggregateSizeForNode(metadata, featureSubset) * 8L
      if (memUsage + nodeMemUsage <= maxMemoryUsage || memUsage == 0) {
        nodeStack.pop()
        mutableNodesForGroup.getOrElseUpdate(treeIndex, new mutable.ArrayBuffer[LearningNode]()) +=
          node
        mutableTreeToNodeToIndexInfo
          .getOrElseUpdate(treeIndex, new mutable.HashMap[Int, NodeIndexInfo]())(node.id)
          = new NodeIndexInfo(numNodesInGroup, featureSubset)
      }
      numNodesInGroup += 1   //we not add the node to mutableNodesForGroup, but we add memUsage here.
      memUsage += nodeMemUsage
    }
    if (memUsage > maxMemoryUsage) {
      // If maxMemoryUsage is 0, we should still allow splitting 1 node.
      logWarning(s""Tree learning is using approximately $memUsage bytes per iteration, which"" +
        s"" exceeds requested limit maxMemoryUsage=$maxMemoryUsage. This allows splitting"" +
        s"" $numNodesInGroup nodes in this iteration."")
    }
{code}

",,apachespark,peng.meng@intel.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 13:57:06 UTC 2017,,,,,,,,,,"0|i3ievj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/17 08:46;srowen;What's inaccurate and why isn't it necessary? Please start with the problem.;;;","04/Aug/17 08:49;peng.meng@intel.com;This is because ""we not add the node to mutableNodesForGroup, but we add memUsage"" in current implemantation.
This logic is not accurate.;;;","04/Aug/17 08:51;peng.meng@intel.com;I will be back home now, will answer your question next week. Thanks [~srowen];;;","04/Aug/17 09:10;peng.meng@intel.com;In the example warning message, the split node shoud be 2621;  ;;;","04/Aug/17 09:13;peng.meng@intel.com;The first data should - nodeMemUsage;;;","10/Aug/17 20:38;srowen;Issue resolved by pull request 18868
[https://github.com/apache/spark/pull/18868];;;","11/Dec/17 13:57;apachespark;User 'mpjlu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18868;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`hive.metastore.warehouse` in --hiveconf is not respected,SPARK-21637,13092397,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,04/Aug/17 07:56,06/Aug/17 00:34,14/Jul/23 06:30,06/Aug/17 00:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"In CliSuite `hive.metastore.warehouse`  at line https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala#L92 since this commit:https://github.com/apache/spark/commit/8f33731e796750e6f60dc9e2fc33a94d29d198b4 . 
` bin/spark-sql --master local --hiveconf hive.metastore.warehouse.dir=/some/dir` will not take affect, because now we respect this property in hadoopconf only but in SparkSQLCliDriver, but we do not add --hiveconf s to hadoopconf.

{code:java}
17/08/04 15:46:53 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is file:/home/hadoop/hzyaoqin/spark-2.2.0-bin-hadoop2.7/spark-warehouse
spark-sql> set hive.metastore.warehouse.dir;
17/08/04 15:46:57 INFO SparkSqlParser: Parsing command: set hive.metastore.warehouse.dir
17/08/04 15:47:00 INFO CodeGenerator: Code generated in 166.354926 ms
hive.metastore.warehouse.dir	/some/dir
Time taken: 2.154 seconds, Fetched 1 row(s)
17/08/04 15:47:00 INFO CliDriver: Time taken: 2.154 seconds, Fetched 1 row(s)
spark-sql> set spark.sql.warehouse.dir;
17/08/04 15:47:13 INFO SparkSqlParser: Parsing command: set spark.sql.warehouse.dir
spark.sql.warehouse.dir	file:/home/hadoop/hzyaoqin/spark-2.2.0-bin-hadoop2.7/spark-warehouse
Time taken: 0.024 seconds, Fetched 1 row(s)
17/08/04 15:47:13 INFO CliDriver: Time taken: 0.024 seconds, Fetched 1 row(s) 

{code}",,Qin Yao,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 06 00:32:34 UTC 2017,,,,,,,,,,"0|i3ies7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/17 00:32;smilegator;https://github.com/apache/spark/pull/18668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reset numRecordsWritten after DiskBlockObjectWriter.commitAndGet called,SPARK-21621,13092079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,coneyliu,coneyliu,03/Aug/17 06:03,07/Aug/17 09:06,14/Jul/23 06:30,07/Aug/17 09:05,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"We should reset numRecordsWritten to zero after DiskBlockObjectWriter.commitAndGet called.
Because when `revertPartialWritesAndClose` be called, we decrease the  written records in `ShuffleWriteMetrics` . However, we decreased the written records to zero, this should be wrong, we should only decreased the number reords after the last `commitAndGet` called. ",,cloud_fan,coneyliu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 07 09:05:43 UTC 2017,,,,,,,,,,"0|i3icu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/17 09:05;cloud_fan;Issue resolved by pull request 18830
[https://github.com/apache/spark/pull/18830];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE...ADD COLUMNS broken in Hive 2.1 for DS tables,SPARK-21617,13092017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,02/Aug/17 22:09,01/Sep/17 18:01,14/Jul/23 06:30,21/Aug/17 22:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"When you have a data source table and you run a ""ALTER TABLE...ADD COLUMNS"" query, Spark will save invalid metadata to the Hive metastore.

Namely, it will overwrite the table's schema with the data frame's schema; that is not desired for data source tables (where the schema is stored in a table property instead).

Moreover, if you use a newer metastore client where METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES is on by default, you actually get an exception:

{noformat}
InvalidOperationException(message:The following columns have types incompatible with the existing columns in their respective positions :
c1)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(MetaStoreUtils.java:615)
	at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:133)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3704)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3675)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy26.alter_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table_with_environmentContext(HiveMetaStoreClient.java:402)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table_with_environmentContext(SessionHiveMetaStoreClient.java:309)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:154)
	at com.sun.proxy.$Proxy27.alter_table_with_environmentContext(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:601)
{noformat}

That exception is handled by Spark in an odd way (see code in {{HiveExternalCatalog.scala}}) which still stores invalid metadata.",,apachespark,JonnyR,KevinZwx,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 01 18:01:05 UTC 2017,,,,,,,,,,"0|i3icgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/17 22:16;vanzin;Here's the full test error from our internal build against 2.1:

{noformat}
15:11:29.602 WARN org.apache.spark.sql.hive.test.TestHiveExternalCatalog: Could not alter schema of table  `default`.`t1` in a Hive compatible way. Updating Hive metastore in Spark SQL specific format.
[snip]
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :
c1
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:624)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:602)
- alter datasource table add columns - partitioned - csv *** FAILED ***
  org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: at least one column must be specified for the table;
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:107)
  at org.apache.spark.sql.hive.HiveExternalCatalog.alterTableSchema(HiveExternalCatalog.scala:656)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.alterTableSchema(SessionCatalog.scala:372)
{noformat}

So the exception above is just a warning, and the problem seems to actually be in how Spark is recovering from that situation (the exception handler in {{HiveExternalCatalog.alterTableSchema}}).
;;;","15/Aug/17 00:12;vanzin;https://github.com/apache/spark/pull/18849;;;","01/Sep/17 18:01;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18849;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken redirect in collaborative filtering docs to databricks training repo,SPARK-21615,13091944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,iblong2iyush,iblong2iyush,iblong2iyush,02/Aug/17 17:06,12/Dec/22 18:10,14/Jul/23 06:30,03/Aug/17 08:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Documentation,MLlib,,,,0,documentation,easyfix,,,,,,,"* Current [MLlib Collaborative Filtering tutorial|https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html] points to broken links to old databricks website.
* Databricks moved all their content to [git repo|https://github.com/databricks/spark-training]
* Two links needs to be fixed:
** [training exercises|https://databricks-training.s3.amazonaws.com/index.html]
** [personalized movie recommendation with spark.mllib|https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html]",,iblong2iyush,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 08:59:02 UTC 2017,,,,,,,,,,"0|i3ic07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 17:27;srowen;Sure, just open a PR. If this is old content and third-party content, we might rethink linking to it in the project docs. But I don't feel strongly about it.;;;","02/Aug/17 17:43;iblong2iyush;[~srowen] Hi, I opened a [PR|https://github.com/apache/spark/pull/18821] but not able to assign the issue to myself. ;;;","02/Aug/17 19:23;gurwls223;User 'singhay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18821;;;","03/Aug/17 01:11;iblong2iyush;[~hyukjin.kwon] That's me, is there any way to link my git account with this ?;;;","03/Aug/17 01:16;gurwls223;It should have been done automatically but there looks a problem for now. Please refer http://apache-spark-developers-list.1001551.n3.nabble.com/Some-PRs-not-automatically-linked-to-JIRAs-td22067.html. Usually, I guess there is nothing to do with git and JIRA.;;;","03/Aug/17 08:59;srowen;Issue resolved by pull request 18821
[https://github.com/apache/spark/pull/18821];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupt records are not handled properly when creating a dataframe from a file,SPARK-21610,13091816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cjm,dmt,dmt,02/Aug/17 08:56,12/Dec/22 18:10,14/Jul/23 06:30,12/Sep/17 13:48,2.0.2,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Consider a jsonl file with 3 records. The third record has a value of type string, instead of int.
{code}
echo '{""field"": 1}
{""field"": 2}
{""field"": ""3""}' >/tmp/sample.json
{code}

Create a dataframe from this file, with a schema that contains ""_corrupt_record"" so that corrupt records are kept.

{code}
import org.apache.spark.sql.types._

val schema = new StructType()
  .add(""field"", ByteType)
  .add(""_corrupt_record"", StringType)

val file = ""/tmp/sample.json""

val dfFromFile = spark.read.schema(schema).json(file)
{code}

Run the following lines from a spark-shell:
{code}
scala> dfFromFile.show(false)
+-----+---------------+
|field|_corrupt_record|
+-----+---------------+
|1    |null           |
|2    |null           |
|null |{""field"": ""3""} |
+-----+---------------+

scala> dfFromFile.filter($""_corrupt_record"".isNotNull).count()
res1: Long = 0

scala> dfFromFile.filter($""_corrupt_record"".isNull).count()
res2: Long = 3
{code}
The expected result is 1 corrupt record and 2 valid records, but the actual one is 0 corrupt record and 3 valid records.

The bug is not reproduced if we create a dataframe from a RDD:
{code}
scala> val rdd = sc.textFile(file)
rdd: org.apache.spark.rdd.RDD[String] = /tmp/sample.json MapPartitionsRDD[92] at textFile at <console>:28

scala> val dfFromRdd = spark.read.schema(schema).json(rdd)
dfFromRdd: org.apache.spark.sql.DataFrame = [field: tinyint, _corrupt_record: string]

scala> dfFromRdd.show(false)
+-----+---------------+
|field|_corrupt_record|
+-----+---------------+
|1    |null           |
|2    |null           |
|null |{""field"": ""3""} |
+-----+---------------+

scala> dfFromRdd.filter($""_corrupt_record"".isNotNull).count()
res5: Long = 1

scala> dfFromRdd.filter($""_corrupt_record"".isNull).count()
res6: Long = 2
{code}
","macOs Sierra 10.12.5

",apachespark,cjm,dmt,goun,panbingkun,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22580,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 13:48:08 UTC 2017,,,,,,,,,,"0|i3ib7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/17 04:26;viirya;I'm mentoring one beginner working on this.;;;","07/Aug/17 06:33;cjm;I have created a pull request for this issue:
[https://github.com/apache/spark/pull/18865];;;","28/Aug/17 16:54;apachespark;User 'jmchung' has created a pull request for this issue:
https://github.com/apache/spark/pull/18865;;;","12/Sep/17 06:55;apachespark;User 'jmchung' has created a pull request for this issue:
https://github.com/apache/spark/pull/19199;;;","12/Sep/17 13:48;gurwls223;Issue resolved by pull request 19199
[https://github.com/apache/spark/pull/19199];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let IntelliJ IDEA correctly detect Language level and Target byte code version,SPARK-21605,13091780,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,baibaichen,baibaichen,baibaichen,02/Aug/17 05:26,03/Aug/17 15:45,14/Jul/23 06:30,03/Aug/17 11:00,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,IDE,maven,,,,,,,"With SPARK-21592, removing source and target properties from maven-compiler-plugin lets IntelliJ IDEA use default Language level and Target byte code version which are 1.4.",,baibaichen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21628,,,,,,,,,SPARK-21592,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 11:00:03 UTC 2017,,,,,,,,,,"0|i3iazr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/17 11:00;srowen;Issue resolved by pull request 18808
[https://github.com/apache/spark/pull/18808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collecting column statistics for datasource tables may fail with java.util.NoSuchElementException,SPARK-21599,13091732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,dkbiswal,dkbiswal,01/Aug/17 23:27,12/Dec/22 18:10,14/Jul/23 06:30,03/Aug/17 16:32,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,1,,,,,,,,,"Collecting column level statistics for non compatible hive tables using 

{code}
ANALYZE TABLE <tableName> FOR COLUMNS <column_list>
{code}

may fail with the following exception.

{code}

key not found: a
java.util.NoSuchElementException: key not found: a
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at scala.collection.AbstractMap.default(Map.scala:59)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at scala.collection.AbstractMap.apply(Map.scala:59)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$alterTableStats$1$$anonfun$apply$mcV$sp$3.apply(HiveExternalCatalog.scala:657)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$alterTableStats$1$$anonfun$apply$mcV$sp$3.apply(HiveExternalCatalog.scala:656)
	at scala.collection.immutable.Map$Map2.foreach(Map.scala:137)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$alterTableStats$1.apply$mcV$sp(HiveExternalCatalog.scala:656)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$alterTableStats$1.apply(HiveExternalCatalog.scala:634)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$alterTableStats$1.apply(HiveExternalCatalog.scala:634)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.alterTableStats(HiveExternalCatalog.scala:634)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.alterTableStats(SessionCatalog.scala:375)
	at org.apache.spark.sql.execution.command.AnalyzeColumnCommand.run(AnalyzeColumnCommand.scala:57)
{code}",,dkbiswal,dongjoon,smilegator,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21627,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:22:04 UTC 2017,,,,,,,,,,"0|i3iap3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 03:44;smilegator;https://github.com/apache/spark/pull/18804;;;","02/Aug/17 19:22;gurwls223;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/18804;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avg event time calculated in progress may be wrong,SPARK-21597,13091696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,01/Aug/17 21:45,01/Feb/19 00:02,14/Jul/23 06:30,02/Aug/17 18:00,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"Right now it stores sum of event times and uses it to calc avg. However, the sum value may be overflow.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26806,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 18:07:07 UTC 2017,,,,,,,,,,"0|i3iah3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 18:07;zsxwing;Resolved by https://github.com/apache/spark/pull/18803;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit the places calling HDFSMetadataLog.get,SPARK-21596,13091641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,01/Aug/17 18:24,12/Dec/22 18:10,14/Jul/23 06:30,09/Aug/17 06:50,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"When I was investigating a flaky test, I realized that many places don't check the return value of `HDFSMetadataLog.get(batchId: Long): Option[T]`. When a batch is supposed to be there, the caller just ignores None rather than throwing an error. If some bug causes a query doesn't generate a batch metadata file, this behavior will hide it and allow the query continuing to run.",,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21760,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:21:29 UTC 2017,,,,,,,,,,"0|i3ia53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 19:21;gurwls223;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18799;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
introduction of spark.sql.windowExec.buffer.spill.threshold in spark 2.2 breaks existing workflow,SPARK-21595,13091590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tejasp,sreiling,sreiling,01/Aug/17 15:29,21/Apr/20 05:14,14/Jul/23 06:30,11/Aug/17 20:02,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,PySpark,,,,0,documentation,regression,,,,,,,"My pyspark code has the following statement:


{code:java}
# assign row key for tracking
df = df.withColumn(
        'association_idx',
        sqlf.row_number().over(
            Window.orderBy('uid1', 'uid2')
        )
    )
{code}


where df is a long, skinny (450M rows, 10 columns) dataframe. So this creates one large window for the whole dataframe to sort over.
In spark 2.1 this works without problem, in spark 2.2 this fails either with out of memory exception or too many open files exception, depending on memory settings (which is what I tried first to fix this).
Monitoring the blockmgr, I see that spark 2.1 creates 152 files, spark 2.2 creates >110,000 files.
In the log I see the following messages (110,000 of these):

{noformat}
17/08/01 08:55:37 INFO UnsafeExternalSorter: Spilling data because number of spilledRecords crossed the threshold 4096
17/08/01 08:55:37 INFO UnsafeExternalSorter: Thread 156 spilling sort data of 64.1 MB to disk (0  time so far)
17/08/01 08:55:37 INFO UnsafeExternalSorter: Spilling data because number of spilledRecords crossed the threshold 4096
17/08/01 08:55:37 INFO UnsafeExternalSorter: Thread 156 spilling sort data of 64.1 MB to disk (1  time so far)
{noformat}

So I started hunting for clues in UnsafeExternalSorter, without luck. What I had missed was this one message:

{noformat}
17/08/01 08:55:37 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
{noformat}

Which allowed me to track down the issue. 
By changing the configuration to include:

{code:java}
spark.sql.windowExec.buffer.spill.threshold	2097152
{code}

I got it to work again and with the same performance as spark 2.1.
I have workflows where I use windowing functions that do not fail, but took a performance hit due to the excessive spilling when using the default of 4096.
I think to make it easier to track down these issues this config variable should be included in the configuration documentation. 
Maybe 4096 is too small of a default value?
",pyspark on linux,hvanhovell,longcao,Rakesh_Shah,sreiling,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 05:14:29 UTC 2020,,,,,,,,,,"0|i3i9tz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 00:40;tejasp;This config was introduced by me in SPARK-13450. The reason why 4096 was used is because before the change it was using 4096 as threshold to switch to `UnsafeExternalSorter` (see WindowExec.scala in https://github.com/apache/spark/pull/16909/files). I don't have real workloads which use WINDOW operator so would defer from proposing a value but I am open to change the default value to something that works well for everyone. After you bumped up the config, how many files were generated ? I want to know what value would effectively create the same number of files as spark 2.1 did.;;;","03/Aug/17 19:59;hvanhovell;The old and the new code are not exactly the same. The old code path would start using a disk spilling buffer when a window would become larger than 4096 rows. The key difference is that old code path would not start to spill at that point, that would only happen when the Spark would get pressed for memory and the memory manager starts to force spills. The current version is overly active and starts spilling at a much earlier stage. We have seen similar problems with customer workloads on our end.

We either need to set this to a more sensible default, or return this to the old behavior.;;;","03/Aug/17 22:36;tejasp;[~hvanhovell] : I am fine with either options you mentioned. 

one more option: Right now the (switch from in-memory to `UnsafeExternalSorter`) and (`UnsafeExternalSorter` spilling to disk) is controlled by a single threshold. If we de-couple those two using separate thresholds, then the ""spill on memory pressure"" behavior will be achieved. The threshold for in-memory can be kept small and keeping the spilling to disk higher will avoid excessive disk spills. This is fairly simple change to do. What do you think ?;;;","04/Aug/17 11:15;sreiling;I have tried out a couple of settings for spark.sql.windowExec.buffer.spill.threshold and I have now settled on 4M as the default for it in my workflows. This gives about the same behavior as spark 2.1. But this is dependent on the amount of spark memory and the size of the rows in the dataframe.
I am not in favor of introducing another threshold for this. If the spilling is delayed, but then happens with the low threshold of 4096 rows, in my case this would still spill 110k files to disk and potentially cause a ""too many open files"" exception (right ?).
Just looking at the spilling behavior, it would be better if the value would not specify the number of rows, but the amount of memory. So instead of 4096 rows, it would specify 500MB of memory, and then spill chunks of 500MB to disk. How many rows this is would change case by case.;;;","04/Aug/17 17:40;tejasp;[~sreiling] Spilling will happen only when _both_ these are met:

- number of records exceeds the limit of in-memory array. Earlier this was hardcoded to be 4096, with the change it will be configurable (default value = 4096).
- there is less memory on the executors due to many consumers OR a spill threshold based on number of records has reached (this was always configurable and defaulted to `UnsafeExternalSorter.DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD`).

This was what used to happen in v2.1 and what I am proposing. The second criteria takes care of looking at the actual memory utilization (spill threshold value used to be defaulted to a super large value so typically the memory pressure situation kicks in before that).

Here is a PR for that : https://github.com/apache/spark/pull/18843 Please take a look and share your feedback.
;;;","21/Apr/20 05:14;Rakesh_Shah;Hi [~sreiling] I am also facing same issue where my shuffle is taking a long time,

Here I am joining two tables in spark, but before this point there is a window function being used.

When I debugged i saw this below info, when I tried to change the property I am not able to change it, it still shows same.

 

20/04/21 04:15:18 INFO Executor: Finished task 935.0 in stage 43.0 (TID 28873). 26714 bytes result sent to driver
20/04/21 04:18:20 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20/04/21 04:18:20 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20/04/21 04:18:20 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20/04/21 04:20:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20/04/21 04:20:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter
20/04/21 04:20:49 INFO ExternalAppendOnlyUnsafeRowArray: Reached spill threshold of 4096 rows, switching to org.apache.spark.util.collection.uns

 

can you please help me with this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken configuration page,SPARK-21593,13091549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,asukhenko,asukhenko,01/Aug/17 13:54,01/Aug/17 18:06,14/Jul/23 06:30,01/Aug/17 18:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,,,,,0,,,,,,,,,"Latest configuration page for Spark 2.2.0 has broken menu list and named anchors.
Compare [2.1.1 docs |https://spark.apache.org/docs/2.1.1/configuration.html] with [Latest docs |https://spark.apache.org/docs/latest/configuration.html]

Or try this link [Configuration # Dynamic Allocation|https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation] with should open Dynamic Allocation part of the page, but doesn't.
!dyn_latest.jpg!

----

!dyn_211.jpg!

",Chrome/Firefox,asukhenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 14:00;asukhenko;doc_211.jpg;https://issues.apache.org/jira/secure/attachment/12879840/doc_211.jpg","01/Aug/17 14:00;asukhenko;doc_latest.jpg;https://issues.apache.org/jira/secure/attachment/12879841/doc_latest.jpg","01/Aug/17 14:00;asukhenko;dyn_211.jpg;https://issues.apache.org/jira/secure/attachment/12879839/dyn_211.jpg","01/Aug/17 14:00;asukhenko;dyn_latest.jpg;https://issues.apache.org/jira/secure/attachment/12879838/dyn_latest.jpg",,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 18:06:23 UTC 2017,,,,,,,,,,"0|i3i9kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 13:55;asukhenko;Latest documentation page with broken menu and working 2.1.1 ;;;","01/Aug/17 14:00;asukhenko;Broken anchors and menu;;;","01/Aug/17 14:07;srowen;I couldn't make out what you were reporting from this descripiton, but I think you mean that anchors after ""Memory Management"" aren't rendering.

I think it might be this change, which didn't quite close an HTML tag just above:
https://github.com/apache/spark/commit/fa7c582e9442b985a0493fb1dd15b3fb9b6031b4#diff-76e731333fb756df3bff5ddb3b731c46R1004;;;","01/Aug/17 14:11;asukhenko;Yes, as well as having {code}### Dynamic Allocation{code} instead of  {code}<h3 id=""dynamic-allocation"">Dynamic Allocation</h3>{code} ;;;","01/Aug/17 15:35;asukhenko;[~srowen] Yes, {code} <td><code>spark.storage.replication.proactive<code></td> {code} is breaking the page.;;;","01/Aug/17 18:06;srowen;Issue resolved by pull request 18793
[https://github.com/apache/spark/pull/18793];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLContext.getConf(key, null) should return null, but it throws NPE",SPARK-21588,13091436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vinodkc,brkyvz,brkyvz,01/Aug/17 02:48,25/May/21 08:13,14/Jul/23 06:30,06/Aug/17 06:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"SQLContext.get(key) for a key that is not defined in the conf, and doesn't have a default value defined, throws a NoSuchElementException. In order to avoid that, I used a null as the default value, which threw a NPE instead. If it is null, it shouldn't try to parse the default value in `getConfString`",,aokolnychyi,apachespark,brkyvz,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 25 08:13:12 UTC 2021,,,,,,,,,,"0|i3i8vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 09:25;vinodkc;[~brkyvz] Can you share sample code and NPE stack trace?;;;","03/Aug/17 20:20;aokolnychyi;I did not manage to reproduce this. I tried:

{code}
spark.sqlContext.getConf(""spark.sql.streaming.checkpointLocation"", null) // null
spark.sqlContext.getConf(""spark.sql.thriftserver.scheduler.pool"", null) // null
spark.sqlContext.getConf(""spark.sql.sources.outputCommitterClass"", null) // null
spark.sqlContext.getConf(""blabla"", null) // null
spark.sqlContext.getConf(""spark.sql.sources.outputCommitterClass"") // <undefined>
{code}

I got a NPE only when I called getConf(key, null) for a parameter with a default value. For example, 
{code}
spark.sqlContext.getConf(""spark.sql.thriftServer.incrementalCollect"", ""<undefined>"") // <undefined>
spark.sqlContext.getConf(""spark.sql.thriftServer.incrementalCollect"", null) // NPE
{code}
;;;","03/Aug/17 20:25;brkyvz;[~vinodkc] [~aokolnychyi]

It happens when the config has a value converter, example `spark.sql.shuffle.partitions`. Basically any non-string sql conf.;;;","03/Aug/17 21:19;aokolnychyi;Sure, but the converter will not be called if the default value that you pass is ""<undefined>"". However, the check can be extended to `defaultValue != null && defaultValue != ""<undefined>""` in the SQLConf#getConfString.;;;","03/Aug/17 21:22;brkyvz;that's what I was proposing. `null` seemed more familiar than `<undefined>` before I looked at the code. ;;;","06/Aug/17 04:31;vinodkc;https://github.com/apache/spark/pull/18852;;;","25/May/21 08:13;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/18852;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter pushdown for EventTime Watermark Operator,SPARK-21587,13091396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joseph.torres,joseph.torres,31/Jul/17 23:16,12/Dec/22 18:10,14/Jul/23 06:30,09/Aug/17 19:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"If I have a streaming query that sets a watermark, then a where() that pertains to a partition column does not prune these partitions and they will all be queried, greatly reducing performance for partitioned tables.",,codingcat,joseph.torres,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 09 19:50:41 UTC 2017,,,,,,,,,,"0|i3i8n3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/17 19:21;gurwls223;User 'joseph-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/18790;;;","09/Aug/17 19:50;tdas;Issue resolved by pull request 18790
[https://github.com/apache/spark/pull/18790];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Application Master marking application status as Failed for Client Mode,SPARK-21585,13091357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pgandhi,pgandhi,pgandhi,31/Jul/17 20:50,17/May/20 18:13,14/Jul/23 06:30,01/Aug/17 13:42,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"Refer https://issues.apache.org/jira/browse/SPARK-21541 for more clarity.

The fix deployed for SPARK-21541 resulted in the Application Master to set the final status of a spark application as Failed for the client mode as the flag 'registered' was not being set to true for client mode.",,pgandhi,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21541,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 13:43:13 UTC 2017,,,,,,,,,,"0|i3i8ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/17 20:51;pgandhi;Currently working on the fix, will file a pull request as soon as it is done.;;;","01/Aug/17 13:38;tgraves;[~srowen]do you know if the github pull request link isn't working anymore?;;;","01/Aug/17 13:43;srowen;Yeah I've noticed -- not sure why or how to fix it. A PR can be linked manually if needed.;;;","01/Aug/17 13:43;tgraves;https://github.com/apache/spark/pull/18788;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug with  `Group by ordinal`,SPARK-21580,13091085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,10110346,10110346,10110346,31/Jul/17 04:07,12/Dec/22 18:11,14/Jul/23 06:30,09/Aug/17 01:20,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"create temporary view data as select * from values
  (1, 1),
  (1, 2),
  (2, 1),
  (2, 2),
  (3, 1),
  (3, 2)
  as data(a, b);

*select 3, 4, sum(b) from data group by 1, 2;*
When running this case, the following exception occurred:
Error in query: GROUP BY position 4 is not in select list (valid range is [1, 3]); line 1 pos 10

But this case can run normally:*select 1, 2, sum(b) from data group by 1, 2;*
",,10110346,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 09 01:20:01 UTC 2017,,,,,,,,,,"0|i3i6pz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/17 04:57;smilegator;https://github.com/apache/spark/pull/18779;;;","02/Aug/17 19:20;gurwls223;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18779;;;","08/Aug/17 23:19;maropu;[~smilegator] [~hyukjin.kwon] Could we close this (the pr already merged)?;;;","09/Aug/17 01:19;gurwls223;Yes, it looks so. Please revert my action if I was mistaken.;;;","09/Aug/17 01:20;gurwls223;Fixed in https://github.com/apache/spark/pull/18779;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark history server leaves incomplete or unreadable history files around forever.,SPARK-21571,13090944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ericvandenbergfb,ericvandenbergfb,ericvandenbergfb,29/Jul/17 01:58,12/Dec/22 18:10,14/Jul/23 06:30,23/Jan/18 17:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"We have noticed that history server logs are sometimes never cleaned up.  The current history server logic *ONLY* cleans up history files if they are completed since in general it doesn't make sense to clean up inprogress history files (after all, the job is presumably still running?)  Note that inprogress history files would generally not be targeted for clean up any way assuming they regularly flush logs and the file system accurately updates the history log last modified time/size, while this is likely it is not guaranteed behavior.

As a consequence of the current clean up logic and a combination of unclean shutdowns, various file system bugs, earlier spark bugs, etc. we have accumulated thousands of these dead history files associated with long since gone jobs.

For example (with spark.history.fs.cleaner.maxAge=14d):

-rw-rw----   3 xxxxxx                                           ooooooo      14382 2016-09-13 15:40 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/qqqqqq1974_ppppppppppp-8812_110586000000195_dev4384_jjjjjjjjjjjj-53982.zstandard
-rw-rw----   3 xxxx                                             ooooooo       5933 2016-11-01 20:16 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/qqqqqq2016_ppppppppppp-8812_126507000000673_dev5365_jjjjjjjjjjjj-65313.lz4
-rw-rw----   3 yyy                                              ooooooo          0 2017-01-19 11:59 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy0057_zzzz326_mmmmmmmmm-57863.lz4.inprogress
-rw-rw----   3 xxxxxxxxx                                        ooooooo          0 2017-01-19 14:17 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy0063_zzzz688_mmmmmmmmm-33246.lz4.inprogress
-rw-rw----   3 yyy                                              ooooooo          0 2017-01-20 10:56 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy1030_zzzz326_mmmmmmmmm-45195.lz4.inprogress
-rw-rw----   3 xxxxxxxxxxxx                                     ooooooo      11955 2017-01-20 17:55 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy1314_wwww54_kkkkkkkkkkkkkk-64671.lz4.inprogress
-rw-rw----   3 xxxxxxxxxxxx                                     ooooooo      11958 2017-01-20 17:55 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy1315_wwww1667_kkkkkkkkkkkkkk-58968.lz4.inprogress
-rw-rw----   3 xxxxxxxxxxxx                                     ooooooo      11960 2017-01-20 17:55 /user/hadoop/xxxxxxxxxxxxxx/spark/logs/yyyyyyyyyyyyyyyy1316_wwww54_kkkkkkkkkkkkkk-48058.lz4.inprogress

Based on the current logic, clean up candidates are skipped in several cases:
1. if a file has 0 bytes, it is completely ignored
2. if a file is in progress and not paresable/can't extract appID, is it completely ignored
3. if a file is complete and but not parseable/can't extract appID, it is completely ignored.

To address this edge case and provide a way to clean out orphaned history files I propose a new configuration option:

spark.history.fs.cleaner.aggressive={true, false}, default is false.

If true, the history server will more aggressively garbage collect history files in cases (1), (2) and (3).  Since the default is false, existing customers won't be affected unless they explicitly opt-in.  If customers have similar leaking garbage over time they have the option of aggressively cleaning up in such cases.  Also note that aggressive clean up may not be appropriate for some customers if they have long running jobs that exceed the cleaner.maxAge time frame and/or have buggy file systems.

Would like to get feedback on if this seems like a reasonable solution.
",,apachespark,blrunner,dongjoon,ericvandenbergfb,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 23 17:54:51 UTC 2018,,,,,,,,,,"0|i3i5un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 16:54;ericvandenbergfb;Link to pull request https://github.com/apache/spark/pull/18791;;;","02/Aug/17 19:21;gurwls223;User 'ericvandenbergfb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18791;;;","16/Nov/17 21:30;apachespark;User 'ericvandenbergfb' has created a pull request for this issue:
https://github.com/apache/spark/pull/19770;;;","23/Jan/18 17:54;vanzin;Fixed with the patch for SPARK-20664 (which includes the code in the PR sent for this bug).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleProgressBar should only be enabled in shells,SPARK-21568,13090908,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,vanzin,vanzin,28/Jul/17 21:54,09/Mar/18 01:01,14/Jul/23 06:30,09/Oct/17 19:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,1,,,,,,,,,"This is the current logic that enables the progress bar:

{code}
    _progressBar =
      if (_conf.getBoolean(""spark.ui.showConsoleProgress"", true) && !log.isInfoEnabled) {
        Some(new ConsoleProgressBar(this))
      } else {
        None
      }
{code}

That is based on the logging level; it just happens to align with the default configuration for shells (WARN) and normal apps (INFO).

But if someone changes the default logging config for their app, this may break; they may silence logs by setting the default level to WARN or ERROR, and a normal application will see a lot of log spam from the progress bar (which is especially bad when output is redirected to a file, as is usually done when running in cluster mode).

While it's possible to disable the progress bar separately, this behavior is not really expected.",,dongjoon,maropu,mboehm7,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 09 01:01:13 UTC 2018,,,,,,,,,,"0|i3i5mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/17 20:03;dongjoon;Hi, [~vanzin].
I'll try this.;;;","26/Aug/17 21:25;dongjoon;PR is ready.;;;","09/Mar/18 01:01;mboehm7;I just upgraded to Spark 2.3 and was about to file a bug for the missing progress indicators when running through spark-submit in yarn client mode. It's not a major issue but from my perspective, the new behavior is inconsistent with the documentation which says that {{--conf spark.ui.showConsoleProgress=true}} is the default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset with Tuple of type alias throws error,SPARK-21567,13090904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,kretes,kretes,28/Jul/17 21:25,12/Dec/22 18:10,14/Jul/23 06:30,08/Aug/17 08:33,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"returning from a map a thing that is a tuple containg another tuple - defined as a type alias - we receive an error.

minimal reproducible case:

having a structure like this:
{code}
object C {
  type TwoInt = (Int,Int)
  def tupleTypeAlias: TwoInt = (1,1)
}
{code}

when I do:
{code}
    Seq(1).toDS().map(_ => ("""",C.tupleTypeAlias))
{code}


I get exception:
{code}
type T1 is not a class
scala.ScalaReflectionException: type T1 is not a class
	at scala.reflect.api.Symbols$SymbolApi$class.asClass(Symbols.scala:275)
	at scala.reflect.internal.Symbols$SymbolContextApiImpl.asClass(Symbols.scala:84)
	at org.apache.spark.sql.catalyst.ScalaReflection$.getClassFromType(ScalaReflection.scala:682)
	at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$dataTypeFor(ScalaReflection.scala:84)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$10.apply(ScalaReflection.scala:614)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$10.apply(ScalaReflection.scala:607)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:607)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$10.apply(ScalaReflection.scala:619)
	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$10.apply(ScalaReflection.scala:607)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:607)
	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:438)
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:71)
	at org.apache.spark.sql.Encoders$.product(Encoders.scala:275)
	at org.apache.spark.sql.LowPrioritySQLImplicits$class.newProductEncoder(SQLImplicits.scala:233)
	at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:33)
{code}

in spark 2.1.1 the last exception was 'head of an empty list'",verified for spark 2.1.1 and 2.2.0 in sbt build,kiszk,kretes,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:22:44 UTC 2017,,,,,,,,,,"0|i3i5lr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/17 04:37;viirya;I tried it. Seems Scala fails to find implicit encoder for type alias. You can manually set the encoder:

{code}
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.catalyst.encoders
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

val tupleTypeAliasEncoder: Encoder[TwoInt] = ExpressionEncoder[(Int, Int)]()
val encoder: Encoder[(String, TwoInt)] = ExpressionEncoder.tuple(
    encoders.encoderFor[String],
    encoders.encoderFor(tupleTypeAliasEncoder))

Seq(1).toDS().map(_ => ("""",C.tupleTypeAlias))(encoder).show

+-----+-----+
|value|   _2|
+-----+-----+
|     |[1,1]|
+-----+-----+

{code};;;","31/Jul/17 03:56;viirya;If there is no more questions about this, we can close this.;;;","31/Jul/17 07:43;kretes;That's good there is a workaround. 

However as I think about that - spark implicits should get encoders right for basic data types and case classes and other primitives like tuples?

I am not sure if it is a Spark bug or a Scala bug but from a user perspective - it is some kind of an unexpected behaviour.;;;","02/Aug/17 07:06;viirya;[~kretes] I've made a mistake when trying to solve this previously, so I was thinking it can't be fixed in Spark side. I think we can have a better solution now.

A PR is submitted to fix this at: https://github.com/apache/spark/pull/18813
;;;","02/Aug/17 19:22;gurwls223;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18813;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
aggregate query fails with watermark on eventTime but works with watermark on timestamp column generated by current_timestamp,SPARK-21565,13090886,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,amit.assudani@gmail.com,amit.assudani@gmail.com,28/Jul/17 20:26,07/Aug/17 20:06,14/Jul/23 06:30,07/Aug/17 20:02,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,1,,,,,,,,,"*Short Description: *

Aggregation query fails with eventTime as watermark column while works with newTimeStamp column generated by running SQL with current_timestamp,

*Exception:*

{code}
Caused by: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(statefulOperators.scala:204)
	at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(statefulOperators.scala:172)
	at org.apache.spark.sql.execution.streaming.state.package$StateStoreOps$$anonfun$1.apply(package.scala:70)
	at org.apache.spark.sql.execution.streaming.state.package$StateStoreOps$$anonfun$1.apply(package.scala:65)
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:64)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
{code}

*Code to replicate:*

{code}
package test

import java.nio.file.{Files, Path, Paths}
import java.text.SimpleDateFormat

import org.apache.spark.sql.types._
import org.apache.spark.sql.{SparkSession}

import scala.collection.JavaConverters._

object Test1 {

  def main(args: Array[String]) {

    val sparkSession = SparkSession
      .builder()
      .master(""local[*]"")
      .appName(""Spark SQL basic example"")
      .config(""spark.some.config.option"", ""some-value"")
      .getOrCreate()

    val sdf = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
    val checkpointPath = ""target/cp1""
    val newEventsPath = Paths.get(""target/newEvents/"").toAbsolutePath
    delete(newEventsPath)
    delete(Paths.get(checkpointPath).toAbsolutePath)
    Files.createDirectories(newEventsPath)


    val dfNewEvents= newEvents(sparkSession)
    dfNewEvents.createOrReplaceTempView(""dfNewEvents"")

    //The below works - Start
//    val dfNewEvents2 = sparkSession.sql(""select *,current_timestamp as newTimeStamp from dfNewEvents "").withWatermark(""newTimeStamp"",""2 seconds"")
//    dfNewEvents2.createOrReplaceTempView(""dfNewEvents2"")
//    val groupEvents = sparkSession.sql(""select symbol,newTimeStamp, count(price) as count1 from dfNewEvents2 group by symbol,newTimeStamp"")
    // End
    
    
    //The below doesn't work - Start
    val dfNewEvents2 = sparkSession.sql(""select * from dfNewEvents "").withWatermark(""eventTime"",""2 seconds"")
     dfNewEvents2.createOrReplaceTempView(""dfNewEvents2"")
      val groupEvents = sparkSession.sql(""select symbol,eventTime, count(price) as count1 from dfNewEvents2 group by symbol,eventTime"")
    // - End
    
    
    val query1 = groupEvents.writeStream
      .outputMode(""append"")
        .format(""console"")
      .option(""checkpointLocation"", checkpointPath)
      .start(""./myop"")

    val newEventFile1=newEventsPath.resolve(""eventNew1.json"")
    Files.write(newEventFile1, List(
      """"""{""symbol"": ""GOOG"",""price"":100,""eventTime"":""2017-07-25T16:00:00.000-04:00""}"""""",
      """"""{""symbol"": ""GOOG"",""price"":200,""eventTime"":""2017-07-25T16:00:00.000-04:00""}""""""
    ).toIterable.asJava)
    query1.processAllAvailable()

    sparkSession.streams.awaitAnyTermination(10000)

  }

  private def newEvents(sparkSession: SparkSession) = {
    val newEvents = Paths.get(""target/newEvents/"").toAbsolutePath
    delete(newEvents)
    Files.createDirectories(newEvents)

    val dfNewEvents = sparkSession.readStream.schema(eventsSchema).json(newEvents.toString)//.withWatermark(""eventTime"",""2 seconds"")
    dfNewEvents
  }

  private val eventsSchema = StructType(List(
    StructField(""symbol"", StringType, true),
    StructField(""price"", DoubleType, true),
    StructField(""eventTime"", TimestampType, false)
  ))

  private def delete(dir: Path) = {
    if(Files.exists(dir)) {
      Files.walk(dir).iterator().asScala.toList
        .map(p => p.toFile)
        .sortWith((o1, o2) => o1.compareTo(o2) > 0)
        .foreach(_.delete)
    }
  }

}


{code}",,a1ray,amit.assudani@gmail.com,gaaldornick,marmbrus,Robin Shao,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 07 20:06:05 UTC 2017,,,,,,,,,,"0|i3i5hr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/17 20:47;a1ray;I believe you need to use a window to group by your event time.;;;","31/Jul/17 21:14;amit.assudani@gmail.com;I am not sure, does it mean we can not use watermark without windows - kind of on a microbatch level ? 

If that is the case, it should not work with current_timestamp and should have validation not to allow without windows.;;;","31/Jul/17 21:36;a1ray;No nothing like the limitations of microbatches. The window can be made trivially small if you want only one timestamp per group, for example {{window(eventTime, ""1 microsecond"")}}

And yes this should probably be checked in analysis if this is the intended limitation.;;;","02/Aug/17 18:12;zsxwing;Thanks for reporting it. I can reproduce the error in a unit test. Still investigating it.;;;","07/Aug/17 20:06;zsxwing;Resolved by https://github.com/apache/spark/pull/18840;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when serializing TaskDescriptions and adding jars,SPARK-21563,13090879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aash,aash,aash,28/Jul/17 20:08,14/Aug/17 14:58,14/Jul/23 06:30,14/Aug/17 14:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"cc [~robert3005]

I was seeing this exception during some running Spark jobs:

{noformat}
16:16:28.294 [dispatcher-event-loop-14] ERROR org.apache.spark.rpc.netty.Inbox - Ignoring error
java.io.EOFException: null
    at java.io.DataInputStream.readFully(DataInputStream.java:197)
    at java.io.DataInputStream.readUTF(DataInputStream.java:609)
    at java.io.DataInputStream.readUTF(DataInputStream.java:564)
    at org.apache.spark.scheduler.TaskDescription$$anonfun$decode$1.apply(TaskDescription.scala:127)
    at org.apache.spark.scheduler.TaskDescription$$anonfun$decode$1.apply(TaskDescription.scala:126)
    at scala.collection.immutable.Range.foreach(Range.scala:160)
    at org.apache.spark.scheduler.TaskDescription$.decode(TaskDescription.scala:126)
    at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(CoarseGrainedExecutorBackend.scala:95)
    at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
    at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:748)
{noformat}

After some debugging, we determined that this is due to a race condition in task serde.  cc [~irashid] [~kayousterhout] who last touched that code in SPARK-19796

The race is between adding additional jars to the SparkContext and serializing the TaskDescription.

Consider this sequence of events:

- TaskSetManager creates a TaskDescription using a reference to the SparkContext's jars: https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L506
- TaskDescription starts serializing, and begins writing jars: https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala#L84
- the size of the jar map is written out: https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala#L63
- _on another thread_: the application adds a jar to the SparkContext's jars list
- then the entries in the jars list are serialized out: https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/scheduler/TaskDescription.scala#L64

The problem now is that the jars list is serialized as having N entries, but actually N+1 entries follow that count!

This causes task deserialization to fail in the executor, with the stacktrace above.

The same issue also likely exists for files, though I haven't observed that and our application does not stress that codepath the same way it did for jar additions.

One fix here is that TaskSetManager could make an immutable copy of the jars list that it passes into the TaskDescription constructor, so that list doesn't change mid-serialization.",,aash,irashid,kiszk,robert3005,Steven Rand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 10 23:45:18 UTC 2017,,,,,,,,,,"0|i3i5g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/17 22:08;aash;And for reference, I added this additional logging to assist in debugging: https://github.com/palantir/spark/pull/238;;;","01/Aug/17 15:12;irashid;Thanks for the detailed report [~aash], makes perfect sense.  I think your proposed solution,  making an immutable copy of the files and jars, is the right thing to do.  I'd just add that you should do it once per TaskSetManager, not once per TaskDescription.

Do you want to take a stab at submitting the PR for this?;;;","10/Aug/17 23:45;aash;Thanks for the thoughts [~irashid] -- I submitted a PR implementing this approach at https://github.com/apache/spark/pull/18913;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GROUP BY don't work with expressions with NVL and nested objects,SPARK-21555,13090698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,v-gerasimov,v-gerasimov,28/Jul/17 06:59,29/Jul/17 17:05,14/Jul/23 06:30,29/Jul/17 17:05,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"{code}
spark.read.json(spark.createDataset(""""""{""foo"":{""foo1"":""value""}}"""""" :: Nil)).createOrReplaceTempView(""test"")
spark.sql(""select nvl(foo.foo1, \""value\""), count(*) from test group by nvl(foo.foo1, \""value\"")"")
{code}

returns exception:
{code}
org.apache.spark.sql.AnalysisException: expression 'test.`foo`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
Aggregate [nvl(foo#4.foo1 AS foo1#8, value)], [nvl(foo#4.foo1 AS foo1#9, value) AS nvl(test.`foo`.`foo1` AS `foo1`, 'value')#11, count(1) AS count(1)#12L]
+- SubqueryAlias test
   +- LogicalRDD [foo#4]

  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:39)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:247)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1$5.apply(CheckAnalysis.scala:253)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:253)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$9.apply(CheckAnalysis.scala:280)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:280)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)
  ... 48 elided
{code}

In Spark 1.6.2 this code works fine. Also it works fine if rewrite it with `coalesce`:
{code}spark.sql(""select coalesce(foo.foo1, \""value\""), count(*) from test group by coalesce(foo.foo1, \""value\"")""){code}",,v-gerasimov,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 15:18:07 UTC 2017,,,,,,,,,,"0|i3i4cv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/17 15:18;viirya;The sync between PR and JIRA seems broken still. I already submitted a PR for this issue at https://github.com/apache/spark/pull/18761.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark's collect fails when getaddrinfo is too slow,SPARK-21551,13090573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,peay,peay,peay,27/Jul/17 17:16,12/Dec/22 18:10,14/Jul/23 06:30,09/Aug/17 21:03,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.3,2.2.1,2.3.0,,PySpark,,,,,0,,,,,,,,,"Pyspark's {{RDD.collect}}, as well as {{DataFrame.toLocalIterator}} and {{DataFrame.collect}} all work by starting an ephemeral server in the driver, and having Python connect to it to download the data.

All three are implemented along the lines of:

{code}
port = self._jdf.collectToPython()
return list(_load_from_socket(port, BatchedSerializer(PickleSerializer())))
{code}

The server has **a hardcoded timeout of 3 seconds** (https://github.com/apache/spark/blob/e26dac5feb02033f980b1e69c9b0ff50869b6f9e/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L695) -- i.e., the Python process has 3 seconds to connect to it from the very moment the driver server starts.

In general, that seems fine, but I have been encountering frequent timeouts leading to `Exception: could not open socket`.

After investigating a bit, it turns out that {{_load_from_socket}} makes a call to {{getaddrinfo}}:

{code}
def _load_from_socket(port, serializer):
    sock = None
    # Support for both IPv4 and IPv6.
    # On most of IPv6-ready systems, IPv6 will take precedence.
    for res in socket.getaddrinfo(""localhost"", port, socket.AF_UNSPEC, socket.SOCK_STREAM):
       .. connect ..
{code}

I am not sure why, but while most such calls to {{getaddrinfo}} on my machine only take a couple milliseconds, about 10% of them take between 2 and 10 seconds, leading to about 10% of jobs failing. I don't think we can always expect {{getaddrinfo}} to return instantly. More generally, Python may sometimes pause for a couple seconds, which may not leave enough time for the process to connect to the server.

Especially since the server timeout is hardcoded, I think it would be best to set a rather generous value (15 seconds?) to avoid such situations.

A {{getaddrinfo}}  specific fix could avoid doing it every single time, or do it before starting up the driver server.
 
cc SPARK-677 [~davies]",,apachespark,frosner,peay,pmarini,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18649,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 17 12:02:04 UTC 2017,,,,,,,,,,"0|i3i3lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/17 17:23;rxin;Do you want to submit a pull request?
;;;","27/Jul/17 17:25;peay;Sure, does 15 seconds sound good?;;;","27/Jul/17 17:26;rxin;Sure.;;;","02/Aug/17 19:19;gurwls223;User 'peay' has created a pull request for this issue:
https://github.com/apache/spark/pull/18752;;;","17/Oct/17 09:26;frosner;Do you guys mind if I backport this also to 2.0.x, 2.1.x, and 2.2.x? We are having some jobs that we don't want to upgrade to 2.3.0 but that are failing regularly because of this problem.

Which branches would that have to go to? branch-2.0, branch-2.1, and branch-2.2?;;;","17/Oct/17 12:02;apachespark;User 'FRosner' has created a pull request for this issue:
https://github.com/apache/spark/pull/19512;;;","17/Oct/17 12:02;apachespark;User 'FRosner' has created a pull request for this issue:
https://github.com/apache/spark/pull/19513;;;","17/Oct/17 12:02;apachespark;User 'FRosner' has created a pull request for this issue:
https://github.com/apache/spark/pull/19514;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark fails to complete job correctly in case of OutputFormat which do not write into hdfs,SPARK-21549,13090560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,szhemzhitsky,szhemzhitsky,szhemzhitsky,27/Jul/17 16:45,16/Oct/17 01:43,14/Jul/23 06:30,07/Oct/17 03:46,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,2,,,,,,,,,"Spark fails to complete job correctly in case of custom OutputFormat implementations.

There are OutputFormat implementations which do not need to use *mapreduce.output.fileoutputformat.outputdir* standard hadoop property.

[But spark reads this property from the configuration|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopMapReduceWriter.scala#L79] while setting up an OutputCommitter
{code:javascript}
val committer = FileCommitProtocol.instantiate(
  className = classOf[HadoopMapReduceCommitProtocol].getName,
  jobId = stageId.toString,
  outputPath = conf.value.get(""mapreduce.output.fileoutputformat.outputdir""),
  isAppend = false).asInstanceOf[HadoopMapReduceCommitProtocol]
committer.setupJob(jobContext)
{code}
... and then uses this property later on while [commiting the job|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L132], [aborting the job|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L141], [creating task's temp path|https://github.com/apache/spark/blob/v2.2.0/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L95]

In that cases when the job completes then following exception is thrown
{code}
Can not create a Path from a null string
java.lang.IllegalArgumentException: Can not create a Path from a null string
  at org.apache.hadoop.fs.Path.checkPathArg(Path.java:123)
  at org.apache.hadoop.fs.Path.<init>(Path.java:135)
  at org.apache.hadoop.fs.Path.<init>(Path.java:89)
  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.absPathStagingDir(HadoopMapReduceCommitProtocol.scala:58)
  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.abortJob(HadoopMapReduceCommitProtocol.scala:141)
  at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:106)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
  at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084)
  ...
{code}

So it seems that all the jobs which use OutputFormats which don't write data into HDFS-compatible file systems are broken.","spark 2.2.0
scala 2.11",apachespark,bikassaha,Deng FEI,elserj,kent2171,mridulm80,qiuzhuang.lian,stevel@apache.org,szhemzhitsky,WeiqingYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MAPREDUCE-6961,,,SPARK-20045,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 14 06:24:04 UTC 2017,,,,,,,,,,"0|i3i3if:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/17 20:38;mridulm80;This affects both mapred (""mapred.output.dir"") and mapreduce (""mapreduce.output.fileoutputformat.outputdir"") based OutputFormat's which do not set the properties referenced and is an incompatibility introduced in spark 2.2

Workaround is to explicitly set the property to a dummy value (which is valid and writable by user - say /tmp).

+CC [~WeiqingYang] 

;;;","21/Aug/17 18:23;szhemzhitsky;[~mridulm80], [~WeiqingYang] does it make sense to implement the provided workaround with valid and writable directory just within SparkHadoopMapRedceWriter if the needed property is not set, to prevent affecting all the jobs which don't write to hdfs, at least until there is a better solution?;;;","19/Sep/17 09:24;stevel@apache.org;Linking to SPARK-20045, which highlights the commit logic, especially the abort code, needs to be resilient to failures, including that of the invoked {{OutputCommitter.abort()}} calls from raising exceptions. While people implementing committers should be expected to write resilient abort routines, you can't rely on it. Same for calling fs.delete()...it could also fall, so wrapping everything in an exception handler would at least make abort resilient.;;;","19/Sep/17 09:43;stevel@apache.org;# you can't rely on the committers having output and temp dirs. Subclasses of {{FileOutputCommitter}} *must*, though there's no official mechanism for querying that because {{getOutputPath()}} is private.
# Hadoop 3.0 has added (MAPREDUCE-6956) a new superclass of {{FileOutputCommitter}}, [PathOutputCommitter|[https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/PathOutputCommitter.java] which pulls up getWorkingDir method to be more general (so that you can have output committers which tell spark and hive where their intermediate data should go, without them being subclasses of FileOutputCommitter.
# I'm happy to pull up {{getOutputPath}} to that class, and if people can give me a patch for it *this week* I'll add it for 3.0 beta 1. 

Regarding the committer here, you might want to think of moving off/subclassing {{HadoopMapReduceCommitProtocol}}. This is what I've done in [PathOutputCommitProtoco|https://github.com/hortonworks-spark/cloud-integration/blob/master/spark-cloud-integration/src/main/scala/com/hortonworks/spark/cloud/PathOutputCommitProtocol.scala], though I can see it's still relying on the superclass to get that properly. Again, if we can patch the new PathOutputCommitter class for a getOutputPath I use that here. And yes, while that new mapreduce will take a long time to surface in spark core, you can use it independently, from later this year..

If you are playing with different committers out of spark's own codebase, pick up the the ORC hive tests from [https://github.com/hortonworks-spark/cloud-integration/tree/master/cloud-examples/src/test/scala/org/apache/spark/sql/sources]. These are just some of the spark sql tests reworked slightly so that they'll work with any FileSystem impl. rather than just local file:// paths

Ping me direct if you are playing with new committers, & look at MAPREDUCE-6823 to see if that'd be useful to you (& how it could be improved, given its still not in the codebase)

;;;","20/Sep/17 13:29;apachespark;User 'szhem' has created a pull request for this issue:
https://github.com/apache/spark/pull/19294;;;","20/Sep/17 13:38;szhemzhitsky;[~mridulm80], [~WeiqingYang], [~stevel@apache.org], 

I've implemented the fix in this PR (https://github.com/apache/spark/pull/19294), which sets user's current working directory (which is typically her home directory in case of distributed filesystems) as output directory.

The patch allows using OutputFormats which write to external systems, databases, etc. by means of RDD API.
I far as I understand the requirement for output paths to be specified is only necessary to allow files to be committed to an absolute output location, that is not the case for output formats which write data to external systems. 
So using user's working directory for such situations seems to be ok.
 
;;;","20/Sep/17 16:29;stevel@apache.org;The {{newTaskTempFileAbsPath()}} method is an interesting spot of code...I'm still trying to work out when it is actually used. Some committers like {{ManifestFileCommitProtocol}} don't support it all. 

However, if it is used, then your patch is going to cause problems if the dest FS != the default FS, because then the bit of the protocol which takes that list of temp files and renames() them into their destination is going to fail. I think you'd be better off having the committer fail fast when an absolute path is asked for;;;","20/Sep/17 20:05;szhemzhitsky;[~stevel@apache.org] I've updated PR to prevent using FileSystems at all. 
Instead, there is just an additional check whether there are absolute files to rename during commit.;;;","07/Oct/17 03:46;mridulm80;Issue resolved by pull request 19294
[https://github.com/apache/spark/pull/19294];;;","13/Oct/17 00:29;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/19487;;;","14/Oct/17 06:24;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/19497;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropDuplicates with watermark yields RuntimeException due to binding failure,SPARK-21546,13090453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,jlaskowski,jlaskowski,27/Jul/17 08:45,12/Dec/22 18:10,14/Jul/23 06:30,02/Aug/17 21:02,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Structured Streaming,,,,,0,,,,,,,,,"With today's master...

The following streaming query with watermark and {{dropDuplicates}} yields {{RuntimeException}} due to failure in binding.

{code}
val topic1 = spark.
  readStream.
  format(""kafka"").
  option(""subscribe"", ""topic1"").
  option(""kafka.bootstrap.servers"", ""localhost:9092"").
  option(""startingoffsets"", ""earliest"").
  load

val records = topic1.
  withColumn(""eventtime"", 'timestamp).  // <-- just to put the right name given the purpose
  withWatermark(eventTime = ""eventtime"", delayThreshold = ""30 seconds""). // <-- use the renamed eventtime column
  dropDuplicates(""value"").  // dropDuplicates will use watermark
                            // only when eventTime column exists
  // include the watermark column => internal design leak?
  select('key cast ""string"", 'value cast ""string"", 'eventtime).
  as[(String, String, java.sql.Timestamp)]

scala> records.explain
== Physical Plan ==
*Project [cast(key#0 as string) AS key#169, cast(value#1 as string) AS value#170, eventtime#157-T30000ms]
+- StreamingDeduplicate [value#1], StatefulOperatorStateInfo(<unknown>,93c3de98-3f85-41a4-8aef-d09caf8ea693,0,0), 0
   +- Exchange hashpartitioning(value#1, 200)
      +- EventTimeWatermark eventtime#157: timestamp, interval 30 seconds
         +- *Project [key#0, value#1, timestamp#5 AS eventtime#157]
            +- StreamingRelation kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]

import org.apache.spark.sql.streaming.{OutputMode, Trigger}
val sq = records.
  writeStream.
  format(""console"").
  option(""truncate"", false).
  trigger(Trigger.ProcessingTime(""10 seconds"")).
  queryName(""from-kafka-topic1-to-console"").
  outputMode(OutputMode.Update).
  start
{code}

{code}
-------------------------------------------
Batch: 0
-------------------------------------------
17/07/27 10:28:58 ERROR Executor: Exception in task 3.0 in stage 13.0 (TID 438)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: eventtime#157-T30000ms
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.bind(GeneratePredicate.scala:45)
	at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.bind(GeneratePredicate.scala:40)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:977)
	at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:370)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.org$apache$spark$sql$execution$streaming$WatermarkSupport$$super$newPredicate(statefulOperators.scala:350)
	at org.apache.spark.sql.execution.streaming.WatermarkSupport$$anonfun$watermarkPredicateForKeys$1.apply(statefulOperators.scala:160)
	at org.apache.spark.sql.execution.streaming.WatermarkSupport$$anonfun$watermarkPredicateForKeys$1.apply(statefulOperators.scala:160)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.sql.execution.streaming.WatermarkSupport$class.watermarkPredicateForKeys(statefulOperators.scala:160)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.watermarkPredicateForKeys$lzycompute(statefulOperators.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.watermarkPredicateForKeys(statefulOperators.scala:350)
	at org.apache.spark.sql.execution.streaming.WatermarkSupport$class.removeKeysOlderThanWatermark(statefulOperators.scala:167)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.removeKeysOlderThanWatermark(statefulOperators.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec$$anonfun$doExecute$4$$anonfun$apply$4$$anonfun$apply$mcV$sp$1.apply$mcV$sp(statefulOperators.scala:403)
	at org.apache.spark.sql.execution.streaming.StateStoreWriter$class.timeTakenMs(statefulOperators.scala:96)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec.timeTakenMs(statefulOperators.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec$$anonfun$doExecute$4$$anonfun$apply$4.apply$mcV$sp(statefulOperators.scala:403)
	at org.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:46)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:35)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Couldn't find eventtime#157-T30000ms in [value#185]
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 49 more
{code}

I'm somehow convinced that watermark support leaks from {{StreamingDeduplicate}} and forces a Spark developer to include extra fields for watermark. I think filter pushdown (for the select) should not be executed for this case or should include the extra {{eventTime}} column (regardless of whether a developer uses it or not).
",,jlaskowski,KevinZwx,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 03 04:08:53 UTC 2017,,,,,,,,,,"0|i3i2un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/17 18:16;zsxwing;Yeah, good catch. The watermark column should be one of the dropDuplicates columns. Otherwise, it never evicts states.;;;","02/Aug/17 19:23;gurwls223;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18822;;;","03/Aug/17 04:08;KevinZwx;[~zsxwing] in my case I hope to use a watermark to expire the state but not use the watermark column to filter duplicate elements. i.e. I want to count the unique access of my website for one day, so I should just store the state of dropDuplicates for one day and drop the state the next day, meanwhile I want to use uuid as the key to drop duplicate elements rather than using (uuid, eventTime) together, but dropDuplicates behaves like the latter right? If so how can I get the right results as I expected?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Logs show incorrect job status for a job that does not create SparkContext,SPARK-21541,13090336,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pgandhi,pgandhi,pgandhi,26/Jul/17 21:22,17/May/20 18:14,14/Jul/23 06:30,28/Jul/17 14:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"If you run a spark job without creating the SparkSession or SparkContext, the spark job logs says it succeeded but yarn says it fails and retries 3 times. Also, since, Application Master unregisters with Resource Manager and exits successfully, it deletes the spark staging directory, so when yarn makes subsequent retries, it fails to find the staging directory and thus, the retries fail.

*Steps:*
1. For example, run a pyspark job without creating SparkSession or SparkContext. 
*Example:*
import sys
from random import random
from operator import add
from pyspark import SparkContext

if __name__ == ""__main__"":
  print(""hello world"")

2. Spark will mark it as FAILED. Got to the UI and check the container logs.

3. You will see the following information in the logs:
spark:
7/07/14 13:22:10 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/07/14 13:22:10 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED

But yarn logs will show:
2017-07-14 01:14:33,203 [AsyncDispatcher event handler] INFO attempt.RMAppAttemptImpl: appattempt_1493735952617_12443844_000001 State change from FINAL_SAVING to FAILED",,pgandhi,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21585,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 15:46:21 UTC 2017,,,,,,,,,,"0|i3i24v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/17 21:23;pgandhi;Currently working on the fix, will file a pull request as soon as it is done.;;;","28/Jul/17 14:42;srowen;Was this change merged? I don't think it was https://github.com/apache/spark/pull/18741;;;","28/Jul/17 15:35;pgandhi;The change has been merged. Thank you.;;;","28/Jul/17 15:46;tgraves;it was merged https://github.com/apache/spark/commit/69ab0e4bddccb461f960fcb48a390a1517e504dd  but I guess the pr link didn't pick it up.

I missed that the title wasn't quite right [Spark-21541] so perhaps jira didn't pick it up properly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PickleException when creating dataframe from python row with empty bytearray,SPARK-21534,13090052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,maver1ck,maver1ck,25/Jul/17 21:31,12/Dec/22 18:11,14/Jul/23 06:30,31/Aug/17 03:56,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"{code}
spark.createDataFrame(spark.sql(""select unhex('') as xx"").rdd.map(lambda x: {""abc"": x.xx})).show()
{code}
This code creates exception. It looks like corner-case.
{code}
net.razorvine.pickle.PickleException: invalid pickle data for bytearray; expected 1 or 2 args, got 0
	at net.razorvine.pickle.objects.ByteArrayConstructor.construct(ByteArrayConstructor.java:20)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:152)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:151)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: net.razorvine.pickle.PickleException: invalid pickle data for bytearray; expected 1 or 2 args, got 0
	at net.razorvine.pickle.objects.ByteArrayConstructor.construct(ByteArrayConstructor.java:20)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:152)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:151)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:234)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
{code}
",,apachespark,bryanc,kiszk,maver1ck,ueshin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 31 03:56:05 UTC 2017,,,,,,,,,,"0|i3i0dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/17 00:47;gurwls223;cc [~zasdfgbnm] and [~ueshin], this one looks related with few PRs we reviewed and worked before.

This was not reproduced in Python 2.7:

{code}
>>> spark.createDataFrame(spark.sql(""select unhex('') as xx"").rdd.map(lambda x: {""abc"": x.xx})).show()

+---+
|abc|
+---+
| []|
+---+
{code}

but Python 3.6:

{code}
>>> spark.createDataFrame(spark.sql(""select unhex('') as xx"").rdd.map(lambda x: {""abc"": x.xx})).show()
net.razorvine.pickle.PickleException: invalid pickle data for bytearray; expected 1 or 2 args, got 0
	at net.razorvine.pickle.objects.ByteArrayConstructor.construct(ByteArrayConstructor.java:20)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:174)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:173)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

;;;","30/Aug/17 09:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19085;;;","31/Aug/17 03:56;gurwls223;Issue resolved by pull request 19085
[https://github.com/apache/spark/pull/19085];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReceiverSupervisorImpl seems to ignore the error code when writing to the WAL,SPARK-21525,13089743,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,mgrover,mgrover,25/Jul/17 00:15,31/Jan/18 19:49,14/Jul/23 06:30,31/Jan/18 19:49,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,DStreams,,,,,0,,,,,,,,,"{{AddBlock}} returns an error code related to whether writing the block to the WAL was successful or not. In cases where a WAL may be unavailable temporarily, the write would fail but it seems like we are not using the return code (see [here|https://github.com/apache/spark/blob/ba8912e5f3d5c5a366cb3d1f6be91f2471d048d2/streaming/src/main/scala/org/apache/spark/streaming/receiver/ReceiverSupervisorImpl.scala#L162]).

For example, when using the Flume Receiver, we should be sending a n'ack back to Flume if the block wasn't written to the WAL. I haven't gone through the full code path yet but at least from looking at the ReceiverSupervisorImpl, it doesn't seem like that return code is being used.",,apachespark,cane,mgrover,panbingkun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 23:33:05 UTC 2018,,,,,,,,,,"0|i3hyhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/18 23:33;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20161;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug of strong wolfe linesearch `init` parameter lose effectiveness,SPARK-21523,13089729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,weichenxu123,weichenxu123,weichenxu123,24/Jul/17 22:56,14/Sep/17 22:10,14/Jul/23 06:30,09/Aug/17 06:49,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,MLlib,,,,,0,,,,,,,,,"We need merge this breeze bugfix into spark because it influence a series of algos in MLlib which use LBFGS.
https://github.com/scalanlp/breeze/pull/651",,ibobak,josephkb,mgrover,mlnick,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,SPARK-21614,SPARK-21919,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 14 22:10:27 UTC 2017,,,,,,,,,,"0|i3hye7:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"24/Jul/17 22:57;weichenxu123;I will work on this once the breeze cut a new version for this bugfix.;;;","24/Jul/17 23:00;josephkb;CC [~yanboliang] [~yuhaoyan] [~dbtsai] making a few people aware of this;;;","28/Jul/17 18:15;srowen;I think this is fairly critical actually -- would like to get this into a 2.2.1 release.;;;","14/Sep/17 22:10;ibobak;For us it is also critical issue:  we faced with the same problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: LauncherServerSuite.testStreamFiltering,SPARK-21522,13089679,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,24/Jul/17 20:12,01/Aug/17 17:07,14/Jul/23 06:30,01/Aug/17 17:07,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.1,2.3.0,,Tests,,,,,0,,,,,,,,,"We ran into this in our internal Jenkins servers. Partial stack trace:

{noformat}
java.net.SocketException: Broken pipe
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1876)
	at java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)
	at java.io.ObjectOutputStream.writeNonProxyDesc(ObjectOutputStream.java:1285)
	at java.io.ObjectOutputStream.writeClassDesc(ObjectOutputStream.java:1230)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1426)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)
	at org.apache.spark.launcher.LauncherConnection.send(LauncherConnection.java:82)
	at org.apache.spark.launcher.LauncherServerSuite.testStreamFiltering(LauncherServerSuite.java:174)
{noformat}
",,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 17:07:36 UTC 2017,,,,,,,,,,"0|i3hy33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 17:07;vanzin;Since the bot didn't add the link: https://github.com/apache/spark/pull/18727;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
overriding afterEach() in DatasetCacheSuite must call super.afterEach(),SPARK-21516,13089396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kiszk,kiszk,kiszk,24/Jul/17 04:57,25/Jul/17 07:23,14/Jul/23 06:30,25/Jul/17 07:23,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When we override {{afterEach()}} method in Testsuite, we have to call {{super.afterEach()}}. This is follow-up of SPARK-21512.",,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 25 07:23:06 UTC 2017,,,,,,,,,,"0|i3hwq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/17 05:49;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18721;;;","25/Jul/17 07:23;srowen;Resolved by https://github.com/apache/spark/pull/18721;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatasetCacheSuite needs to execute unpersistent after executing peristent,SPARK-21512,13089290,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,23/Jul/17 06:43,24/Jul/17 04:53,14/Jul/23 06:30,23/Jul/17 18:32,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 23 09:38:03 UTC 2017,,,,,,,,,,"0|i3hw2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/17 09:07;kiszk;When {{DatasetCacheSuite}} is executed, the following warning messages appear. Unpersistent dataset is made persistent in the second test case {{""persist and then rebind right encoder when join 2 datasets""}} after the first test case {{""get storage level""}} made it persistent.
Thus, we run these test cases, the second case does not perform to make dataset persistent. This is because in 
 When we run only the second case, it performs to make dataset persistent. It is not good to change behavior of the second test suite. The first test case should correctly make dataset unpersistent.

{code}
01:52:48.595 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.
01:52:48.692 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.
{code};;;","23/Jul/17 09:38;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18719;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation on 'Spark Streaming Custom Receivers' has error in example code,SPARK-21508,13089244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,remisharoon,remisharoon,remisharoon,22/Jul/17 13:15,29/Jul/17 12:27,14/Jul/23 06:30,29/Jul/17 12:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,,,,,0,,,,,,,,,"The example code provided in the documentation on 'Spark Streaming Custom Receivers' has an error.
https://spark.apache.org/docs/latest/streaming-custom-receivers.html

{code}
// Assuming ssc is the StreamingContext
val customReceiverStream = ssc.receiverStream(new CustomReceiver(host, port))
val words = lines.flatMap(_.split("" ""))
...
{code}

instead of {code}lines.flatMap(_.split("" "")){code} it should be {code}customReceiverStream.flatMap(_.split("" "")){code}",,remisharoon,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Sat Jul 29 12:26:36 UTC 2017,,,,,,,,,,"0|i3hvsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/17 17:29;srowen;Seems OK [~remisharoon], you can make a pull request;;;","24/Jul/17 04:45;remisharoon;Thankyou [~srowen] , I will raise the PR.
I am new here, how can I assign this ticket to my name? does is require any user privileges?;;;","24/Jul/17 08:08;srowen;You don't need it assigned, not until you resolve it.;;;","29/Jul/17 12:11;remisharoon;[~srowen] , I raised a pull request for this 
https://github.com/apache/spark/pull/18770;;;","29/Jul/17 12:26;srowen;Issue resolved by pull request 18770
[https://github.com/apache/spark/pull/18770];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI shows incorrect task status for a killed Executor Process,SPARK-21503,13089157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pgandhi,pgandhi,pgandhi,21/Jul/17 20:57,09/Aug/17 05:48,14/Jul/23 06:30,09/Aug/17 05:48,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"The executor tab on Spark UI page shows task as completed when an executor process that is running that task is killed using the kill command.

Steps:
1. Run a big Spark job. As an example, I ran a pyspark job with the following command:
$SPARK_HOME/bin/spark-submit --master yarn --deploy-mode cluster --queue default --num-executors 10 --driver-memory 2G --conf spark.pyspark.driver.python=./Python3/bin/python --conf spark.pyspark.python=./Python3/bin/python --archives hdfs:///user/USERNAME/Python3.zip#Python3 ~/pi.py

2. Go to the UI to see which executors are running.

3. Do an ssh to each of the executor hosts and kill the java process running on the respective port mentioned in the UI using the following command:

kill <pid> OR kill -9 <pid>",,apachespark,cloud_fan,pgandhi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 09 05:48:33 UTC 2017,,,,,,,,,,"0|i3hv93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 20:58;pgandhi;Currently working on the fix, will file a pull request as soon as it is done.;;;","21/Jul/17 21:23;apachespark;User 'pgandhi999' has created a pull request for this issue:
https://github.com/apache/spark/pull/18707;;;","09/Aug/17 05:48;cloud_fan;Issue resolved by pull request 18707
[https://github.com/apache/spark/pull/18707];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
--supervise causing frameworkId conflicts in mesos cluster mode,SPARK-21502,13089095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,21/Jul/17 16:32,24/Jul/17 18:12,14/Jul/23 06:30,24/Jul/17 18:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Mesos,,,,,0,,,,,,,,,"This issue is described here: https://dcosjira.atlassian.net/browse/SPARK-8
The --supervise option is actually broken in mesos cluster mode. 

The error message you get when you re-launch a driver is:
17/05/16 13:26:09 ERROR MesosCoarseGrainedSchedulerBackend: Mesos error: Framework has been removed
Exception in thread ""Thread-13"" org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Framework has been removed
  at org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:459)
  at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.error(MesosCoarseGrainedSchedulerBackend.scala:569)
I0516 13:26:09.003342    92 sched.cpp:2021] Asked to abort the driver
I0516 13:26:09.003362    92 sched.cpp:1217] Aborting framework '1b9cb705-2e80-4354-8f3b-a4cffac7dac3-0003-driver-20170516131736-0004'
17/05/16 13:26:09 INFO MesosCoarseGrainedSchedulerBackend: driver.run() returned with code DRIVER_ABORTED
17/05/16 13:26:09 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Error starting driver, DRIVER_ABORTED
  at org.apache.spark.scheduler.cluster.mesos.MesosSchedulerUtils$$anon$1.run(MesosSchedulerUtils.scala:125)

",,apachespark,skonto,susanxhuynh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 21 17:22:03 UTC 2017,,,,,,,,,,"0|i3huvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 16:33;skonto;I have a patch I will submit a PR shortly.;;;","21/Jul/17 17:22;apachespark;User 'skonto' has created a pull request for this issue:
https://github.com/apache/spark/pull/18705;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark shuffle index cache size should be memory based,SPARK-21501,13089089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sanket991,tgraves,tgraves,21/Jul/17 16:21,21/Oct/20 13:11,14/Jul/23 06:30,23/Aug/17 16:51,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Shuffle,Spark Core,,,,0,,,,,,,,,"Right now the spark shuffle service has a cache for index files. It is based on a # of files cached (spark.shuffle.service.index.cache.entries). This can cause issues if people have a lot of reducers because the size of each entry can fluctuate based on the # of reducers. 

We saw an issues with a job that had 170000 reducers and it caused NM with spark shuffle service to use 700-800MB or memory in NM by itself.

We should change this cache to be memory based and only allow a certain memory size used. When I say memory based I mean the cache should have a limit of say 100MB.",,avs75,daemon,devaraj,kiszk,larsfrancke,renxunsaky,sanket991,sitalkedia@gmail.com,Steven Rand,tgraves,vrushalic,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 13:11:42 UTC 2020,,,,,,,,,,"0|i3huu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/17 14:56;kiszk;I guess that to use Spark 2.1 or later version alleviates this issue by SPARK-15074;;;","24/Jul/17 18:01;tgraves;The issue was actually introduced with SPARK-15074.  Updated the affects version. I was thinking that was added earlier so thanks for pointing out.

That cache should be memory based not # of entries based.;;;","25/Jul/17 01:08;kiszk;I see. I misunderstood the description.
You expect that memory cache would be enabled even when # of entries is larger than {{spark.shuffle.service.index.cache.entries}} if the total cache size is not large.;;;","25/Jul/17 13:10;tgraves;We want to change it from a # of entries to a size of memory.  So the cache is enabled but has a max size of 200MB for instance. This way you can size that based on the memory size of the YARN NM where the spark shuffle service runs.  If I have a 1GB heap on my NM this ensures it only uses 200MB.  With the # of entries you can't guarantee it won't use all 1GB of your heap because the size of each entry is dependent upon the # of reducers in that job.;;;","25/Jul/17 19:57;sanket991;Hi I am working on this issue just to avoid any redundancies if any thanks;;;","13/Feb/18 14:45;renxunsaky;Hi guys,

Could you tell me how to figure out how many memory the NM with spark shuffle service has used ? And how to know a spark job has used how many reducers ?

Because I have the same problem recently and I want to get a list of spark jobs by sorting by number of reducers.

 

Thanks.

Regards,

Xun REN.;;;","21/Oct/20 13:11;larsfrancke;Just FYI for others stumbling across this: This has a bug in how the memory is calculated and might use way more than the 100MB it intends to.

See SPARK-33206 for details.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
quick start  -> one  py demo have some bug in code ,SPARK-21498,13088975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,lizhaoch,lizhaoch,lizhaoch,21/Jul/17 06:08,25/Jul/17 12:20,14/Jul/23 06:30,25/Jul/17 09:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Examples,,,,,0,beginner,,,,,,,,"http://spark.apache.org/docs/latest/quick-start.html

Self-Contained Applications：

spark = SparkSession.builder().appName(appName).master(master).getOrCreate()

should change to:

spark = SparkSession.builder.appName(""SimpleApp"").getOrCreate()
",,apachespark,lizhaoch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/17 07:09;lizhaoch;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12878577/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,"
http://spark.apache.org/docs/latest/quick-start.html


Self-Contained Applications：

spark = SparkSession.builder().appName(appName).master(master).getOrCreate()
should change to:
spark = SparkSession.builder.appName(""SimpleApp"").getOrCreate()
",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 25 12:20:41 UTC 2017,,,,,,,,,,"0|i3hu4v:",9223372036854775807,,,,,bvkeirsb,,,,,,,,,,,,,,,,,,,"21/Jul/17 06:58;srowen;Agree, that looks more consistent with the other examples.;;;","21/Jul/17 07:03;lizhaoch;ha, ,i continue my leaning

------------------ 原始邮件 ------------------
发件人: ""Sean Owen (JIRA)"" <jira@apache.org>;
发送时间: 2017年7月21日(星期五) 14:59
收件人: ""lizhaoc"" <lizhaoc@qq.com>;
主题: [jira] [Commented] (SPARK-21498) quick start  -> one  py demo havesome bug in code




    [ https://issues.apache.org/jira/browse/SPARK-21498?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16095872#comment-16095872 ] 

Sean Owen commented on SPARK-21498:
-----------------------------------

Agree, that looks more consistent with the other examples.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
;;;","21/Jul/17 17:49;srowen;Do you want to make a pull request?;;;","21/Jul/17 22:54;lizhaoch;yes,that's very good.
how should i to do?

------------------ 原始邮件 ------------------
发件人: ""Sean Owen (JIRA)"" <jira@apache.org>;
发送时间: 2017年7月22日(星期六) 1:50
收件人: ""lizhaoc"" <lizhaoc@qq.com>;
主题: [jira] [Updated] (SPARK-21498) quick start  -> one  py demo havesome bug in code




     [ https://issues.apache.org/jira/browse/SPARK-21498?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sean Owen updated SPARK-21498:
------------------------------
    Target Version/s:   (was: 2.2.0)
       Fix Version/s:     (was: 2.2.0)

Do you want to make a pull request?




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
;;;","22/Jul/17 08:31;srowen;http://spark.apache.org/contributing.html;;;","22/Jul/17 14:04;lizhaoch;OK，let me try!

------------------ 原始邮件 ------------------
发件人: ""Sean Owen (JIRA)"" <jira@apache.org>;
发送时间: 2017年7月22日(星期六) 16:32
收件人: ""lizhaoc"" <lizhaoc@qq.com>;
主题: [jira] [Commented] (SPARK-21498) quick start  -> one  py demo havesome bug in code




    [ https://issues.apache.org/jira/browse/SPARK-21498?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16097172#comment-16097172 ] 

Sean Owen commented on SPARK-21498:
-----------------------------------

http://spark.apache.org/contributing.html




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
;;;","24/Jul/17 07:09;lizhaoch;I submit a version about  quick start.md

!screenshot-1.png!;;;","24/Jul/17 07:28;apachespark;User 'lizhaoch' has created a pull request for this issue:
https://github.com/apache/spark/pull/18722;;;","25/Jul/17 09:54;srowen;Issue resolved by pull request 18722
[https://github.com/apache/spark/pull/18722];;;","25/Jul/17 10:00;lizhaoch;When can I see the changes submitted? :D:D:D
http://spark.apache.org/docs/latest/quick-start.html ;;;","25/Jul/17 12:20;lizhaoch;I am reading your book  <Advanced Analytics with Spark> , very good ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.2.0 AES encryption not working with External shuffle,SPARK-21494,13088938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,uditme,uditme,21/Jul/17 00:16,17/May/20 18:21,14/Jul/23 06:30,26/Jul/17 00:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Block Manager,Shuffle,Spark Core,,,1,,,,,,,,,"Spark’s new AES based authentication mechanism does not seem to work when configured with external shuffle service on YARN. 

Here is the stack trace for the error we see in the driver logs:
ERROR YarnScheduler: Lost executor 40 on ip-10-167-104-125.ec2.internal: Unable to create executor due to Unable to register with external shuffle server due to: java.lang.IllegalArgumentException: Authentication failed.
                at org.apache.spark.network.crypto.AuthRpcHandler.receive(AuthRpcHandler.java:125)
                at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:157)
                at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:105)
                at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
                at org.spark_project.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
                at org.spark_project.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
                at org.spark_project.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
 
Here are the settings we are configuring in ‘spark-defaults’ and ‘yarn-site’:
spark.network.crypto.enabled true
spark.network.crypto.saslFallback false
spark.authenticate               true
 
Turning on DEBUG logs for class ‘org.apache.spark.network.crypto’ on both Spark and YARN side is not giving much information either about why authentication fails. The driver and node manager logs have been attached to the JIRA.",AWS EMR,apachespark,djiangxu,juanrh,uditme,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 00:18;uditme;logs.zip;https://issues.apache.org/jira/secure/attachment/12878281/logs.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 21 17:49:03 UTC 2017,,,,,,,,,,"0|i3htwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 06:51;srowen;Are you sure your auth config is correct? that's just saying authentication failed.;;;","21/Jul/17 16:34;vanzin;[~srowen] I already asked all these questions on the mailing list. There's an actual bug and I'm looking into it.;;;","21/Jul/17 17:49;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18706;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in SortMergeJoin,SPARK-21492,13088904,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,zhzhan,zhzhan,20/Jul/17 21:34,08/Jun/20 17:37,14/Jul/23 06:30,22/Oct/19 11:09,2.2.0,2.3.0,2.3.1,3.0.0,,,,,,,,,,,,,,,,,,,,,,,2.4.5,3.0.0,,,,SQL,,,,,3,,,,,,,,,"In SortMergeJoin, if the iterator is not exhausted, there will be memory leak caused by the Sort. The memory is not released until the task end, and cannot be used by other operators causing performance drop or OOM.",,apachespark,bmscicho,cane,cloud_fan,dongjoon,guru.bobbi@gmail.com,jcdauchy,jiangxb1987,kiszk,maropu,mshen,mstuder,taoluo,tgraves,toopt4,xiaojuwu,zhzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27558,,,,,,,SPARK-24657,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 11:09:05 UTC 2019,,,,,,,,,,"0|i3htp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/17 21:39;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18694;;;","20/Jul/17 21:43;zhzhan;root cause: In the SortMergeJoin, inner/leftOuter/rightOuter, one side of the SortedIter may not exhausted, that chunk of the memory thus cannot be released, causing memory leak and performance degradtion.;;;","20/Oct/17 11:54;bmscicho;Here's a script that exposes memory leak during SortMergeJoin in Spark 2.2.0, maybe it will be helpful.
Memory leak happens when the following code is executed in spark-shell (a local one). {{--conf spark.sql.autoBroadcastJoinThreshold=-1}} may be needed to ensure proper join type.

{noformat}
import org.apache.spark.sql.types._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

val table1Key = ""t1_key""
val table1Value = ""t1_value""

val table2Key = ""t2_key""
val table2Value = ""t2_value""

val table1Schema = StructType(List(
    StructField(table1Key, IntegerType),
    StructField(table1Value, DoubleType)
));

val table2Schema = StructType(List(
    StructField(table2Key, IntegerType),
    StructField(table2Value, DoubleType)
));

val table1 = spark.sqlContext.createDataFrame(
    rowRDD = spark.sparkContext.parallelize(Seq(
        Row(1, 2.0)
    )),
    schema = table1Schema
);

val table2 = spark.sqlContext.createDataFrame(
    rowRDD = spark.sparkContext.parallelize(Seq(
        Row(1, 4.0)
    )),
    schema = table2Schema
);


val t1 = table1.repartition(col(table1Key)).groupBy(table1Key).avg()
val t2 = table2.repartition(col(table2Key)).groupBy(table2Key).avg()

val joinedDF = t1 join t2 where t1(table1Key) === t2(table2Key)

joinedDF.explain()
// == Physical Plan ==
// *SortMergeJoin [t1_key#2], [t2_key#9], Inner
// :- *Sort [t1_key#2 ASC NULLS FIRST], false, 0
// :  +- *HashAggregate(keys=[t1_key#2], functions=[avg(cast(t1_key#2 as bigint)), avg(t1_value#3)])
// :     +- *HashAggregate(keys=[t1_key#2], functions=[partial_avg(cast(t1_key#2 as bigint)), partial_avg(t1_value#3)])
// :        +- Exchange hashpartitioning(t1_key#2, 200)
// :           +- *Filter isnotnull(t1_key#2)
// :              +- Scan ExistingRDD[t1_key#2,t1_value#3]
// +- *Sort [t2_key#9 ASC NULLS FIRST], false, 0
//    +- *HashAggregate(keys=[t2_key#9], functions=[avg(cast(t2_key#9 as bigint)), avg(t2_value#10)])
//       +- *HashAggregate(keys=[t2_key#9], functions=[partial_avg(cast(t2_key#9 as bigint)), partial_avg(t2_value#10)])
//          +- Exchange hashpartitioning(t2_key#9, 200)
//             +- *Filter isnotnull(t2_key#9)
//                +- Scan ExistingRDD[t2_key#9,t2_value#10]

joinedDF.show()
// The 'show' action yields a lot of:
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 8
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 9
// 17/10/19 08:17:39 WARN executor.Executor: Managed memory leak detected; size = 4194304 bytes, TID = 11
{noformat}
;;;","08/Feb/19 18:57;taoluo;If SortMergeJoinScanner doesn't consume the iterator from UnsafeExternalRowSorter entirely, the memory that UnsafeExternalSorter acquired from TaskMemoryManager will never be released. This leads to a memory leak, spills, and OOME. A page will be held per partition of the unused iterator.



{code:java}
from pyspark.sql.functions import rand, col

spark.conf.set(""spark.sql.join.preferSortMergeJoin"", ""true"")
spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", -1)

r1 = spark.range(1, 1001).select(col(""id"").alias(""timestamp1""))
r1 = r1.withColumn('value', rand())
r2 = spark.range(1000, 2001).select(col(""id"").alias(""timestamp2""))
r2 = r2.withColumn('value2', rand())
joined = r1.join(r2, r1.timestamp1 == r2.timestamp2, ""inner"")
joined = joined.coalesce(1)
joined.explain()
joined.show(){code}
 
{{== Physical Plan == Coalesce 1 +- *(5) SortMergeJoin [timestamp1#52L|#52L], [timestamp2#59L|#59L], Inner :- *(2) Sort [timestamp1#52L ASC NULLS FIRST|#52L ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(timestamp1#52L, 200) : +- *(1) Project [id#50L AS timestamp1#52L, rand(-4732263137869282482) AS value#54|#50L AS timestamp1#52L, rand(-4732263137869282482) AS value#54] : +- *(1) Range (1, 1001, step=1, splits=4) +- *(4) Sort [timestamp2#59L ASC NULLS FIRST|#59L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(timestamp2#59L, 200) +- *(3) Project [id#57L AS timestamp2#59L, rand(-3625198886289022666) AS value2#61|#57L AS timestamp2#59L, rand(-3625198886289022666) AS value2#61] +- *(3) Range (1000, 2001, step=1, splits=4)}}

{{org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 21, 10.100.100.10, executor 0): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0}}
 

Using broadcast succeeds:
{code:java}
#broadcast
joined = r1.join(broadcast(r2), r1.timestamp1 == r2.timestamp2, ""inner""){code}
Running on Spark 2.4. 

 

Or if you prefer Scala:
{code:java}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, rand}

spark.conf.set(""spark.sql.join.preferSortMergeJoin"", ""true"")
spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", -1)

var r1 = spark.range(1, 1001).select(col(""id"").alias(""timestamp1""))
r1 = r1.withColumn(""value"", rand())
var r2 = spark.range(1000, 2001).select(col(""id"").alias(""timestamp2""))
r2 = r2.withColumn(""value2"", rand())
var joined = r1.join(r2, col(""timestamp1"") === col(""timestamp2""), ""inner"")
joined = joined.coalesce(1)
joined.explain()
joined.show(){code}
 

 

Just reproduced it in standalone mode using [https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz,] Python 3.7. Same code as above.

 

Succeeds with 1 thread: ./bin/pyspark --master local[1]

Fails with >1 thread: ./bin/pyspark --master local[4]
{code:java}
SparkSession available as 'spark'.

>>> from pyspark.sql.functions import rand, col

>>>

>>> spark.conf.set(""spark.sql.join.preferSortMergeJoin"", ""true"")

>>> spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", -1)

>>>

>>> r1 = spark.range(1, 1001).select(col(""id"").alias(""timestamp1""))

>>> r1 = r1.withColumn('value', rand())

>>> r2 = spark.range(1000, 1001).select(col(""id"").alias(""timestamp2""))

>>> r2 = r2.withColumn('value2', rand())

>>> joined = r1.join(r2, r1.timestamp1 == r2.timestamp2, ""inner"")

>>> joined = joined.coalesce(1)

>>> joined.show(){code}
 
{code:java}
[Stage 2:>                                                          (0 + 1) / 1]2019-02-06 17:12:27 WARN  TaskMemoryManager:304 - Failed to allocate a page (1900544 bytes), try again.

2019-02-06 17:12:27 ERROR Executor:91 - Exception in task 0.0 in stage 2.0 (TID 6)

org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 28 bytes of memory, got 0

at org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)

at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:119)

at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:383)

at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:407)

at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:135){code}
 ;;;","08/Feb/19 19:00;taoluo;Can someone add 'affects version' 2.4.0 as well? ;;;","11/Feb/19 21:06;taoluo;I'll take a stab at this jira, should have something to review today or tomorrow. 

[https://github.com/apache/spark/pull/23762] (took some inspiration from Zhan's patch)

I'd appreciate a review, and pointers on modifying code-gen portion.;;;","13/Feb/19 00:04;taoluo;cc [~tejasp], [~kiszk] for input on code generation to address the memory leak. 

[https://github.com/apache/spark/pull/23762] 

 ;;;","21/Feb/19 09:19;cloud_fan;[~taoluo] Thanks for the detailed explanation! I kind of agree that this is a memory leak, although the memory can be released when the task completes.

The problematic pattern is: an operator consumes only a part of records from its child, so that the child can't release the last page which stores the last record it outputs. The child has no idea if the last record has been consumed by the parent or not, so it's not safe to release the last page, as doing so would make the last record corrupted. SMJ and limit are 2 places that I can think of that have this pattern.

So we need a mechanism to allow the parent to tell the child that it can release all the resources. Closable iterator is an option here, but we should not hack it in SMJ only.;;;","27/Feb/19 16:07;guru.bobbi@gmail.com;i ran the sample code provided on spark 2.0.2 and it used sortedMergeJoin but did not lead to memory leaks. So wondering what has changed between spark 2.0.2 and spark 2.2 that is leading to this OOM error? ;;;","18/Mar/19 12:04;xiaojuwu;Any updates? Do you have any discussion on the general fix instead of hack in SMJ?;;;","29/Apr/19 17:23;taoluo;The problem is that the task won't complete because of memory being leaked (You can see from the simple example above)
Secondly, it's not just the last page, it's every page with records from unused iterators. 
Can we increase the priority of this bug? SMJ is a pretty integral part of Spark SQL, and it seems like no progress is being made on this bug, which is causing jobs to fail and has no workaround. 

I don't think that it's a hack: the argument seems to be that limit also needs to fixed, so let's not fix this bug until that is also fixed, meanwhile this issue has been lingering since at least July 2017. 
This would fix a memory leak and improve performance from not spilling unnecessarily. Why don't we target this fix for SMJ first, since it's pretty isolated to UnsafeExternalRowIterator in SMJ, run it through all the test cases, and make additional changes as necessary in the future. 

I've been porting [this PR|https://github.com/apache/spark/pull/23762] onto my production Spark cluster for the last 3 months, but I'm hoping we can get some sort of fix into 3.0 at least.

I started a discussion thread here, hopefully people can jump in:
http://apache-spark-developers-list.1001551.n3.nabble.com/Memory-leak-in-SortMergeJoin-td27152.html
;;;","20/May/19 21:24;jiangxb1987;I'm working on this, will post a simple design doc in a few days.;;;","12/Sep/19 05:07;cane;Any progress of this issue? [~jiangxb1987]
We also encountered this problem;;;","22/Sep/19 01:35;mshen;We also saw this issue happening in our cluster.

Based on the [~taoluo] 's patch, we worked on a patch which fixes this issue for when codegen is enabled.

[https://github.com/apache/spark/pull/25888]

Would appreciate comments on this.

[~taoluo] [~cloud_fan] [~jiangxb1987];;;","14/Oct/19 16:55;mshen;We have deployed the latest version of the PR in [https://github.com/apache/spark/pull/25888] in LinkedIn's production clusters for a week now.

With the most recent changes, all corner cases seem to have been handled.

We are seeing jobs previously failing due to this issue now able to complete.

We have also observed a general reduction of spills during join in our cluster.

Want to see if the community is also working on a fix of this issue, and if so whether there's a timeline for the fix.

[~cloud_fan] [~jiangxb1987] [~taoluo];;;","14/Oct/19 17:12;mshen;Want to further clarify the scope of the fix in PR [https://github.com/apache/spark/pull/25888].

Based on previous work by [~taoluo], this PR further fixes the issue for SMJ codegen.

[~hvanhovell] raised 2 concerns in [~taoluo]'s PR in [https://github.com/apache/spark/pull/23762]:
 # This only works for a SMJ with Sorts as its direct input.
 # Not sure if it safe to assume that you can close an underlying child like this.

The fix in PR [https://github.com/apache/spark/pull/25888] should have addressed concern #2, i.e. it guarantees safeness on closing the iterator for a Sort operator early.

This fix does not yet propagate the requests to close iterators of both child operators of a SMJ throughout the plan tree to reach the Sort operators.

However, with our experiences in operating all Spark workloads at LinkedIn, it is mostly common for SMJ not having Sort as its direct input when there are multiple SMJs stacked together.

In this case, even if we are not yet propagating the requests, each SMJ can still properly handle its local child operators which would still help to release the resources early.;;;","22/Oct/19 11:09;cloud_fan;Issue resolved by pull request 26164
[https://github.com/apache/spark/pull/26164];;;",,,,,,,,,,,,,,,,,,,,,,
SparkLauncher may fail to redirect streams,SPARK-21490,13088893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,20/Jul/17 20:54,02/Aug/17 19:06,14/Jul/23 06:30,02/Aug/17 19:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"While investigating a different issue, I noticed that the redirection handling in SparkLauncher is faulty.

In the default case or when users use only log redirection, things should work fine.

But if users try to redirect just the stdout of a child process without redirecting stderr, the launcher won't detect that and the child process may hang because stderr is not being read and its buffer fills up.

The code should detect these cases and redirect any streams that haven't been explicitly redirected by the user.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 20 21:57:04 UTC 2017,,,,,,,,,,"0|i3htmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/17 21:57;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18696;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outer join filter pushdown in null supplying table when condition is on one of the joined columns,SPARK-21479,13088628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maryannxue,anbhole,anbhole,20/Jul/17 03:13,16/May/19 10:17,14/Jul/23 06:30,18/Apr/18 02:38,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,Optimizer,SQL,,,,2,,,,,,,,,"Here are two different query plans - 


{code:java}

df1 = spark.createDataFrame([{ ""a"": 1, ""b"" : 2}, { ""a"": 3, ""b"" : 4}])
df2 = spark.createDataFrame([{ ""a"": 1, ""c"" : 5}, { ""a"": 3, ""c"" : 6}, { ""a"": 5, ""c"" : 8}])

df1.join(df2, ['a'], 'right_outer').where(""b = 2"").explain()

== Physical Plan ==
*Project [a#16299L, b#16295L, c#16300L]
+- *SortMergeJoin [a#16294L], [a#16299L], Inner
   :- *Sort [a#16294L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(a#16294L, 4)
   :     +- *Filter ((isnotnull(b#16295L) && (b#16295L = 2)) && isnotnull(a#16294L))
   :        +- Scan ExistingRDD[a#16294L,b#16295L]
   +- *Sort [a#16299L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#16299L, 4)
         +- *Filter isnotnull(a#16299L)
            +- Scan ExistingRDD[a#16299L,c#16300L]


df1 = spark.createDataFrame([{ ""a"": 1, ""b"" : 2}, { ""a"": 3, ""b"" : 4}])
df2 = spark.createDataFrame([{ ""a"": 1, ""c"" : 5}, { ""a"": 3, ""c"" : 6}, { ""a"": 5, ""c"" : 8}])

df1.join(df2, ['a'], 'right_outer').where(""a = 1"").explain()

== Physical Plan ==
*Project [a#16314L, b#16310L, c#16315L]
+- SortMergeJoin [a#16309L], [a#16314L], RightOuter
   :- *Sort [a#16309L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(a#16309L, 4)
   :     +- Scan ExistingRDD[a#16309L,b#16310L]
   +- *Sort [a#16314L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(a#16314L, 4)
         +- *Filter (isnotnull(a#16314L) && (a#16314L = 1))
            +- Scan ExistingRDD[a#16314L,c#16315L]
{code}


If condition on b can be pushed down on df1 then why not condition on a?",,anbhole,aokolnychyi,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21645,SPARK-27741,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 02:38:55 UTC 2018,,,,,,,,,,"0|i3hrzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/17 20:13;aokolnychyi;I used the following code to investigate:

{code}
    val inputSchema1 = StructType(
      StructField(""col1"", StringType) ::
      StructField(""col2"", IntegerType) ::
      Nil)

    val inputSchema2 = StructType(
      StructField(""col1"", StringType) ::
      StructField(""col3"", StringType) ::
      Nil)

    val rdd1 = sc.parallelize(1 to 3).map(v => Row(s""value $v"", v))
    val df1 = spark.createDataFrame(rdd1, inputSchema1)
    val rdd2 = sc.parallelize(1 to 3).map(v => Row(s""value $v"", ""some value""))
    val df2 = spark.createDataFrame(rdd2, inputSchema2)
    
    // 1st use case
    df1.join(df2, Seq(""col1""), ""right_outer"").where(""col2 = 2"").explain(true)
    // 2nd use case
    df1.join(df2, Seq(""col1""), ""right_outer"").where(""col1 = 'value 2'"").explain(true)
{code}

It is important to notice that the actual join type in the first case is `inner` and not `rigth_outer`. This happens due to the `EliminateOuterJoin` rule, which sees that `col2 = 2` filters out non-matching rows on the left side of the join. Once the join type is changed, the `PushPredicateThroughJoin` rule pushes `col2 = 2` to the left relation. The analyzed and optimized logical plans are:

{noformat}
== Analyzed Logical Plan ==
col1: string, col2: int, col3: string
Filter (col2#3 = 2)
+- Project [col1#9, col2#3, col3#10]
   +- Join RightOuter, (col1#2 = col1#9)
      :- LogicalRDD [col1#2, col2#3]
      +- LogicalRDD [col1#9, col3#10]

== Optimized Logical Plan ==
Project [col1#9, col2#3, col3#10]
+- Join Inner, (col1#2 = col1#9)
   :- Filter ((isnotnull(col2#3) && (col2#3 = 2)) && isnotnull(col1#2))
   :  +- LogicalRDD [col1#2, col2#3]
   +- Filter isnotnull(col1#9)
      +- LogicalRDD [col1#9, col3#10]

{noformat}

The second case is different. The join type stays the same (i.e., `right_outer`) and the analyzed logical plan looks like:

{noformat}
== Analyzed Logical Plan ==
col1: string, col2: int, col3: string
Filter (col1#9 = value 2)
+- Project [col1#9, col2#3, col3#10]
   +- Join RightOuter, (col1#2 = col1#9)
      :- LogicalRDD [col1#2, col2#3]
      +- LogicalRDD [col1#9, col3#10]
{noformat}

`col1#9` from the Filter belongs to the right relation. After `PushPredicateThroughJoin` we have:

{noformat}
Join RightOuter, (col1#2 = col1#9)
:- LogicalRDD [col1#2, col2#3]
+- Filter (isnotnull(col1#9) && (col1#9 = value 2))
   +- LogicalRDD [col1#9, col3#10]
{noformat}


In theory, `InferFiltersFromConstraints` is capable of inferring `(col1#2 = value 2)` from `(col1#9 = value 2, col1#2 = col1#9)`. However, not in this case since the join type is `right_outer` and `InferFiltersFromConstraints` will process only constraints from the right relation (i.e., `(isnotnull(col1#9) && (col1#9 = value 2))`), which is not enough to infer `(col1#2 = value 2)`.

It seems like this is done on purpose and it is expected behavior even though additional `(col1#2 = value 2)` on the left relation would be logically correct here (as far as I understand).;;;","28/Jul/17 09:26;anbhole;So here is the actual use case - 

{code:java}
spark = SparkSession.builder.getOrCreate()

df1 = spark.createDataFrame([{ ""x"" : 'c1', ""a"": 1, ""b"" : 2}, { ""x"" : 'c2', ""a"": 3, ""b"" : 4}])
df2 = spark.createDataFrame([{ ""x"" : 'c1', ""a"": 1, ""c"" : 5}, { ""x"" : 'c1', ""a"": 3, ""c"" : 6}, { ""x"" : 'c2', ""a"": 5, ""c"" : 8}])

df1.join(df2, ['x', 'a'], 'right_outer').where(""b = 2"").explain()

df1.join(df2, ['x', 'a'], 'right_outer').where(""b = 2"").show()

print 

df1 = spark.createDataFrame([{ ""x"" : 'c1', ""a"": 1, ""b"" : 2}, { ""x"" : 'c2', ""a"": 3, ""b"" : 4}])
df2 = spark.createDataFrame([{ ""x"" : 'c1', ""a"": 1, ""c"" : 5}, { ""x"" : 'c1', ""a"": 3, ""c"" : 6}, { ""x"" : 'c2', ""a"": 5, ""c"" : 8}])


df1.join(df2, ['x', 'a'], 'right_outer').where(""x = 'c1'"").explain()

df1.join(df2, ['x', 'a'], 'right_outer').where(""x = 'c1'"").show()
{code}

Output - 

{code:java}
== Physical Plan ==
*Project [x#458, a#456L, b#450L, c#457L]
+- *SortMergeJoin [x#451, a#449L], [x#458, a#456L], Inner
   :- *Sort [x#451 ASC NULLS FIRST, a#449L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(x#451, a#449L, 4)
   :     +- *Filter (((isnotnull(b#450L) && (b#450L = 2)) && isnotnull(x#451)) && isnotnull(a#449L))
   :        +- Scan ExistingRDD[a#449L,b#450L,x#451]
   +- *Sort [x#458 ASC NULLS FIRST, a#456L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(x#458, a#456L, 4)
         +- *Filter (isnotnull(x#458) && isnotnull(a#456L))
            +- Scan ExistingRDD[a#456L,c#457L,x#458]
+---+---+---+---+
|  x|  a|  b|  c|
+---+---+---+---+
| c1|  1|  2|  5|
+---+---+---+---+


== Physical Plan ==
*Project [x#490, a#488L, b#482L, c#489L]
+- SortMergeJoin [x#483, a#481L], [x#490, a#488L], RightOuter
   :- *Sort [x#483 ASC NULLS FIRST, a#481L ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(x#483, a#481L, 4)
   :     +- Scan ExistingRDD[a#481L,b#482L,x#483]
   +- *Sort [x#490 ASC NULLS FIRST, a#488L ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(x#490, a#488L, 4)
         +- *Filter (isnotnull(x#490) && (x#490 = c1))
            +- Scan ExistingRDD[a#488L,c#489L,x#490]
+---+---+----+---+
|  x|  a|   b|  c|
+---+---+----+---+
| c1|  1|   2|  5|
| c1|  3|null|  6|
+---+---+----+---+
{code}

As you can see filter on 'x' column does not get pushed down. In our cases, 'x' is a company id in an multi tenant system and it is extremely important to pass this filter to both dataframes or else it fetches the entire data for both the tables.
;;;","12/Mar/18 23:19;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/20805;;;","13/Mar/18 21:37;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/20816;;;","17/Apr/18 06:33;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/21083;;;","18/Apr/18 02:38;cloud_fan;Issue resolved by pull request 20816
[https://github.com/apache/spark/pull/20816];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Output of StructuredStreaming tables don't respect user specified schema when reading back the table,SPARK-21463,13088047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,18/Jul/17 20:16,20/Jul/17 18:21,14/Jul/23 06:30,20/Jul/17 18:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,Structured Streaming,,,,0,,,,,,,,,"When using the MetadataLogFileIndex to read back a table, we don't respect the user provided schema as the proper column types. This can lead to issues when trying to read strings that look like dates that get truncated to DateType, or longs being truncated to IntegerType, just because a long value doesn't exist.",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 20:41:03 UTC 2017,,,,,,,,,,"0|i3hoev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 20:41;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/18676;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add batchId to the json of StreamingQueryProgress,SPARK-21462,13088045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,tdas,tdas,18/Jul/17 20:13,18/Jul/17 23:30,14/Jul/23 06:30,18/Jul/17 23:30,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,Batch id is present in the StreamingQueryProgress but not represented in the generated json.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 23:30:00 UTC 2017,,,,,,,,,,"0|i3hoef:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"18/Jul/17 20:41;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/18675;;;","18/Jul/17 23:30;tdas;Issue resolved by pull request 18675
[https://github.com/apache/spark/pull/18675];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalCatalog.listPartitions should correctly handle partition values with dot,SPARK-21457,13087948,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,18/Jul/17 14:48,19/Jul/17 00:11,14/Jul/23 06:30,19/Jul/17 00:11,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 15:08:06 UTC 2017,,,,,,,,,,"0|i3hnsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 15:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18671;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveConf in SparkSQLCLIDriver doesn't respect spark.hadoop.some.hive.variables,SPARK-21451,13087843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,18/Jul/17 06:39,06/Aug/17 00:33,14/Jul/23 06:30,06/Aug/17 00:33,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala#L83 is not copying properties configured for hadoop/hive by --conf or in spak-default.conf using spark.hadoop.foo=bar

",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 06:47:03 UTC 2017,,,,,,,,,,"0|i3hn5j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 06:47;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/18668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive client's SessionState was not closed properly  in HiveExternalCatalog,SPARK-21449,13087816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,18/Jul/17 03:21,16/Mar/21 11:27,14/Jul/23 06:30,16/Mar/21 02:42,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,3.2.0,,,,,SQL,,,,,0,,,,,,,,,close the sessionstate to clear `hive.downloaded.resources.dir` and else.,,apachespark,Qin Yao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 11:27:15 UTC 2021,,,,,,,,,,"0|i3hmzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 03:28;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/18666;;;","15/Mar/21 02:13;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31833;;;","16/Mar/21 02:42;yumwang;Issue resolved by pull request 31833
https://github.com/apache/spark/pull/31833;;;","16/Mar/21 11:26;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31850;;;","16/Mar/21 11:27;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31850;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark history server fails to render compressed inprogress history file in some cases.,SPARK-21447,13087798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ericvandenbergfb,ericvandenbergfb,ericvandenbergfb,18/Jul/17 01:05,25/Jul/17 18:51,14/Jul/23 06:30,25/Jul/17 18:50,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Web UI,,,,,0,,,,,,,,,"We've observed the Spark History Server sometimes fails to load event data from a compressed .inprogress spark history file.  Note the existing logic in ReplayListenerBus is to read each line, if it can't json parse the last line and it's inprogress (maybeTruncated) then it is accepted as best effort.

In the case of compressed files, the output stream will compress on the fly json serialized event data.  The output is periodically flushed to disk when internal buffers are full.  A consequence of that is a partially compressed frame may be flushed, and not being a complete frame, it can not be decompressed.  If the spark history server attempts to read such an .inprogress compressed file it will throw an EOFException.  This is really analogous to the case of failing to json parse the last line in the file (because the full line was not flushed), the difference is that since the file is compressed it is possible the compression frame was not flushed, and trying to decompress a partial frame fails in a different way the code doesn't currently handle.

17/07/13 17:24:59 ERROR FsHistoryProvider: Exception encountered when attempting to load application log hdfs://********/user/hadoop/******/spark/logs/job_**********-*************-*****.lz4.inprogress
java.io.EOFException: Stream ended prematurely
        at org.apache.spark.io.LZ4BlockInputStream.readFully(LZ4BlockInputStream.java:230)
        at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:203)
        at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
        at java.io.InputStreamReader.read(InputStreamReader.java:184)
        at java.io.BufferedReader.fill(BufferedReader.java:161)
        at java.io.BufferedReader.readLine(BufferedReader.java:324)
        at java.io.BufferedReader.readLine(BufferedReader.java:389)
        at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:72)
        at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:836)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
        at org.apache.spark.scheduler.ReplayListenerBus.replay(ReplayListenerBus.scala:66)
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$replay(FsHistoryProvider.scala:601)
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$mergeApplicationListing(FsHistoryProvider.scala:409)
        at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$checkForLogs$3$$anon$4.run(FsHistoryProvider.scala:310)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

 ",Spark History Server,apachespark,ericvandenbergfb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11170,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 17:20:03 UTC 2017,,,,,,,,,,"0|i3hmvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 17:20;apachespark;User 'ericvandenbergfb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18673;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] JDBC Postgres fetchsize parameter ignored again,SPARK-21446,13087788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DFFuture,DFFuture,DFFuture,18/Jul/17 00:01,19/Jul/17 21:48,14/Jul/23 06:30,19/Jul/17 21:48,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"This bug was fixed in https://github.com/apache/spark/pull/9861
but reappears in spark 2.1 +
The following code may not be executed in org.apache.spark.sql.jdbc.PostgresDialect because ""properties"" is ""asConnectionProperties"" but ""asConnectionProperties"" can not have ""fetchsize""
{code:java}
if (properties.getOrElse(JDBCOptions.JDBC_BATCH_FETCH_SIZE, ""0"").toInt > 0) {
      connection.setAutoCommit(false)
}
{code}
 ",,apachespark,DFFuture,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 03:14:04 UTC 2017,,,,,,,,,,"0|i3hmtb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/17 03:14;apachespark;User 'DFFuture' has created a pull request for this issue:
https://github.com/apache/spark/pull/18665;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NotSerializableException thrown by UTF8String.IntWrapper,SPARK-21445,13087782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,17/Jul/17 23:47,26/Jul/17 06:31,14/Jul/23 06:30,18/Jul/17 04:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"{code}
Caused by: java.io.NotSerializableException: org.apache.spark.unsafe.types.UTF8String$IntWrapper
Serialization stack:
    - object not serializable (class: org.apache.spark.unsafe.types.UTF8String$IntWrapper, value: org.apache.spark.unsafe.types.UTF8String$IntWrapper@326450e)
    - field (class: org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToInt$1, name: result$2, type: class org.apache.spark.unsafe.types.UTF8String$IntWrapper)
    - object (class org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToInt$1, <function1>)
{code}

Not exactly sure in which specific case this exception is thrown, because I couldn't come up with a simple reproducer yet, but will include a test in the PR",,apachespark,brkyvz,cloud_fan,jinxing6042@126.com,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 26 06:31:06 UTC 2017,,,,,,,,,,"0|i3hmrz:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"17/Jul/17 23:49;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/18660;;;","18/Jul/17 04:10;cloud_fan;Issue resolved by pull request 18660
[https://github.com/apache/spark/pull/18660];;;","25/Jul/17 08:48;jinxing6042@126.com;With this change, I'm still seeing exception below:
java.io.InvalidClassException: org.apache.spark.unsafe.types.UTF8String$IntWrapper; class invalid for deserialization
	at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)
	at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1775)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745);;;","25/Jul/17 09:50;cloud_fan;[~jinxing6042@126.com] can you post the code snippet to reproduce it? ;;;","25/Jul/17 11:50;jinxing6042@126.com;I'm not sure how to reproduce, I will try.;;;","26/Jul/17 06:31;jinxing6042@126.com;Sorry, I report the exception by mistake.
With the change, it works well for me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch failure due to node reboot causes job failure,SPARK-21444,13087778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,sitalkedia@gmail.com,sitalkedia@gmail.com,17/Jul/17 23:38,17/May/20 17:48,14/Jul/23 06:30,18/Jul/17 03:41,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"We started seeing this issue after merging the PR - https://github.com/apache/spark/pull/17955. 

This PR introduced a change to keep the map-output statuses only in the map output tracker and whenever there is a fetch failure, the scheduler tries to invalidate the map-output statuses by talking all the the block manager slave end point. However, in case the fetch failure is because node reboot, the block manager slave end point would not be reachable and so the driver fails the job as a result. See exception below -


{code}
2017-07-15 05:20:50,255 WARN  storage.BlockManagerMaster (Logging.scala:logWarning(87)) - Failed to remove broadcast 10 with removeFromMaster = true - Connection reset by peer
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)
2017-07-15 05:20:50,275 ERROR scheduler.DAGSchedulerEventProcessLoop (Logging.scala:logError(91)) - DAGSchedulerEventProcessLoop failed; shutting down SparkContext
org.apache.spark.SparkException: Exception thrown in awaitResult
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
        at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:143)
        at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:271)
        at org.apache.spark.broadcast.TorrentBroadcast.doDestroy(TorrentBroadcast.scala:165)
        at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:111)
        at org.apache.spark.broadcast.Broadcast.destroy(Broadcast.scala:98)
        at org.apache.spark.ShuffleStatus.invalidateSerializedMapOutputStatusCache(MapOutputTracker.scala:197)
        at org.apache.spark.ShuffleStatus.removeMapOutput(MapOutputTracker.scala:105)
        at org.apache.spark.MapOutputTrackerMaster.unregisterMapOutput(MapOutputTracker.scala:420)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1324)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1715)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1673)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1662)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:192)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
        at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
        at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
        at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
        at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,igor.berman,joshrosen,sitalkedia@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20715,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 19 08:48:07 UTC 2019,,,,,,,,,,"0|i3hmr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/17 23:40;sitalkedia@gmail.com;cc - [~joshrosen];;;","17/Jul/17 23:40;sitalkedia@gmail.com;Any idea how to fix this issue?;;;","17/Jul/17 23:44;joshrosen;I'm going to adjust the ""affects versions"" on this because it only affects Spark 2.3.0 / master and not 2.0.2.;;;","17/Jul/17 23:50;joshrosen;I spot the problem: in the old code, we removed the broadcast variable by calling {{BroadcastManager.unbroadcast}} with {{blocking=false}}, but the new code simply calls {{Broadcast.destroy()}} which is capable of failing with an exception. It sounds like a reasonable fix would be to call {{destroy(blocking = false)}} and wrap the call in a ""try and log exceptions"" block since it should be fine to ignore cleanup failures here (the context cleaner should eventually handle final cleanup even if we didn't explicitly destroy here). I can submit a PR for this.;;;","18/Jul/17 00:06;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/18662;;;","19/May/19 08:48;igor.berman;this Jira affects 2.2.3 version as well 

as commit causing problem exists in 2.2.3 too (also Jira marked to be fixed under 2.2.3) https://issues.apache.org/jira/browse/SPARK-20715

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Codegen in SortMergeJoinExec results failures in some cases,SPARK-21441,13087581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,donnyzone,donnyzone,donnyzone,17/Jul/17 13:11,19/Jul/17 13:56,14/Jul/23 06:30,19/Jul/17 13:54,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"We noticed that the codegen mechanism in SortMergeJoinExec caused job fails in some cases. The below simple example demonstrates this issue. 

The query joins two relations with conditions containing a HiveUDF (i.e., base64) in OR predicates. 

{code:sql}
SELECT ca_zip
FROM customer, customer_address
WHERE customer.c_current_addr_sk = customer_address.ca_address_sk
AND (base64(ca_zip) = '85669' OR customer.c_birth_month > 2)
{code}


Physical plan before execution

*Project [ca_zip#27]
+- *SortMergeJoin [c_current_addr_sk#4], [ca_address_sk#18], Inner, ((HiveSimpleUDF#Base64(ca_zip#27) = 85669) || (c_birth_month#12 > 2))
   :- *Sort [c_current_addr_sk#4 ASC NULLS FIRST], false, 0
   :  +- Exchange hashpartitioning(c_current_addr_sk#4, 200)
   :     +- *Filter isnotnull(c_current_addr_sk#4)
   :        +- HiveTableScan [c_current_addr_sk#4, c_birth_month#12], MetastoreRelation test, customer
   +- *Sort [ca_address_sk#18 ASC NULLS FIRST], false, 0
      +- Exchange hashpartitioning(ca_address_sk#18, 200)
         +- *Filter isnotnull(ca_address_sk#18)
            +- HiveTableScan [ca_address_sk#18, ca_zip#27], MetastoreRelation test, customer_address


By default, the query will fail and throws the following exception:

{code:java}

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times
...............................................................................................
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.unsafe.types.UTF8String.getBytes(UTF8String.java:229)
	at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:821)
	at java.lang.String.valueOf(String.java:2994)
	at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:200)
	at scala.collection.TraversableOnce$$anonfun$addString$1.apply(TraversableOnce.scala:359)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.addString(TraversableOnce.scala:357)
	at scala.collection.AbstractTraversable.addString(Traversable.scala:104)
	at scala.collection.TraversableOnce$class.mkString(TraversableOnce.scala:323)
	at scala.collection.AbstractTraversable.mkString(Traversable.scala:104)
	at scala.collection.TraversableLike$class.toString(TraversableLike.scala:600)
	at scala.collection.SeqLike$class.toString(SeqLike.scala:682)
	at scala.collection.AbstractSeq.toString(Seq.scala:41)
	at java.lang.String.valueOf(String.java:2994)
	at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:200)
	at org.apache.spark.sql.hive.HiveSimpleUDF$$anonfun$eval$1.apply(hiveUDFs.scala:179)
	at org.apache.spark.sql.hive.HiveSimpleUDF$$anonfun$eval$1.apply(hiveUDFs.scala:179)
	at org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54)
	at org.apache.spark.sql.hive.HiveSimpleUDF.logInfo(hiveUDFs.scala:130)
	at org.apache.spark.sql.hive.HiveSimpleUDF.eval(hiveUDFs.scala:179)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	...................................................................
{code}


However, when we close the codegen (i.e., spark.sql.codegen.wholeStage=false, spark.sql.codegen=false), it works well.

",,apachespark,cloud_fan,dmcwhorter,donnyzone,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 19 13:54:09 UTC 2017,,,,,,,,,,"0|i3hljb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/17 13:15;apachespark;User 'DonnyZone' has created a pull request for this issue:
https://github.com/apache/spark/pull/18656;;;","19/Jul/17 05:07;viirya;Btw, I think the priority of this issue should not be critical. I lower it to major.;;;","19/Jul/17 13:54;cloud_fan;Issue resolved by pull request 18656
[https://github.com/apache/spark/pull/18656];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reviving broken partial functions in UDF in PySpark,SPARK-21432,13087467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Jul/17 02:51,12/Dec/22 17:51,14/Jul/23 06:30,18/Jul/17 01:04,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"This is related with SPARK-21394

We also happened to break partial function support in UDF.

Spark 2.1:

{code}
>>> from pyspark.sql import functions
>>> from functools import partial
>>>
>>>
>>> partial_func = partial(lambda x: x, x=1)
>>> udf = functions.udf(partial_func)
>>> spark.range(1).select(udf()).show()
+---------+
|partial()|
+---------+
|        1|
+---------+
{code}

master:

{code}
>>> from pyspark.sql import functions
>>> from functools import partial
>>>
>>>
>>> partial_func = partial(lambda x: x, x=1)
>>> udf = functions.udf(partial_func)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/sql/functions.py"", line 2154, in udf
    return _udf(f=f, returnType=returnType)
  File "".../spark/python/pyspark/sql/functions.py"", line 2145, in _udf
    return udf_obj._wrapped()
  File "".../spark/python/pyspark/sql/functions.py"", line 2099, in _wrapped
    @functools.wraps(self.func, assigned=assignments)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: 'functools.partial' object has no attribute '__module__'
{code}",,apachespark,kwunlyou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 17 03:05:04 UTC 2017,,,,,,,,,,"0|i3hktz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/17 03:05;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliSessionState never be recognized because of IsolatedClientLoader,SPARK-21428,13087437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,16/Jul/17 15:49,22/Sep/17 14:06,14/Jul/23 06:30,19/Sep/17 11:36,1.5.2,1.6.3,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When using bin/spark-sql with the builtin hive jars, we are expecting to reuse the instance of    CliSessionState.
{quote}
        // In `SparkSQLCLIDriver`, we have already started a `CliSessionState`,
        // which contains information like configurations from command line. Later
        // we call `SparkSQLEnv.init()` there, which would run into this part again.
        // so we should keep `conf` and reuse the existing instance of `CliSessionState`.
{quote}
Actually it never ever happened since SessionState.get()  at https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L138 will always be null by IsolatedClientLoader.

The SessionState.start was called many times, which will creates `hive.exec.strachdir`, see the following case...

{code:java}
spark git:(master) bin/spark-sql --conf spark.sql.hive.metastore.jars=builtin
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/07/16 23:29:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/16 23:29:04 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/07/16 23:29:04 INFO ObjectStore: ObjectStore, initialize called
17/07/16 23:29:04 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/07/16 23:29:04 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/07/16 23:29:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
17/07/16 23:29:06 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:06 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:07 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:07 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:07 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/07/16 23:29:07 INFO ObjectStore: Initialized ObjectStore
17/07/16 23:29:07 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/07/16 23:29:07 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/07/16 23:29:08 INFO HiveMetaStore: Added admin role in metastore
17/07/16 23:29:08 INFO HiveMetaStore: Added public role in metastore
17/07/16 23:29:08 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/07/16 23:29:08 INFO HiveMetaStore: 0: get_all_databases
17/07/16 23:29:08 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_all_databases
17/07/16 23:29:08 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/07/16 23:29:08 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
17/07/16 23:29:08 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:08 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/a2c40e42-08e2-4023-8464-3432ed690184_resources
17/07/16 23:29:08 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/a2c40e42-08e2-4023-8464-3432ed690184
17/07/16 23:29:08 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/Kent/a2c40e42-08e2-4023-8464-3432ed690184
17/07/16 23:29:08 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/a2c40e42-08e2-4023-8464-3432ed690184/_tmp_space.db
17/07/16 23:29:08 INFO SparkContext: Running Spark version 2.3.0-SNAPSHOT
17/07/16 23:29:08 INFO SparkContext: Submitted application: SparkSQL::10.0.0.8
17/07/16 23:29:08 INFO SecurityManager: Changing view acls to: Kent
17/07/16 23:29:08 INFO SecurityManager: Changing modify acls to: Kent
17/07/16 23:29:08 INFO SecurityManager: Changing view acls groups to:
17/07/16 23:29:08 INFO SecurityManager: Changing modify acls groups to:
17/07/16 23:29:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Kent); groups with view permissions: Set(); users  with modify permissions: Set(Kent); groups with modify permissions: Set()
17/07/16 23:29:09 INFO Utils: Successfully started service 'sparkDriver' on port 51369.
17/07/16 23:29:09 INFO SparkEnv: Registering MapOutputTracker
17/07/16 23:29:09 INFO SparkEnv: Registering BlockManagerMaster
17/07/16 23:29:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/07/16 23:29:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/07/16 23:29:09 INFO DiskBlockManager: Created local directory at /private/var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/blockmgr-8bf36015-6c27-4b4b-b90b-126f7a59b5a0
17/07/16 23:29:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/07/16 23:29:09 INFO SparkEnv: Registering OutputCommitCoordinator
17/07/16 23:29:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/07/16 23:29:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.8:4040
17/07/16 23:29:09 INFO Executor: Starting executor ID driver on host localhost
17/07/16 23:29:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51371.
17/07/16 23:29:09 INFO NettyBlockTransferService: Server created on 10.0.0.8:51371
17/07/16 23:29:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/07/16 23:29:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.8, 51371, None)
17/07/16 23:29:09 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.8:51371 with 366.3 MB RAM, BlockManagerId(driver, 10.0.0.8, 51371, None)
17/07/16 23:29:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.8, 51371, None)
17/07/16 23:29:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.8, 51371, None)
17/07/16 23:29:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/Kent/Documents/spark/spark-warehouse').
17/07/16 23:29:10 INFO SharedState: Warehouse path is 'file:/Users/Kent/Documents/spark/spark-warehouse'.
17/07/16 23:29:10 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/07/16 23:29:10 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/07/16 23:29:10 INFO ObjectStore: ObjectStore, initialize called
17/07/16 23:29:11 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/07/16 23:29:11 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/07/16 23:29:12 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
17/07/16 23:29:13 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:13 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:13 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:13 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:13 INFO Query: Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
17/07/16 23:29:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/07/16 23:29:13 INFO ObjectStore: Initialized ObjectStore
17/07/16 23:29:13 INFO HiveMetaStore: Added admin role in metastore
17/07/16 23:29:13 INFO HiveMetaStore: Added public role in metastore
17/07/16 23:29:13 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/07/16 23:29:13 INFO HiveMetaStore: 0: get_all_databases
17/07/16 23:29:13 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_all_databases
17/07/16 23:29:13 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/07/16 23:29:13 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*
17/07/16 23:29:13 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MResourceUri"" is tagged as ""embedded-only"" so does not have its own datastore table.
17/07/16 23:29:13 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/27f3c8f8-38d1-4b64-9ade-b41a07937937_resources
17/07/16 23:29:13 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/27f3c8f8-38d1-4b64-9ade-b41a07937937
17/07/16 23:29:13 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/Kent/27f3c8f8-38d1-4b64-9ade-b41a07937937
17/07/16 23:29:13 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/27f3c8f8-38d1-4b64-9ade-b41a07937937/_tmp_space.db
17/07/16 23:29:13 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/Users/Kent/Documents/spark/spark-warehouse
17/07/16 23:29:13 INFO HiveMetaStore: 0: get_database: default
17/07/16 23:29:13 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_database: default
17/07/16 23:29:14 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/4b589938-6df6-4a9f-b044-99da6b1c76b4_resources
17/07/16 23:29:14 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/4b589938-6df6-4a9f-b044-99da6b1c76b4
17/07/16 23:29:14 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/Kent/4b589938-6df6-4a9f-b044-99da6b1c76b4
17/07/16 23:29:14 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/4b589938-6df6-4a9f-b044-99da6b1c76b4/_tmp_space.db
17/07/16 23:29:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/Users/Kent/Documents/spark/spark-warehouse
17/07/16 23:29:14 INFO HiveMetaStore: 0: get_database: global_temp
17/07/16 23:29:14 INFO audit: ugi=Kent	ip=unknown-ip-addr	cmd=get_database: global_temp
17/07/16 23:29:14 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
17/07/16 23:29:14 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/1317e8bf-5a10-4758-bd56-3196ad9ae95e_resources
17/07/16 23:29:14 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/1317e8bf-5a10-4758-bd56-3196ad9ae95e
17/07/16 23:29:14 INFO SessionState: Created local directory: /var/folders/k2/04p4k4ws73l6711h_mz2_tq00000gn/T/Kent/1317e8bf-5a10-4758-bd56-3196ad9ae95e
17/07/16 23:29:14 INFO SessionState: Created HDFS directory: /tmp/hive/Kent/1317e8bf-5a10-4758-bd56-3196ad9ae95e/_tmp_space.db
17/07/16 23:29:14 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/Users/Kent/Documents/spark/spark-warehouse
17/07/16 23:29:14 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
{code}",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22102,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 19 03:52:04 UTC 2017,,,,,,,,,,"0|i3hknb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/17 16:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/18648;;;","17/Aug/17 16:27;cloud_fan;Issue resolved by pull request 18648
[https://github.com/apache/spark/pull/18648];;;","28/Aug/17 09:36;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19068;;;","28/Aug/17 09:37;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/19068;;;","19/Sep/17 03:52;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19273;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failure due to unsupported hex literals. ,SPARK-21426,13087401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,16/Jul/17 00:33,16/Jul/17 16:44,14/Jul/23 06:30,16/Jul/17 16:43,2.0.3,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,,,,,Tests,,,,,0,,,,,,,,,"SPARK 2.0 does not support hex literal. Thus, the test case failed after backporting https://github.com/apache/spark/pull/18571",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 16 00:34:03 UTC 2017,,,,,,,,,,"0|i3hkfb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/17 00:34;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18643;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Depend on Apache ORC 1.4.0,SPARK-21422,13087308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Jul/17 21:38,16/Aug/17 06:08,14/Jul/23 06:30,16/Aug/17 06:08,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,"Like Parquet, this issue aims to depend on the latest Apache ORC 1.4 for Apache Spark 2.3. There are key benefits for now.

- Stability: Apache ORC 1.4.0 has many fixes and we can depend on ORC community more.
- Maintainability: Reduce the Hive dependency and can remove old legacy code later.

Later, we can get the following two key benefits by adding new ORCFileFormat in SPARK-20728, too.

- Usability: User can use ORC data sources without hive module, i.e, -Phive.
- Speed: Use both Spark ColumnarBatch and ORC RowBatch together. This is faster than the current implementation in Spark.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,SPARK-20682,SPARK-20728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 14 22:06:05 UTC 2017,,,,,,,,,,"0|i3hjuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/17 22:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElementException: None.get in DataSourceScanExec with sun.io.serialization.extendedDebugInfo=true,SPARK-21418,13087211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,darabos,darabos,14/Jul/17 14:17,21/Sep/17 12:13,14/Jul/23 06:30,04/Sep/17 21:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"I don't have a minimal reproducible example yet, sorry. I have the following lines in a unit test for our Spark application:

{code}
val df = mySparkSession.read.format(""jdbc"")
  .options(Map(""url"" -> url, ""dbtable"" -> ""test_table""))
  .load()
df.show
println(df.rdd.collect)
{code}

The output shows the DataFrame contents from {{df.show}}. But the {{collect}} fails:

{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.util.NoSuchElementException: None.get
java.util.NoSuchElementException: None.get
  at scala.None$.get(Option.scala:347)
  at scala.None$.get(Option.scala:345)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.org$apache$spark$sql$execution$DataSourceScanExec$$redact(DataSourceScanExec.scala:70)
  at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:54)
  at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:52)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.simpleString(DataSourceScanExec.scala:52)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.simpleString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString(QueryPlan.scala:349)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.org$apache$spark$sql$execution$DataSourceScanExec$$super$verboseString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.execution.DataSourceScanExec$class.verboseString(DataSourceScanExec.scala:60)
  at org.apache.spark.sql.execution.RowDataSourceScanExec.verboseString(DataSourceScanExec.scala:75)
  at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:556)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.generateTreeString(WholeStageCodegenExec.scala:451)
  at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:576)
  at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:480)
  at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:477)
  at org.apache.spark.sql.catalyst.trees.TreeNode.toString(TreeNode.scala:474)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
  at scala.collection.immutable.List$SerializationProxy.writeObject(List.scala:468)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
  at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
  at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
  at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
  at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
  at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1003)
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:930)
  at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:874)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1677)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{noformat}

Looks like this is due to https://github.com/apache/spark/commit/91fa80fe8a2480d64c430bd10f97b3d44c007bcc#diff-2a91a9a59953aa82fa132aaf45bd731bR69 from https://issues.apache.org/jira/browse/SPARK-20070. It tries to redact sensitive information from {{explain}} output. (We are not trying to explain anything here, so I doubt it is meant to be running in this case.) When it needs to access some configuration, it tries to take it from the ""current"" Spark session, which it just reads from a thread-local variable. We appear to be on a thread where this variable is not set.

My impression is that SPARK-20070 unintentionally introduced a very hard constraint on multi-threaded Spark applications.",,apachespark,darabos,jxiang,kiszk,Koraseg,robert3005,tigerquoll,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 12:13:58 UTC 2017,,,,,,,,,,"0|i3hj9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/17 16:43;kiszk;I am curious why {{java.io.ObjectOutputStream.writeOrdinaryObject}} calls {{toString}} method. Do you specify some option to run this program for JVM?;;;","16/Jul/17 15:08;darabos;I'm on holiday without a computer through the coming week, but I'll try to
dig deeper after that.

I do recall that we enable a JVM flag for printing extra details on
serialization errors. Now I wonder if that flag collects string forms even
when no error happens. I guess I should not be surprised: if it did not,
there would be no reason to ever disable this feature.

That already suggests an easy workaround :). Thanks!;;;","04/Sep/17 15:49;darabos;Sorry for the delay. I can confirm that removing {{-Dsun.io.serialization.extendedDebugInfo=true}} is the fix. We only use this flag when running unit tests, but it's very useful for debugging serialization issues. It happens often in Spark that you accidentally include something in a closure that cannot be serialized. It's hard to figure out without this flag what caused that.;;;","04/Sep/17 17:31;srowen;I think we could easily make this code a little more defensive so that this doesn't result in an error. It's just trying to check if a config exists in SparkConf and there's no particular need for this to fail.;;;","04/Sep/17 17:34;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/19123;;;","13/Sep/17 23:14;tigerquoll;I'm getting the same stackdump in 2.2.0 while using apache-avro (current snapshot) in structured streaming with a globbed hdfs input path.  I do not get this error if I use a non-globbed input path.

I should note that this is without setting sun.io.serialization.extendedDebugInfo at all.;;;","14/Sep/17 08:36;darabos;Sean's fix should cover you no matter what triggers the unexpected {{toString}} call. You could try building from {{master}} (or taking a nightly from https://spark.apache.org/developer-tools.html#nightly-builds) to confirm that this is the case.;;;","21/Sep/17 12:13;Koraseg;There is still a place in FileSourceScanExec.scala where None.get error theoretically could appear.
{code:java}
  val needsUnsafeRowConversion: Boolean = if (relation.fileFormat.isInstanceOf[ParquetSource]) {
    SparkSession.getActiveSession.get.sessionState.conf.parquetVectorizedReaderEnabled
  } else {
    false
  }
{code}

I think it is worth defending this val initialization as well (setting a default configuration value in case of None). Although I didn't encounter any None.get errors so far after applying this patch.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buffer in SlidingWindowFunctionFrame could be big though window is small,SPARK-21414,13087093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,14/Jul/17 06:09,19/Jul/17 13:37,14/Jul/23 06:30,19/Jul/17 13:37,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"In {{SlidingWindowFunctionFrame}}, it is now adding all rows to the buffer for which the input row value is equal to or less than the output row upper bound, then drop all rows from the buffer for which the input row value is smaller than the output row lower bound.
This could result in the buffer is very big though the window is small. 

For example: 
{{select a, b, sum(a) over (partition by b order by a range between 1000000 following and 1000001 following) from table}}",,apachespark,cloud_fan,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 19 13:37:12 UTC 2017,,,,,,,,,,"0|i3hijj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/17 06:16;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/18634;;;","19/Jul/17 13:37;cloud_fan;Issue resolved by pull request 18634
[https://github.com/apache/spark/pull/18634];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to get new HDFS delegation tokens in AMCredentialRenewer,SPARK-21411,13087078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,14/Jul/17 03:16,17/May/20 18:14,14/Jul/23 06:30,18/Jul/17 18:44,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"In the current {{YARNHadoopDelegationTokenManager}}, {{FileSystem}} to which to get tokens are created out of KDC logged UGI, using these {{FileSystem}} to get new tokens will lead to exception. The main is that Spark code trying to get new tokens using non-kerberized UGI, and Hadoop can only offer new tokens in kerberized UGI. To fix this issue, we should lazily create these {{FileSystem}} within KDC logged UGI.

{noformat}
WARN AMCredentialRenewer: Failed to write out new credentials to HDFS, will try again in an hour! If this happens too often tasks will fail.
org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7087)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:676)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:998)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2345)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
	at org.apache.hadoop.ipc.Client.call(Client.java:1498)
	at org.apache.hadoop.ipc.Client.call(Client.java:1398)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at com.sun.proxy.$Proxy10.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:980)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:291)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:203)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:185)
	at com.sun.proxy.$Proxy11.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:1041)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1688)
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:549)
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:527)
	at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2400)
	at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider$$anonfun$fetchDelegationTokens$1.apply(HadoopFSDelegationTokenProvider.scala:97)
	at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider$$anonfun$fetchDelegationTokens$1.apply(HadoopFSDelegationTokenProvider.scala:95)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:94)
	at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider.fetchDelegationTokens(HadoopFSDelegationTokenProvider.scala:95)
	at org.apache.spark.deploy.security.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.scala:46)
	at org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anonfun$obtainDelegationTokens$2.apply(HadoopDelegationTokenManager.scala:111)
	at org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anonfun$obtainDelegationTokens$2.apply(HadoopDelegationTokenManager.scala:109)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainDelegationTokens(HadoopDelegationTokenManager.scala:109)
	at org.apache.spark.deploy.yarn.security.YARNHadoopDelegationTokenManager.obtainDelegationTokens(YARNHadoopDelegationTokenManager.scala:56)
	at org.apache.spark.deploy.yarn.security.AMCredentialRenewer$$anon$2.run(AMCredentialRenewer.scala:177)
	at org.apache.spark.deploy.yarn.security.AMCredentialRenewer$$anon$2.run(AMCredentialRenewer.scala:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.spark.deploy.yarn.security.AMCredentialRenewer.org$apache$spark$deploy$yarn$security$AMCredentialRenewer$$writeNewCredentialsToHDFS(AMCredentialRenewer.scala:174)
	at org.apache.spark.deploy.yarn.security.AMCredentialRenewer$$anon$1.run(AMCredentialRenewer.scala:107)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 14 04:04:03 UTC 2017,,,,,,,,,,"0|i3hig7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/17 03:18;jerryshao;This issue is introduced by SPARK-20434.;;;","14/Jul/17 04:04;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18633;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster mode doesn't work with --packages [Mesos],SPARK-21403,13086913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,skonto,skonto,skonto,13/Jul/17 14:24,13/Jul/17 17:37,14/Jul/23 06:30,13/Jul/17 17:37,1.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Mesos,,,,,0,,,,,,,,,"From the mailing list:
{quote}
Another problem I ran into that you also might is that --packages doesn't
work with --deploy-mode cluster.  It downloads the packages to a temporary
location on the node running spark-submit, then passes those paths to the
node that is running the Driver, but since that isn't the same machine, it
can't find anything and fails.  The driver process *should* be the one
doing the downloading, but it isn't. I ended up having to create a fat JAR
with all of the dependencies to get around that one.
{quote}

The problem is that we currently don't upload jars to the cluster. It seems to fix this we either (1) do upload jars, or (2) just run the packages code on the driver side. I slightly prefer (2) because it's simpler.",,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-12559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-07-13 14:24:15.0,,,,,,,,,,"0|i3hhfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix java array of structs deserialization,SPARK-21402,13086910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vofque,tomron,tomron,13/Jul/17 14:12,19/Oct/18 17:24,14/Jul/23 06:30,18/Oct/18 22:12,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.3,2.3.3,2.4.0,,,SQL,,,,,1,,,,,,,,,"I have the following schema in a dataset -

root
 |-- userId: string (nullable = true)
 |-- data: map (nullable = true)
 |    |-- key: string
 |    |-- value: struct (valueContainsNull = true)
 |    |    |-- startTime: long (nullable = true)
 |    |    |-- endTime: long (nullable = true)
 |-- offset: long (nullable = true)


 And I have the following classes (+ setter and getters which I omitted for simplicity) -


 
{code:java}
public class MyClass {

    private String userId;

    private Map<String, MyDTO> data;

    private Long offset;
 }

public class MyDTO {

    private long startTime;
    private long endTime;

}
{code}


I collect the result the following way - 


{code:java}
        Encoder<MyClass> myClassEncoder = Encoders.bean(MyClass.class);
        Dataset<MyClass> results = raw_df.as(myClassEncoder);
        List<MyClass> lst = results.collectAsList();

{code}
        
I do several calculations to get the result I want and the result is correct all through the way before I collect it.
This is the result for - 


{code:java}
results.select(results.col(""data"").getField(""2017-07-01"").getField(""startTime"")).show(false);

{code}

|data[2017-07-01].startTime|data[2017-07-01].endTime|
+-----------------------------+--------------+
|1498854000                |1498870800              |


This is the result after collecting the reuslts for - 


{code:java}
MyClass userData = results.collectAsList().get(0);
MyDTO userDTO = userData.getData().get(""2017-07-01"");
System.out.println(""userDTO startTime: "" + userDTO.getStartTime());
System.out.println(""userDTO endTime: "" + userDTO.getEndTime());

{code}

--
data startTime: 1498870800
data endTime: 1498854000

I tend to believe it is a spark issue. Would love any suggestions on how to bypass it.","mac os
spark 2.1.1
Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_121",apachespark,cloud_fan,dongjoon,kdhuria,kiszk,praetp,steven.aerts,tomron,vofque,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25772,,,,SPARK-21747,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 19 17:24:35 UTC 2018,,,,,,,,,,"0|i3hhf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/17 18:24;srowen;Your code kind of suggests you're reading this data as a different bean class -- getSleepStartTime isn't defined otherwise. Is that the explanation? then the mapping of fields would just be based on order, I think.;;;","13/Jul/17 18:35;tomron;Sorry, I was trying to hide my original code so it will be more understandable (changed it now). It reads the class it is suppose to read but when collecting it the fields are mixed. ;;;","21/Jul/17 11:52;tomron;I have a small insight regarding this possible issue as I bumped into something the appeared similar to me.
If the class we are creating instances for has a method getX when X is not a member of this class it seems that something get messed up.
E.g. - I had another class with data member which is Long and static data member of type Double, I have method of the following - 

{code:java}
public double getX() {
        return longDataMember / staticDouble;
    }

{code}

This alone was enough to mess my class and to assign the value of 103079215144 to all the longDataMember.;;;","04/Aug/17 09:48;tomron;Additional comment when there are multiple datatypes which are not easily convert to one other (e.g double <-> string) the system get crazy (as well the instruction in the error message didn't help) - 

{code:bash}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0xa) at pc=0x000000011be827bc, pid=992, tid=0x0000000000001c03
#
# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode bsd-amd64 compressed oops)
# Problematic frame:
# v  ~StubRoutines::jlong_disjoint_arraycopy
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
{code}

;;;","10/Aug/17 09:02;praetp;I can confirm this problem persists in Spark 2.2.0: fields get all swapped when you use the bean encoder on a dataset with an array of structs. A plain struct works, an array of structs does not. Pretty big issue if you ask me.

{noformat}
root
 |-- writeKey: string (nullable = false)
 |-- id: string (nullable = false)
 |-- type: string (nullable = false)
 |-- ssid: string (nullable = false)
+------------+------+--------+--------+
|writeKey    |id    |type    |ssid    |
+------------+------+--------+--------+
|someWriteKey|someId|someType|someSSID|
+------------+------+--------+--------+
{noformat}

When I convert into a struct, everything is still fine:

{noformat}
root
 |-- writeKey: string (nullable = false)
 |-- nodes: struct (nullable = false)
 |    |-- id: string (nullable = false)
 |    |-- type: string (nullable = false)
 |    |-- ssid: string (nullable = false)

+------------+--------------------------+
|writeKey    |nodes                     |
+------------+--------------------------+
|someWriteKey|[someId,someType,someSSID]|
+------------+--------------------------+
{noformat}

When I do a groupBy on writeKey and a collect_set() on the nodes, we get:

{noformat}
root
 |-- writeKey: string (nullable = false)
 |-- nodes: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- id: string (nullable = false)
 |    |    |-- type: string (nullable = false)
 |    |    |-- ssid: string (nullable = false)

+------------+----------------------------+
|writeKey    |nodes                       |
+------------+----------------------------+
|someWriteKey|[[someId,someType,someSSID]]|
+------------+----------------------------+
{noformat}

When I convert  this to Java...

{code:java}
Dataset<Row> dfArray = dfStruct.groupBy(""writeKey"")            .agg(functions.collect_set(""nodes"").alias(""nodes""));
Encoder<Topology> topologyEncoder = Encoders.bean(Topology.class);
Dataset<Topology> datasetMultiple = dfArray.as(topologyEncoder);
System.out.println(datasetMultiple.first());
{code}
This prints:

{noformat}
Topology{writeKey='someWriteKey', nodes=[Node{id='someId', type='someSSID', ssid='someType'}]}
{noformat}
You can clearly see the type and ssid fields were swapped.

POJO classes:
{code:java}
 public static class Topology {
        private String writeKey;
        private List<Node> nodes;

        public Topology() {
        }

        public String getWriteKey() {
            return writeKey;
        }

        public void setWriteKey(String writeKey) {
            this.writeKey = writeKey;
        }

        public List<Node> getNodes() {
            return nodes;
        }

        public void setNodes(List<Node> nodes) {
            this.nodes = nodes;
        }

        @Override
        public String toString() {
            return ""Topology{"" +
                    ""writeKey='"" + writeKey + '\'' +
                    "", nodes="" + nodes +
                    '}';
        }
    }

    public static class Node {
        private String id;
        private String type;
        private String ssid;

        public Node() {
        }

        public String getId() {
            return id;
        }

        public void setId(String id) {
            this.id = id;
        }

        public String getType() {
            return type;
        }

        public void setType(String type) {
            this.type = type;
        }

        public String getSsid() {
            return ssid;
        }

        public void setSsid(String ssid) {
            this.ssid = ssid;
        }


        @Override
        public String toString() {
            return ""Node{"" +
                    ""id='"" + id + '\'' +
                    "", type='"" + type + '\'' +
                    "", ssid='"" + ssid + '\'' +
                    '}';
        }
    }
{code}

;;;","10/Aug/17 10:16;praetp;It seems changing the order of the fields in the struct can give some improvements but when I add more fields, the problem just gets worse - some fields just never get filled in or twice.;;;","03/Oct/18 13:32;praetp;Still there in Spark 2.3.1.;;;","11/Oct/18 11:56;vofque;Working on this issue.

The problem can be solved by using unresolved expressions instead of resolved expressions for ArrayType and MapType when building java bean deserializer.

In this case struct data type of array elements or map's keys/values is defined during ""analysis"" and is based on actual data, not on java bean.

The same approach is currently used to define struct data types of nested structs. In case of array elements or map's keys/values struct data types are currently extracted from java beans, hence we get incorrect field order.;;;","12/Oct/18 12:07;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22708;;;","12/Oct/18 12:08;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22708;;;","16/Oct/18 11:43;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22745;;;","16/Oct/18 11:44;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22745;;;","17/Oct/18 15:41;cloud_fan;Issue resolved by pull request 22708
[https://github.com/apache/spark/pull/22708];;;","18/Oct/18 14:20;vofque;I guess, only array case has been fixed yet, the initial map case is still in progress.;;;","18/Oct/18 14:29;cloud_fan;Can you create a ticket for the map case? Basically what we need is to resolve the todo at https://github.com/apache/spark/blob/v2.4.0-rc3/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L202;;;","18/Oct/18 14:34;cloud_fan;oh this ticket reports the bug for map type. I'm reopening it. Can you create a ticket for the array type?;;;","18/Oct/18 15:08;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22767;;;","18/Oct/18 15:09;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22768;;;","18/Oct/18 15:09;apachespark;User 'vofque' has created a pull request for this issue:
https://github.com/apache/spark/pull/22768;;;","18/Oct/18 20:46;dongjoon;I cloned this JIRA issue. Can we do like the followings? [~cloud_fan] and [~vofque] and [~srowen] and [~tomron]

- SPARK-21402 (this) for `Fix java array of structs deserialization`
- SPARK-25772 (new cloned one) for the original Map issue?;;;","18/Oct/18 20:48;dongjoon;We already committed with SPARK JIRA ID, SPARK-21402. Too bad.;;;","19/Oct/18 06:07;cloud_fan;LGTM, thanks for doing it [~dongjoon] !;;;","19/Oct/18 17:24;dongjoon;Thank you for confirming, [~cloud_fan]!;;;",,,,,,,,,,,,,,,,
Spark shouldn't ignore user defined output committer in append mode,SPARK-21400,13086886,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,robert3005,robert3005,13/Jul/17 11:45,27/Jul/17 03:16,14/Jul/23 06:30,27/Jul/17 03:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"In https://issues.apache.org/jira/browse/SPARK-8578 we decided to override user defined output committers in append mode. The reasoning was that there's some output committers that can lead to correctness issues. Since then we have removed DirectParquetOutputCommitter (the biggest known offender) from codebase and rely on default implementations.

I believe that we shouldn't be restricting this anymore and users should understand that if they're overwriting this configuration they have tested their committer for correctness. This unblocks using more sophisticated and performant output committers without need to overwrite file format implementations.",,aash,apachespark,robert3005,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 27 03:15:57 UTC 2017,,,,,,,,,,"0|i3hh9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/17 11:48;apachespark;User 'robert3005' has created a pull request for this issue:
https://github.com/apache/spark/pull/18621;;;","27/Jul/17 03:15;robert3005;Fixed in [https://github.com/apache/spark/pull/18689 ];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reviving broken callable objects in UDF in PySpark,SPARK-21394,13086708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,12/Jul/17 21:31,12/Dec/22 17:51,14/Jul/23 06:30,18/Jul/17 01:04,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"After SPARK-19161, we happened to break callable objects as UDFs in Python as below:

{code}
>>> from pyspark.sql import functions
>>> class F(object):
...     def __call__(self, x):
...         return x
...
>>> foo = F()
>>> foo(1)
1
>>> udf = functions.udf(foo)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../spark/python/pyspark/sql/functions.py"", line 2142, in udf
    return _udf(f=f, returnType=returnType)
  File "".../spark/python/pyspark/sql/functions.py"", line 2133, in _udf
    return udf_obj._wrapped()
  File "".../spark/python/pyspark/sql/functions.py"", line 2090, in _wrapped
    @functools.wraps(self.func)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: F instance has no attribute '__name__'
{code}


Note that this works in Spark 2.1 as below:

{code}
>>> from pyspark.sql import functions
>>> class F(object):
...     def __call__(self, x):
...         return x
...
>>> foo = F()
>>> foo(1)
1
>>> udf = functions.udf(foo)
>>> spark.range(1).select(udf(""id"")).show()
+-----+
|F(id)|
+-----+
|    0|
+-----+
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 21:54:03 UTC 2017,,,,,,,,,,"0|i3hg6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/17 21:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.2 + YARN without spark.yarn.jars / spark.yarn.archive fails,SPARK-21384,13086457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,devaraj,holden,holden,12/Jul/17 03:42,17/May/20 18:13,14/Jul/23 06:30,20/Sep/17 23:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"In making the updated version of Spark 2.2 + YARN it seems that the auto packaging of JARS based on SPARK_HOME isn't quite working (which results in a warning anyways). You can see the build failure in travis at https://travis-ci.org/holdenk/spark-testing-base/builds/252656109 (I've reproed it locally).

This results in an exception like:


{code}
17/07/12 03:14:11 WARN ResourceLocalizationService: { file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip, 1499829249000, ARCHIVE, null } failed: File file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip does not exist
java.io.FileNotFoundException: File file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:359)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/07/12 03:14:11 WARN NMAuditLogger: USER=travis	OPERATION=Container Finished - Failed	TARGET=ContainerImpl	RESULT=FAILURE	DESCRIPTION=Container failed with state: LOCALIZATION_FAILED	APPID=application_1499829231193_0001	CONTAINERID=container_1499829231193_0001_01_000001
17/07/12 03:14:11 WARN DefaultContainerExecutor: delete returned false for path: [/home/travis/build/holdenk/spark-testing-base/target/com.holdenkarau.spark.testing.YARNCluster/com.holdenkarau.spark.testing.YARNCluster-localDir-nm-0_0/usercache/travis/filecache/11]
17/07/12 03:14:11 WARN DefaultContainerExecutor: delete returned false for path: [/home/travis/build/holdenk/spark-testing-base/target/com.holdenkarau.spark.testing.YARNCluster/com.holdenkarau.spark.testing.YARNCluster-localDir-nm-0_0/usercache/travis/filecache/11_tmp]
17/07/12 03:14:13 WARN ResourceLocalizationService: { file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip, 1499829249000, ARCHIVE, null } failed: File file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip does not exist
java.io.FileNotFoundException: File file:/tmp/spark-0dc9dd59-dd7f-48fc-be2c-11a1bbd57d70/__spark_libs__8035392745283841054.zip does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)
	at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)
	at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:359)
	at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

The work around of course is to set one of spark.yarn.jars / spark.yarn.archive.",,apachespark,devaraj,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 06 00:27:03 UTC 2017,,,,,,,,,,"0|i3hemv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/17 00:27;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/19141;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN can allocate too many executors,SPARK-21383,13086435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DjvuLee,tgraves,tgraves,12/Jul/17 02:15,17/May/20 18:14,14/Jul/23 06:30,25/Jul/17 17:24,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"The YarnAllocator doesn't properly track containers being launched but not yet running.  If it takes time to launch the containers on the NM they don't show up as numExecutorsRunning, but they are already out of the Pending list, so if the allocateResources call happens again it can think it has missing executors even when it doesn't (they just haven't been launched yet).

This was introduced by SPARK-12447 

Where it check for missing:
https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L297

Only updates the numRunningExecutors after NM has started it:
https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L524

Thus if the NM is slow or the network is slow, it can miscount and start additional executors.",,apachespark,devaraj,dongjoon,jincheng,tgraves,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21562,,,,,,,,,,,,,SPARK-12447,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 17 08:01:03 UTC 2017,,,,,,,,,,"0|i3hehz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/17 02:17;tgraves;Note we saw this with dynamic allocation off.   Its easy to reproduce if you modify runAllocatedContainers to have something like:

{noformat}
             new ExecutorRunnable(
                  Some(container),
                  conf,
                  sparkConf,
                  driverUrl,
                  executorId,
                  executorHostname,
                  executorMemory,
                  executorCores,
                  appAttemptId.getApplicationId.toString,
                  securityMgr,
                  localResources
                ).run()
                logInfo(""Delaying before updating internal to reproduce bug"")
                try {
                  Thread.sleep(10000)
                } catch {
                  case e => logError(""exception"", e)
                }
                updateInternalState()
{noformat}
;;;","17/Jul/17 08:01;apachespark;User 'djvulee' has created a pull request for this issue:
https://github.com/apache/spark/pull/18651;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jars specified with --jars or --packages are not added into AM's system classpath,SPARK-21377,13086349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,yeshavora,yeshavora,11/Jul/17 18:48,17/May/20 18:14,14/Jul/23 06:30,17/Jul/17 20:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"In this issue we have a long running Spark application with secure HBase, which requires {{HBaseCredentialProvider}} to get tokens periodically, we specify HBase related jars with {{\--packages}}, but these dependencies are not added into AM classpath, so when {{HBaseCredentialProvider}} tries to initialize HBase connections to get tokens, it will be failed.

Currently because jars specified with {{\--jars}} or {{\--packages}} are not added into AM classpath, the only way to extend AM classpath is to use ""spark.driver.extraClassPath"" which supposed to be used in yarn cluster mode.

So here we should figure out a solution  either to put these dependencies to AM classpath or to extend AM classpath with correct configuration.",,apachespark,jerryshao,tgraves,vanzin,yeshavora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 12 23:54:04 UTC 2017,,,,,,,,,,"0|i3hdzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/17 18:58;srowen;Why does that suggest that it's not finding a class? I don't see how this is separate from https://issues.apache.org/jira/browse/SPARK-21376 that you just created?;;;","11/Jul/17 19:57;jerryshao;[~srowen] this is a separate issue to SPARK-21376. In this issue we have a long running Spark application with secure HBase, which requires {{HBaseCredentialProvider}} to get tokens periodically, we specify HBase related jars with {{\--packages}}, but these dependencies are not added into AM classpath, so when {{HBaseCredentialProvider}} tries to initialize HBase connections to get tokens, it will be failed.;;;","11/Jul/17 20:02;jerryshao;SPARK-21376 and here are both security issues, but the root cause and fix is different. So that's why we created two JIRAs.;;;","11/Jul/17 21:10;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18602;;;","11/Jul/17 21:35;vanzin;bq. we specify HBase related jars with --packages, but these dependencies are not added into AM classpath

I'm a little confused about how the code in the PR is supposed to solve this. Specifying {{\-\-packages}} doesn't automatically make jars available to the AM. The files are just distributed to the cluster, but you still need code to make the AM's class loader see those jars. There's code to do that for the driver ({{ApplicationMaster.startUserApplication}}) but not for the AM ({{ApplicationMaster.runExecutorLauncher}}.;;;","11/Jul/17 21:54;jerryshao;Thanks [~vanzin] for your comment.

Your comment is correct, specifying {{\--packages}} will not add jars to AM, my original thought is to add main jar and secondary jars automatically into AM classpath, but this will break the usage of ""spark.driver.userClassPathFirst"". So my proposal is to manually specify AM extra classpath with ""spark.yarn.am.extraClassPath"" manually, for example specifying HBase classpath with this configuration. This requires HBase dependencies existed in cluster, but it may not impact user application's classpath.;;;","11/Jul/17 21:58;vanzin;bq. my original thought is to add main jar and secondary jars automatically into AM classpath, but this will break the usage of ""spark.driver.userClassPathFirst""

Can you expand on that? That config does not (or at least should not) apply to the client AM; and you shouldn't be adding the jars to the classpath, but to a new classloader that is set as the ""context class loader"" of the AM, just like it is in the case of the driver, in the method I referenced before.;;;","11/Jul/17 22:13;jerryshao;My original purpose is to add jars uploaded by distributed cache to AM classpath with ""$\{PWD\}/*"" in AM container setup both in client and cluster mode, but I guess it may affect the use of ""spark.driver.userClassPathFirst"" in cluster mode or contaminate the existing AM classpath, since now jars will also be existed in AM classpath.

So your suggestion is that we use another context loader to load these jars, and specifically used for ServiceCredentialProvider, am I right?;;;","11/Jul/17 22:17;vanzin;bq. So your suggestion is that we use another context loader to load these jars

Yes that would be my first choice. If for some reason that does not work, then you can modify the classpath like you suggest for the AM only, appending the jars (not prepending) so that they don't overwrite Spark's versions of conflicting libraries.;;;","12/Jul/17 23:54;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18616;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token is not renewed in yarn client process in cluster mode,SPARK-21376,13086348,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,yeshavora,yeshavora,11/Jul/17 18:47,17/May/20 18:14,14/Jul/23 06:30,13/Jul/17 22:26,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"STR:
* Set below config in spark-default.conf
{code}
spark.yarn.security.credentials.hbase.enabled true
spark.hbase.connector.security.credentials.enabled false{code}
* Set below config in hdfs-site.xml
{code}
'dfs.namenode.delegation.token.max-lifetime':'43200000'
'dfs.namenode.delegation.token.renew-interval':'28800000' {code}
* Run HDFSWordcount streaming app in yarn-cluster mode  for 25 hrs. 

After 25 hours, noticing that HDFS Wordcount job is hitting HDFS_DELEGATION_TOKEN renewal issue. 
{code}
17/06/28 10:49:47 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 230 for hrt_qa) is expired
17/06/28 10:49:47 WARN Client: Failed to cleanup staging dir hdfs://mycluster0/user/hrt_qa/.sparkStaging/application_1498539861056_0015
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 230 for hrt_qa) is expired
        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)
        at org.apache.hadoop.ipc.Client.call(Client.java:1498){code}
",,apachespark,jerryshao,krisden,tgraves,yeshavora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 13 04:06:03 UTC 2017,,,,,,,,,,"0|i3hdz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/17 19:50;jerryshao;I will work on this, thanks [~yeshavora].;;;","12/Jul/17 14:40;tgraves;Can you please clarify the title and description?  What do you mean by ""in yarn client process in cluster mode""?  I assume you were running in yarn cluster mode but what is the yarn client process?  the application master?;;;","12/Jul/17 16:38;jerryshao;Hi [~tgraves], it is the local yarn launcher process which will launch Spark application on yarn cluster. The problem here is that local launcher process will always keep the initial token and not get renewed, so when application is killed then local launcher process will try to delete the staging files, and using this initial token will be failed in long running scenario.;;;","12/Jul/17 18:28;tgraves;so you are referring to the org.apache.spark.launcher.SparkLauncher code that is launching a yarn cluster mode job?  or what do you mean by ""local yarn launcher process""?;;;","12/Jul/17 18:31;jerryshao;I'm referrring to o.a.s.deploy.yarn.Client this class, it will monitor yarn application and try to delete staging files when application is finished.;;;","13/Jul/17 04:06;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18617;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading globbed paths from S3 into DF doesn't work if filesystem caching is disabled,SPARK-21374,13086291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrey.t,andrey.t,andrey.t,11/Jul/17 15:24,07/Aug/17 18:21,14/Jul/23 06:30,05/Aug/17 05:41,2.0.2,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,1,,,,,,,,,"*Motivation:*
In my case I want to disable filesystem cache to be able to change S3's access key and secret key on the fly to read from buckets with different permissions. This works perfectly fine for RDDs but doesn't work for DFs.

*Example (works for RDD but fails for DataFrame):*

{code:java}
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {

    val awsAccessKeyId = ""something""
    val awsSecretKey = ""something else""

    val conf = new SparkConf().setAppName(""Simple Application"").setMaster(""local[*]"")

    val sc = new SparkContext(conf)
    sc.hadoopConfiguration.set(""fs.s3.awsAccessKeyId"", awsAccessKeyId)
    sc.hadoopConfiguration.set(""fs.s3.awsSecretAccessKey"", awsSecretKey)
    sc.hadoopConfiguration.setBoolean(""fs.s3.impl.disable.cache"",true)
    sc.hadoopConfiguration.set(""fs.s3.impl"",""org.apache.hadoop.fs.s3native.NativeS3FileSystem"")
    sc.hadoopConfiguration.set(""fs.s3.buffer.dir"",""/tmp"")

    val spark = SparkSession.builder().config(conf).getOrCreate()

    val rddFile = sc.textFile(""s3://bucket/file.csv"").count // ok
    val rddGlob = sc.textFile(""s3://bucket/*"").count // ok
    val dfFile = spark.read.format(""csv"").load(""s3://bucket/file.csv"").count // ok
    
    val dfGlob = spark.read.format(""csv"").load(""s3://bucket/*"").count 
    // IllegalArgumentExcepton. AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively)
    // of a s3 URL, or by setting the fs.s3.awsAccessKeyId or fs.s3.awsSecretAccessKey properties (respectively).
   
    sc.stop()
  }
}

{code}",,andrey.t,apachespark,stevel@apache.org,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 01 13:35:58 UTC 2017,,,,,,,,,,"0|i3hdmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/17 21:43;zsxwing;Yeah, org.apache.spark.deploy.SparkHadoopUtil.globPath uses a wrong Hadoop configuration. Welcome to submit a PR to fix it.

Right now as a workaround, you can use the following codes to set your keys:

{code}
val conf = org.apache.spark.deploy.SparkHadoopUtil.get.conf
conf.set(...)
{code};;;","13/Jul/17 11:54;apachespark;User 'andrey-tpt' has created a pull request for this issue:
https://github.com/apache/spark/pull/18623;;;","13/Jul/17 12:37;stevel@apache.org;This is possibly a sign that your new configuration isn't having its auth values picked up, or they are incorrect (i.e properties are wrong). Its working for enabled caching as some other codepath has set them up with the right properties, and so when used in the DF, the previous params are picked up.

# if using Hadoop 2.7.x JARs, switch to s3a and use s3a in the URLs & settings. You don't need to set the fs.s3a.impl field either; done for yoiu.
# if you can upgrade to Hadoop 2.8 binaries, you can use per-bucket configuration; this does exactly what you want: lets you configure different auth details for different buckets, without having to play these games. See [https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#Configuring_different_S3_buckets] and [https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.1/bk_cloud-data-access/content/s3-per-bucket-configs.html]


going to 2.8 binaries (or anything with the feature backported to a 2.7.x variant) should solve your problem without you having to worry about what you are seeing here.;;;","19/Jul/17 17:05;andrey.t;[~stevel@apache.org] 

Indeed, while working on PR and debugging the code I see that code works only accidentally because caching is turned on by default.

1. Thanks for the advice. I doubt that it's related to type of the filesystem - I've only mentioned filesystem explicitly to show why ""awsAcceesKeyId"" not ""access.key"" is used with ""s3"" scheme in the example. Sorry for the confusion. 
 
2. Unfortunately, it's not that easy - this example is only simplified version of what happens in my project. We don't have information about which buckets user will try to access in interactive mode so I can not enumerate them all in configuration. ;;;","01/Aug/17 13:35;stevel@apache.org;I understand...the patch shows the issue. Its only working in some codepaths because the (authenticated) S3 FS instance was already created and cached.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use Scala classes in external shuffle service,SPARK-21369,13086072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,10/Jul/17 20:44,17/May/20 18:13,14/Jul/23 06:30,11/Jul/17 03:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Shuffle,Spark Core,YARN,,,0,,,,,,,,,"Right now the external shuffle service uses Scala Tuple2. However, the Scala library won't be shaded into the yarn shuffle assembly jar. Then when the codes are called, it will throw ClassNotFoundException.

Right now it's safe because we disabled spark.reducer.maxReqSizeShuffleToMem by default. However,  to allow using spark.reducer.maxReqSizeShuffleToMem for Yarn users, we should remove all usages of Tuples.",,apachespark,cloud_fan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 03:27:38 UTC 2017,,,,,,,,,,"0|i3hc9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/17 20:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18593;;;","11/Jul/17 03:27;cloud_fan;Issue resolved by pull request 18593
[https://github.com/apache/spark/pull/18593];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R older version of Roxygen2 on Jenkins,SPARK-21367,13086022,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shaneknapp,felixcheung,felixcheung,10/Jul/17 17:41,13/May/19 18:16,14/Jul/23 06:30,10/May/19 16:55,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SparkR,,,,,0,,,,,,,,,"Getting this message from a recent build.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/79461/console
Warning messages:
1: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
2: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
* installing *source* package 'SparkR' ...
** R

We have been running with 5.0.1 and haven't changed for a year.
NOTE: Roxygen 6.x has some big changes and IMO we should not move to that yet.",,dongjoon,felixcheung,shaneknapp,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/17 18:38;shaneknapp;R.paks;https://issues.apache.org/jira/secure/attachment/12876674/R.paks",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 13 18:13:06 UTC 2019,,,,,,,,,,"0|i3hbz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/17 17:44;felixcheung;[~shaneknapp]
could you check? thanks!;;;","10/Jul/17 18:25;shaneknapp;we haven't ever explicitly installed any version of Roxygen2, so whatever is there was installed via deps on other packages.

that being said, i should be able to upgrade this to 5.0.0 pretty easily.  let me check w/[~shivaram] before proceeding, however.;;;","10/Jul/17 18:35;shivaram;I just checked the transitive dependencies and I think it should be fine to manually install roxygen 5.0.0;;;","10/Jul/17 18:36;shaneknapp;copy that.  installing this version now...;;;","10/Jul/17 18:37;shaneknapp;...and this is done.;;;","10/Jul/17 21:19;dongjoon;Hi, All.
After this, many builds seems to fail consecutively.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/
{code}
This patch fails SparkR unit tests.
{code};;;","10/Jul/17 21:44;shaneknapp;reverting now.;;;","10/Jul/17 21:49;shaneknapp;reverted;;;","10/Jul/17 21:49;shivaram;[~dongjoon] Do you have a particular output file that we can use to debug ?;;;","10/Jul/17 21:51;shaneknapp;ill take a closer look at this tomorrow.;;;","10/Jul/17 21:56;shaneknapp;shiv:  some of the builds that launched post-upgrade were showing a v4 version of roxygen2 being imported (from https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/79476/console)

```First time using roxygen2 4.0. Upgrading automatically...```

this build launched after the upgrade, so i'd rather roll back and un-break things and take a closer look in a little bit (i have an appt in ~30 mins).  i'll check back later this afternoon and see if anything has turned up.;;;","10/Jul/17 22:20;dongjoon;Thank you, [~shaneknapp].
It seems to be `Had CRAN check errors; see logs.`.
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/79474/console
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/79473/console
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/79472/console
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/79471/console;;;","11/Jul/17 06:01;felixcheung;I'm not sure exactly why yet, but comparing the working and non-working build

working:
{code}
First time using roxygen2 4.0. Upgrading automatically...
Writing SparkDataFrame.Rd
Writing printSchema.Rd
Writing schema.Rd
Writing explain.Rd
...
Warning messages:
1: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
2: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
* installing *source* package 'SparkR' ...
{code}

not working:
{code}
First time using roxygen2 4.0. Upgrading automatically...
There were 50 or more warnings (use warnings() to see the first 50)
* installing *source* package 'SparkR' ...
{code}

Bascially, the .Rd files are not getting created (because of warnings that are not captured)
That cause the CRAN check to fail with 
""checking for missing documentation entries ... WARNING
Undocumented code objects:
  '%<=>%' 'add_months' 'agg' 'approxCountDistinc""

(which would be completely expected - without Rd files it will not have the documentation hence the check will fail)

;;;","11/Jul/17 06:04;felixcheung;And I'm pretty sure we should build with Roxygen2 5.0.1

https://github.com/apache/spark/blob/master/R/pkg/DESCRIPTION#L60
RoxygenNote: 5.0.1;;;","11/Jul/17 06:22;felixcheung;I think I found the first error, it's one build before the build failures listed above, 79470

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/79470/console

{code}
Updating roxygen version in  /home/jenkins/workspace/SparkPullRequestBuilder/R/pkg/DESCRIPTION 
Deleting AFTSurvivalRegressionModel-class.Rd
Deleting ALSModel-class.Rd
...
There were 50 or more warnings (use warnings() to see the first 50)
{code}

Whereas this build from mid June
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/SparkPullRequestBuilder/78020/console

Does NOT have this ""Need roxygen2 >= 5.0.0 but loaded version is 4.1.1"" message in the console output;;;","11/Jul/17 18:03;shaneknapp;ok, so i dug through the old build logs and the warning started happening during the R version rollback that happened on june 20th.  here's the first build that had the warning in it:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/78353/

note the timing:  
- R accidentally upgraded @ ~7am june 20th (affected packages:  R-java-3.1.1-7, R-3.1.1-7, R-java-devel-3.1.1-7, libRmath-devel-3.1.1-7, R-core-devel-3.1.1-7, libRmath-3.1.1-7, R-core-3.1.1-7,  R-devel-3.1.1-7).
- no warnings between ~7am and ~9pm
- we started getting the Roxygen2 warnings @ ~9pm (the build linked above)
- i rolled back R ~830pm to the 3.1.1* versions

sadly, all the logs from the build failures yesterday are gone, so i'll see if i can reproduce locally on another centos box set up the same way...  and since i will need to deploy this box from scratch, it'll take a couple of hours.

Y R BUILDS SOMETIMES SO HARD?;;;","11/Jul/17 18:16;felixcheung;*SOMETIMES*? :);;;","11/Jul/17 18:28;shivaram;From [~felixcheung]'s earlier parsing of the error messages, my guess is that the knitr package probably has some issues working with roxygen2 5.0.0 ? In other words I wonder if the problem is caused due to incompatible versions across packages. 

[~shaneknapp] Could you list down all the versions of packages installed right now on Jenkins ? We can then try to use that and see if we can piece together why this problem comes up;;;","11/Jul/17 18:39;shaneknapp;attached is the output from Rscript -e 'installed.packages()';;;","11/Jul/17 18:44;shaneknapp;here's what we install during server setup:
{noformat}
Rscript -e 'install.packages(""digest"", repos=""http://cran.stat.ucla.edu/"")'
Rscript -e 'install.packages(""testthat"", repos=""http://cran.stat.ucla.edu/"")'
Rscript -e 'install.packages(""knitr"", repos=""http://cran.stat.ucla.edu/"")'
Rscript -e 'install.packages(""devtools"", repos=""http://cran.stat.ucla.edu/"")'
Rscript -e 'devtools::install_github(""jimhester/lintr"")'
Rscript -e 'install.packages(""e1071"", repos=""http://cran.stat.ucla.edu/"")'
Rscript -e 'install.packages(""rmarkdown"", repos=""http://cran.stat.ucla.edu/"")'
$YUM pandoc
$YUM pandoc-citeproc
{noformat}

note:  pandoc and pandoc-citeproc were installed via yum, but did not get updated on june 20th.
{noformat}
$ pssh -h jenkins_workers.txt -i -t 0 ""grep pandoc /var/log/yum.log""
[1] 11:43:49 [FAILURE] amp-jenkins-worker-04 Exited with error code 1
[2] 11:43:49 [FAILURE] amp-jenkins-worker-05 Exited with error code 1
[3] 11:43:49 [FAILURE] amp-jenkins-worker-07 Exited with error code 1
[4] 11:43:49 [FAILURE] amp-jenkins-worker-03 Exited with error code 1
[5] 11:43:49 [FAILURE] amp-jenkins-worker-06 Exited with error code 1
[6] 11:43:49 [FAILURE] amp-jenkins-worker-02 Exited with error code 1
[7] 11:43:49 [FAILURE] amp-jenkins-worker-08 Exited with error code 1
[8] 11:43:50 [FAILURE] amp-jenkins-worker-01 Exited with error code 1
{noformat};;;","11/Jul/17 18:55;shaneknapp;here's what version of knitr is installed:
{noformat}
knitr        ""knitr""        ""/usr/lib64/R/library"" ""1.16""       NA
knitr        ""R (>= 3.1.0)""
knitr        ""evaluate (>= 0.10), digest, highr, markdown, stringr (>= 0.6),\nyaml, methods, tools""
{noformat}

EDIT:  i actually reinstalled all of the R packages after i downgraded R to 3.1.1...  that's why the dirs are showing the june 20th modification timestamp.

-looking in /usr/lib64/R/library, i see that it got updated on june 20th:-
;;;","11/Jul/17 18:59;shivaram;Is there anyway to get more info on the pandoc error ? I think that is the root cause of the problem - Error code 1 does not seem to show up anything useful in searches;;;","11/Jul/17 19:02;shaneknapp;[~shivaram] that error code 1 is grep failing to find anything matching pandoc in /var/log/yum.log (meaning pandoc and pandoc-citeproc weren't updating when i did the system update).

here are the versions we have installed:
{noformat}
[root@amp-jenkins-worker-05 R]# rpm -qa | grep pandoc
pandoc-citeproc-0.9.1.1-1.el6.x86_64
pandoc-1.17.0.3-1.el6.x86_64
{noformat}

EDIT:  i just checked the archived yum logs and these two packages are at the same version as when installed last year.

https://issues.apache.org/jira/browse/SPARK-17420;;;","03/Aug/17 17:31;felixcheung;still seeing it

Warning messages:
1: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
2: In check_dep_version(pkg, version, compare) :
  Need roxygen2 >= 5.0.0 but loaded version is 4.1.1
* installing *source* package 'SparkR' ...

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/80213/console;;;","10/May/19 16:54;shaneknapp;after deciding to clear out my JIRA queue this morning, i found this unresolved ticket and just installed roxygen2 from source on all of the jenkins workers:


{noformat}
$ pssh -h jenkins_workers.txt -i ""/home/eecs/sknapp/r-wtf.sh""
[1] 09:54:30 [SUCCESS] amp-jenkins-worker-02
[1] ‘5.0.1’
[2] 09:54:30 [SUCCESS] amp-jenkins-worker-05
[1] ‘5.0.1’
[3] 09:54:30 [SUCCESS] amp-jenkins-worker-06
[1] ‘5.0.1’
[4] 09:54:30 [SUCCESS] amp-jenkins-worker-04
[1] ‘5.0.1’
[5] 09:54:31 [SUCCESS] amp-jenkins-worker-03
[1] ‘5.0.1’
[6] 09:54:31 [SUCCESS] amp-jenkins-worker-01
[1] ‘5.0.1’
{noformat}
;;;","12/May/19 23:08;felixcheung;great thx!;;;","13/May/19 18:13;dongjoon;Thank you, [~shaneknapp]!;;;",,,,,,,,,,,,
FileInputDStream not remove out of date RDD,SPARK-21357,13085840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lovepocky,lovepocky,lovepocky,10/Jul/17 04:31,29/Jul/17 12:28,14/Jul/23 06:30,29/Jul/17 12:27,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,DStreams,,,,,0,,,,,,,,,"The method in org.apache.spark.streaming.dstream.FileInputDSteam.clearMetadata at line 166
will not remove out of date RDDs in generatedRDDs.
This will cause leak of memory and OOM.",,apachespark,bomeng,lovepocky,Robin Shao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 29 12:27:54 UTC 2017,,,,,,,,,,"0|i3haun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/17 05:40;apachespark;User 'shaofei007' has created a pull request for this issue:
https://github.com/apache/spark/pull/18718;;;","29/Jul/17 12:27;srowen;Issue resolved by pull request 18718
[https://github.com/apache/spark/pull/18718];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INPUT FILE related functions do not support more than one sources,SPARK-21354,13085812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,09/Jul/17 22:58,17/Jul/17 06:59,14/Jul/23 06:30,17/Jul/17 06:59,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"{noformat}
hive> select *, INPUT__FILE__NAME FROM t1, t2;
FAILED: SemanticException Column INPUT__FILE__NAME Found in more than One Tables/Subqueries
{noformat}

The build-in functions {{input_file_name}}, {{input_file_block_start}}, {{input_file_block_length}} do not support more than one sources, like what Hive does. Currently, we do not block it and the outputs are ambiguous.",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 17 06:59:08 UTC 2017,,,,,,,,,,"0|i3haof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/17 23:04;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18580;;;","17/Jul/17 06:59;cloud_fan;Issue resolved by pull request 18580
[https://github.com/apache/spark/pull/18580];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the error message when the number of arguments is wrong when invoking a UDF,SPARK-21350,13085744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,09/Jul/17 03:53,11/Jul/17 03:20,14/Jul/23 06:30,11/Jul/17 03:20,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Got a confusing error message when the number of arguments is wrong when invoking a UDF. 

{noformat}
    val df = spark.emptyDataFrame
    spark.udf.register(""foo"", (_: String).length)
    df.selectExpr(""foo(2, 3, 4)"")
{noformat}
{noformat}
org.apache.spark.sql.UDFSuite$$anonfun$9$$anonfun$apply$mcV$sp$12 cannot be cast to scala.Function3
java.lang.ClassCastException: org.apache.spark.sql.UDFSuite$$anonfun$9$$anonfun$apply$mcV$sp$12 cannot be cast to scala.Function3
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.<init>(ScalaUDF.scala:109)
{noformat}
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 03:20:25 UTC 2017,,,,,,,,,,"0|i3ha9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/17 03:57;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18574;;;","11/Jul/17 03:20;cloud_fan;Issue resolved by pull request 18574
[https://github.com/apache/spark/pull/18574];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSessionBuilderSuite should clean up stopped sessions,SPARK-21345,13085596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,07/Jul/17 21:00,09/Jul/17 03:11,14/Jul/23 06:30,08/Jul/17 12:28,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,Tests,,,,0,,,,,,,,,"`SparkSessionBuilderSuite` should clean up stopped sessions. Otherwise, it leaves behind some stopped `SparkContext`s interfereing with other test suites using `ShardSQLContext`.",,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 08 20:38:04 UTC 2017,,,,,,,,,,"0|i3h9cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/17 21:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18567;;;","08/Jul/17 12:28;cloud_fan;Issue resolved by pull request 18567
[https://github.com/apache/spark/pull/18567];;;","08/Jul/17 20:38;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinaryType comparison does signed byte array comparison,SPARK-21344,13085589,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,shubhamc,shubhamc,07/Jul/17 20:15,16/Jul/17 16:45,14/Jul/23 06:30,16/Jul/17 16:45,2.0.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.1,,,SQL,,,,,0,,,,,,,,,"BinaryType used by Spark SQL defines ordering using signed byte comparisons. This can lead to unexpected behavior. Consider the following code snippet that shows this error:

{code}
case class TestRecord(col0: Array[Byte])
def convertToBytes(i: Long): Array[Byte] = {
    val bb = java.nio.ByteBuffer.allocate(8)
    bb.putLong(i)
    bb.array
  }
def test = {
    val sql = spark.sqlContext
    import sql.implicits._
    val timestamp = 1498772083037L
    val data = (timestamp to timestamp + 1000L).map(i => TestRecord(convertToBytes(i)))
    val testDF = sc.parallelize(data).toDF
    val filter1 = testDF.filter(col(""col0"") >= convertToBytes(timestamp) && col(""col0"") < convertToBytes(timestamp + 50L))
    val filter2 = testDF.filter(col(""col0"") >= convertToBytes(timestamp + 50L) && col(""col0"") < convertToBytes(timestamp + 100L))
    val filter3 = testDF.filter(col(""col0"") >= convertToBytes(timestamp) && col(""col0"") < convertToBytes(timestamp + 100L))
    assert(filter1.count == 50)
    assert(filter2.count == 50)
    assert(filter3.count == 100)
}
{code}


",,apachespark,dongjoon,kiszk,maropu,shubhamc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 08 09:30:04 UTC 2017,,,,,,,,,,"0|i3h9av:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/17 23:23;dongjoon;Hi, [~shubhamc].
Thank you for reporting. I just change the priority since `Blocker` is set by committers.;;;","07/Jul/17 23:26;dongjoon;Also, this cannot be blocker because Spark 2.0.0 seems to have the same behavior according to your test code.
{code}
scala> test
java.lang.AssertionError: assertion failed
  at scala.Predef$.assert(Predef.scala:156)
  at test(<console>:37)
  ... 48 elided

scala> sc.version
res1: String = 2.0.0
{code};;;","08/Jul/17 07:12;kiszk;I will work for this if anyone has finished a PR.;;;","08/Jul/17 07:25;dongjoon;+1 ! :);;;","08/Jul/17 07:32;srowen;Bytes are signed, as are other int types in Spark SQL / Hive. Is this not normal then?;;;","08/Jul/17 09:30;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18571;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refine the document for spark.reducer.maxReqSizeShuffleToMem,SPARK-21343,13085544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,07/Jul/17 15:55,08/Jul/17 16:28,14/Jul/23 06:30,08/Jul/17 16:28,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"In current code, reducer can break the old shuffle service when {{spark.reducer.maxReqSizeShuffleToMem}} is enabled. Let's refine document.",,apachespark,cloud_fan,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 08 16:28:49 UTC 2017,,,,,,,,,,"0|i3h91b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/17 02:33;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/18566;;;","08/Jul/17 16:28;cloud_fan;Issue resolved by pull request 18566
[https://github.com/apache/spark/pull/18566];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DownloadCallback to work well with RetryingBlockFetcher,SPARK-21342,13085538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,07/Jul/17 15:33,10/Jul/17 13:13,14/Jul/23 06:30,10/Jul/17 13:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,When {{RetryingBlockFetcher}} retries fetching blocks. There could be two {{DownloadCallbacks}} download the same content to the same target file. It could cause {{ShuffleBlockFetcherIterator}} reading a partial result.,,apachespark,cloud_fan,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 10 13:12:25 UTC 2017,,,,,,,,,,"0|i3h8zz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/17 15:39;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/18565;;;","10/Jul/17 13:12;cloud_fan;Issue resolved by pull request 18565
[https://github.com/apache/spark/pull/18565];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell --packages option does not add jars to classpath on windows,SPARK-21339,13085516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,devaraj,gblankendal,gblankendal,07/Jul/17 14:06,10/Aug/17 16:49,14/Jul/23 06:30,01/Aug/17 20:43,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Shell,Windows,,,,0,,,,,,,,,"I am unable to import symbols from dependencies specified with the packages option:

{code}
spark-shell --packages ""com.datastax.spark:spark-cassandra-connector_2.11:2.0.2"" --conf spark.jars.ivy=""c:/tmp/ivy2"" --verbose
{code}

This results in:

{code}
scala> import com.datastax.spark.connector._
<console>:23: error: object datastax is not a member of package com
       import com.datastax.spark.connector._
                  ^
{code}

NOTE: It is working as expected when running on Linux but not on Windows.

Complete verbose output:

{code}
> spark-shell --packages ""com.datastax.spark:spark-cassandra-connector_2.11:2.0.2"" --conf spark.jars.ivy=""c:/tmp/ivy2"" --
verbose
Using properties file: null
Parsed arguments:
  master                  local[*]
  deployMode              null
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          null
  driverMemory            null
  driverCores             null
  driverExtraClassPath    null
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               org.apache.spark.repl.Main
  primaryResource         spark-shell
  name                    Spark shell
  childArgs               []
  jars                    null
  packages                com.datastax.spark:spark-cassandra-connector_2.11:2.0.2
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file null:
  spark.jars.ivy -> c:/tmp/ivy2


Ivy Default Cache set to: c:\tmp\ivy2\cache
The jars for the packages stored in: c:\tmp\ivy2\jars
:: loading settings :: url = jar:file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
        confs: [default]
        found com.datastax.spark#spark-cassandra-connector_2.11;2.0.2 in local-m2-cache
        found com.twitter#jsr166e;1.1.0 in local-m2-cache
        found commons-beanutils#commons-beanutils;1.9.3 in central
        found commons-collections#commons-collections;3.2.2 in local-m2-cache
        found org.joda#joda-convert;1.2 in local-m2-cache
        found joda-time#joda-time;2.3 in central
        found io.netty#netty-all;4.0.33.Final in local-m2-cache
        found org.scala-lang#scala-reflect;2.11.8 in local-m2-cache
:: resolution report :: resolve 378ms :: artifacts dl 8ms
        :: modules in use:
        com.datastax.spark#spark-cassandra-connector_2.11;2.0.2 from local-m2-cache in [default]
        com.twitter#jsr166e;1.1.0 from local-m2-cache in [default]
        commons-beanutils#commons-beanutils;1.9.3 from central in [default]
        commons-collections#commons-collections;3.2.2 from local-m2-cache in [default]
        io.netty#netty-all;4.0.33.Final from local-m2-cache in [default]
        joda-time#joda-time;2.3 from central in [default]
        org.joda#joda-convert;1.2 from local-m2-cache in [default]
        org.scala-lang#scala-reflect;2.11.8 from local-m2-cache in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   8   |   0   |   0   |   0   ||   8   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
        confs: [default]
        0 artifacts copied, 8 already retrieved (0kB/11ms)
Main class:
org.apache.spark.repl.Main
Arguments:

System properties:
spark.jars.ivy -> c:/tmp/ivy2
SPARK_SUBMIT -> true
spark.app.name -> Spark shell
spark.jars -> file:/c:/tmp/ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar,file:/c:/tmp/ivy2/jars/com.twitter_jsr166e-1.1.0.jar,file:/c:/tmp/ivy2/jars/commons-beanutils_commons-beanutils-1.9.3.jar,file:/c:/tmp/ivy2/jars/org.joda_joda-convert-1.2.jar,file:/c:/tmp/ivy2/jars/joda-time_joda-time-2.3.jar,file:/c:/tmp/ivy2/jars/io.netty_netty-all-4.0.33.Final.jar,file:/c:/tmp/ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar,file:/c:/tmp/ivy2/jars/commons-collections_commons-collections-3.2.2.jar
spark.submit.deployMode -> client
spark.master -> local[*]
Classpath elements:
c:\tmp\ivy2\jars\com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar
c:\tmp\ivy2\jars\com.twitter_jsr166e-1.1.0.jar
c:\tmp\ivy2\jars\commons-beanutils_commons-beanutils-1.9.3.jar
c:\tmp\ivy2\jars\org.joda_joda-convert-1.2.jar
c:\tmp\ivy2\jars\joda-time_joda-time-2.3.jar
c:\tmp\ivy2\jars\io.netty_netty-all-4.0.33.Final.jar
c:\tmp\ivy2\jars\org.scala-lang_scala-reflect-2.11.8.jar
c:\tmp\ivy2\jars\commons-collections_commons-collections-3.2.2.jar


Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/07 15:45:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/07 15:45:28 WARN General: Plugin (Bundle) ""org.datanucleus"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-core-3.2.10.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar.""
17/07/07 15:45:28 WARN General: Plugin (Bundle) ""org.datanucleus.api.jdo"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar.""
17/07/07 15:45:28 WARN General: Plugin (Bundle) ""org.datanucleus.store.rdbms"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar.""
17/07/07 15:45:31 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://192.168.56.1:4040
Spark context available as 'sc' (master = local[*], app id = local-1499435127578).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.

scala>

scala> import com.datastax.spark.connector._
<console>:23: error: object datastax is not a member of package com
       import com.datastax.spark.connector._
                  ^
{code}

The behaviour is different when we add the downloaded jar explicitly to the classpath with spark.driver.extraClassPath

{code}
spark-shell --conf spark.driver.extraClassPath=""c:\tmp\ivy2\jars\com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar"" --verbose
{code}

Complete output:
{code}
spark-shell --conf spark.driver.extraClassPath=""c:\tmp\ivy2\jars\com.data
stax.spark_spark-cassandra-connector_2.11-2.0.2.jar"" --verbose
Using properties file: null
Parsed arguments:
  master                  local[*]
  deployMode              null
  executorMemory          null
  executorCores           null
  totalExecutorCores      null
  propertiesFile          null
  driverMemory            null
  driverCores             null
  driverExtraClassPath    c:\tmp\ivy2\jars\com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar
  driverExtraLibraryPath  null
  driverExtraJavaOptions  null
  supervise               false
  queue                   null
  numExecutors            null
  files                   null
  pyFiles                 null
  archives                null
  mainClass               org.apache.spark.repl.Main
  primaryResource         spark-shell
  name                    Spark shell
  childArgs               []
  jars                    null
  packages                null
  packagesExclusions      null
  repositories            null
  verbose                 true

Spark properties used, including those specified through
 --conf and those from the properties file null:
  spark.driver.extraClassPath -> c:\tmp\ivy2\jars\com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar


Main class:
org.apache.spark.repl.Main
Arguments:

System properties:
SPARK_SUBMIT -> true
spark.app.name -> Spark shell
spark.jars ->
spark.submit.deployMode -> client
spark.master -> local[*]
spark.driver.extraClassPath -> c:\tmp\ivy2\jars\com.datastax.spark_spark-cassandra-connector_2.11-2.0.2.jar
Classpath elements:



Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/07/07 16:05:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/07 16:05:18 WARN General: Plugin (Bundle) ""org.datanucleus"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-core-3.2.10.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar.""
17/07/07 16:05:18 WARN General: Plugin (Bundle) ""org.datanucleus.api.jdo"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-api-jdo-3.2.6.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar.""
17/07/07 16:05:18 WARN General: Plugin (Bundle) ""org.datanucleus.store.rdbms"" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar"" is already registered, and you are trying to register an identical plugin located at URL ""file:/C:/hadoop/spark-2.1.1-bin-hadoop2.7/bin/../jars/datanucleus-rdbms-3.2.9.jar.""
17/07/07 16:05:21 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://192.168.56.1:4040
Spark context available as 'sc' (master = local[*], app id = local-1499436317287).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import com.datastax.spark.connector._
import com.datastax.spark.connector._

{code}
",Windows 10 Enterprise x64,apachespark,devaraj,gblankendal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18648,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Jul 21 21:59:03 UTC 2017,,,,,,,,,,"0|i3h8v3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/17 21:59;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/18708;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AggregatedDialect doesn't override isCascadingTruncateTable() method,SPARK-21338,13085492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxing,ogonchar,ogonchar,07/Jul/17 11:35,20/Sep/17 04:56,14/Jul/23 06:30,19/Sep/17 16:28,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"org.apache.spark.sql.jdbc.JdbcDialect's method:
*def isCascadingTruncateTable(): Option[Boolean] = None*
is not overriden in org.apache.spark.sql.jdbc.AggregatedDialect class.
Because of this issue - *+when you add  more than one dialect Spark doesn't truncate table because isCascadingTruncateTable always returns default None for Aggregated Dialect+*.
Can be tracked when you write dataset in jdbc mode with SaveMode.Overwrite and extra JdbcDialect added via JdbcDialects.registerDialect(...); method.
Then inside of org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider on line 65 in createRelation method it will never get to line 67 even if isTruncate returns true.
It will only get inside if you have one JdbcDialect and AggregatedDialect is not used.
Fix would be to overrite this method in AggregatedDialect and also to reduce value (I guess with and clause same as in overriden getHandle method) e.g. 
override def isCascadingTruncateTable(): Option[Boolean]=
    Some(dialects.map(_.isCascadingTruncateTable()).reduce(_ && _))",,apachespark,ogonchar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 20 04:56:03 UTC 2017,,,,,,,,,,"0|i3h8pr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/17 06:29;ogonchar;Can anyone check this issue?;;;","17/Sep/17 00:33;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/19256;;;","20/Sep/17 04:56;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/19286;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support un-aliased subquery,SPARK-21335,13085401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Jul/17 04:02,27/Jun/18 03:41,14/Jul/23 06:30,07/Jul/17 12:05,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,release-notes,,,,,,,,"un-aliased subquery is supported by Spark SQL for a long time. Its semantic was not well defined and has confusing behaviors, and it's not a standard SQL syntax, so we disallowed it in https://issues.apache.org/jira/browse/SPARK-20690 .

However, this is a breaking change, and we do have existing queries using un-aliased subquery. We should add the support back and fix its semantic.

After the fix, there is no syntax change from branch 2.2 to master, but we invalid a weird use case:
{{SELECT v.i from (SELECT i FROM v)}}. Now this query will throw analysis exception because users should not be able to use the qualifier inside a subquery.",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20690,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 27 03:41:05 UTC 2018,,,,,,,,,,"0|i3h85j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/17 04:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18559;;;","07/Jul/17 12:05;cloud_fan;Issue resolved by pull request 18559
[https://github.com/apache/spark/pull/18559];;;","27/Jun/18 03:41;apachespark;User 'cnZach' has created a pull request for this issue:
https://github.com/apache/spark/pull/21647;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
joinWith documents and analysis allow invalid join types,SPARK-21333,13085360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coreywoodfield,coreywoodfield,coreywoodfield,07/Jul/17 00:03,14/Dec/20 17:49,14/Jul/23 06:30,19/Jul/17 22:23,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Documentation,,,,,0,,,,,,,,,"joinWith returns a Dataset<Tuple2>, but left_semi and left_anti (which only return things from one side of the join) are listed as acceptable join types in the documentation, and an error is not thrown until spark gets to the joining part, rather than in the analysis phase. These join types should be removed from the documentation, and this error should be caught earlier",,apachespark,coreywoodfield,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,SPARK-33778,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 00:06:03 UTC 2017,,,,,,,,,,"0|i3h7wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/17 00:06;apachespark;User 'coreywoodfield' has created a pull request for this issue:
https://github.com/apache/spark/pull/18462;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result type inferred for some decimal expressions,SPARK-21332,13085354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,ashkapsky,ashkapsky,06/Jul/17 23:42,12/Dec/17 18:56,14/Jul/23 06:30,18/Jul/17 04:10,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.1,2.3.0,,SQL,,,,,0,correctness,,,,,,,,"Decimal expressions do not always follow the type inference rules explained in DecimalPrecision.scala.  An incorrect result type is produced when the expressions contains  more than 2 decimals.

For example:
spark-sql> CREATE TABLE Decimals(decimal_26_6 DECIMAL(26,6)); 
...
spark-sql> describe decimals;
...
decimal_26_6	decimal(26,6)	NULL
spark-sql> explain select decimal_26_6 * decimal_26_6 from decimals;
...
== Physical Plan ==
*Project [CheckOverflow((decimal_26_6#99 * decimal_26_6#99), DecimalType(38,12)) AS (decimal_26_6 * decimal_26_6)#100]
+- HiveTableScan [decimal_26_6#99], MetastoreRelation default, decimals

However:
spark-sql> explain select decimal_26_6 * decimal_26_6 * decimal_26_6 from decimals;
...
== Physical Plan ==
*Project [CheckOverflow((cast(CheckOverflow((decimal_26_6#104 * decimal_26_6#104), DecimalType(38,12)) as decimal(26,6)) * decimal_26_6#104), DecimalType(38,12)) AS ((decimal_26_6 * decimal_26_6) * decimal_26_6)#105]
+- HiveTableScan [decimal_26_6#104], MetastoreRelation default, decimals

The expected result type is DecimalType(38,18).

In Hive 1.1.0:
hive> explain select decimal_26_6 * decimal_26_6 from decimals;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimals
          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
          Select Operator
            expressions: (decimal_26_6 * decimal_26_6) (type: decimal(38,12))
            outputColumnNames: _col0
            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
            ListSink

Time taken: 0.772 seconds, Fetched: 17 row(s)
hive> explain select decimal_26_6 * decimal_26_6 * decimal_26_6 from decimals;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimals
          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
          Select Operator
            expressions: ((decimal_26_6 * decimal_26_6) * decimal_26_6) (type: decimal(38,18))
            outputColumnNames: _col0
            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
            ListSink

Time taken: 0.064 seconds, Fetched: 17 row(s)",,aokolnychyi,apachespark,ashkapsky,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22755,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 10 08:44:04 UTC 2017,,,,,,,,,,"0|i3h7vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/17 10:09;aokolnychyi;I know the root cause and will submit a PR soon.;;;","10/Jul/17 08:44;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/18583;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad partitioning does not allow to read a JDBC table with extreme values on the partition column,SPARK-21330,13085225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,parmesan,parmesan,06/Jul/17 13:09,12/Dec/22 18:10,14/Jul/23 06:30,04/Aug/17 07:58,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"When using ""extreme"" values in the partition column (like having a randomly generated long number) overflow might happen, leading to the following warning message:

{code}WARN JDBCRelation: The number of partitions is reduced because the specified number of partitions is less than the difference between upper bound and lower bound. Updated number of partitions: -1559072469251914524; Input number of partitions: 20; Lower bound: -7701345953623242445; Upper bound: 9186325650834394647.{code}

When this happens, no data is read from the table.

This happens because of the following check in {{org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation.scala}}:
{code}if ((upperBound - lowerBound) >= partitioning.numPartitions){code}

Funny thing is that we worry about overflows a few lines later:
{code}    // Overflow and silliness can happen if you subtract then divide.
    // Here we get a little roundoff, but that's (hopefully) OK.{code}

A better check would be:
{code}if ((upperBound - partitioning.numPartitions) >= lowerBound){code}",,a1ray,parmesan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 04 07:58:44 UTC 2017,,,,,,,,,,"0|i3h733:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/17 19:19;a1ray;https://github.com/apache/spark/pull/18800;;;","02/Aug/17 19:21;gurwls223;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/18800;;;","04/Aug/17 07:58;srowen;Issue resolved by pull request 18800
[https://github.com/apache/spark/pull/18800];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayConstructor should handle an array of typecode 'l' as long rather than int in Python 2.,SPARK-21327,13085140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,06/Jul/17 09:13,07/Jul/17 05:05,14/Jul/23 06:30,07/Jul/17 05:05,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"Currently {{ArrayConstructor}} handles an array of typecode {{'l'}} as {{int}} when converting Python object in Python 2 into Java object, so if the value is larger than {{Integer.MAX_VALUE}} or smaller than {{Integer.MIN_VALUE}} then the overflow occurs.

{code}
import array
data = [Row(l=array.array('l', [-9223372036854775808, 0, 9223372036854775807]))]
df = spark.createDataFrame(data)
df.show(truncate=False)
{code}

{code}
+----------+
|l         |
+----------+
|[0, 0, -1]|
+----------+
{code}

This should be:

{code}
+----------------------------------------------+
|l                                             |
+----------------------------------------------+
|[-9223372036854775808, 0, 9223372036854775807]|
+----------------------------------------------+
{code}
",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 05:05:56 UTC 2017,,,,,,,,,,"0|i3h6k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/17 09:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18553;;;","07/Jul/17 05:05;ueshin;Issue resolved by pull request 18553
[https://github.com/apache/spark/pull/18553];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeExternalRowSorter.RowComparator memory leak,SPARK-21319,13084853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,jebaker,jebaker,05/Jul/17 14:20,02/Jan/18 19:27,14/Jul/23 06:30,27/Jul/17 15:02,2.0.0,2.1.0,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When we wish to sort within partitions, we produce an UnsafeExternalRowSorter. This contains an UnsafeExternalSorter, which contains the UnsafeExternalRowComparator.

The UnsafeExternalSorter adds a task completion listener which performs any additional required cleanup. The upshot of this is that we maintain a reference to the UnsafeExternalRowSorter.RowComparator until the end of the task.

The RowComparator looks like

{code:java}
  private static final class RowComparator extends RecordComparator {
    private final Ordering<InternalRow> ordering;
    private final int numFields;
    private final UnsafeRow row1;
    private final UnsafeRow row2;

    RowComparator(Ordering<InternalRow> ordering, int numFields) {
      this.numFields = numFields;
      this.row1 = new UnsafeRow(numFields);
      this.row2 = new UnsafeRow(numFields);
      this.ordering = ordering;
    }

    @Override
    public int compare(Object baseObj1, long baseOff1, Object baseObj2, long baseOff2) {
      // TODO: Why are the sizes -1?
      row1.pointTo(baseObj1, baseOff1, -1);
      row2.pointTo(baseObj2, baseOff2, -1);
      return ordering.compare(row1, row2);
    }
}
{code}

which means that this will contain references to the last baseObjs that were passed in, and without tracking them for purposes of memory allocation.

We have a job which sorts within partitions and then coalesces partitions - this has a tendency to OOM because of the references to old UnsafeRows that were used during the sorting.

Attached is a screenshot of a memory dump during a task - our JVM has two executor threads.

It can be seen that we have 2 references inside of row iterators, and 11 more which are only known in the task completion listener or as part of memory management.",,apachespark,cloud_fan,jebaker,kiszk,robert3005,schlosna,wkinney,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/17 14:27;jebaker;0001-SPARK-21319-Fix-memory-leak-in-UnsafeExternalRowSort.patch;https://issues.apache.org/jira/secure/attachment/12875770/0001-SPARK-21319-Fix-memory-leak-in-UnsafeExternalRowSort.patch","05/Jul/17 14:20;jebaker;hprof.png;https://issues.apache.org/jira/secure/attachment/12875769/hprof.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 02 19:27:22 UTC 2018,,,,,,,,,,"0|i3h4sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/17 14:27;jebaker;This is my proposed fix for this issue.;;;","05/Jul/17 14:34;apachespark;User 'j-baker' has created a pull request for this issue:
https://github.com/apache/spark/pull/18543;;;","19/Jul/17 08:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18679;;;","27/Jul/17 15:02;cloud_fan;Issue resolved by pull request 18679
[https://github.com/apache/spark/pull/18679];;;","02/Jan/18 19:27;wkinney;Is there a workaround for this for version 2.2.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeRow writeToStream has incorrect offsetInByteArray calculation for non-zero offset,SPARK-21312,13084684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,swale,swale,swale,05/Jul/17 06:51,06/Jul/17 06:50,14/Jul/23 06:30,06/Jul/17 06:48,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,0,,,,,,,,,"The following code in UnsafeRow has an incorrect offset calculation:

{code}
  public void writeToStream(OutputStream out, byte[] writeBuffer) throws IOException {
    if (baseObject instanceof byte[]) {
      int offsetInByteArray = (int) (Platform.BYTE_ARRAY_OFFSET - baseOffset);
{code}

which should be:

{code}
      int offsetInByteArray = (int) (baseOffset - Platform.BYTE_ARRAY_OFFSET);
{code}
",,apachespark,cloud_fan,swale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 06:48:57 UTC 2017,,,,,,,,,,"0|i3h3rb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/17 07:08;apachespark;User 'sumwale' has created a pull request for this issue:
https://github.com/apache/spark/pull/18535;;;","06/Jul/17 06:48;cloud_fan;Issue resolved by pull request 18535
[https://github.com/apache/spark/pull/18535];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OneVsRest Conceals Columns That May Be Relevant To Underlying Classifier,SPARK-21306,13084593,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,facai,cathalgarvey,cathalgarvey,04/Jul/17 16:00,12/Dec/22 18:10,14/Jul/23 06:30,28/Jul/17 02:24,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.1,2.3.0,,ML,,,,,0,classification,ml,,,,,,,"Hi folks, thanks for Spark! :)

I've been learning to use `ml` and `mllib`, and I've encountered a block while trying to use `ml.classification.OneVsRest` with `ml.classification.LogisticRegression`. Basically, [here in the code|https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala#L320], only two columns are being extracted and fed to the underlying classifiers.. however with some configurations, more than two columns are required.

Specifically: I want to do multiclass learning with Logistic Regression, on a very imbalanced dataset. In my dataset, I have lots of imbalances, so I was planning to use weights. I set a column, `""weight""`, as the inverse frequency of each field, and I configured my `LogisticRegression` class to use this column, then put it in a `OneVsRest` wrapper.

However, `OneVsRest` strips all but two columns out of a dataset before training, so I get an error from within `LogisticRegression` that it can't find the `""weight""` column.

It would be nice to have this fixed! I can see a few ways, but a very conservative fix would be to include a parameter in `OneVsRest.fit` for additional columns to `select` before passing to the underlying model.

Thanks!",,apachespark,cathalgarvey,facai,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:19:53 UTC 2017,,,,,,,,,,"0|i3h37b:",9223372036854775807,,,,,yanboliang,,,,,,,,2.3.0,,,,,,,,,,,"05/Jul/17 08:21;mlnick;This is definitely an issue. I don't think it is an issue with columns other than {{weight}}, right? 

The easy fix would be to simply use the entire dataset in the {{fit}} logic, though that would probably result in caching unnecessary data. If it is only the weight col that is really impacted, we could just add a check for that param on the classifier and if present, select the weight column also.;;;","06/Jul/17 05:39;facai;I agree with [~mlnick]. It seems that we will get in trouble only when `weight` is expected.

How about adding a `setWeightCol` like LogisticRegression?;;;","06/Jul/17 05:44;facai;[~cathalgarvey] By the way, since LogisticRegression in ml supports multi-class directly by using multinomial loss, I believe that it might be better than OneVsRest.;;;","06/Jul/17 08:45;cathalgarvey;Hi [~mlnick] & [~facai] - Passing-through the weights column would fix my issue, yes. I'd be content with that.

I'm only concerned that it's a patch over an issue that may continue to trouble future classifiers that may be written. Perhaps a mention in the documentation that it filters columns would help others diagnose this if it ever surfaces again?

Thanks for your advice about using LogisticRegression for multiclass without using OVR; I'll try it ""bare"" and see what happens. :)

Thanks for the help!;;;","06/Jul/17 09:59;apachespark;User 'facaiy' has created a pull request for this issue:
https://github.com/apache/spark/pull/18554;;;","02/Aug/17 19:19;gurwls223;User 'facaiy' has created a pull request for this issue:
https://github.com/apache/spark/pull/18763;;;","02/Aug/17 19:19;gurwls223;User 'facaiy' has created a pull request for this issue:
https://github.com/apache/spark/pull/18764;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalMapToCatalyst should null-check map key prior to converting to internal value.,SPARK-21300,13084439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,04/Jul/17 07:08,05/Jul/17 03:26,14/Jul/23 06:30,05/Jul/17 03:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,{{ExternalMapToCatalyst}} should null-check map key prior to converting to internal value to throw an appropriate Exception instead of something like NPE.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 07:14:04 UTC 2017,,,,,,,,,,"0|i3h293:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/17 07:14;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18524;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot use Int.MIN_VALUE as Spark SQL fetchsize,SPARK-21287,13084233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wwg28103,maver1ck,maver1ck,03/Jul/17 12:33,25/Oct/19 00:11,14/Jul/23 06:30,24/Oct/19 19:36,2.1.1,2.2.3,2.3.4,2.4.4,3.0.0,,,,,,,,,,,,,,,,,,,,,,2.4.5,3.0.0,,,,SQL,,,,,2,,,,,,,,,"MySQL JDBC driver gives possibility to not store ResultSet in memory.
We can do this by setting fetchSize to Int.MIN_VALUE.
Unfortunately this configuration isn't correct in Spark.
{code}
java.lang.IllegalArgumentException: requirement failed: Invalid value `-2147483648` for parameter `fetchsize`. The minimum value is 0. When the value is 0, the JDBC driver ignores the value and does the estimates.
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:105)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:34)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:125)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:166)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:206)
	at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
{code}

https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-implementation-notes.html",,apachespark,bestcastor,dongjoon,jincheng,maver1ck,smilegator,wwg28103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 24 19:36:24 UTC 2019,,,,,,,,,,"0|i3h0zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/17 12:51;apachespark;User 'maver1ck' has created a pull request for this issue:
https://github.com/apache/spark/pull/18515;;;","03/Jul/17 12:54;srowen;Yeah, I'm familiar with this special value.

Is it not equivalent to setting fetch size to 1? or 0? I don't think so, but I don't recall why. Would it work in your situation and therefore be the right way to do this for, likely, most callers?

I can see an argument for removing this assertion, but, the JDBC API says that negative values aren't allowed. This is really nonstandard MySQL behavior, and it'd be good to find a better way to recommend to users.;;;","03/Jul/17 13:04;maver1ck;No.  It's not the same like setting 0 or 1.
Every other value makes MySQL JDBC driver to store whole ResultSet in memory.

Maybe we can just remove this assertion ?
I think this is not so super popular configuration option.;;;","03/Jul/17 13:10;srowen;It's not supposed to do that right -- you're saying the MySQL driver doesn't quite use the value as intended?
Hm, so any MySQL user would have the whole resultset pulled back into memory every time? that seems like a problem, yeah, but surely MySQL has fixed this?
If not, I could see this being a big enough deal that it has to be allowed as a value even if it is wrong.;;;","03/Jul/17 13:59;maver1ck;Quote
{quote}
By default, ResultSets are completely retrieved and stored in memory. In most cases this is the most efficient way to operate and, due to the design of the MySQL network protocol, is easier to implement. If you are working with ResultSets that have a large number of rows or large values and cannot allocate heap space in your JVM for the memory required, you can tell the driver to stream the results back one row at a time.
{quote}
https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-implementation-notes.html;;;","03/Jul/17 14:14;srowen;I know, but this is what fetch size is supposed to control of course. It need not be all or nothing. I am not sure why they didn't just let a fetch size of 1 select this behavior. Does any nonpositive value actually get this behavior? That would be nice.;;;","03/Jul/17 16:46;smilegator;This value is very specific to MySQL. Since we are supporting different dialects, we could introduce a dialect-specific checking logics.;;;","03/Jul/17 17:30;srowen;Yeah, maybe a method to validate or even set the fetch size that is overridden by the MySQL dialect. That could be used where {{stmt.setFetchSize(options.fetchSize)}} is called in JDBCRDD. And then remove the validation in JDBCOptions.;;;","28/Jan/19 19:57;bestcastor;Do we have any following up for this issue? MySQL is a tier-one database. ;;;","28/Jan/19 19:59;smilegator;[~bestcastor] Feel free to submit a PR ;;;","28/Jan/19 20:01;bestcastor;[~smilegator] Ok. Anything we should follow up to submit the PR? ;;;","23/Oct/19 15:02;wwg28103;[~smilegator]  [~srowen] Just submitted a PR for this : [https://github.com/apache/spark/pull/26230]

Please help review.;;;","24/Oct/19 19:36;dongjoon;Issue resolved by pull request 26244
[https://github.com/apache/spark/pull/26244];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
rename SessionCatalog.registerFunction parameter name,SPARK-21284,13084164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Jul/17 07:53,03/Jul/17 17:52,14/Jul/23 06:30,03/Jul/17 17:52,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 03 07:56:03 UTC 2017,,,,,,,,,,"0|i3h0k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/17 07:56;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileOutputStream should be created as append mode,SPARK-21283,13084112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,03/Jul/17 04:11,04/Jul/17 01:17,14/Jul/23 06:30,04/Jul/17 01:17,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"`FileAppender` is used to write `stderr` and `stdout` files  in `ExecutorRunner`.
 But before writing `ErrorStream` into the the `stderr` file, the header information has been written into ,if  FileOutputStream is  not created as append mode, the  header information will be lost",,10110346,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 01:17:03 UTC 2017,,,,,,,,,,"0|i3h08n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/17 04:13;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18507;;;","04/Jul/17 01:17;cloud_fan;Issue resolved by pull request 18507
[https://github.com/apache/spark/pull/18507];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failure in 2.0,SPARK-21282,13084107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,03/Jul/17 03:15,03/Jul/17 05:29,14/Jul/23 06:30,03/Jul/17 05:29,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,,,,,SQL,,,,,0,,,,,,,,,"There is a test failure after backporting a fix from 2.2 to 2.0, because the automatically generated column names are different. 

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-branch-2.0-test-maven-hadoop-2.2/lastCompletedBuild/testReport/
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 03 05:29:22 UTC 2017,,,,,,,,,,"0|i3h07j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/17 03:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18506;;;","03/Jul/17 05:29;cloud_fan;Issue resolved by pull request 18506
[https://github.com/apache/spark/pull/18506];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot create empty typed array column,SPARK-21281,13084072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,revolucion09,revolucion09,02/Jul/17 19:22,08/Jul/17 06:07,14/Jul/23 06:30,08/Jul/17 06:07,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Hi all

I am running this piece of code


{code:java}
val data = spark.read.parquet(""somedata.parquet"")
data.withColumn(""my_new_column"", array().cast(""array<string>"")).show
{code}

and it works fine

{code:java}
+------+---------+--------------------+---+
|itemid|sentiment|                text|my_new_column|
+------+---------+--------------------+---+
|     1|        0|                 ...| []|
|     2|        0|                 ...| []|
|     3|        1|              omg...| []|
|     4|        0|          .. Omga...| []|
{code}

but when I do

{code:java}
val data = spark.read.parquet(""somedata.parquet"")
import org.apache.spark.sql.types._
data.withColumn(""my_new_column"", array().cast(""array<int>"").show
{code}

I get:

{code:java}
scala.MatchError: NullType (of class org.apache.spark.sql.types.NullType$)
  at org.apache.spark.sql.catalyst.expressions.Cast.castToInt(Cast.scala:264)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:433)
  at org.apache.spark.sql.catalyst.expressions.Cast.castArray(Cast.scala:380)
  at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:437)
  at org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:447)
  at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:447)
  at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:449)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:325)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:50)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:43)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionDown$1(QueryPlan.scala:248)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:258)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:262)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:262)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:267)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:267)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:43)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:42)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:42)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:41)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)
  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2791)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
  ... 50 elided
{code}",,apachespark,kiszk,maropu,revolucion09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 03 15:22:03 UTC 2017,,,,,,,,,,"0|i3gzzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/17 01:44;maropu;You want to do this?
{code}
scala> spark.range(1).withColumn(""c"", lit(Array.empty[Int])).printSchema
root
 |-- id: long (nullable = false)
 |-- c: array (nullable = false)
 |    |-- element: integer (containsNull = true)


scala> spark.range(1).withColumn(""c"", lit(Array.empty[Int])).show
+---+---+
| id|  c|
+---+---+
|  0| []|
+---+---+
{code};;;","03/Jul/17 01:47;maropu;Honestly, I'm not sure the query (array with no argument) in the ticket is valid one. map has also the same behaviour with array  (elementType in NullType).
{code}
scala> spark.range(1).withColumn(""c"", array()).printSchema
root
 |-- id: long (nullable = false)
 |-- c: array (nullable = false)
 |    |-- element: null (containsNull = false)


scala> spark.range(1).withColumn(""c"", map()).printSchema
root
 |-- id: long (nullable = false)
 |-- c: map (nullable = false)
 |    |-- key: null
 |    |-- value: null (valueContainsNull = false)
{code};;;","03/Jul/17 06:42;revolucion09;[~maropu] That works, for array. Thanks, but looks like it won't for Map


{code:java}
scala> data.withColumn(""foo"", lit(Map.empty[String, String])).show
java.lang.RuntimeException: Unsupported literal type class scala.collection.immutable.Map$EmptyMap$ Map()
  at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:75)
  at org.apache.spark.sql.functions$.lit(functions.scala:101)
  ... 48 elided

{code}
;;;","03/Jul/17 06:56;maropu;yea, that's known issue. In master, this has been fixed and you can do this;
{code}
scala> spark.range(1).withColumn(""c"", typedLit(Map.empty[Int, Int])).printSchema
root
 |-- id: long (nullable = false)
 |-- c: map (nullable = false)
 |    |-- key: integer
 |    |-- value: integer (valueContainsNull = false)


scala> spark.range(1).withColumn(""c"", typedLit(Map.empty[Int, Int])).show
+---+-----+
| id|    c|
+---+-----+
|  0|Map()|
+---+-----+
{code};;;","03/Jul/17 15:22;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18516;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to Py4J 0.10.6,SPARK-21278,13084037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Jul/17 04:18,09/May/18 18:04,14/Jul/23 06:30,05/Jul/17 23:36,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.2,2.3.0,,,PySpark,,,,,0,,,,,,,,,"This issue aims to bump Py4J in order to fix the following float/double bug.
Py4J 0.10.5 fixes this (https://github.com/bartdag/py4j/issues/272).

{code}
>>> df = spark.range(1)
>>> df.select(df['id'] + 17.133574204226083).show()
+--------------------+
|(id + 17.1335742042)|
+--------------------+
|       17.1335742042|
+--------------------+
{code}

{code}
>>> df = spark.range(1)
>>> df.select(df['id'] + 17.133574204226083).show()
+-------------------------+
|(id + 17.133574204226083)|
+-------------------------+
|       17.133574204226083|
+-------------------------+
{code}",,apachespark,dongjoon,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22337,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 05 23:36:10 UTC 2017,,,,,,,,,,"0|i3gzrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/17 04:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18502;;;","04/Jul/17 04:34;dongjoon;Due to [another Py4J bug|https://github.com/bartdag/py4j/issues/278] on 0.10.5, we should wait until Py4J 0.10.6.
;;;","04/Jul/17 14:36;dongjoon;0.10.6 will be release at the end of this week. I'll try with 0.10.6.;;;","05/Jul/17 18:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18546;;;","05/Jul/17 23:36;holden;Issue resolved by pull request 18546
[https://github.com/apache/spark/pull/18546];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeJoin LeftAnti does not update numOutputRows,SPARK-21272,13083944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,30/Jun/17 22:29,10/Jul/17 16:35,14/Jul/23 06:30,10/Jul/17 16:35,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"Output rows metric not updated in one of the branches.
PR pending.",,apachespark,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 30 22:34:04 UTC 2017,,,,,,,,,,"0|i3gz7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/17 22:34;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/18494;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsafeRow.hashCode assertion when sizeInBytes not multiple of 8,SPARK-21271,13083887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,bograd,bograd,30/Jun/17 18:35,27/Jul/17 07:28,14/Jul/23 06:30,27/Jul/17 07:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"The method is:

{code}
public int hashCode() {
    return Murmur3_x86_32.hashUnsafeWords(baseObject, baseOffset, sizeInBytes, 42);
  }
{code}

but sizeInBytes is not always a multiple of 8 (in which case hashUnsafeWords throws assertion) - for example here: {code}FixedLengthRowBasedKeyValueBatch.appendRow{code}

The fix could be to use hashUnsafeBytes or to use hashUnsafeWords but on a prefix that is multiple of 8.
",,apachespark,bograd,cloud_fan,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 27 07:27:57 UTC 2017,,,,,,,,,,"0|i3gyuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/17 01:30;kiszk;In {{UnsafeRow}}, the length of fixed part should be a multiple of 8. However, the length of variable part may not require this regulation.
[~cloud_fan] What do you think?;;;","01/Jul/17 01:53;cloud_fan;We do have this regulation for var-length part in UnsafeRow, because var-length data is always word-aligned. This is also why we store the length information, otherwise we can just calculate the length by subtracting 2 adjacent offsets.;;;","01/Jul/17 02:32;kiszk;I see. For var-length part, its regulation (or specification) is a multiple of 4. Should we use {{hashUnsafeBytes}} instead of {{Murmur3_x86_32.hashUnsafeWords}} for {{UnsafeRow.hashCode()}}?
;;;","01/Jul/17 02:39;cloud_fan;For word-aligned I mean 8-bytes aligned, so the size of var-lengh data is always a multiple of 8.;;;","01/Jul/17 02:56;kiszk;I see. This issue comes from the violation such as [a code|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java#L65] (e.g. length + 4). Should we detect all of these code pieces by inserting an {{assert(length % 8 == 0)}} at {{pointTo()}}?;;;","01/Jul/17 03:01;cloud_fan;yea we should. BTW the code seems wrong to be, the length of value row should be {{vlen}} instead of {{vlen + 4}};;;","01/Jul/17 03:17;kiszk;I see. I will work for this.
Thank you for letting us how to fix it. I has been thinking about ""+ 4"". I also saw the similar code in [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java#L68];;;","02/Jul/17 08:32;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18503;;;","27/Jul/17 07:27;cloud_fan;Issue resolved by pull request 18503
[https://github.com/apache/spark/pull/18503];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Omitting columns with 'how' specified in join in PySpark throws NPE,SPARK-21264,13083697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,30/Jun/17 08:06,12/Dec/22 17:51,14/Jul/23 06:30,04/Jul/17 02:35,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,,,,,0,,,,,,,,,"{code}
>>> spark.conf.set(""spark.sql.crossJoin.enabled"", ""false"")
>>> spark.range(1).join(spark.range(1), how=""inner"").show()
Traceback (most recent call last):
...
py4j.protocol.Py4JJavaError: An error occurred while calling o66.join.
: java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.join(Dataset.scala:931)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...

>>> spark.conf.set(""spark.sql.crossJoin.enabled"", ""true"")
>>> spark.range(1).join(spark.range(1), how=""inner"").show()
...
py4j.protocol.Py4JJavaError: An error occurred while calling o84.join.
: java.lang.NullPointerException
	at org.apache.spark.sql.Dataset.join(Dataset.scala:931)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
...
{code}

Omitting columns as above throws an exception.

This works in 2.0.2:

{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.0.2
      /_/

Using Python version 2.7.10 (default, Jul 30 2016 19:40:32)
SparkSession available as 'spark'.
>>> spark.range(1).join(spark.range(1), how=""inner"").show()
+---+---+
| id| id|
+---+---+
|  0|  0|
+---+---+
{code}

but looks not from Spark 2.1.0.

It sounds a trivial small regression:

",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14761,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 02:35:38 UTC 2017,,,,,,,,,,"0|i3gxon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/17 08:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18484;;;","04/Jul/17 02:35;ueshin;Issue resolved by pull request 18484
[https://github.com/apache/spark/pull/18484];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberFormatException is not thrown while converting an invalid string to float/double,SPARK-21263,13083694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,Navya Krishnappa,Navya Krishnappa,30/Jun/17 07:40,12/Dec/22 18:11,14/Jul/23 06:30,11/Jul/17 10:11,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Java API,,,,,0,,,,,,,,,"When reading a below-mentioned data by specifying user-defined schema, exception is not thrown. Refer the details :

*Data:* 
'PatientID','PatientName','TotalBill'
'1000','Patient1','10u000'
'1001','Patient2','30000'
'1002','Patient3','40000'
'1003','Patient4','50000'
'1004','Patient5','60000'

*Source code*: 
Dataset dataset = sparkSession.read().schema(schema)
.option(INFER_SCHEMA, ""true"")
.option(DELIMITER, "","")
.option(QUOTE, ""\"""")
.option(MODE, Mode.PERMISSIVE)
.csv(sourceFile);

When we collect the dataset data: 
dataset.collectAsList();

*Schema1*: 
[StructField(PatientID,IntegerType,true), StructField(PatientName,StringType,true), StructField(TotalBill,IntegerType,true)]
*Result *: Throws NumerFormatException 
Caused by: java.lang.NumberFormatException: For input string: ""10u000""

*Schema2*: 
[StructField(PatientID,IntegerType,true), StructField(PatientName,StringType,true), StructField(TotalBill,DoubleType,true)]
*Actual Result*: 
""PatientID"": 1000,
""NumberOfVisits"": ""400"",
""TotalBill"": 10,
*Expected Result*: Should throw NumberFormatException for input string ""10u000""",,apachespark,falaki,Navya Krishnappa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 10:11:22 UTC 2017,,,,,,,,,,"0|i3gxnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/17 08:01;srowen;This is because:

{code}
java.text.NumberFormat.getInstance.parse(""10u000"")
foo: Number = 10
{code}

This is the code path used to parse a double in this case, and the Java libaries will just stop parsing if they can't interpret the rest of the input.

[~hyukjin.kwon] did you write it this way to account for varying decimal separators? instead of just {{.toDouble}} or something? it's hard-coded to a US locale though.;;;","30/Jun/17 09:48;gurwls223;The code path should be here - https://github.com/apache/spark/blob/0b903caef3183c5113feb09995874f6a07aa6698/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/UnivocityParser.scala#L121-L126

I believe this is there since it is a thrid party in the first place - https://github.com/databricks/spark-csv/commit/21b3d6233514894911147343be87df0cd42000eb;;;","30/Jun/17 09:53;gurwls223;Hm.. losing data sounds not quite good to me but I am unclear if we should call it a bug, in which case I believe we can fix it.;;;","30/Jun/17 10:56;srowen;CC [~falaki] as well for the original code

Yeah, tough one. The original code is trying to handle Locale, as I expected. The Spark version does not as (for other good reasons) it is not sensitive to the machine's locale.

I think the right behavior is therefore to fail on this type of input. I think it's more a fix than behavior change, IMHO, because getting ""10"" out of ""10u000"" silently doesn't sound like a good idea.

We could use {{.toDouble}}. We can also keep the current code but check whether it consumed all the input by checking {{ParsePosition}} afterwards. I note that, for example, the current code would parse ""10e3"" as ""10"", whereas {{.toDouble}} would parse as 10000.0. So using the latter does introduce small behavior changes, but again, it seems less surprising to parse that correctly as scientific notation, like standard JVM parsing routines would?;;;","03/Jul/17 10:28;Navya Krishnappa;[~sowen] & [~hyukjin.kwon] Thanks for your comments. Let me know the resolution for this issue.;;;","03/Jul/17 10:34;srowen;I believe I favor taking the small behavior change to fix the more important parsing problem, and change these to use .toDouble and .toFloat;;;","04/Jul/17 22:54;falaki;[~sowen] note that user specified the mode to be PERMISSIVE. In this mode CSV data source will try to ignore errors and return some result. If the mode is FAILFAST, it should throw an exception. I see the permissiveness of different modes as follows:

{code}
PERMISSIVE > DROPMALFORMED > FAILFAST
{code}

Here we have different behavior for {{IntegerType}} vs. {{DoubleType}}. That needs to be fixed and behavior should be consistent.;;;","04/Jul/17 23:08;gurwls223;Let me propose a change soon but Hm.. [~falaki], however, isn't it unrelated with the parse mode as it is parsed correctly but the parsed one lost data partially?

{code}
scala> spark.read.schema(""a DOUBLE"").option(""mode"", ""FAILFAST"").csv(Seq(""10u12"").toDS).show()
+----+
|   a|
+----+
|10.0|
+----+
{code}

{code}
scala> spark.read.schema(""a INT"").option(""mode"", ""FAILFAST"").csv(Seq(""10u12"").toDS).show()
...
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.
...
Caused by: java.lang.NumberFormatException: For input string: ""10u12""
{code};;;","04/Jul/17 23:35;gurwls223;Ah, sorry, the original JIRA describes why it does not throw an exception. Yes, it should return {{null}} instead.;;;","04/Jul/17 23:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18532;;;","05/Jul/17 06:10;srowen;[~hyukjin.kwon] [~falaki] Yeah I saw ""PERMISSIVE"" but I don't think that comes into play here. There is actually no parsing error at all, and ""10u000"" is considered correctly parsed as ""10"".;;;","11/Jul/17 10:11;srowen;Issue resolved by pull request 18532
[https://github.com/apache/spark/pull/18532];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Window result incorrect using complex object with spilling,SPARK-21258,13083604,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,hvanhovell,hvanhovell,29/Jun/17 21:37,19/Jul/17 22:10,14/Jul/23 06:30,30/Jun/17 04:35,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,hvanhovell,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 05 16:40:53 UTC 2017,,,,,,,,,,"0|i1u552:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"29/Jun/17 22:13;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/18470;;;","30/Jun/17 04:35;cloud_fan;Issue resolved by pull request 18470
[https://github.com/apache/spark/pull/18470];;;","05/Jul/17 16:40;yhuai;Since this change is not in branch-2.1, I am removing 2.1.2 from the list of fix versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when creating encoder for enum,SPARK-21255,13083492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mike0sv,mike0sv,mike0sv,29/Jun/17 15:27,12/Dec/22 18:11,14/Jul/23 06:30,25/Aug/17 06:23,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Java API,,,,,0,,,,,,,,,"When you try to create an encoder for Enum type (or bean with enum property) via Encoders.bean(...), it fails with NullPointerException at TypeToken:495.
I did a little research and it turns out, that in JavaTypeInference:126 following code 

{code:java}
val beanInfo = Introspector.getBeanInfo(typeToken.getRawType)
val properties = beanInfo.getPropertyDescriptors.filterNot(_.getName == ""class"")
val fields = properties.map { property =>
          val returnType = typeToken.method(property.getReadMethod).getReturnType
          val (dataType, nullable) = inferDataType(returnType)
          new StructField(property.getName, dataType, nullable)
        }
(new StructType(fields), true)
{code}

filters out properties named ""class"", because we wouldn't want to serialize that. But enum types have another property of type Class named ""declaringClass"", which we are trying to inspect recursively. Eventually we try to inspect ClassLoader class, which has property ""defaultAssertionStatus"" with no read method, which leads to NPE at TypeToken:495.

I think adding property name ""declaringClass"" to filtering will resolve this.","org.apache.spark:spark-core_2.10:2.1.0
org.apache.spark:spark-sql_2.10:2.1.0",apachespark,mike0sv,shengzhixia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23862,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 04 16:44:48 UTC 2018,,,,,,,,,,"0|i3gwfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/17 15:55;srowen;Is the change to omit declaringClass too?;;;","29/Jun/17 22:49;mike0sv;Yes, but there may be other problems (at least I cannot guarantee there won't be), since it seems like enums were never used before.;;;","30/Jun/17 08:07;srowen;[~mike0sv] OK, feel free to open a PR to exclude it and ideally a test case to exercise this path. That may be all there is to it.
Also CC [~hyukjin.kwon] on this one as I think you may have touched this code last.;;;","30/Jun/17 09:09;gurwls223;Thanks for cc'ing me. It looks this was introduced long time ago in the first place.
I double checked {{declaringClass}} and https://docs.oracle.com/javase/8/docs/api/java/lang/Enum.html and it works if we filter out. It sounds good to me too.

The reproducer I used is as below:

{code}
  public enum A {
    B(""www.google.com"");

    private String url;

    private A(String url) {
      this.url = url;
    }

    public String getUrl() {
      return url;
    }

    public void setUrl(String url) {
      this.url = url;
    }
  }

  @Test
  public void testEnum() {
    List<A> data = Arrays.asList(A.B);
    spark.createDataFrame(data, A.class).show();
  }
{code}

After the change:

{code}
+--------------+
|           url|
+--------------+
|www.google.com|
+--------------+
{code}
;;;","30/Jun/17 13:48;apachespark;User 'mike0sv' has created a pull request for this issue:
https://github.com/apache/spark/pull/18488;;;","25/Aug/17 06:23;srowen;Issue resolved by pull request 18488
[https://github.com/apache/spark/pull/18488];;;","28/Aug/17 06:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19066;;;","28/Aug/17 06:38;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19066;;;","04/Apr/18 16:44;shengzhixia;created a follow up issue to add the java enum support in scala api: SPARK-23862;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History UI: Taking over 1 minute for initial page display,SPARK-21254,13083481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,2ooom,2ooom,2ooom,29/Jun/17 14:46,12/Dec/22 18:10,14/Jul/23 06:30,04/Aug/17 07:20,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Web UI,,,,,0,,,,,,,,,"Currently on the first page load (if there is no limit set) the whole jobs execution history is loaded since the begging of the time. On large amount of rows returned (10k+) page load time grows dramatically, causing 1min+ delay in Chrome and freezing the process in Firefox, Safari and IE.
A simple inspection in Chrome shows that network is not an issue here and only causes a small latency (<1s) while most of the time is spend in UI  processing the results according to chrome devtools:
!screenshot-1.png!

",,2ooom,ajbozarth,apachespark,devaraj,vanzin,zhouyejoe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18085,,,,,,,,,,,,,,"29/Jun/17 14:46;2ooom;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12875058/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 16:54:04 UTC 2017,,,,,,,,,,"0|i3gwd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/17 15:56;srowen;Duplicate of lots of JIRAs related to making the initial read faster;;;","30/Jun/17 08:27;2ooom;Hi [~srowen], could you please provide tickets number for the similar issues, we made a fix in our own fork https://github.com/criteo-forks/spark/commit/a2a22f52d08ff9fb5736220853af2f31160a5ebe and I'm trying to push it to master branch.;;;","30/Jun/17 08:58;srowen;I linked the umbrella above?;;;","30/Jun/17 09:50;2ooom;Apologies didn't notice attached ticket.
I agree, SPARK-18085 will solves problem on a larger scale (fixing the HTTP API and major rework of the UI), while the bug identified here is specific to frontend templating, making parsing of large number of jobs very inefficient. Which is actually quite easy to fix (detaching grid from document before all modifications are done). And If later implementation of SPARK-18085 will solve the issue in a better way I think that there is still a benefit from the quick win meanwhile.

We actually raised the issue and [resolved|https://github.com/criteo-forks/spark/commit/a2a22f52d08ff9fb5736220853af2f31160a5ebe] it internally in our own fork since the history page usage was unbearable and hugely decreasing productivity on daily basis as the size grew up to 13+k rows. I just wanted to submit PR to the master of apache/spark repo, but I was told it won't be accepted without a ticket.

So maybe we could keep the ticket just for sake of submitting a PR?;;;","30/Jun/17 10:57;srowen;Do none of the sub-tickets overlap with this? CC [~vanzin] who is leading all that;;;","30/Jun/17 12:13;2ooom;Doesn't seem that there is direct overlap with any of the sub-issues. It seems like the the whole SPARK-18085 is just attacking the problem of history page slowness though server rework. I think this ticket could actually go under umbrella and contribute to solution via client-side improvements;;;","07/Jul/17 18:55;vanzin;I've re-opened this because right, there's no specific task currently tracking this. The work in SPARK-18085 doesn't directly affect this - the part of the code that handles this is not really touched by that work.

In fact this could be fixed independently of SPARK-18085.;;;","02/Aug/17 19:20;gurwls223;User '2ooom' has created a pull request for this issue:
https://github.com/apache/spark/pull/18783;;;","04/Aug/17 07:20;srowen;Issue resolved by pull request 18783
[https://github.com/apache/spark/pull/18783];;;","28/Aug/17 16:54;apachespark;User '2ooom' has created a pull request for this issue:
https://github.com/apache/spark/pull/18860;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot fetch big blocks to disk ,SPARK-21253,13083428,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,yumwang,yumwang,29/Jun/17 10:51,30/Jun/17 13:29,14/Jul/23 06:30,30/Jun/17 02:58,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Spark *cluster* can reproduce, *local* can't:

1. Start a spark context with {{spark.reducer.maxReqSizeShuffleToMem=1K}}:
{code:actionscript}
$ spark-shell --conf spark.reducer.maxReqSizeShuffleToMem=1K --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
{code}

2. A shuffle:
{code:actionscript}
scala> sc.parallelize(0 until 3000000, 10).repartition(2001).count()
{code}

The error messages:

{noformat}
org.apache.spark.shuffle.FetchFailedException: Failed to send request for 1649611690367_2 to yhd-jqhadoop166.int.yihaodian.com/10.17.28.166:7337: java.io.IOException: Connection reset by peer
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:442)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:418)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:59)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
        at scala.collection.AbstractIterator.to(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to send request for 1649611690367_2 to yhd-jqhadoop166.int.yihaodian.com/10.17.28.166:7337: java.io.IOException: Connection reset by peer
        at org.apache.spark.network.client.TransportClient.lambda$stream$1(TransportClient.java:196)
        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)
        at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)
        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)
        at io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:163)
        at io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:93)
        at io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:28)
        at org.apache.spark.network.client.TransportClient.stream(TransportClient.java:183)
        at org.apache.spark.network.shuffle.OneForOneBlockFetcher$1.onSuccess(OneForOneBlockFetcher.java:123)
        at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:176)
        at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
        at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
{noformat}

",,apachespark,cloud_fan,glenn.strycker@gmail.com,kiszk,yumwang,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19659,,,,,,,,,,,,,,"29/Jun/17 11:50;yumwang;ui-thread-dump-jqhadoop221-154.gif;https://issues.apache.org/jira/secure/attachment/12875043/ui-thread-dump-jqhadoop221-154.gif",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 30 03:47:04 UTC 2017,,,,,,,,,,"0|i3gw1b:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"29/Jun/17 10:56;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/18466;;;","29/Jun/17 11:55;yumwang;It may be hang for a {{spark-sql}} application also:

!ui-thread-dump-jqhadoop221-154.gif!;;;","29/Jun/17 19:01;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18467;;;","29/Jun/17 19:53;zsxwing;[~q79969786] did you run Spark 2.2.0-rcX on Yarn which has a Spark 2.1.* shuffle service?;;;","29/Jun/17 22:26;yumwang;I checked it, all jars are latest 2.2.0-rcX.;;;","30/Jun/17 00:01;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18472;;;","30/Jun/17 02:58;cloud_fan;Issue resolved by pull request 18472
[https://github.com/apache/spark/pull/18472];;;","30/Jun/17 03:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.sql.kafka010.KafkaSourceSuite.assign from specific offsets (failOnDataLoss: true),SPARK-21248,13083313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,28/Jun/17 23:02,06/Jul/17 01:29,14/Jul/23 06:30,06/Jul/17 01:29,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"{code}
org.scalatest.exceptions.TestFailedException:  Stream Thread Died: null java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)  scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)  scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)  scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)  org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)  org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)  org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)  org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:108)  org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:335)   == Progress ==    AssertOnQuery(<condition>, )    CheckAnswer: [-20],[-21],[-22],[0],[1],[2],[11],[12],[22]    StopStream    StartStream(ProcessingTime(0),org.apache.spark.util.SystemClock@6c63901,Map())    CheckAnswer: [-20],[-21],[-22],[0],[1],[2],[11],[12],[22]    AddKafkaData(topics = Set(topic-7), data = WrappedArray(30, 31, 32, 33, 34), message = )    CheckAnswer: [-20],[-21],[-22],[0],[1],[2],[11],[12],[22],[30],[31],[32],[33],[34]    StopStream  == Stream == Output Mode: Append Stream state: not started Thread state: dead  java.lang.InterruptedException  at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)  at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)  at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)  at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)  at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)  at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:108)  at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:335)  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:375)  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)   == Sink == 0: [-20] [-21] [-22] [22] [11] [12] [0] [1] [2] 1: [30] 2: [33] [31] [32] [34]   == Plan ==   
{code}


See https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6/3173/testReport/junit/org.apache.spark.sql.kafka010/KafkaSourceSuite/assign_from_specific_offsets__failOnDataLoss__true_/",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 28 23:07:04 UTC 2017,,,,,,,,,,"0|i3gvbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/17 23:07;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18461;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InSet incorrect handling of structs,SPARK-21228,13082826,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,27/Jun/17 13:50,08/Jul/17 12:15,14/Jul/23 06:30,06/Jul/17 17:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"In InSet it's possible that hset contains GenericInternalRows while child returns UnsafeRows (and vice versa). InSet uses hset.contains (both in doCodeGen and eval) which will always be false in this case.

The following code reproduces the problem:
{code}
spark.conf.set(""spark.sql.optimizer.inSetConversionThreshold"", ""2"") // the default is 10 which requires a longer query text to repro

spark.range(1, 10).selectExpr(""named_struct('a', id, 'b', id) as a"").createOrReplaceTempView(""A"")

sql(""select * from (select min(a) as minA from A) A where minA in (named_struct('a', 1L, 'b', 1L),named_struct('a', 2L, 'b', 2L),named_struct('a', 3L, 'b', 3L))"").show // the Aggregate here will return UnsafeRows while the list of structs that will become hset will be GenericInternalRows
+----+
|minA|
+----+
+----+
{code}

In.doCodeGen uses compareStructs and seems to work. In.eval might not work but not sure how to reproduce.

{code}
spark.conf.set(""spark.sql.optimizer.inSetConversionThreshold"", ""3"") // now it will not use InSet
sql(""select * from (select min(a) as minA from A) A where minA in (named_struct('a', 1L, 'b', 1L),named_struct('a', 2L, 'b', 2L),named_struct('a', 3L, 'b', 3L))"").show

+-----+
| minA|
+-----+
|[1,1]|
+-----+
{code}

Solution could be either to do safe<->unsafe conversion in InSet or not trigger InSet optimization at all in this case.
Need to investigate if In.eval is affected.
",,apachespark,bograd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 11:49:03 UTC 2017,,,,,,,,,,"0|i3gsbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/17 14:46;bograd;I tested manually (since there is no flag to disable codegen for expressions) that In.eval also fails, so only In.doCodeGen appears correct.;;;","27/Jun/17 15:06;bograd;InSubquery.doCodeGen is using InSet directly (although InSubquery itself is never used) so a fix should consider this too.;;;","28/Jun/17 13:02;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/18455;;;","07/Jul/17 11:49;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/18563;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
decrease the Mem using for variable 'tasks' in function resourceOffers,SPARK-21225,13082765,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yzg37166,yzg37166,yzg37166,27/Jun/17 09:29,29/Jun/17 12:56,14/Jul/23 06:30,29/Jun/17 12:54,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"    In the function 'resourceOffers', It declare a variable 'tasks' for storage the tasks which have  allocated a executor. It declared like this:
*{color:#d04437}val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores)){color}*

But, I think this code only conside a situation for that one task per core. If the user config the ""spark.task.cpus"" as 2 or 3, It really don't need so much space. I think It can motify as follow:

{color:#14892c}*val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](Math.ceil(o.cores*1.0/CPUS_PER_TASK).toInt))*{color}",,apachespark,cloud_fan,yzg37166,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 29 12:54:34 UTC 2017,,,,,,,,,,"0|i3gry7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/17 11:17;apachespark;User 'JackYangzg' has created a pull request for this issue:
https://github.com/apache/spark/pull/18434;;;","27/Jun/17 11:44;apachespark;User 'JackYangzg' has created a pull request for this issue:
https://github.com/apache/spark/pull/18435;;;","29/Jun/17 12:54;cloud_fan;Issue resolved by pull request 18435
[https://github.com/apache/spark/pull/18435];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread-safety issue in FsHistoryProvider ,SPARK-21223,13082730,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gostop_zlx,gostop_zlx,gostop_zlx,27/Jun/17 07:24,30/Jun/17 18:29,14/Jul/23 06:30,30/Jun/17 18:28,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Currently, Spark HistoryServer use a HashMap named fileToAppInfo in class FsHistoryProvider to store the map of eventlog path and attemptInfo. 
When use ThreadPool to Replay the log files in the list and merge the list of old applications with new ones, multi thread may update fileToAppInfo at the same time, which may cause Thread-safety issues.
{code:java}
for (file <- logInfos) {
       tasks += replayExecutor.submit(new Runnable {
        override def run(): Unit = mergeApplicationListing(file)
     })
 }
{code}
",,apachespark,gostop_zlx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21078,,,,,,,,,,,,,,"29/Jun/17 03:34;gostop_zlx;historyserver_jstack.txt;https://issues.apache.org/jira/secure/attachment/12875011/historyserver_jstack.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 30 18:28:54 UTC 2017,,,,,,,,,,"0|i3grqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/17 07:40;apachespark;User 'zenglinxi0615' has created a pull request for this issue:
https://github.com/apache/spark/pull/18430;;;","27/Jun/17 11:39;srowen;[~gostop_zlx] this overlaps a lot with SPARK-21078. Can you look at that one and possibly address the comment there too?;;;","28/Jun/17 10:41;gostop_zlx;[~sowen] ok, i will check SPARK-21078 first.;;;","29/Jun/17 03:37;gostop_zlx;BTW, this cause an infinite loop problem when we restart historyserver and replaying event logs of spark apps.;;;","30/Jun/17 18:28;srowen;Issue resolved by pull request 18430
[https://github.com/apache/spark/pull/18430];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task retry occurs on same executor due to race condition with blacklisting,SPARK-21219,13082643,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ericvandenbergfb,ericvandenbergfb,ericvandenbergfb,26/Jun/17 21:41,17/May/20 17:46,14/Jul/23 06:30,10/Jul/17 06:41,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"When a task fails it is (1) added into the pending task list and then (2) corresponding black list policy is enforced (ie, specifying if it can/can't run on a particular node/executor/etc.)  Unfortunately the ordering is such that retrying the task could assign the task to the same executor, which, incidentally could be shutting down and immediately fail the retry.   Instead the order should be (1) the black list state should be updated and then (2) the task assigned, ensuring that the black list policy is properly enforced.

The attached logs demonstrate the race condition.

See spark_executor.log.anon:

1. Task 55.2 fails on the executor

17/06/20 13:25:07 ERROR Executor: Exception in task 55.2 in stage 5.0 (TID 39575)
java.lang.OutOfMemoryError: Java heap space

2. Immediately the same executor is assigned the retry task:

17/06/20 13:25:07 INFO CoarseGrainedExecutorBackend: Got assigned task 39651
17/06/20 13:25:07 INFO Executor: Running task 55.3 in stage 5.0 (TID 39651)

3. The retry task of course fails since the executor is also shutting down due to the original task 55.2 OOM failure.

See the spark_driver.log.anon:

The driver processes the lost task 55.2:

17/06/20 13:25:07 WARN TaskSetManager: Lost task 55.2 in stage 5.0 (TID 39575, foobar####.masked-server.com, executor attempt_foobar####.masked-server.com-####_####_####_####.masked-server.com-####_####_####_####_0): java.lang.OutOfMemoryError: Java heap space

The driver then receives the ExecutorLostFailure for the retry task 55.3 (although it's obfuscated in these logs, the server info is same...)

17/06/20 13:25:10 WARN TaskSetManager: Lost task 55.3 in stage 5.0 (TID 39651, foobar####.masked-server.com, executor attempt_foobar####.masked-server.com-####_####_####_####.masked-server.com-####_####_####_####_0): ExecutorLostFailure (executor attempt_foobar####.masked-server.com-####_####_####_####.masked-server.com-####_####_####_####_0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
",,apachespark,cloud_fan,ericvandenbergfb,jsoltren,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/17 21:42;ericvandenbergfb;spark_driver.log.anon;https://issues.apache.org/jira/secure/attachment/12874566/spark_driver.log.anon","26/Jun/17 21:42;ericvandenbergfb;spark_executor.log.anon;https://issues.apache.org/jira/secure/attachment/12874567/spark_executor.log.anon",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 22:58:04 UTC 2017,,,,,,,,,,"0|i3gr7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/17 22:27;apachespark;User 'ericvandenbergfb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18427;;;","10/Jul/17 06:41;cloud_fan;Issue resolved by pull request 18427
[https://github.com/apache/spark/pull/18427];;;","11/Jul/17 22:13;jsoltren;I think it would be good to backport this to 2.2 and 2.1. Any objections? I'll start putting together backport PRs shortly, assuming there are none.;;;","11/Jul/17 22:58;apachespark;User 'jsoltren' has created a pull request for this issue:
https://github.com/apache/spark/pull/18604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming DataFrames fail to join with Hive tables,SPARK-21216,13082609,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,26/Jun/17 18:57,27/Jul/18 17:22,14/Jul/23 06:30,28/Jun/17 17:46,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"The following code will throw a cryptic exception:

{code}
import org.apache.spark.sql.execution.streaming.MemoryStream
    import testImplicits._

    implicit val _sqlContext = spark.sqlContext

    Seq((1, ""one""), (2, ""two""), (4, ""four"")).toDF(""number"", ""word"").createOrReplaceTempView(""t1"")
    // Make a table and ensure it will be broadcast.
    sql(""""""CREATE TABLE smallTable(word string, number int)
          |ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
          |STORED AS TEXTFILE
        """""".stripMargin)

    sql(
      """"""INSERT INTO smallTable
        |SELECT word, number from t1
      """""".stripMargin)

    val inputData = MemoryStream[Int]
    val joined = inputData.toDS().toDF()
      .join(spark.table(""smallTable""), $""value"" === $""number"")

    val sq = joined.writeStream
      .format(""memory"")
      .queryName(""t2"")
      .start()
    try {
      inputData.addData(1, 2)

      sq.processAllAvailable()
    } finally {
      sq.stop()
    }
{code}

If someone creates a HiveSession, the planner in `IncrementalExecution` doesn't take into account the Hive scan strategies",,apachespark,brkyvz,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 27 17:22:54 UTC 2018,,,,,,,,,,"0|i3gqzr:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"26/Jun/17 19:56;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/18426;;;","27/Jul/18 17:22;rspitzer;For anyone else searching, this also fixes custom Spark Strategies added via spark.sql.extensions not being applied in a Structured Streaming Context.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeException with Set and Case Class in Spark 2.1.1,SPARK-21204,13082344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,leoromanovsky,leoromanovsky,24/Jun/17 23:36,20/Aug/17 08:50,14/Jul/23 06:30,06/Jul/17 17:08,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Optimizer,SQL,,,,0,,,,,,,,,"When attempting to produce a Dataset containing a Set, such as with:

{code:java}
dbData
  .groupBy(""userId"")
  .agg(functions.collect_set(""friendId"") as ""friendIds"")
  .as[(Int, Set[Int])]
{code}

An exception occurs. This can be avoided by casting to a Seq, but sometimes it makes more logical sense have a Set, especially when using the collect_set aggregation operation. Additionally, I am unable to write this Dataset to a Cassandra table containing a Set column without first converting to an RDD.


{code:java}
[error] Exception in thread ""main"" java.lang.UnsupportedOperationException: No Encoder found for Set[Int]
[error] - field (class: ""scala.collection.immutable.Set"", name: ""_2"")
[error] - root class: ""scala.Tuple2""
[error] 	at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:602)
[error] 	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$9.apply(ScalaReflection.scala:596)
[error] 	at org.apache.spark.sql.catalyst.ScalaReflection$$anonfun$9.apply(ScalaReflection.scala:587)
[error] 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
[error] 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
[error] 	at scala.collection.immutable.List.foreach(List.scala:381)
[error] 	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:252)
[error] 	at scala.collection.immutable.List.flatMap(List.scala:344)
[error] 	at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:587)
[error] 	at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:425)
[error] 	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:71)
[error] 	at org.apache.spark.sql.Encoders$.product(Encoders.scala:275)
[error] 	at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:49)
{code}


I think the resolution to this might be similar to adding the Map type - https://github.com/apache/spark/pull/16986",,apachespark,cloud_fan,kiszk,leoromanovsky,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17414,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:08:24 UTC 2017,,,,,,,,,,"0|i3gpd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/17 10:17;maropu;I think the query is invalid:
{code}
scala> Seq((1, 1), (1, 2)).toDF(""id"", ""value"")
  .groupBy(""id"")
  .agg(functions.collect_set(""value"") as ""value"")
  .printSchema

root
 |-- id: integer (nullable = false)
 |-- value: array (nullable = true)
 |    |-- element: integer (containsNull = true)
{code}
You want to cast array to set?;;;","25/Jun/17 10:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18416;;;","26/Jun/17 02:11;viirya;[~maropu] I think It's more likely to turn to use set as the domain object of the Dataset. Currently SparkSQL encoders don't support Set.;;;","26/Jun/17 02:27;maropu;ok, I'll check your pr later. Thanks!;;;","06/Jul/17 17:08;cloud_fan;Issue resolved by pull request 18416
[https://github.com/apache/spark/pull/18416];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong results of insertion of Array of Struct,SPARK-21203,13082284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,24/Jun/17 03:27,19/Jul/17 22:11,14/Jul/23 06:30,24/Jun/17 14:37,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
      spark.sql(
        """"""
          |CREATE TABLE `tab1`
          |(`custom_fields` ARRAY<STRUCT<`id`: BIGINT, `value`: STRING>>)
          |USING parquet
        """""".stripMargin)
      spark.sql(
        """"""
          |INSERT INTO `tab1`
          |SELECT ARRAY(named_struct('id', 1, 'value', 'a'), named_struct('id', 2, 'value', 'b'))
        """""".stripMargin)

      spark.sql(""SELECT custom_fields.id, custom_fields.value FROM tab1"").show()
{noformat}

The returned result is wrong:
{noformat}
Row(Array(2, 2), Array(""b"", ""b""))
{noformat}
",,apachespark,kiszk,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 24 03:38:04 UTC 2017,,,,,,,,,,"0|i3gp07:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"24/Jun/17 03:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18412;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
releaseAllLocksForTask should synchronize the whole method,SPARK-21188,13082003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coyotehills,liufeng.ee@gmail.com,liufeng.ee@gmail.com,23/Jun/17 05:20,29/Jun/17 23:03,14/Jul/23 06:30,29/Jun/17 23:03,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Block Manager,Spark Core,,,,0,,,,,,,,,"Since the objects readLocksByTask, writeLocksByTask and infos are coupled and supposed to be modified by other threads concurrently, all the read and writes of them in the releaseAllLocksForTask method should be protected by a single synchronized block. The fine-grained synchronization in the current code can cause some test flakiness.",,apachespark,liufeng.ee@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 23 05:32:03 UTC 2017,,,,,,,,,,"0|i3gn93:",9223372036854775807,,,,,joshrosen,,,,,,,,,,,,,,,,,,,"23/Jun/17 05:32;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18400;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suppress memory leak errors reported by netty,SPARK-21181,13081832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Dhruve Ashar,Dhruve Ashar,Dhruve Ashar,22/Jun/17 16:23,30/Aug/19 21:43,14/Jul/23 06:30,23/Jun/17 17:37,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,Input/Output,,,,,0,,,,,,,,,"We are seeing netty report memory leak erros like the one below after switching to 2.1. 

{code}
ERROR ResourceLeakDetector: LEAK: ByteBuf.release() was not called before it's garbage-collected. Enable advanced leak reporting to find out where the leak occurred. To enable advanced leak reporting, specify the JVM option '-Dio.netty.leakDetection.level=advanced' or call ResourceLeakDetector.setLevel() See http://netty.io/wiki/reference-counted-objects.html for more information.
{code}

Looking a bit deeper, Spark is not leaking any memory here, but it is confusing for the user to see the error message in the driver logs. 

After enabling, '-Dio.netty.leakDetection.level=advanced', netty reveals the SparkSaslServer to be the source of these leaks.

Sample trace :https://gist.github.com/dhruve/b299ebc35aa0a185c244a0468927daf1",,apachespark,Dhruve Ashar,thangamani.murugasamy@epsilon.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 30 21:43:48 UTC 2019,,,,,,,,,,"0|i3gm73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/17 16:49;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/18392;;;","23/Jun/17 14:23;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/18407;;;","30/Aug/19 21:43;thangamani.murugasamy@epsilon.com;I have same problem in Spark 2.3

 

RROR util.ResourceLeakDetector: LEAK: ByteBuf.release() was not called before it's garbage-collected. See http://netty.io/wiki/reference-counted-objects.html for more information.
Recent access records:
[Stage 0:===============================================>         (25 + 5) / 30]19/08/30 16:39:07 ERROR datasources.FileFormatWriter: Aborting job null.
java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
        at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master UI hangs with spark.ui.reverseProxy=true if the master node has many CPUs,SPARK-21176,13081695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,IngoSchuster,IngoSchuster,IngoSchuster,22/Jun/17 08:27,01/Jul/17 10:43,14/Jul/23 06:30,30/Jun/17 03:17,2.1.0,2.1.1,2.2.0,2.2.1,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Web UI,,,,,1,network,web-ui,,,,,,,"In reverse proxy mode, Sparks exhausts the Jetty thread pool if the master node has too many cpus or the cluster has too many executers:

For each ProxyServlet, Jetty creates Selector threads: minimum 4, maximum half the number of available CPUs:
{{this(Math.max(1, Runtime.getRuntime().availableProcessors() / 2));}}
(see https://github.com/eclipse/jetty.project/blob/0c8273f2ca1f9bf2064cd9c4c939d2546443f759/jetty-client/src/main/java/org/eclipse/jetty/client/http/HttpClientTransportOverHTTP.java)

In reverse proxy mode, a proxy servlet is set up for each executor.
I have a system with 7 executors and 88 CPUs on the master node. Jetty tries to instantiate 7*44 = 309 selector threads just for the reverse proxy servlets, but since the QueuedThreadPool is initialized with 200 threads by default, the UI gets stuck.

I have patched JettyUtils.scala to extend the thread pool ( {{val pool = new QueuedThreadPool(400)}}). With this hack, the UI works.

Obviously, the Jetty defaults are meant for a real web server. If that has 88 CPUs, you do certainly expect a lot of traffic.
For the Spark admin UI however, there will rarely be concurrent accesses for the same application or the same executor.
I therefore propose to dramatically reduce the number of selector threads that get instantiated - at least by default.

I will propose a fix in a pull request.","ppc64le GNU/Linux, POWER8, only master node is reachable externally other nodes are in an internal network",apachespark,cloud_fan,farrellee,IngoSchuster,zgl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20853,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 01 10:43:03 UTC 2017,,,,,,,,,,"0|i3glcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/17 08:12;IngoSchuster;Best way I can see to control the number of selectors, is to overwrite the newHttpClient() method of ProxyServlet in our JettyUtils.scala.
It's a pitty that the constructor of ProxyServlet does not allow to overwrite the selector default.
Maybe something to clean up the code in future, I have opened an respective request  for Jetty: https://github.com/eclipse/jetty.project/issues/1643;;;","27/Jun/17 12:25;apachespark;User 'IngoSchuster' has created a pull request for this issue:
https://github.com/apache/spark/pull/18437;;;","30/Jun/17 03:17;cloud_fan;Issue resolved by pull request 18437
[https://github.com/apache/spark/pull/18437];;;","01/Jul/17 10:43;apachespark;User 'aosagie' has created a pull request for this issue:
https://github.com/apache/spark/pull/18499;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.tryWithSafeFinallyAndFailureCallbacks throws IllegalArgumentException: Self-suppression not permitted,SPARK-21170,13081646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,devaraj,devaraj,devaraj,22/Jun/17 04:46,01/Jul/17 14:55,14/Jul/23 06:30,01/Jul/17 14:55,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"{code:xml}
17/06/20 22:49:39 ERROR Executor: Exception in task 225.0 in stage 1.0 (TID 27225)
java.lang.IllegalArgumentException: Self-suppression not permitted
        at java.lang.Throwable.addSuppressed(Throwable.java:1043)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1400)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

{code:xml}
17/06/20 22:52:32 INFO scheduler.TaskSetManager: Lost task 427.0 in stage 1.0 (TID 27427) on 192.168.1.121, executor 12: java.lang.IllegalArgumentException (Self-suppression not permitted) [duplicate 1]
17/06/20 22:52:33 INFO scheduler.TaskSetManager: Starting task 427.1 in stage 1.0 (TID 27764, 192.168.1.122, executor 106, partition 427, PROCESS_LOCAL, 4625 bytes)
17/06/20 22:52:33 INFO scheduler.TaskSetManager: Lost task 186.0 in stage 1.0 (TID 27186) on 192.168.1.122, executor 106: java.lang.IllegalArgumentException (Self-suppression not permitted) [duplicate 2]
17/06/20 22:52:38 INFO scheduler.TaskSetManager: Starting task 186.1 in stage 1.0 (TID 27765, 192.168.1.121, executor 9, partition 186, PROCESS_LOCAL, 4625 bytes)
17/06/20 22:52:38 WARN scheduler.TaskSetManager: Lost task 392.0 in stage 1.0 (TID 27392, 192.168.1.121, executor 9): java.lang.IllegalArgumentException: Self-suppression not permitted
	at java.lang.Throwable.addSuppressed(Throwable.java:1043)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1400)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1145)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$12.apply(PairRDDFunctions.scala:1125)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Here it is trying to suppress the same Throwable instance and causing to throw the IllegalArgumentException which masks the original exception.

I think it should not add to the suppressed if it is the same instance.",,apachespark,devaraj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 01 14:55:11 UTC 2017,,,,,,,,,,"0|i3gl1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/17 05:17;apachespark;User 'devaraj-kavali' has created a pull request for this issue:
https://github.com/apache/spark/pull/18384;;;","01/Jul/17 14:55;srowen;Resolved by https://github.com/apache/spark/pull/18384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaRDD should always set kafka clientId.,SPARK-21168,13081634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,liuzhaokun,dixingxing@yeah.net,dixingxing@yeah.net,22/Jun/17 03:30,23/Apr/18 18:57,14/Jul/23 06:30,23/Apr/18 18:57,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,DStreams,,,,,0,,,,,,,,,"I found KafkaRDD not set kafka client.id in ""fetchBatch"" method (FetchRequestBuilder will set clientId to empty by default),  normally this will affect nothing, but in our case ,we use clientId at kafka server side, so we have to rebuild spark-streaming-kafka。",,apachespark,dixingxing@yeah.net,koeninger,liuzhaokun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 18:57:52 UTC 2018,,,,,,,,,,"0|i3gkz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/17 07:39;liuzhaokun;[~srowen]
Can I create a new PR to fix this problem?;;;","05/Dec/17 01:31;liuzhaokun;[~dixingxing@yeah.net]
Hi,as your PR are not in progess,can I create a new PR to fix this problems?;;;","05/Dec/17 03:08;apachespark;User 'liu-zhaokun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19887;;;","23/Apr/18 18:57;koeninger;Issue resolved by pull request 19887
[https://github.com/apache/spark/pull/19887];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path is not decoded correctly when reading output of FileSink,SPARK-21167,13081603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,22/Jun/17 00:03,03/Jul/17 11:15,14/Jul/23 06:30,22/Jun/17 07:00,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,Structured Streaming,,,,,0,,,,,,,,,"When reading output of FileSink, path is not decoded correctly. So if the path has some special characters, such as spaces, Spark cannot read it.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 18:24:04 UTC 2017,,,,,,,,,,"0|i3gks7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/17 00:07;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18381;;;","22/Jun/17 18:24;apachespark;User 'dijingran' has created a pull request for this issue:
https://github.com/apache/spark/pull/18383;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to write into partitioned hive table due to attribute reference not working with cast on partition column,SPARK-21165,13081529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,irashid,irashid,21/Jun/17 19:45,13/Oct/17 05:10,14/Jul/23 06:30,23/Jun/17 12:45,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"A simple ""insert into ... select"" involving partitioned hive tables fails.  Here's a simpler repro which doesn't involve hive at all -- this succeeds on 2.1.1, but fails on 2.2.0-rc5:

{noformat}
spark.sql(""""""SET hive.exec.dynamic.partition.mode=nonstrict"""""")
spark.sql(""""""DROP TABLE IF EXISTS src"""""")
spark.sql(""""""DROP TABLE IF EXISTS dest"""""")
spark.sql(""""""
CREATE TABLE src (first string, word string)
  PARTITIONED BY (length int)
"""""")

spark.sql(""""""
INSERT INTO src PARTITION(length) VALUES
  ('a', 'abc', 3),
  ('b', 'bcde', 4),
  ('c', 'cdefg', 5)
"""""")

spark.sql(""""""
  CREATE TABLE dest (word string, length int)
    PARTITIONED BY (first string)
"""""")

spark.sql(""""""
  INSERT INTO TABLE dest PARTITION(first) SELECT word, length, cast(first as string) as first FROM src
"""""")
{noformat}

The exception is

{noformat}
17/06/21 14:25:53 WARN TaskSetManager: Lost task 1.0 in stage 4.0 (TID 10, localhost, executor driver): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute
, tree: first#74
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$$anonfun$bind$1.apply(GenerateOrdering.scala:49)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$$anonfun$bind$1.apply(GenerateOrdering.scala:49)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$.bind(GenerateOrdering.scala:49)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering$.bind(GenerateOrdering.scala:43)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:884)
        at org.apache.spark.sql.execution.SparkPlan.newOrdering(SparkPlan.scala:363)
        at org.apache.spark.sql.execution.SortExec.createSorter(SortExec.scala:63)
        at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:102)
        at org.apache.spark.sql.execution.SortExec$$anonfun$1.apply(SortExec.scala:101)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:320)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Couldn't find first#74 in [word#76,length#77,first#75]
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        ... 40 more
{noformat}

The key to making this fail is the {{cast(first as string) as first}}.  Doing the same thing on any other column doesn't matter, it only matters on {{first}} (which is the partition column for the destination table).

Here's the explain plan from 2.2.0:
{noformat}
== Parsed Logical Plan ==
'InsertIntoTable 'UnresolvedRelation `dest`, Map(first -> None), false, false
+- 'Project ['word, 'length, cast('first as string) AS first#85]
   +- 'UnresolvedRelation `src`

== Analyzed Logical Plan ==
InsertIntoHiveTable CatalogTable(
Database: default
Table: dest
Owner: irashid
Created: Wed Jun 21 14:25:13 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/dest
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`first`]
Schema: root
-- word: string (nullable = true)
-- length: integer (nullable = true)
-- first: string (nullable = true)
), Map(first -> None), false, false
   +- Project [word#89, length#90, cast(first#88 as string) AS first#85]
      +- SubqueryAlias src
         +- CatalogRelation CatalogTable(
Database: default
Table: src
Owner: irashid
Created: Wed Jun 21 14:25:11 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Statistics: 9223372036854775807 bytes
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/src
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`length`]
Schema: root
-- first: string (nullable = true)
-- word: string (nullable = true)
-- length: integer (nullable = true)
), [first#88, word#89], [length#90]

== Optimized Logical Plan ==                                                                                                                                                    [68/2430]
InsertIntoHiveTable CatalogTable(
Database: default
Table: dest
Owner: irashid
Created: Wed Jun 21 14:25:13 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/dest
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`first`]
Schema: root
-- word: string (nullable = true)
-- length: integer (nullable = true)
-- first: string (nullable = true)
), Map(first -> None), false, false
   +- Project [word#89, length#90, cast(first#88 as string) AS first#85]
      +- SubqueryAlias src
         +- CatalogRelation CatalogTable(
Database: default
Table: src
Owner: irashid
Created: Wed Jun 21 14:25:11 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Statistics: 9223372036854775807 bytes
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/src
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`length`]
Schema: root
-- first: string (nullable = true)
-- word: string (nullable = true)
-- length: integer (nullable = true)
), [first#88, word#89], [length#90]

== Physical Plan ==
ExecutedCommand
   +- InsertIntoHiveTable CatalogTable(
Database: default
Table: dest
Owner: irashid
Created: Wed Jun 21 14:25:13 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/dest
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`first`]
Schema: root
-- word: string (nullable = true)
-- length: integer (nullable = true)
-- first: string (nullable = true)
), Map(first -> None), false, false
         +- Project [word#89, length#90, cast(first#88 as string) AS first#85]
            +- SubqueryAlias src
               +- CatalogRelation CatalogTable(
Database: default
Table: src
Owner: irashid
Created: Wed Jun 21 14:25:11 CDT 2017
Last Access: Wed Dec 31 18:00:00 CST 1969
Type: MANAGED
Provider: hive
Properties: [serialization.format=1]
Statistics: 9223372036854775807 bytes
Location: file:/Users/irashid/github/pub/spark/spark-warehouse/src
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.TextInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Partition Provider: Catalog
Partition Columns: [`length`]
Schema: root
-- first: string (nullable = true)
-- word: string (nullable = true)
-- length: integer (nullable = true)
), [first#88, word#89], [length#90]|
{noformat}

And from 2.1.1:
{noformat}
== Parsed Logical Plan ==
'InsertIntoTable 'UnresolvedRelation `dest`, Map(first -> None), OverwriteOptions(false,Map()), false
+- 'Project ['word, 'length, cast('first as string) AS first#55]
   +- 'UnresolvedRelation `src`

== Analyzed Logical Plan ==
InsertIntoTable MetastoreRelation default, dest, Map(first -> None), OverwriteOptions(false,Map()), false
+- Project [word#60, length#58, cast(first#59 as string) AS first#55]
   +- MetastoreRelation default, src

== Optimized Logical Plan ==
InsertIntoTable MetastoreRelation default, dest, Map(first -> None), OverwriteOptions(false,Map()), false
+- Project [word#60, length#58, first#59]
   +- MetastoreRelation default, src

== Physical Plan ==
InsertIntoHiveTable MetastoreRelation default, dest, Map(first -> None), false, false
+- HiveTableScan [word#60, length#58, first#59], MetastoreRelation default, src
{noformat}

While this example query is somewhat contrived, this is really iimportant because if you try to do the same thing where {{src}} was created by hive, then the query fails with the same error.  In that case, the explain plan looks like:

{noformat}
== Parsed Logical Plan ==
'InsertIntoTable 'UnresolvedRelation `dest`, Map(first -> None), false, false
+- 'Project ['word, 'length, 'first]
   +- 'UnresolvedRelation `src`

== Analyzed Logical Plan ==
InsertIntoHiveTable `my_test`.`dest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Map(first -> None), false, false
   +- Project [cast(word#49 as string) AS word#54, cast(length#50 as int) AS length#55, cast(first#48 as string) AS first#56]
      +- Project [word#49, length#50, first#48]
         +- SubqueryAlias src
            +- CatalogRelation `my_test`.`src`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [first#48, word#49], [length#50]

== Optimized Logical Plan ==
InsertIntoHiveTable `my_test`.`dest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Map(first -> None), false, false
   +- Project [cast(word#49 as string) AS word#54, cast(length#50 as int) AS length#55, cast(first#48 as string) AS first#56]
      +- Project [word#49, length#50, first#48]
         +- SubqueryAlias src
            +- CatalogRelation `my_test`.`src`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [first#48, word#49], [length#50]

== Physical Plan ==
ExecutedCommand
   +- InsertIntoHiveTable `my_test`.`dest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Map(first -> None), false, false
         +- Project [cast(word#49 as string) AS word#54, cast(length#50 as int) AS length#55, cast(first#48 as string) AS first#56]
            +- Project [word#49, length#50, first#48]
               +- SubqueryAlias src
                  +- CatalogRelation `my_test`.`src`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [first#48, word#49], [length#50]
{noformat}",,apachespark,cloud_fan,irashid,jonathak,sameerag,smilegator,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 12 14:09:04 UTC 2017,,,,,,,,,,"0|i3gkcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/17 23:38;smilegator;Unable to reproduce it in the current master branch. Will try to use 2.2 RC5 later;;;","22/Jun/17 00:09;smilegator;2.2 branch failed with the same error.;;;","22/Jun/17 06:33;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18386;;;","23/Jun/17 12:45;cloud_fan;Issue resolved by pull request 18386
[https://github.com/apache/spark/pull/18386];;;","12/Oct/17 14:09;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame.toPandas should respect the data type,SPARK-21163,13081465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,21/Jun/17 15:45,14/Apr/18 23:01,14/Jul/23 06:30,22/Jun/17 08:22,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,,,,,,,,,,,apachespark,cloud_fan,emtl97,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 14 23:01:38 UTC 2018,,,,,,,,,,"0|i3gjy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/17 15:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18378;;;","22/Jun/17 08:22;cloud_fan;Issue resolved by pull request 18378
[https://github.com/apache/spark/pull/18378];;;","14/Apr/18 23:01;emtl97;Had a question: in Spark 2.2.1, if I do a .toPandas on a Spark DataFrame with column integer type, the dtypes in pandas is int64.  Whereas in in Spark 2.3.0 they ints are converted to int32. I ran the below in Spark 2.2.1 and 2.3.0:

```
df = spark.sparkContext.parallelize([(i, ) for i in [1, 2, 3]]).toDF([""a""]).select(sf.col('a').cast('int')).toPandas()
df.dtypes
```
Is this intended? We ran into as we have unit tests in a project that passed in Spark 2.2.1 that fail in Spark 2.3.0

Left a comment on github:

[https://github.com/apache/spark/pull/18378/files/d8ba5452539c5fd5b650b7f5e51e467aabc33739#diff-6fc344560230bf0ef711bb9b5573f1faR1775]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cluster mode, driver throws connection refused exception submitted by SparkLauncher",SPARK-21159,13081294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,teclusky@gmail.com,teclusky@gmail.com,21/Jun/17 02:26,24/Jun/17 05:37,14/Jul/23 06:30,24/Jun/17 05:24,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,Spark Submit,,,,0,,,,,,,,,"When an spark application submitted by SparkLauncher#startApplication method, this will get a SparkAppHandle. In the test environment, the launcher runs on server A, if it runs in Client mode, everything is ok. In cluster mode, the launcher will run on Server A, and the driver will be run on Server B, in this scenario, when initialize SparkContext, a LauncherBackend will try to connect to the launcher application via specified port and ip address. the problem is the implementation of LauncherBackend uses loopback ip to connect which is 127.0.0.1. this will cause the connection refused as server B never ran the launcher. 

The expected behavior is the LauncherBackend should use Server A's Ip address to connect for reporting the running status.

Below is the stacktrace:
17/06/20 17:24:37 ERROR SparkContext: Error initializing SparkContext.
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at org.apache.spark.launcher.LauncherBackend.connect(LauncherBackend.scala:43)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:60)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:156)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
17/06/20 17:24:37 INFO SparkUI: Stopped Spark web UI at http://172.25.108.62:4040
17/06/20 17:24:37 INFO StandaloneSchedulerBackend: Shutting down all executors
17/06/20 17:24:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
17/06/20 17:24:37 ERROR Utils: Uncaught exception in thread main
java.lang.NullPointerException
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:214)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:116)
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:467)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1588)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1825)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:587)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
17/06/20 17:24:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/06/20 17:24:37 INFO MemoryStore: MemoryStore cleared
17/06/20 17:24:37 INFO BlockManager: BlockManager stopped
17/06/20 17:24:37 INFO BlockManagerMaster: BlockManagerMaster stopped
17/06/20 17:24:37 WARN MetricsSystem: Stopping a MetricsSystem that is not running
17/06/20 17:24:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/06/20 17:24:37 INFO SparkContext: Successfully stopped SparkContext
17/06/20 17:24:37 ERROR MongoPilotTask: error occurred group{2}:task(222)
java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:244)
	at org.apache.spark.launcher.LauncherBackend.connect(LauncherBackend.scala:43)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:60)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:156)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:509)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
 ","Server A-Master
Server B-Slave",apachespark,cloud_fan,teclusky@gmail.com,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 24 05:24:27 UTC 2017,,,,,,,,,,"0|i3giw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/17 02:33;vanzin;standalone cluster mode applications are not supported through {{startApplication}} yet. See SPARK-11033.

I don't think they should fail, so I'm not going to make this a duplicate yet, in case there's something to fix here.;;;","21/Jun/17 07:52;teclusky@gmail.com;thank you for your reply. it should use launcher's IP address to connect rather than driver's IP address, as the launcher and driver will not run on the same server in cluster mode. ;;;","21/Jun/17 16:16;vanzin;No, that should not be it. That's not how the launcher works internally.

I'm only keeping this open because app's shouldn't fail because this feature is not implemented.;;;","22/Jun/17 23:54;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18397;;;","23/Jun/17 06:10;teclusky@gmail.com;Great, thank you.;;;","24/Jun/17 05:24;cloud_fan;Issue resolved by pull request 18397
[https://github.com/apache/spark/pull/18397];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persistent view stored in Hive metastore should be case preserving.,SPARK-21150,13081020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,smilegator,smilegator,20/Jun/17 06:43,20/Jun/17 16:16,14/Jul/23 06:30,20/Jun/17 16:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{noformat}
    withView(""view1"") {
      spark.sql(""CREATE VIEW view1 AS SELECT 1 AS cAsEpReSeRvE, 2 AS aBcD"")
      val metadata = new MetadataBuilder().putString(types.HIVE_TYPE_STRING, ""int"").build()

      val expectedSchema = StructType(List(
        StructField(""cAsEpReSeRvE"", IntegerType, nullable = false, metadata),
        StructField(""aBcD"", IntegerType, nullable = false, metadata)))
      assert(spark.table(""view1"").schema == expectedSchema, ""Schema should match"")
      checkAnswer(
        sql(""select aBcD, cAsEpReSeRvE from view1""),
        Row(2, 1))
    }
{noformat}

The column names of persistent view stored in Hive metastore should be case preserving.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 08:53:03 UTC 2017,,,,,,,,,,"0|i3gh7b:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"20/Jun/17 08:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18360;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the schema of socket/rate source can not be set.,SPARK-21147,13080978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,Robin Shao,Robin Shao,20/Jun/17 01:43,12/Dec/22 17:35,14/Jul/23 06:30,21/Jun/17 17:52,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"The schema set for DataStreamReader can not work. The code is shown as below:
val line = ss.readStream.format(""socket"")
.option(""ip"",xxx)
.option(""port"",xxx)
.schema( StructField(""name"",StringType)::StructField(""area"",StringType)::Nil)
.load
line.printSchema

The printSchema prints:
root
|--value:String(nullable=true)

According to the code, it should print the schema set by schema().

Suggestion from Michael Armbrust:
throw an exception saying that you can't set schema here.
","Win7,spark 2.1.0",apachespark,lwlin,Robin Shao,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 16:24:03 UTC 2017,,,,,,,,,,"0|i3ggxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/17 16:24;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18365;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Restarted queries reuse same StateStoreProvider, causing multiple concurrent tasks to update same StateStore",SPARK-21145,13080956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,19/Jun/17 22:54,23/Jun/17 07:43,14/Jul/23 06:30,23/Jun/17 07:43,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"StateStoreProvider instances are loaded on-demand in a executor when a query is started. When a query is restarted, the loaded provider instance will get reused. Now, there is a non-trivial chance, that the task of the previous query run is still running, while the tasks of the restarted run has started. So for a stateful partition, there may be two concurrent tasks related to the same stateful partition, and there for using the same provider instance. This can lead to inconsistent results and possibly random failures, as state store implementations are not designed to be thread-safe.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 23 07:43:40 UTC 2017,,,,,,,,,,"0|i3ggt3:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"19/Jun/17 23:36;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/18355;;;","22/Jun/17 23:36;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/18396;;;","23/Jun/17 07:43;tdas;Issue resolved by pull request 18355
[https://github.com/apache/spark/pull/18355];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected results when the data schema and partition schema have the duplicate columns,SPARK-21144,13080942,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,smilegator,smilegator,19/Jun/17 21:39,23/Jun/17 16:31,14/Jul/23 06:30,23/Jun/17 16:31,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{noformat}
    withTempPath { dir =>
      val basePath = dir.getCanonicalPath
      spark.range(0, 3).toDF(""foo"").write.parquet(new Path(basePath, ""foo=1"").toString)
      spark.range(0, 3).toDF(""foo"").write.parquet(new Path(basePath, ""foo=a"").toString)
      spark.read.parquet(basePath).show()
    }
{noformat}

The result of the above case is
{noformat}
+---+
|foo|
+---+
|  1|
|  1|
|  a|
|  a|
|  1|
|  a|
+---+
{noformat}
",,apachespark,felixcheung,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 21 10:36:04 UTC 2017,,,,,,,,,,"0|i3ggpz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"19/Jun/17 21:41;smilegator;cc [~maropu];;;","20/Jun/17 00:52;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18356;;;","20/Jun/17 02:02;maropu;okay, I'm currently looking into this.;;;","20/Jun/17 07:36;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17758;;;","21/Jun/17 10:36;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18375;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot delete staging dir when the clusters of ""spark.yarn.stagingDir"" and ""spark.hadoop.fs.defaultFS"" are different ",SPARK-21138,13080788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sharkd,sharkd,sharkd,19/Jun/17 11:11,02/Dec/22 22:52,14/Jul/23 06:30,19/Jun/17 22:13,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,2.3.0,,Spark Core,YARN,,,,0,,,,,,,,,"When I set different clusters for ""spark.hadoop.fs.defaultFS"" and ""spark.yarn.stagingDir"" as follows：

{code:java}
spark.hadoop.fs.defaultFS  hdfs://tl-nn-tdw.tencent-distribute.com:54310
spark.yarn.stagingDir hdfs://ss-teg-2-v2/tmp/spark
{code}

I got following logs:

{code:java}
17/06/19 17:55:48 INFO SparkContext: Successfully stopped SparkContext
17/06/19 17:55:48 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/06/19 17:55:48 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.
17/06/19 17:55:48 INFO ApplicationMaster: Deleting staging directory hdfs://ss-teg-2-v2/tmp/spark/.sparkStaging/application_1496819138021_77618
17/06/19 17:55:48 ERROR Utils: Uncaught exception in thread Thread-2
java.lang.IllegalArgumentException: Wrong FS: hdfs://ss-teg-2-v2/tmp/spark/.sparkStaging/application_1496819138021_77618, expected: hdfs://tl-nn-tdw.tencent-distribute.com:54310
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:642)
	at org.apache.hadoopcdh3.hdfs.Cdh3DistributedFileSystem.getPathName(Cdh3DistributedFileSystem.java:197)
	at org.apache.hadoopcdh3.hdfs.Cdh3DistributedFileSystem.access$000(Cdh3DistributedFileSystem.java:81)
	at org.apache.hadoopcdh3.hdfs.Cdh3DistributedFileSystem$10.doCall(Cdh3DistributedFileSystem.java:644)
	at org.apache.hadoopcdh3.hdfs.Cdh3DistributedFileSystem$10.doCall(Cdh3DistributedFileSystem.java:640)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoopcdh3.hdfs.Cdh3DistributedFileSystem.delete(Cdh3DistributedFileSystem.java:640)
	at org.apache.hadoop.fs.FilterFileSystem.delete(FilterFileSystem.java:216)
	at org.apache.spark.deploy.yarn.ApplicationMaster.org$apache$spark$deploy$yarn$ApplicationMaster$$cleanupStagingDir(ApplicationMaster.scala:545)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anonfun$run$1.apply$mcV$sp(ApplicationMaster.scala:233)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}
",,apachespark,sharkd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-41313,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 29 06:00:14 UTC 2022,,,,,,,,,,"0|i1u54z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/17 11:20;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18352;;;","29/Nov/22 05:59;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 05:59;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 05:59;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;","29/Nov/22 06:00;apachespark;User 'xinglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38832;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HighlyCompressedMapStatus#writeExternal throws NPE,SPARK-21133,13080631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yumwang,yumwang,yumwang,18/Jun/17 07:31,12/Sep/17 05:32,14/Jul/23 06:30,20/Jun/17 01:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Reproduce, set {{set spark.sql.shuffle.partitions>2000}} with shuffle, for simple:

{code:sql}
spark-sql --executor-memory 12g --driver-memory 8g --executor-cores 7   -e ""
  set spark.sql.shuffle.partitions=2001;
  drop table if exists spark_hcms_npe;
  create table spark_hcms_npe as select id, count(*) from big_table group by id;
""
{code}

Error logs:
{noformat}
17/06/18 15:00:27 ERROR Utils: Exception encountered
java.lang.NullPointerException
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)
        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
17/06/18 15:00:27 ERROR MapOutputTrackerMaster: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)
        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
        ... 17 more
17/06/18 15:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.17.47.20:50188
17/06/18 15:00:27 ERROR Utils: Exception encountered
java.lang.NullPointerException
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)
        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)
        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)
        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

{noformat}
",,apachespark,cloud_fan,drewrobb,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 05:32:02 UTC 2017,,,,,,,,,,"0|i3get3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"18/Jun/17 07:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/18343;;;","20/Jun/17 01:23;cloud_fan;Issue resolved by pull request 18343
[https://github.com/apache/spark/pull/18343];;;","12/Sep/17 04:23;drewrobb;Thanks for the fix on this, but I don't think the fix version is correct at 2.2.0, as it is reproducible in 2.2.0. ;;;","12/Sep/17 05:15;cloud_fan;This patch was merged at June, and Spark 2.2.0 was released at July, I think this fix is in 2.2.0. Can you double check?;;;","12/Sep/17 05:32;drewrobb;My mistake, you are absolutely correct. I had some locally cached rc build of 2.2.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DISTINCT modifier of function arguments should not be silently ignored,SPARK-21132,13080621,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,18/Jun/17 02:36,19/Jun/17 07:52,14/Jul/23 06:30,19/Jun/17 07:52,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,DISTINCT modifier of function arguments should not be silently ignored when it is not being supported. ,,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 19 07:52:00 UTC 2017,,,,,,,,,,"0|i3geqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/17 02:39;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18340;;;","19/Jun/17 07:52;cloud_fan;Issue resolved by pull request 18340
[https://github.com/apache/spark/pull/18340];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arguments of SQL function call should not be named expressions,SPARK-21129,13080607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,17/Jun/17 20:14,30/Jun/17 21:26,14/Jul/23 06:30,30/Jun/17 21:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Function argument should not be named expressions. It could cause misleading error message.

{noformat}
spark-sql> select count(distinct c1, distinct c2) from t1;
{noformat}
{noformat}
Error in query: cannot resolve '`distinct`' given input columns: [c1, c2]; line 1 pos 26;
'Project [unresolvedalias('count(c1#30, 'distinct), None)]
+- SubqueryAlias t1
   +- CatalogRelation `default`.`t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [c1#30, c2#31]
{noformat}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 17 20:16:03 UTC 2017,,,,,,,,,,"0|i3genz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/17 20:16;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18338;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The configuration which named ""spark.core.connection.auth.wait.timeout"" hasn't been used in spark",SPARK-21126,13080556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,liuzhaokun,liuzhaokun,liuzhaokun,17/Jun/17 01:36,03/Jul/17 11:15,14/Jul/23 06:30,18/Jun/17 07:32,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Documentation,Spark Core,,,,0,,,,,,,,,"The configuration which named ""spark.core.connection.auth.wait.timeout"" hasn't been used in spark,so I think it should be removed from configuration.md.",,apachespark,liuzhaokun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 18 07:32:56 UTC 2017,,,,,,,,,,"0|i3gecn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/17 01:41;apachespark;User 'liu-zhaokun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18333;;;","17/Jun/17 05:57;srowen;[~liuzhaokun] please fill out JIRAs more carefully. This can't be a ""Major Bug"".;;;","18/Jun/17 07:32;srowen;Issue resolved by pull request 18333
[https://github.com/apache/spark/pull/18333];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong user shown in UI when using kerberos,SPARK-21124,13080512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,16/Jun/17 20:48,19/Jun/17 21:42,14/Jul/23 06:30,19/Jun/17 21:42,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,,"When submitting an app to a kerberos-secured cluster, the OS user and the user running the application may differ. Although it may also happen in cluster mode depending on the cluster manager's configuration, it's more common in client mode.

The UI should show enough information about user running the application to correctly identify the actual user. The ""app user"" can be easily retrieved via {{Utils.getCurrentUserName()}}, so it's mostly a matter of how to record this information (for showing in replayed applications) and how to present it in the UI.",,ajbozarth,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14483,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 16 22:11:03 UTC 2017,,,,,,,,,,"0|i3ge2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/17 21:05;ajbozarth;Though a separate issue, I believe the open PR for SPARK-14483 actually fixes this as well;;;","16/Jun/17 21:19;vanzin;Hmm, yeah, it seems like that code would solve this, but I have the same concern as you - in a normal application the user information all over the place will just clutter up the UI.

Anyway, I'll link both bugs and if that goes in I'll close this.;;;","16/Jun/17 21:21;vanzin;Also, the changes in SPARK-14483 also mask which is the actual user of the application running in the cluster (i.e. who actually is reading / writing files).

The thrift server doesn't do per-job impersonation, as far as I know, so it would still be useful to show the kerberos user the application is running as somewhere.;;;","16/Jun/17 22:11;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18331;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unset table properties should keep the table comment,SPARK-21119,13080334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Jun/17 06:58,16/Jun/17 15:07,14/Jul/23 06:30,16/Jun/17 15:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 16 07:01:03 UTC 2017,,,,,,,,,,"0|i3gczb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/17 07:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18325;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE SET TBLPROPERTIES should not overwrite COMMENT,SPARK-21112,13080234,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,15/Jun/17 21:49,16/Jun/17 02:11,14/Jul/23 06:30,16/Jun/17 02:11,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,{{ALTER TABLE SET TBLPROPERTIES}} should not overwrite the COMMENT even if the input does not have the property of `COMMENT`,,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 16 02:11:50 UTC 2017,,,,,,,,,,"0|i3gcd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/17 22:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18318;;;","16/Jun/17 02:11;cloud_fan;Issue resolved by pull request 18318
[https://github.com/apache/spark/pull/18318];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refresh command is too aggressive in parsing,SPARK-21102,13079943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,rxin,rxin,15/Jun/17 01:13,23/Aug/17 18:24,14/Jul/23 06:30,24/Jul/17 07:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,starter,,,,,,,,"SQL REFRESH command parsing is way too aggressive:

{code}
    | REFRESH TABLE tableIdentifier                                    #refreshTable
    | REFRESH .*?                                                      #refreshResource
{code}

We should change it so it takes the whole string (without space), or a quoted string.


",,aokolnychyi,apachespark,maropu,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 21:24:05 UTC 2017,,,,,,,,,,"0|i3gakf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/17 18:58;aokolnychyi;Hi [~rxin],

I took a look at this issue and have a prototype that fixes this. It is available [here| https://github.com/aokolnychyi/spark/commit/fc2b7c02fab7f570ae3ca080ae1c2c9502300de7]. I am not sure that my current implementation is the most optimal, so any feedback is appreciated. My first idea was to make the grammar as strict as possible. Unfortunately, there were some problems. I tried the approach below:

SqlBase.g4

{noformat}
...
    | REFRESH TABLE tableIdentifier                                    #refreshTable
    | REFRESH resourcePath                                             #refreshResource
...

resourcePath
    : STRING
    | (IDENTIFIER | number | nonReserved | '/' | '-')+ // other symbols can be added if needed
    ;
{noformat}

It is not flexible enough and requires to explicitly mention all possible symbols. Therefore, I came up with the approach that was mentioned at the beginning.

Let me know your opinion on which one is better.;;;","19/Jun/17 19:02;rxin;Can you submit a pull request so we can discuss the details of implementation there?
;;;","20/Jun/17 21:24;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/18368;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error running Hive temporary UDTF on latest Spark 2.2,SPARK-21101,13079933,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,dyzhou,dyzhou,15/Jun/17 00:00,25/Oct/17 06:00,14/Jul/23 06:30,25/Oct/17 06:00,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"I'm using temporary UDTFs on Spark 2.2, e.g.

CREATE TEMPORARY FUNCTION myudtf AS 'com.foo.MyUdtf' USING JAR 'hdfs:///path/to/udf.jar'; 

But when I try to invoke it, I get the following error:

{noformat}
17/06/14 19:43:50 ERROR SparkExecuteStatementOperation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.AnalysisException: No handler for Hive UDF 'com.foo.MyUdtf': java.lang.NullPointerException; line 1 pos 7
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:266)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Any help appreciated, thanks.",,apachespark,maropu,yumwang,zzr1000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-12377,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 13:16:05 UTC 2017,,,,,,,,,,"0|i3gaif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/17 01:17;maropu;You just don't pass your uber-jar into spark? Or, you mean the query above worked well on previous spark?;;;","15/Jun/17 01:27;dyzhou;Hi [~maropu]

>>You just don't pass your uber-jar into spark? 
Sorry not sure what you meant -- could you clarify your question?

>>Or, you mean the query above worked well on previous spark?
I did not try it with earlier versions, but likely the same behavior I think.;;;","15/Jun/17 01:30;maropu;See https://www.mail-archive.com/user@spark.apache.org/msg61009.html;;;","15/Jun/17 02:03;dyzhou;Hi [~maropu], yes I saw this one, but in my case, I'm using JDBC Thrift server to invoke the UDTF, not using Spark-shell.  So is there a way to pass my JAR to the Thrift server?;;;","15/Jun/17 04:14;maropu;Since JIRA is not a place for questions, you better ask in spark-user. I'll close this because this does not seem to be a bug. If you find this is a bug, feel free to reopen this. Thanks.;;;","15/Jun/17 05:53;dyzhou;Hi [~maropu],

>> I'll close this because this seems to be a bug.

This sounds bizarre, maybe you meant it wasn't a bug, but anyway, I did not start by asking a question, I started by reporting an error which is probably a bug.  What is your justification that it is NOT a bug and what is your justification of closing it as 'not a problem' when you don't even seem to understand it?;;;","15/Jun/17 06:12;srowen;[~dyzhou] did you read the link he posted?
This does not seem like a bug if you're not even passing your jar to the app. I would also close it.;;;","15/Jun/17 06:16;dyzhou;Hi [~sowen],

>> did you read the link he posted?

Yes I did, but did you read my response?  

I'm not using spark shell, I'm using spark thrift server, with USING JAR syntax:

CREATE TEMPORARY FUNCTION myudtf AS 'com.foo.MyUdtf' USING JAR 'hdfs:///path/to/udf.jar';;;;","15/Jun/17 07:47;zzr1000;Hi  Dayou Zhou ,  Jira is a place to post an almost certain bug , not a place to ask question . From now ,the question you ask cannot be see as a bug . . .;;;","15/Jun/17 16:53;dyzhou;Hi [~zhangzr1026], I'm still waiting for someone (anyone) to explain to me why this is not a bug, but whatever.  If this is how you treat people who love Spark, use Spark, and are trying to help make it better, than fine.;;;","15/Jun/17 17:01;srowen;[~dyzhou] I see your reply. The thrift server should just be another job that is spark-submit-ted. So I think you can in fact use {{--jars}} to add JARs to its classpath. That is what the guidance is getting at here. It is a bit more of a question therefore than JIRA issue, but I see why you're not convinced of that, but the way forward is to give the idea in that link a try next. 

Generally: I would only treat comments from committers or regular contributors as authoritative.;;;","15/Jun/17 20:26;dyzhou;Hi [~srowen], thanks for the helpful and constructive comment.  So yes I have also tried starting STS using --jars option, i.e.

./start-thriftserver.sh --jars /path/to/udf.jar

and have also verified that by doing this, I no longer need to specify USING JAR when creating my udf, i.e.

CREATE TEMPORARY FUNCTION myudtf AS 'com.foo.MyUdtf'

However, the bad news is that when I invoke the udf, I get exactly the same error as before, i.e.

>> No handler for Hive UDF 'com.foo.MyUdtf': java.lang.NullPointerException; line 1 pos 7

So I have reported what I wanted to report and I will leave the authorities to decide whether this is a bug or a 'question' (even though I do have an opinion on which).  Thanks for your help.;;;","18/Jun/17 09:48;yumwang;[~dyzhou], Can you try to override https://github.com/apache/hive/blob/release-2.0.0/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTF.java#L70

It works for me:
{code:sql}
add jar hdfs://nameservice1/tmp/wym/hive-exec-1.1.0-cdh5.4.3.jar;
CREATE TEMPORARY FUNCTION spark_21101 AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDTFStack';
select spark_21101(2,'A',10,date '2015-01-01','B',20,date '2016-01-01');
{code}


Ref: https://github.com/apache/hive/blob/release-2.0.0/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFStack.java;;;","19/Jun/17 05:54;dyzhou;Hi [~q79969786], thank you kindly for your response.  I was not aware of this 'other' version of initialize() method, and will try your suggestion tomorrow.;;;","19/Jun/17 08:53;viirya;-May I ask what Hive version your UDTF is based on?- Nvm, might not very related.;;;","04/Jul/17 13:16;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/18527;;;",,,,,,,,,,,,,,,,,,,,,,,
Multiple gapply execution occasionally failed in SparkR ,SPARK-21093,13079711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,14/Jun/17 09:27,12/Dec/22 18:11,14/Jul/23 06:30,08/Jul/17 21:26,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,"On Centos 7.2.1511 with R 3.4.0/3.3.0, multiple execution of {{gapply}} looks failed as below:

{code}
 Welcome to
    ____              __
   / __/__  ___ _____/ /__
  _\ \/ _ \/ _ `/ __/  '_/
 /___/ .__/\_,_/_/ /_/\_\   version  2.3.0-SNAPSHOT
    /_/


 SparkSession available as 'spark'.
> df <- createDataFrame(list(list(1L, 1, ""1"", 0.1)), c(""a"", ""b"", ""c"", ""d""))
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
17/06/14 18:21:01 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))
  a b c   d
1 1 1 1 0.1
> collect(gapply(df, ""a"", function(key, x) { x }, schema(df)))

Error in handleErrors(returnStatus, conn) :
  org.apache.spark.SparkException: Job aborted due to stage failure: Task 98 in stage 14.0 failed 1 times, most recent failure: Lost task 98.0 in stage 14.0 (TID 1305, localhost, executor driver): org.apache.spark.SparkException: R computation failed with

        at org.apache.spark.api.r.RRunner.compute(RRunner.scala:108)
        at org.apache.spark.sql.execution.FlatMapGroupsInRExec$$anonfun$13.apply(objects.scala:432)
        at org.apache.spark.sql.execution.FlatMapGroupsInRExec$$anonfun$13.apply(objects.scala:414)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.a

...

*** buffer overflow detected ***: /usr/lib64/R/bin/exec/R terminated
======= Backtrace: =========
/lib64/libc.so.6(__fortify_fail+0x37)[0x7fe699b3f597]
/lib64/libc.so.6(+0x10c750)[0x7fe699b3d750]
/lib64/libc.so.6(+0x10e507)[0x7fe699b3f507]
/usr/lib64/R/modules//internet.so(+0x6015)[0x7fe689bb7015]
/usr/lib64/R/modules//internet.so(+0xe81e)[0x7fe689bbf81e]
/usr/lib64/R/lib/libR.so(+0xbd1b6)[0x7fe69c54a1b6]
/usr/lib64/R/lib/libR.so(+0x1104d0)[0x7fe69c59d4d0]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(Rf_eval+0x354)[0x7fe69c5ad2f4]
/usr/lib64/R/lib/libR.so(+0x123f8e)[0x7fe69c5b0f8e]
/usr/lib64/R/lib/libR.so(Rf_eval+0x589)[0x7fe69c5ad529]
/usr/lib64/R/lib/libR.so(+0x1254ce)[0x7fe69c5b24ce]
/usr/lib64/R/lib/libR.so(+0x1104d0)[0x7fe69c59d4d0]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(Rf_eval+0x817)[0x7fe69c5ad7b7]
/usr/lib64/R/lib/libR.so(+0x1256d1)[0x7fe69c5b26d1]
/usr/lib64/R/lib/libR.so(+0x1552e9)[0x7fe69c5e22e9]
/usr/lib64/R/lib/libR.so(+0x11062a)[0x7fe69c59d62a]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(+0x11ee91)[0x7fe69c5abe91]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(+0x11ee91)[0x7fe69c5abe91]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(+0x11ee91)[0x7fe69c5abe91]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(+0x11ee91)[0x7fe69c5abe91]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x120a7e)[0x7fe69c5ada7e]
/usr/lib64/R/lib/libR.so(+0x11ee91)[0x7fe69c5abe91]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(Rf_eval+0x198)[0x7fe69c5ad138]
/usr/lib64/R/lib/libR.so(+0x1221af)[0x7fe69c5af1af]
/usr/lib64/R/lib/libR.so(+0x119101)[0x7fe69c5a6101]
/usr/lib64/R/lib/libR.so(+0x12649e)[0x7fe69c5b349e]
/usr/lib64/R/lib/libR.so(+0x12744b)[0x7fe69c5b444b]
/usr/lib64/R/lib/libR.so(Rf_eval+0x589)[0x7fe69c5ad529]
/usr/lib64/R/lib/libR.so(Rf_ReplIteration+0x232)[0x7fe69c5d6e12]
======= Memory map: ========
00400000-00401000 r-xp 00000000 08:04 55839004975                        /usr/lib64/R/bin/exec/R
00600000-00601000 r--p 00000000 08:04 55839004975                        /usr/lib64/R/bin/exec/R
00601000-00602000 rw-p 00001000 08:04 55839004975                        /usr/lib64/R/bin/exec/R
01c55000-047ce000 rw-p 00000000 00:00 0                                  [heap]
047ce000-04852000 rw-p 00000000 00:00 0                                  [heap]
7fe685e06000-7fe685e0b000 r-xp 00000000 08:04 2164504660                 /usr/lib64/R/library/parallel/libs/parallel.so
7fe685e0b000-7fe68600a000 ---p 00005000 08:04 2164504660                 /usr/lib64/R/library/parallel/libs/parallel.so
7fe68600a000-7fe68600b000 r--p 00004000 08:04 2164504660                 /usr/lib64/R/library/parallel/libs/parallel.so
7fe68600b000-7fe68600c000 rw-p 00005000 08:04 2164504660                 /usr/lib64/R/library/parallel/libs/parallel.so
7fe68600c000-7fe686018000 r-xp 00000000 08:04 64425907345                /usr/lib64/libnss_files-2.17.so
7fe686018000-7fe686217000 ---p 0000c000 08:04 64425907345                /usr/lib64/libnss_files-2.17.so
7fe686217000-7fe686218000 r--p 0000b000 08:04 64425907345                /usr/lib64/libnss_files-2.17.so
7fe686218000-7fe686219000 rw-p 0000c000 08:04 64425907345                /usr/lib64/libnss_files-2.17.so
7fe686219000-7fe68621f000 rw-p 00000000 00:00 0
7fe68621f000-7fe686221000 r-xp 00000000 08:04 64424510590                /usr/lib64/libfreebl3.so
7fe686221000-7fe686420000 ---p 00002000 08:04 64424510590                /usr/lib64/libfreebl3.so
7fe686420000-7fe686421000 r--p 00001000 08:04 64424510590                /usr/lib64/libfreebl3.so
7fe686421000-7fe686422000 rw-p 00002000 08:04 64424510590                /usr/lib64/libfreebl3.so
7fe686422000-7fe68642a000 r-xp 00000000 08:04 64425907331                /usr/lib64/libcrypt-2.17.so
7fe68642a000-7fe686629000 ---p 00008000 08:04 64425907331                /usr/lib64/libcrypt-2.17.so
7fe686629000-7fe68662a000 r--p 00007000 08:04 64425907331                /usr/lib64/libcrypt-2.17.so
7fe68662a000-7fe68662b000 rw-p 00008000 08:04 64425907331                /usr/lib64/libcrypt-2.17.so
7fe68662b000-7fe686659000 rw-p 00000000 00:00 0
7fe686659000-7fe68667d000 r-xp 00000000 08:04 64433322030                /usr/lib64/libselinux.so.1
7fe68667d000-7fe68687c000 ---p 00024000 08:04 64433322030                /usr/lib64/libselinux.so.1
7fe68687c000-7fe68687d000 r--p 00023000 08:04 64433322030                /usr/lib64/libselinux.so.1
7fe68687d000-7fe68687e000 rw-p 00024000 08:04 64433322030                /usr/lib64/libselinux.so.1
7fe68687e000-7fe686880000 rw-p 00000000 00:00 0
7fe686880000-7fe68689c000 r-xp 00000000 08:04 64424511832                /usr/lib64/libsasl2.so.3.0.0
7fe68689c000-7fe686a9b000 ---p 0001c000 08:04 64424511832                /usr/lib64/libsasl2.so.3.0.0
7fe686a9b000-7fe686a9c000 r--p 0001b000 08:04 64424511832                /usr/lib64/libsasl2.so.3.0.0
7fe686a9c000-7fe686a9d000 rw-p 0001c000 08:04 64424511832                /usr/lib64/libsasl2.so.3.0.0
7fe686a9d000-7fe686ab3000 r-xp 00000000 08:04 64425907355                /usr/lib64/libresolv-2.17.so
7fe686ab3000-7fe686cb3000 ---p 00016000 08:04 64425907355                /usr/lib64/libresolv-2.17.so
7fe686cb3000-7fe686cb4000 r--p 00016000 08:04 64425907355                /usr/lib64/libresolv-2.17.so
7fe686cb4000-7fe686cb5000 rw-p 00017000 08:04 64425907355                /usr/lib64/libresolv-2.17.so
7fe686cb5000-7fe686cb7000 rw-p 00000000 00:00 0
7fe686cb7000-7fe686cba000 r-xp 00000000 08:04 64424511994                /usr/lib64/libkeyutils.so.1.5
7fe686cba000-7fe686eb9000 ---p 00003000 08:04 64424511994                /usr/lib64/libkeyutils.so.1.5
7fe686eb9000-7fe686eba000 r--p 00002000 08:04 64424511994                /usr/lib64/libkeyutils.so.1.5
7fe686eba000-7fe686ebb000 rw-p 00003000 08:04 64424511994                /usr/lib64/libkeyutils.so.1.5
7fe686ebb000-7fe686ec8000 r-xp 00000000 08:04 64424692791                /usr/lib64/libkrb5support.so.0.1
7fe686ec8000-7fe6870c8000 ---p 0000d000 08:04 64424692791                /usr/lib64/libkrb5support.so.0.1
7fe6870c8000-7fe6870c9000 r--p 0000d000 08:04 64424692791                /usr/lib64/libkrb5support.so.0.1
7fe6870c9000-7fe6870ca000 rw-p 0000e000 08:04 64424692791                /usr/lib64/libkrb5support.so.0.1
7fe6870ca000-7fe687288000 r-xp 00000000 08:04 64424692795                /usr/lib64/libcrypto.so.1.0.1e
7fe687288000-7fe687488000 ---p 001be000 08:04 64424692795                /usr/lib64/libcrypto.so.1.0.1e
7fe687488000-7fe6874a2000 r--p 001be000 08:04 64424692795                /usr/lib64/libcrypto.so.1.0.1e
7fe6874a2000-7fe6874ae000 rw-p 001d8000 08:04 64424692795                /usr/lib64/libcrypto.so.1.0.1e
7fe6874ae000-7fe6874b2000 rw-p 00000000 00:00 0
7fe6874b2000-7fe687515000 r-xp 00000000 08:04 64424692797                /usr/lib64/libssl.so.1.0.1e
7fe687515000-7fe687714000 ---p 00063000 08:04 64424692797                /usr/lib64/libssl.so.1.0.1e
7fe687714000-7fe687718000 r--p 00062000 08:04 64424692797                /usr/lib64/libssl.so.1.0.1e
7fe687718000-7fe68771f000 rw-p 00066000 08:04 64424692797                /usr/lib64/libssl.so.1.0.1e
7fe68771f000-7fe68776f000 r-xp 00000000 08:04 64425430157                /usr/lib64/libldap-2.4.so.2.10.3
7fe68776f000-7fe68796e000 ---p 00050000 08:04 64425430157                /usr/lib64/libldap-2.4.so.2.10.3
7fe68796e000-7fe687971000 r--p 0004f000 08:04 64425430157                /usr/lib64/libldap-2.4.so.2.10.3
7fe687971000-7fe687972000 rw-p 00052000 08:04 64425430157                /usr/lib64/libldap-2.4.so.2.10.3
7fe687972000-7fe687980000 r-xp 00000000 08:04 64425430152                /usr/lib64/liblber-2.4.so.2.10.3
7fe687980000-7fe687b7f000 ---p 0000e000 08:04 64425430152                /usr/lib64/liblber-2.4.so.2.10.3
7fe687b7f000-7fe687b80000 r--p 0000d000 08:04 64425430152                /usr/lib64/liblber-2.4.so.2.10.3
7fe687b80000-7fe687b81000 rw-p 0000e000 08:04 64425430152                /usr/lib64/liblber-2.4.so.2.10.3
7fe687b81000-7fe687b84000 r-xp 00000000 08:04 64424511895                /usr/lib64/libcom_err.so.2.1
7fe687b84000-7fe687d83000 ---p 00003000 08:04 64424511895                /usr/lib64/libcom_err.so.2.1
7fe687d83000-7fe687d84000 r--p 00002000 08:04 64424511895                /usr/lib64/libcom_err.so.2.1
7fe687d84000-7fe687d85000 rw-p 00003000 08:04 64424511895                /usr/lib64/libcom_err.so.2.1
7fe687d85000-7fe687db4000 r-xp 00000000 08:04 64424692789                /usr/lib64/libk5crypto.so.3.1
7fe687db4000-7fe687fb3000 ---p 0002f000 08:04 64424692789                /usr/lib64/libk5crypto.so.3.1
7fe687fb3000-7fe687fb5000 r--p 0002e000 08:04 64424692789                /usr/lib64/libk5crypto.so.3.1
7fe687fb5000-7fe687fb6000 rw-p 00030000 08:04 64424692789                /usr/lib64/libk5crypto.so.3.1
7fe687fb6000-7fe687fb7000 rw-p 00000000 00:00 0
7fe687fb7000-7fe68808c000 r-xp 00000000 08:04 64424512023                /usr/lib64/libkrb5.so.3.3
7fe68808c000-7fe68828c000 ---p 000d5000 08:04 64424512023                /usr/lib64/libkrb5.so.3.3
7fe68828c000-7fe688299000 r--p 000d5000 08:04 64424512023                /usr/lib64/libkrb5.so.3.3
7fe688299000-7fe68829c000 rw-p 000e2000 08:04 64424512023                /usr/lib64/libkrb5.so.3.3
7fe68829c000-7fe6882e5000 r-xp 00000000 08:04 64424513555                /usr/lib64/libgssapi_krb5.so.2.2
7fe6882e5000-7fe6884e5000 ---p 00049000 08:04 64424513555                /usr/lib64/libgssapi_krb5.so.2.2
7fe6884e5000-7fe6884e6000 r--p 00049000 08:04 64424513555                /usr/lib64/libgssapi_krb5.so.2.2
7fe6884e6000-7fe6884e8000 rw-p 0004a000 08:04 64424513555                /usr/lib64/libgssapi_krb5.so.2.2
7fe6884e8000-7fe688522000 r-xp 00000000 08:04 64424511817                /usr/lib64/libnspr4.so
7fe688522000-7fe688721000 ---p 0003a000 08:04 64424511817                /usr/lib64/libnspr4.so
7fe688721000-7fe688722000 r--p 00039000 08:04 64424511817                /usr/lib64/libnspr4.so
7fe688722000-7fe688724000 rw-p 0003a000 08:04 64424511817                /usr/lib64/libnspr4.so
7fe688724000-7fe688726000 rw-p 00000000 00:00 0
7fe688726000-7fe68872a000 r-xp 00000000 08:04 64424511819                /usr/lib64/libplc4.so
7fe68872a000-7fe688929000 ---p 00004000 08:04 64424511819                /usr/lib64/libplc4.so
7fe688929000-7fe68892a000 r--p 00003000 08:04 64424511819                /usr/lib64/libplc4.so
7fe68892a000-7fe68892b000 rw-p 00004000 08:04 64424511819                /usr/lib64/libplc4.so
7fe68892b000-7fe68892e000 r-xp 00000000 08:04 64424511821                /usr/lib64/libplds4.so
7fe68892e000-7fe688b2d000 ---p 00003000 08:04 64424511821                /usr/lib64/libplds4.so
7fe688b2d000-7fe688b2e000 r--p 00002000 08:04 64424511821                /usr/lib64/libplds4.so
7fe688b2e000-7fe688b2f000 rw-p 00003000 08:04 64424511821                /usr/lib64/libplds4.so
7fe688b2f000-7fe688b55000 r-xp 00000000 08:04 64424511822                /usr/lib64/libnssutil3.so
7fe688b55000-7fe688d54000 ---p 00026000 08:04 64424511822                /usr/lib64/libnssutil3.so
7fe688d54000-7fe688d5a000 r--p 00025000 08:04 64424511822                /usr/lib64/libnssutil3.so
7fe688d5a000-7fe688d5b000 rw-p 0002b000 08:04 64424511822                /usr/lib64/libnssutil3.so
7fe688d5b000-7fe688e79000 r-xp 00000000 08:04 64426776728                /usr/lib64/libnss3.so
7fe688e79000-7fe689078000 ---p 0011e000 08:04 64426776728                /usr/lib64/libnss3.so
7fe689078000-7fe68907d000 r--p 0011d000 08:04 64426776728                /usr/lib64/libnss3.so
7fe68907d000-7fe68907f000 rw-p 00122000 08:04 64426776728                /usr/lib64/libnss3.so
7fe68907f000-7fe689081000 rw-p 00000000 00:00 0
7fe689081000-7fe6890a5000 r-xp 00000000 08:04 64424993837                /usr/lib64/libsmime3.so
7fe6890a5000-7fe6892a4000 ---p 00024000 08:04 64424993837                /usr/lib64/libsmime3.so
7fe6892a4000-7fe6892a7000 r--p 00023000 08:04 64424993837                /usr/lib64/libsmime3.so
7fe6892a7000-7fe6892a8000 rw-p 00026000 08:04 64424993837                /usr/lib64/libsmime3.so
7fe6892a8000-7fe6892e6000 r-xp 00000000 08:04 64426776729                /usr/lib64/libssl3.so
7fe6892e6000-7fe6894e5000 ---p 0003e000 08:04 64426776729                /usr/lib64/libssl3.so
7fe6894e5000-7fe6894e9000 r--p 0003d000 08:04 64426776729                /usr/lib64/libssl3.so
7fe6894e9000-7fe6894ea000 rw-p 00041000 08:04 64426776729                /usr/lib64/libssl3.so
7fe6894ea000-7fe6894eb000 rw-p 00000000 00:00 0
7fe6894eb000-7fe689513000 r-xp 00000000 08:04 64424963118                /usr/lib64/libssh2.so.1.0.1
7fe689513000-7fe689713000 ---p 00028000 08:04 64424963118                /usr/lib64/libssh2.so.1.0.1
7fe689713000-7fe689714000 r--p 00028000 08:04 64424963118                /usr/lib64/libssh2.so.1.0.1
7fe689714000-7fe689715000 rw-p 00029000 08:04 64424963118                /usr/lib64/libssh2.so.1.0.1
7fe689715000-7fe689747000 r-xp 00000000 08:04 64424512526                /usr/lib64/libidn.so.11.6.11
7fe689747000-7fe689946000 ---p 00032000 08:04 64424512526                /usr/lib64/libidn.so.11.6.11
7fe689946000-7fe689947000 r--p 00031000 08:04 64424512526                /usr/lib64/libidn.so.11.6.11
7fe689947000-7fe689948000 rw-p 00032000 08:04 64424512526                /usr/lib64/libidn.so.11.6.11
7fe689948000-7fe6899ad000 r-xp 00000000 08:04 64434945109                /usr/lib64/libcurl.so.4.3.0
7fe6899ad000-7fe689bad000 ---p 00065000 08:04 64434945109                /usr/lib64/libcurl.so.4.3.0
7fe689bad000-7fe689baf000 r--p 00065000 08:04 64434945109                /usr/lib64/libcurl.so.4.3.0
7fe689baf000-7fe689bb0000 rw-p 00067000 08:04 64434945109                /usr/lib64/libcurl.so.4.3.0
7fe689bb0000-7fe689bb1000 rw-p 00000000 00:00 0
7fe689bb1000-7fe689bc4000 r-xp 00000000 08:04 66581724169                /usr/lib64/R/modules/internet.so
7fe689bc4000-7fe689dc3000 ---p 00013000 08:04 66581724169                /usr/lib64/R/modules/internet.so
7fe689dc3000-7fe689dc4000 r--p 00012000 08:04 66581724169                /usr/lib64/R/modules/internet.so
7fe689dc4000-7fe689dc5000 rw-p 00013000 08:04 66581724169                /usr/lib64/R/modules/internet.so
7fe689dc5000-7fe68dfe6000 rw-p 00000000 00:00 0
7fe68e02a000-7fe68e164000 rw-p 00000000 00:00 0
7fe68e164000-7fe68e16c000 r-xp 00000000 08:04 6445979743                 /usr/lib64/R/library/methods/libs/methods.so
7fe68e16c000-7fe68e36b000 ---p 00008000 08:04 6445979743                 /usr/lib64/R/library/methods/libs/methods.so
7fe68e36b000-7fe68e36c000 r--p 00007000 08:04 6445979743                 /usr/lib64/R/library/methods/libs/methods.so
7fe68e36c000-7fe68e36d000 rw-p 00008000 08:04 6445979743                 /usr/lib64/R/library/methods/libs/methods.so
7fe68e36d000-7fe68e39a000 rw-p 00000000 00:00 0
7fe68e39a000-7fe68e58f000 r-xp 00000000 08:04 19333693112                /usr/lib64/R/lib/libRlapack.so
7fe68e58f000-7fe68e78e000 ---p 001f5000 08:04 19333693112                /usr/lib64/R/lib/libRlapack.so
7fe68e78e000-7fe68e78f000 r--p 001f4000 08:04 19333693112                /usr/lib64/R/lib/libRlapack.so
7fe68e78f000-7fe68e790000 rw-p 001f5000 08:04 19333693112                /usr/lib64/R/lib/libRlapack.so
7fe68e790000-7fe68e836000 r-xp 00000000 08:04 45100675141                /usr/lib64/R/library/stats/libs/stats.so
7fe68e836000-7fe68ea36000 ---p 000a6000 08:04 45100675141                /usr/lib64/R/library/stats/libs/stats.so
7fe68ea36000-7fe68ea38000 r--p 000a6000 08:04 45100675141                /usr/lib64/R/library/stats/libs/stats.so
7fe68ea38000-7fe68ea3a000 rw-p 000a8000 08:04 45100675141                /usr/lib64/R/library/stats/libs/stats.so
7fe68ea3a000-7fe68ebc0000 rw-p 00000000 00:00 0
7fe68ebc0000-7fe68ebfd000 r-xp 00000000 08:04 6445979741                 /usr/lib64/R/library/graphics/libs/graphics.so
7fe68ebfd000-7fe68edfd000 ---p 0003d000 08:04 6445979741                 /usr/lib64/R/library/graphics/libs/graphics.so
7fe68edfd000-7fe68edfe000 r--p 0003d000 08:04 6445979741                 /usr/lib64/R/library/graphics/libs/graphics.so
7fe68edfe000-7fe68edff000 rw-p 0003e000 08:04 6445979741                 /usr/lib64/R/library/graphics/libs/graphics.so
7fe68edff000-7fe68ee4c000 rw-p 00000000 00:00 0
7fe68ee4c000-7fe68ee80000 r-xp 00000000 08:04 57982776471                /usr/lib64/R/library/grDevices/libs/grDevices.so
7fe68ee80000-7fe68f080000 ---p 00034000 08:04 57982776471                /usr/lib64/R/library/grDevices/libs/grDevices.so
7fe68f080000-7fe68f085000 r--p 00034000 08:04 57982776471                /usr/lib64/R/library/grDevices/libs/grDevices.so
7fe68f085000-7fe68f087000 rw-p 00039000 08:04 57982776471                /usr/lib64/R/library/grDevices/libs/grDevices.so
7fe68f087000-7fe68f0ba000 rw-p 00000000 00:00 0
7fe68f0ba000-7fe68f0c4000 r-xp 00000000 08:04 62293320708                /usr/lib64/R/library/utils/libs/utils.so
7fe68f0c4000-7fe68f2c3000 ---p 0000a000 08:04 62293320708                /usr/lib64/R/library/utils/libs/utils.so
7fe68f2c3000-7fe68f2c4000 r--p 00009000 08:04 62293320708                /usr/lib64/R/library/utils/libs/utils.so
7fe68f2c4000-7fe68f2c5000 rw-p 0000a000 08:04 62293320708                /usr/lib64/R/library/utils/libs/utils.so
7fe68f2c5000-7fe68f39f000 rw-p 00000000 00:00 0
7fe68f3f7000-7fe68f705000 rw-p 00000000 00:00 0
7fe68f705000-7fe695c2c000 r--p 00000000 08:04 53687238663                /usr/lib/locale/locale-archive
7fe695c2c000-7fe695d15000 r-xp 00000000 08:04 64424511848                /usr/lib64/libstdc++.so.6.0.19
7fe695d15000-7fe695f15000 ---p 000e9000 08:04 64424511848                /usr/lib64/libstdc++.so.6.0.19
7fe695f15000-7fe695f1e000 r--p 000e9000 08:04 64424511848                /usr/lib64/libstdc++.so.6.0.19
7fe695f1e000-7fe695f20000 rw-p 000f2000 08:04 64424511848                /usr/lib64/libstdc++.so.6.0.19
7fe695f20000-7fe695f35000 rw-p 00000000 00:00 0
7fe695f35000-7fe697308000 r-xp 00000000 08:04 64424511959                /usr/lib64/libicudata.so.50.1.2
7fe697308000-7fe697507000 ---p 013d3000 08:04 64424511959                /usr/lib64/libicudata.so.50.1.2
7fe697507000-7fe697508000 r--p 013d2000 08:04 64424511959                /usr/lib64/libicudata.so.50.1.2
7fe697508000-7fe697509000 rw-p 013d3000 08:04 64424511959                /usr/lib64/libicudata.so.50.1.2
7fe697509000-7fe69752e000 r-xp 00000000 08:04 64424511873                /usr/lib64/libtinfo.so.5.9
7fe69752e000-7fe69772e000 ---p 00025000 08:04 64424511873                /usr/lib64/libtinfo.so.5.9
7fe69772e000-7fe697732000 r--p 00025000 08:04 64424511873                /usr/lib64/libtinfo.so.5.9
7fe697732000-7fe697733000 rw-p 00029000 08:04 64424511873                /usr/lib64/libtinfo.so.5.9
7fe697733000-7fe697748000 r-xp 00000000 08:04 64434760744                /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7fe697748000-7fe697947000 ---p 00015000 08:04 64434760744                /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7fe697947000-7fe697948000 r--p 00014000 08:04 64434760744                /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7fe697948000-7fe697949000 rw-p 00015000 08:04 64434760744                /usr/lib64/libgcc_s-4.8.5-20150702.so.1
7fe697949000-7fe697b39000 r-xp 00000000 08:04 64424511961                /usr/lib64/libicui18n.so.50.1.2
7fe697b39000-7fe697d39000 ---p 001f0000 08:04 64424511961                /usr/lib64/libicui18n.so.50.1.2
7fe697d39000-7fe697d45000 r--p 001f0000 08:04 64424511961                /usr/lib64/libicui18n.so.50.1.2
7fe697d45000-7fe697d47000 rw-p 001fc000 08:04 64424511961                /usr/lib64/libicui18n.so.50.1.2
7fe697d47000-7fe697d48000 rw-p 00000000 00:00 0
7fe697d48000-7fe697eac000 r-xp 00000000 08:04 64424511973                /usr/lib64/libicuuc.so.50.1.2
7fe697eac000-7fe6980ac000 ---p 00164000 08:04 64424511973                /usr/lib64/libicuuc.so.50.1.2
7fe6980ac000-7fe6980bc000 r--p 00164000 08:04 64424511973                /usr/lib64/libicuuc.so.50.1.2
7fe6980bc000-7fe6980bd000 rw-p 00174000 08:04 64424511973                /usr/lib64/libicuuc.so.50.1.2
7fe6980bd000-7fe6980c1000 rw-p 00000000 00:00 0
7fe6980c1000-7fe6980c4000 r-xp 00000000 08:04 64425907333                /usr/lib64/libdl-2.17.so
7fe6980c4000-7fe6982c3000 ---p 00003000 08:04 64425907333                /usr/lib64/libdl-2.17.so
7fe6982c3000-7fe6982c4000 r--p 00002000 08:04 64425907333                /usr/lib64/libdl-2.17.so
7fe6982c4000-7fe6982c5000 rw-p 00003000 08:04 64425907333                /usr/lib64/libdl-2.17.so
7fe6982c5000-7fe6982cc000 r-xp 00000000 08:04 64425907357                /usr/lib64/librt-2.17.so
7fe6982cc000-7fe6984cb000 ---p 00007000 08:04 64425907357                /usr/lib64/librt-2.17.so
7fe6984cb000-7fe6984cc000 r--p 00006000 08:04 64425907357                /usr/lib64/librt-2.17.so
7fe6984cc000-7fe6984cd000 rw-p 00007000 08:04 64425907357                /usr/lib64/librt-2.17.so
7fe6984cd000-7fe6984e2000 r-xp 00000000 08:04 64425230795                /usr/lib64/libz.so.1.2.7
7fe6984e2000-7fe6986e1000 ---p 00015000 08:04 64425230795                /usr/lib64/libz.so.1.2.7
7fe6986e1000-7fe6986e2000 r--p 00014000 08:04 64425230795                /usr/lib64/libz.so.1.2.7
7fe6986e2000-7fe6986e3000 rw-p 00015000 08:04 64425230795                /usr/lib64/libz.so.1.2.7
7fe6986e3000-7fe6986f2000 r-xp 00000000 08:04 64424511910                /usr/lib64/libbz2.so.1.0.6
7fe6986f2000-7fe6988f1000 ---p 0000f000 08:04 64424511910                /usr/lib64/libbz2.so.1.0.6
7fe6988f1000-7fe6988f2000 r--p 0000e000 08:04 64424511910                /usr/lib64/libbz2.so.1.0.6
7fe6988f2000-7fe6988f3000 rw-p 0000f000 08:04 64424511910                /usr/lib64/libbz2.so.1.0.6
7fe6988f3000-7fe698918000 r-xp 00000000 08:04 64432774884                /usr/lib64/liblzma.so.5.2.2
7fe698918000-7fe698b17000 ---p 00025000 08:04 64432774884                /usr/lib64/liblzma.so.5.2.2
7fe698b17000-7fe698b18000 r--p 00024000 08:04 64432774884                /usr/lib64/liblzma.so.5.2.2
7fe698b18000-7fe698b19000 rw-p 00025000 08:04 64432774884                /usr/lib64/liblzma.so.5.2.2
7fe698b19000-7fe698b79000 r-xp 00000000 08:04 64424511837                /usr/lib64/libpcre.so.1.2.0
7fe698b79000-7fe698d78000 ---p 00060000 08:04 64424511837                /usr/lib64/libpcre.so.1.2.0
7fe698d78000-7fe698d79000 r--p 0005f000 08:04 64424511837                /usr/lib64/libpcre.so.1.2.0
7fe698d79000-7fe698d7a000 rw-p 00060000 08:04 64424511837                /usr/lib64/libpcre.so.1.2.0
7fe698d7a000-7fe698d89000 r-xp 00000000 08:04 64432775393                /usr/lib64/libtre.so.5.0.0
7fe698d89000-7fe698f88000 ---p 0000f000 08:04 64432775393                /usr/lib64/libtre.so.5.0.0
7fe698f88000-7fe698f89000 r--p 0000e000 08:04 64432775393                /usr/lib64/libtre.so.5.0.0
7fe698f89000-7fe698f8a000 rw-p 0000f000 08:04 64432775393                /usr/lib64/libtre.so.5.0.0
7fe698f8a000-7fe698fc6000 r-xp 00000000 08:04 64424511924                /usr/lib64/libreadline.so.6.2
7fe698fc6000-7fe6991c6000 ---p 0003c000 08:04 64424511924                /usr/lib64/libreadline.so.6.2
7fe6991c6000-7fe6991c8000 r--p 0003c000 08:04 64424511924                /usr/lib64/libreadline.so.6.2
7fe6991c8000-7fe6991ce000 rw-p 0003e000 08:04 64424511924                /usr/lib64/libreadline.so.6.2
7fe6991ce000-7fe6991d0000 rw-p 00000000 00:00 0
7fe6991d0000-7fe69920b000 r-xp 00000000 08:04 64424589353                /usr/lib64/libquadmath.so.0.0.0
7fe69920b000-7fe69940a000 ---p 0003b000 08:04 64424589353                /usr/lib64/libquadmath.so.0.0.0
7fe69940a000-7fe69940b000 r--p 0003a000 08:04 64424589353                /usr/lib64/libquadmath.so.0.0.0
7fe69940b000-7fe69940c000 rw-p 0003b000 08:04 64424589353                /usr/lib64/libquadmath.so.0.0.0
7fe69940c000-7fe69950d000 r-xp 00000000 08:04 64425907335                /usr/lib64/libm-2.17.so
7fe69950d000-7fe69970c000 ---p 00101000 08:04 64425907335                /usr/lib64/libm-2.17.so
7fe69970c000-7fe69970d000 r--p 00100000 08:04 64425907335                /usr/lib64/libm-2.17.so
7fe69970d000-7fe69970e000 rw-p 00101000 08:04 64425907335                /usr/lib64/libm-2.17.so
7fe69970e000-7fe69982d000 r-xp 00000000 08:04 64425157647                /usr/lib64/libgfortran.so.3.0.0
7fe69982d000-7fe699a2d000 ---p 0011f000 08:04 64425157647                /usr/lib64/libgfortran.so.3.0.0
7fe699a2d000-7fe699a2f000 r--p 0011f000 08:04 64425157647                /usr/lib64/libgfortran.so.3.0.0
7fe699a2f000-7fe699a31000 rw-p 00121000 08:04 64425157647                /usr/lib64/libgfortran.so.3.0.0
7fe699a31000-7fe699be8000 r-xp 00000000 08:04 64426776661                /usr/lib64/libc-2.17.so
7fe699be8000-7fe699de8000 ---p 001b7000 08:04 64426776661                /usr/lib64/libc-2.17.so
7fe699de8000-7fe699dec000 r--p 001b7000 08:04 64426776661                /usr/lib64/libc-2.17.so
7fe699dec000-7fe699dee000 rw-p 001bb000 08:04 64426776661                /usr/lib64/libc-2.17.so
7fe699dee000-7fe699df3000 rw-p 00000000 00:00 0
7fe699df3000-7fe699e09000 r-xp 00000000 08:04 64425907353                /usr/lib64/libpthread-2.17.so
7fe699e09000-7fe69a009000 ---p 00016000 08:04 64425907353                /usr/lib64/libpthread-2.17.so
7fe69a009000-7fe69a00a000 r--p 00016000 08:04 64425907353                /usr/lib64/libpthread-2.17.so
7fe69a00a000-7fe69a00b000 rw-p 00017000 08:04 64425907353                /usr/lib64/libpthread-2.17.so
7fe69a00b000-7fe69a00f000 rw-p 00000000 00:00 0
7fe69a00f000-7fe69a034000 r-xp 00000000 08:04 64424513594                /usr/lib64/libgomp.so.1.0.0
7fe69a034000-7fe69a233000 ---p 00025000 08:04 64424513594                /usr/lib64/libgomp.so.1.0.0
7fe69a233000-7fe69a234000 r--p 00024000 08:04 64424513594                /usr/lib64/libgomp.so.1.0.0
7fe69a234000-7fe69a235000 rw-p 00025000 08:04 64424513594                /usr/lib64/libgomp.so.1.0.0
7fe69a235000-7fe69c267000 r-xp 00000000 08:04 19333693109                /usr/lib64/R/lib/libRblas.so
7fe69c267000-7fe69c466000 ---p 02032000 08:04 19333693109                /usr/lib64/R/lib/libRblas.so
7fe69c466000-7fe69c46a000 r--p 02031000 08:04 19333693109                /usr/lib64/R/lib/libRblas.so
7fe69c46a000-7fe69c482000 rw-p 02035000 08:04 19333693109                /usr/lib64/R/lib/libRblas.so
7fe69c482000-7fe69c48d000 rw-p 00000000 00:00 0
7fe69c48d000-7fe69c73b000 r-xp 00000000 08:04 19333693111                /usr/lib64/R/lib/libR.so
7fe69c73b000-7fe69c93a000 ---p 002ae000 08:04 19333693111                /usr/lib64/R/lib/libR.so
7fe69c93a000-7fe69c958000 r--p 002ad000 08:04 19333693111                /usr/lib64/R/lib/libR.so
7fe69c958000-7fe69c964000 rw-p 002cb000 08:04 19333693111                /usr/lib64/R/lib/libR.so
7fe69c964000-7fe69cab5000 rw-p 00000000 00:00 0
7fe69cab5000-7fe69cad6000 r-xp 00000000 08:04 64424511800                /usr/lib64/ld-2.17.so
7fe69cade000-7fe69ccba000 rw-p 00000000 00:00 0
7fe69ccba000-7fe69ccbb000 rw-p 00000000 00:00 0
7fe69ccbb000-7fe69ccbc000 r--p 00000000 08:04 49394932767                /usr/lib64/R/library/translations/en/LC_MESSAGES/R.mo
7fe69ccbc000-7fe69cccc000 rwxp 00000000 00:00 0
7fe69cccc000-7fe69ccd3000 r--s 00000000 08:04 4295610652                 /usr/lib64/gconv/gconv-modules.cache
7fe69ccd3000-7fe69ccd6000 rw-p 00000000 00:00 0
7fe69ccd6000-7fe69ccd7000 r--p 00021000 08:04 64424511800                /usr/lib64/ld-2.17.so
7fe69ccd7000-7fe69ccd8000 rw-p 00022000 08:04 64424511800                /usr/lib64/ld-2.17.so
7fe69ccd8000-7fe69ccd9000 rw-p 00000000 00:00 0
7fff43955000-7fff439b1000 rw-p 00000000 00:00 0                          [stack]
7fff439f2000-7fff439f4000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
{code}

The related test - https://github.com/apache/spark/blob/master/R/pkg/tests/fulltests/test_sparkSQL.R#L3007-L3095 passes in 

Windows Server 2012 R2 / R 3.3.1
macOS Sierra 10.12.3 / R 3.4.0
macOS Sierra 10.12.3 / R 3.2.3
Ubuntu 17 / R 3.3

failed in 

Centos 7.2.1511 with R 3.4.0/3.3.3","CentOS 7.2.1511 / R 3.4.0, CentOS 7.2.1511 / R 3.3.3",apachespark,bdwyer,felixcheung,mlnick,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 29 09:15:03 UTC 2017,,,,,,,,,,"0|i3g953:",9223372036854775807,,,,,felixcheung,,,,,,,,2.3.0,,,,,,,,,,,"14/Jun/17 09:30;gurwls223;cc [~nick.pentreath@gmail.com] and [~felixcheung].;;;","14/Jun/17 16:44;shivaram;So it looks like the R worker process crashes on CentOS and that leads to the task failures. I think the only way to debug this might be to get a core dump from the R process, attach gdb to it and see the stack trace at the time of the crash ?;;;","15/Jun/17 03:01;gurwls223;I am taking a look here gdb with bt:

In case of CentOS above,

{code}
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-80.el7
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-redhat-linux-gnu"".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/lib64/R/bin/exec/R...Reading symbols from /usr/lib64/R/bin/exec/R...(no debugging symbols found)...done.
(no debugging symbols found)...done.
[New LWP 25284]
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Core was generated by `/usr/lib64/R/bin/exec/R --slave --no-restore --vanilla --file=/home/hyukjinkwon'.
Program terminated with signal 6, Aborted.
#0  0x00007fbdffb545f7 in raise () from /lib64/libc.so.6
Missing separate debuginfos, use: debuginfo-install R-core-3.4.0-2.el7.x86_64
(gdb) where
#0  0x00007fbdffb545f7 in raise () from /lib64/libc.so.6
#1  0x00007fbdffb55ce8 in abort () from /lib64/libc.so.6
#2  0x00007fbdffb94327 in __libc_message () from /lib64/libc.so.6
#3  0x00007fbdffc2d597 in __fortify_fail () from /lib64/libc.so.6
#4  0x00007fbdffc2b750 in __chk_fail () from /lib64/libc.so.6
#5  0x00007fbdffc2d507 in __fdelt_warn () from /lib64/libc.so.6
#6  0x00007fbdefca5015 in R_SockConnect () from /usr/lib64/R/modules//internet.so
#7  0x00007fbdefcad81e in sock_open () from /usr/lib64/R/modules//internet.so
#8  0x00007fbe026381b6 in do_sockconn () from /usr/lib64/R/lib/libR.so
#9  0x00007fbe0268b4d0 in bcEval () from /usr/lib64/R/lib/libR.so
#10 0x00007fbe0269b138 in Rf_eval () from /usr/lib64/R/lib/libR.so
#11 0x00007fbe0269d1af in R_execClosure () from /usr/lib64/R/lib/libR.so
#12 0x00007fbe0269b2f4 in Rf_eval () from /usr/lib64/R/lib/libR.so
#13 0x00007fbe0269ef8e in do_set () from /usr/lib64/R/lib/libR.so
#14 0x00007fbe0269b529 in Rf_eval () from /usr/lib64/R/lib/libR.so
#15 0x00007fbe026a04ce in do_eval () from /usr/lib64/R/lib/libR.so
#16 0x00007fbe0268b4d0 in bcEval () from /usr/lib64/R/lib/libR.so
#17 0x00007fbe0269b138 in Rf_eval () from /usr/lib64/R/lib/libR.so
#18 0x00007fbe0269d1af in R_execClosure () from /usr/lib64/R/lib/libR.so
#19 0x00007fbe02694101 in bcEval () from /usr/lib64/R/lib/libR.so
#20 0x00007fbe0269b138 in Rf_eval () from /usr/lib64/R/lib/libR.so
#21 0x00007fbe0269ba7e in forcePromise () from /usr/lib64/R/lib/libR.so
#22 0x00007fbe0269b7b7 in Rf_eval () from /usr/lib64/R/lib/libR.so
#23 0x00007fbe026a06d1 in do_withVisible () from /usr/lib64/R/lib/libR.so
#24 0x00007fbe026d02e9 in do_internal () from /usr/lib64/R/lib/libR.so
---Type <return> to continue, or q <return> to quit---
{code}

Another thing I found is, this looks actually reproduced in my Mac too. If the command above is executed multiple times (for my Mac, it has to be executed (14~16-ish times) but the error message looks apparently different. However, my wild guess is the root cause is the same. 
;;;","15/Jun/17 18:14;gurwls223;In case of my Mac, it looks the problem is here - https://github.com/apache/spark/blob/2881a2d1d1a650a91df2c6a01275eba14a43b42a/R/pkg/inst/worker/daemon.R#L45-L53

This code path looks particularly busy for {{dapply}} and {{gapply}}. I could reproduce the issue for both APIs too.

{code}
df <- createDataFrame(list(list(1L, 1, ""1"", 0.1)), c(""a"", ""b"", ""c"", ""d""))
collect(dapply(repartition(df, 200), function(x) { x }, schema(df)))
{code}

It looks processes / sockets are being closed fine but the pipes remind. This looks leading to an error, such as, ""Error in parallel:::mcfork() :"". I checked this via {{watch -n 0.1 ""lsof -c R | wc -l""}} in my Mac.

Running the code below multiple times (this should be varied for ulimit ...):

{code}
for(i in 0:200) {
  p <- parallel:::mcfork()
  if (inherits(p, ""masterProcess"")) {
    tools::pskill(Sys.getpid(), tools::SIGUSR1)
    parallel:::mcexit(0L)
  }
}
{code}

reproduced the problem. Please correct me if I am wrong. I suspect an issue in R. I will update a comment after testing this on CentOS soon.;;;","15/Jun/17 21:30;shivaram;Thanks [~hyukjin.kwon] -- thats a very useful debugging notes. In addition to filing the bug in R, I am wondering if there is some thing we can do in our SparkR code to mitigate this. Could we say add a sleep or pause before the gapply tests ? Or in other words do the pipes / sockets disappear after some time ?;;;","16/Jun/17 00:45;gurwls223;This does disappear in a certain condition which I could not verify; however, the `daemon.R` process keeps the pipes eventually when it is particularly executed hot.

Executing the SparkR codes:

{code}
501  2041  1968   0  -:--AM ttys000    0:01.50 /Library/Frameworks/R.framework/Resources/bin/exec/R --slave --no-restore --vanilla --file=.../spark/R/lib/SparkR/worker/daemon.R
{code}


{code}
lsof -p 2041
{code}

{code}
...
R       2041 hyukjinkwon    7   PIPE 0x15b5dd8f99c8d451     16384
R       2041 hyukjinkwon    8   PIPE 0x15b5dd8f99c00911     16384
R       2041 hyukjinkwon    9   PIPE 0x15b5dd8f940201d1     16384
R       2041 hyukjinkwon   10   PIPE 0x15b5dd8f9401f811     16384
R       2041 hyukjinkwon   11   PIPE 0x15b5dd8f99deec11     16384
R       2041 hyukjinkwon   12   PIPE 0x15b5dd8f96a18851     16384
R       2041 hyukjinkwon   13   PIPE 0x15b5dd8f99deee51     16384
R       2041 hyukjinkwon   14   PIPE 0x15b5dd8f99dee851     16384
R       2041 hyukjinkwon   15   PIPE 0x15b5dd8f99df13d1     16384
R       2041 hyukjinkwon   16   PIPE 0x15b5dd8f99c8ce51     16384
R       2041 hyukjinkwon   17   PIPE 0x15b5dd8f96a18f11     16384
R       2041 hyukjinkwon   18   PIPE 0x15b5dd8f96a195d1     16384
R       2041 hyukjinkwon   19   PIPE 0x15b5dd8f99e73d91     16384
R       2041 hyukjinkwon   20   PIPE 0x15b5dd8f96a18c11     16384
R       2041 hyukjinkwon   21   PIPE 0x15b5dd8f99df0651     16384
R       2041 hyukjinkwon   22   PIPE 0x15b5dd8f99def691     16384
R       2041 hyukjinkwon   23   PIPE 0x15b5dd8f99e75f51     16384
R       2041 hyukjinkwon   24   PIPE 0x15b5dd8f99def091     16384
R       2041 hyukjinkwon   25   PIPE 0x15b5dd8f99e74d51     16384
R       2041 hyukjinkwon   26   PIPE 0x15b5dd8f99e75591     16384
{code}

{code}
lsof -p 2041 | wc -l
{code}

This number keeps increasing/decreasing but eventually consistently increasing.

Same thing happens in CentOS too (in this case, looks increasing more aggressively.

{code}
R       29617 root   16r  FIFO      0,8       0t0    40330039 pipe
R       29617 root   17r  FIFO      0,8       0t0    40330197 pipe
R       29617 root   18r  FIFO      0,8       0t0    40330041 pipe
R       29617 root   19r  FIFO      0,8       0t0    40330067 pipe
R       29617 root   20w  FIFO      0,8       0t0    40326795 pipe
R       29617 root   21r  FIFO      0,8       0t0    40326800 pipe
R       29617 root   22r  FIFO      0,8       0t0    40330191 pipe
R       29617 root   23r  FIFO      0,8       0t0    40330047 pipe
R       29617 root   24r  FIFO      0,8       0t0    40330049 pipe
R       29617 root   25r  FIFO      0,8       0t0    40330011 pipe
R       29617 root   26w  FIFO      0,8       0t0    40326801 pipe
R       29617 root   27r  FIFO      0,8       0t0    40330199 pipe
R       29617 root   28r  FIFO      0,8       0t0    40330051 pipe
R       29617 root   29r  FIFO      0,8       0t0    40326808 pipe
R       29617 root   30w  FIFO      0,8       0t0    40330124 pipe
R       29617 root   31r  FIFO      0,8       0t0    40326810 pipe
R       29617 root   32r  FIFO      0,8       0t0    40330133 pipe
R       29617 root   33w  FIFO      0,8       0t0    40330012 pipe
{code} 

One workaround only to pass the test is, I think, avoid reuse the `daemon.R` by setting {{spark.sparkr.use.daemon}} to {{false}}. I will double check and will propose this to fix it for now.;;;","16/Jun/17 01:13;gurwls223;BTW, {{mcfork}} in R looks opening a pipe ahead but the existing logic does not properly close it when it is executed hot. This is why I suspect it is an R issue.;;;","16/Jun/17 02:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18320;;;","17/Jun/17 09:13;gurwls223;This potentially affects many R functionalities. I increased the priority.;;;","21/Jun/17 13:50;mlnick;Just adding the info from test failure report from the 2.2.0-RC4 vote thread:

R - 3.3.0
OpenJDK Runtime Environment (build 1.8.0_111-b15)
CentOS 7.2.1511

;;;","25/Jun/17 18:10;felixcheung;since this is a very core change to SparkR, we agree it might not be best to push to 2.2 branch right now as we are so close to releasing 2.2.0, and let it cooks for a bit first
(though RC sign off might still run into this issue then);;;","29/Jun/17 03:09;felixcheung;this was reverted.

we seems to be getting random test termination with error code -10 after this is merged.;;;","29/Jun/17 03:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18463;;;","29/Jun/17 09:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18465;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Table properties are not shown in DESC EXTENDED/FORMATTED,SPARK-21089,13079648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,14/Jun/17 05:09,19/Jul/17 22:12,14/Jul/23 06:30,16/Jun/17 16:42,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Since both table properties and storage properties share the same key values, table properties are not shown in the output of DESC EXTENDED/FORMATTED when the storage properties are not empty. 
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 05:11:03 UTC 2017,,,,,,,,,,"0|i3g8r3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"14/Jun/17 05:11;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18294;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to read the partitioned table created by Spark 2.1,SPARK-21085,13079600,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,smilegator,smilegator,smilegator,13/Jun/17 23:25,14/Jun/17 08:28,14/Jul/23 06:30,14/Jun/17 08:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Spark 2.2 is unable to read the partitioned table created by Spark 2.1 when the table schema does not put the partitioning column at the end of the schema. 
{noformat}
assert(partitionFields.map(_.name) == partitionColumnNames)
{noformat}

The codes are from the following files:

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala#L234-L236

When reading the table metadata from the metastore, we also need to reorder the columns. 
",,apachespark,cloud_fan,sameerag,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 08:28:53 UTC 2017,,,,,,,,,,"0|i3g8gf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"14/Jun/17 05:21;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18295;;;","14/Jun/17 08:28;cloud_fan;Issue resolved by pull request 18295
[https://github.com/apache/spark/pull/18295];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LibSVM load just one input file,SPARK-21066,13079245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,darion,darion,12/Jun/17 15:40,12/Dec/22 17:35,14/Jul/23 06:30,07/Jul/17 04:25,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,ML,,,,,0,,,,,,,,,"Currently when we using SVM to train dataset we found the input files limit only one .

The file store on the Distributed File System such as HDFS is split into mutil piece and I think this limit is not necessary .

 We can join input paths into a string split with comma. ",,apachespark,cloud_fan,darion,facai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 04:25:28 UTC 2017,,,,,,,,,,"0|i3g69j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/17 17:24;srowen;CC [~lian cheng] 

I don't immediately see why the relation can't examine multiple files to compute the number of features, either.;;;","13/Jun/17 10:43;apachespark;User 'darionyaphet' has created a pull request for this issue:
https://github.com/apache/spark/pull/18288;;;","20/Jun/17 08:12;facai;Hi, [~darion] .

If `numFeatures` is specified, multiple files are OK. 
{code}
val df = spark.read.format(""libsvm"")
  .option(""numFeatures"", ""780"")
  .load(""data/mllib/sample_libsvm_data.txt"")
{code}

see: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.source.libsvm.LibSVMDataSource;;;","20/Jun/17 08:19;facai;[~sowen] I believe that the API has explained well in details.

 If unspecified or nonpositive, the number of features will be determined automatically at the cost of one additional pass.

The best way to solve the problem is to modify the misleading message of exception: to suggest user to specify `numFeatures`, rather than warn user to go away.;;;","23/Jun/17 03:50;facai;Downgrade to Trivial since `numFeatures` should work.;;;","06/Jul/17 16:52;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18556;;;","07/Jul/17 04:25;cloud_fan;Issue resolved by pull request 18556
[https://github.com/apache/spark/pull/18556];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the default value bug in NettyBlockTransferServiceSuite,SPARK-21064,13079191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,DjvuLee,DjvuLee,DjvuLee,12/Jun/17 11:54,03/Jul/17 11:15,14/Jul/23 06:30,13/Jun/17 14:56,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,Tests,,,,,0,,,,,,,,,,,apachespark,DjvuLee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 13 14:56:41 UTC 2017,,,,,,,,,,"0|i3g5xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/17 11:55;DjvuLee;The defalut value for `spark.port.maxRetries` is 100, but we use the 10
in the suite file.;;;","12/Jun/17 12:02;apachespark;User 'djvulee' has created a pull request for this issue:
https://github.com/apache/spark/pull/18279;;;","12/Jun/17 12:19;apachespark;User 'djvulee' has created a pull request for this issue:
https://github.com/apache/spark/pull/18280;;;","13/Jun/17 14:56;srowen;Issue resolved by pull request 18280
[https://github.com/apache/spark/pull/18280];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LikeSimplification can NPE on null pattern,SPARK-21059,13079136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,12/Jun/17 08:11,10/Jul/17 23:40,14/Jul/23 06:30,12/Jun/17 21:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 12 08:14:04 UTC 2017,,,,,,,,,,"0|i3g5lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/17 08:14;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18273;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not use a PascalDistribution in countApprox,SPARK-21057,13078956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,lovasoa,lovasoa,11/Jun/17 16:36,14/Jun/17 08:01,14/Jul/23 06:30,14/Jun/17 08:01,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"I was reading the source of Spark, and found this:
https://github.com/apache/spark/blob/v2.1.1/core/src/main/scala/org/apache/spark/partial/CountEvaluator.scala#L50-L72

This is the function that estimates the probability distribution of the total count of elements in an RDD given the count of only some partitions.

This function does a strange thing: when the number of elements counted so far is less than 10 000, it models the total count with a negative binomial (Pascal) law, else, it models it with a Poisson law.

Modeling our number of uncounted elements with a negative binomial law is like saying that we ran over elements, counting only some, and stopping after having counted a given number of elements.
But this does not model what really happened.  Our counting was limited in time, not in number of counted elements, and we can't count only some of the elements in a partition.

I propose to use the Poisson distribution in every case, as it can be justified under the hypothesis that the number of elements in each partition is independent and follows a Poisson law.",,apachespark,lovasoa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 08:01:30 UTC 2017,,,,,,,,,,"0|i3g5c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/17 18:36;srowen;That's not the intended interpretation of the negative binomial. The hypothetical is that you pick elements from the total data set at random, and a fraction p of the time you 'succeed ' in picking from among the elements already counted. The rest of the time you 'fail'. But if this goes on long enough to reach the observed count as the number of successes, then the number of failures models the size of the rest of the data.

Certainly, they both have the same expected value (good). I recall this had something to do with problems for very small counts, but this doesn't appear to be a problem in this code, now. If anything the condition should be a function of r and p. I also can't go back and see a clear theoretical justification. I agree, the Poisson analysis seems fine even for small p and r.

Try the change and see how it affects the tests.;;;","11/Jun/17 22:43;lovasoa;{quote}
The hypothetical is that you pick elements from the total data set at random, and a fraction p of the time you 'succeed ' in picking from among the elements already counted.
The rest of the time you 'fail'. But if this goes on long enough to reach the observed count as the number of successes, then the number of failures models the size of the rest of the data.
{quote}

The thing is, if you pick elements until you've picked all the ones that were counted, and don't want to pick the same element twice, then the probability of picking a 'counted' element is not constant. The more you pick counted elements, the less the probability to pick a counted element next time. 

If you don't care about picking the same element twice, then you may well pick every time the same counted element and stop. The number of uncounted elements you picked before ending is not the total number of uncounted elements.

This model doesn't model the random process we are trying to describe. The fact it has a correct expected value (and thus doesn’t give nonsensical results) doesn't mean anything. Most probability laws could be made to fit our expected value.

I currently don’t have a lot of free time as I'm finishing my master thesis, but I will make a pull request as soon as I find the time. Thank you for your quick answer !;;;","11/Jun/17 22:57;srowen;I don't know if that's relevant to the analysis. Nothing is literally picking elements with or without replacement. Neither does the Poisson model literally model what's actually taking place, either. The argument isn't solely that the expect value is correct, of course. Still I don't see the reason NB was introduced here, and it looks no longer useful or even valid. I have a PR ready anyway as I needed to test it.;;;","12/Jun/17 09:59;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/18276;;;","14/Jun/17 08:01;srowen;Issue resolved by pull request 18276
[https://github.com/apache/spark/pull/18276];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support grouping__id,SPARK-21055,13078932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cenyuhai,cenyuhai,cenyuhai,11/Jun/17 09:17,21/Oct/17 06:33,14/Jul/23 06:30,20/Oct/17 16:29,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Now, spark doesn't support grouping__id, spark provide another function grouping_id() to workaround. 
If use grouping_id(), many scripts need to change and supporting  grouping__id is very easy, why not?",spark2.1.1,apachespark,cenyuhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21858,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 21 06:33:04 UTC 2017,,,,,,,,,,"0|i3g56v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/17 12:10;apachespark;User 'cenyuhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/18270;;;","30/Aug/17 15:27;apachespark;User 'cenyuhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/19087;;;","21/Oct/17 06:33;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/19546;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ml word2vec write has overflow issue in calculating numPartitions,SPARK-21050,13078907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,11/Jun/17 00:34,03/Jul/17 11:15,14/Jul/23 06:30,12/Jun/17 21:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,ML,,,,,0,,,,,,,,,"The method calculateNumberOfPartitions() uses Int, not Long (unlike the MLlib version), so it is very easily to have an overflow in calculating the number of partitions for ML persistence.",,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19247,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 12 21:27:55 UTC 2017,,,,,,,,,,"0|i3g51b:",9223372036854775807,,,,,,,,,,,,,2.2.1,2.3.0,,,,,,,,,,"11/Jun/17 00:37;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/18265;;;","12/Jun/17 21:27;josephkb;Issue resolved by pull request 18265
[https://github.com/apache/spark/pull/18265];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark executor blocked instead of throwing exception because exception occur when python worker send exception info to PythonRDD in Python 2+,SPARK-21045,13078844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,advancedxy,wangzjie1,wangzjie1,10/Jun/17 08:36,12/Dec/22 18:10,14/Jul/23 06:30,20/Sep/19 23:10,2.0.1,2.0.2,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,PySpark,,,,,0,,,,,,,,,"My pyspark program is always blocking in product yarn cluster. Then I jstack and found :

{code}
""Executor task launch worker for task 0"" #60 daemon prio=5 os_prio=31 tid=0x00007fb2f44e3000 nid=0xa003 runnable [0x0000000123b4a000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
        at java.net.SocketInputStream.read(SocketInputStream.java:170)
        at java.net.SocketInputStream.read(SocketInputStream.java:141)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
        - locked <0x00000007acab1c98> (a java.io.BufferedInputStream)
        at java.io.DataInputStream.readInt(DataInputStream.java:387)
        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:190)
        at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
        at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

It is blocking in socket read.  I view the log on blocking executor and found error:

{code}
Traceback (most recent call last):
  File ""/opt/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 178, in main
    write_with_length(traceback.format_exc().encode(""utf-8""), outfile)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe4 in position 618: ordinal not in range(128)
{code}

Finally I found the problem:

{code:title=worker.py|borderStyle=solid}
    # 178 line in spark 2.1.1
    except Exception:
        try:
            write_int(SpecialLengths.PYTHON_EXCEPTION_THROWN, outfile)
            write_with_length(traceback.format_exc().encode(""utf-8""), outfile)
        except IOError:
            # JVM close the socket
            pass
        except Exception:
            # Write the error to stderr if it happened while serializing
            print(""PySpark worker failed with exception:"", file=sys.stderr)
            print(traceback.format_exc(), file=sys.stderr)
{code}

when write_with_length(traceback.format_exc().encode(""utf-8""), outfile) occur exception like UnicodeDecodeError, the python worker can't send the trace info, but when the PythonRDD get PYTHON_EXCEPTION_THROWN, It should read the trace info length next. So it is blocking.

{code:title=PythonRDD.scala|borderStyle=solid}
    # 190 line in spark 2.1.1
    case SpecialLengths.PYTHON_EXCEPTION_THROWN =>
     // Signals that an exception has been thrown in python
     val exLength = stream.readInt()  // It is possible to be blocked
{code}

{color:red}
We can triggle the bug use simple program:
{color}
{code:title=test.py|borderStyle=solid}
    spark = SparkSession.builder.master('local').getOrCreate()
    rdd = spark.sparkContext.parallelize(['中']).map(lambda x: x.encode(""utf8""))
    rdd.collect()
{code}","It has problem only in Python 2+. 
Python 3+ is ok.
",apachespark,wangzjie1,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 20 23:10:46 UTC 2019,,,,,,,,,,"0|i3g4nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/17 16:06;apachespark;User 'dataknocker' has created a pull request for this issue:
https://github.com/apache/spark/pull/18261;;;","10/Jun/17 16:32;apachespark;User 'dataknocker' has created a pull request for this issue:
https://github.com/apache/spark/pull/18262;;;","16/Jun/17 06:44;apachespark;User 'dataknocker' has created a pull request for this issue:
https://github.com/apache/spark/pull/18261;;;","16/Jun/17 06:57;apachespark;User 'dataknocker' has created a pull request for this issue:
https://github.com/apache/spark/pull/18324;;;","20/Sep/19 23:10;gurwls223;Fixed at https://github.com/apache/spark/pull/25847;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"With whole-stage codegen, SparkSession.range()'s behavior is inconsistent with SparkContext.range()",SPARK-21041,13078759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,rednaxelafx,rednaxelafx,09/Jun/17 20:28,03/Jul/17 11:15,14/Jul/23 06:30,12/Jun/17 13:01,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"When whole-stage codegen is enabled, in face of integer overflow, SparkSession.range()'s behavior is inconsistent with when codegen is turned off, while the latter is consistent with SparkContext.range()'s behavior.

The following Spark Shell session shows the inconsistency:
{code:java}
scala> sc.range
   def range(start: Long,end: Long,step: Long,numSlices: Int): org.apache.spark.rdd.RDD[Long]

scala> spark.range
                                                                                                     
def range(start: Long,end: Long,step: Long,numPartitions: Int): org.apache.spark.sql.Dataset[Long]   
def range(start: Long,end: Long,step: Long): org.apache.spark.sql.Dataset[Long]                      
def range(start: Long,end: Long): org.apache.spark.sql.Dataset[Long]                                 
def range(end: Long): org.apache.spark.sql.Dataset[Long] 

scala> sc.range(java.lang.Long.MAX_VALUE - 3, java.lang.Long.MIN_VALUE + 2, 1).collect
res1: Array[Long] = Array()

scala> spark.range(java.lang.Long.MAX_VALUE - 3, java.lang.Long.MIN_VALUE + 2, 1).collect
res2: Array[Long] = Array(9223372036854775804, 9223372036854775805, 9223372036854775806)

scala> spark.conf.set(""spark.sql.codegen.wholeStage"", false)

scala> spark.range(java.lang.Long.MAX_VALUE - 3, java.lang.Long.MIN_VALUE + 2, 1).collect
res5: Array[Long] = Array()
{code}",,apachespark,dongjoon,kiszk,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21044,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 10 02:09:14 UTC 2017,,,,,,,,,,"0|i3g44f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/17 22:37;dongjoon;Oh, interesting. I'll take a look. [~rednaxelafx].;;;","09/Jun/17 22:57;dongjoon;Yep. It looks like the generated code has a logical bug.;;;","10/Jun/17 01:29;dongjoon;I'll make a PR for this.;;;","10/Jun/17 02:00;dongjoon;Although this is a correctness issue, this is not a regression because this exists since Spark 2.0.
In general, I'm trying to remove this kind of invalid `Range` from optimizer in SPARK-21044.;;;","10/Jun/17 02:09;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18257;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix the potential OOM in UnsafeExternalSorter,SPARK-21033,13078540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,09/Jun/17 08:40,14/May/20 01:38,14/Jul/23 06:30,30/Oct/17 17:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"In `UnsafeInMemorySorter`, one record may take 32 bytes: 1 `long` for pointer, 1 `long` for key-prefix, and another 2 `long`s as the temporary buffer for radix sort.

In `UnsafeExternalSorter`, we set the `DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD` to be `1024 * 1024 * 1024 / 2`, and hoping the max size of point array to be 8 GB. However this is wrong, `1024 * 1024 * 1024 / 2 * 32` is actually 16 GB, and if we grow the point array before reach this limitation, we may hit the max-page-size error.

Users may see exception like this on large dataset:
{code}
Caused by: java.lang.IllegalArgumentException: Cannot allocate a page with more than 17179869176 bytes
at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:241)
at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:121)
at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:374)
at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:396)
at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:94)
...
{code}",,apachespark,clehene,cloud_fan,devaraj,fanyunbojerry,maropu,rchukh,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22438,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 01:38:46 UTC 2020,,,,,,,,,,"0|i3g2rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/17 08:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18251;;;","19/Oct/17 18:41;clehene;I think this may be responsible for other problems, such as not being able to allocate memory while running in a container as well as getting killed from exceeding max memory.

{noformat}
17/10/19 18:15:39 INFO memory.TaskMemoryManager: Memory used in task 6317340
17/10/19 18:15:39 INFO memory.TaskMemoryManager: Acquired by org.apache.spark.shuffle.sort.ShuffleExternalSorter@2c98b15b: 32.0 KB
17/10/19 18:15:39 INFO memory.TaskMemoryManager: Acquired by org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@566144b7: 64.0 KB
17/10/19 18:15:39 INFO memory.TaskMemoryManager: Acquired by org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@ea479ad: 13.3 GB
17/10/19 18:15:39 INFO memory.TaskMemoryManager: 0 bytes of memory were used by task 6317340 but are not associated with specific consumers
17/10/19 18:15:39 INFO memory.TaskMemoryManager: 14496792576 bytes of memory are used for execution and 198127044 bytes of memory are used for storage
17/10/19 18:15:39 ERROR executor.Executor: Exception in task 6.0 in stage 320.2 (TID 6317340)
java.lang.OutOfMemoryError: Unable to acquire 65536 bytes of memory, got 0
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:98)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:126)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:153)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:120)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.<init>(UnsafeExternalRowSorter.java:82)
	at org.apache.spark.sql.execution.SortExec.createSorter(SortExec.scala:87)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.init(Unknown Source)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(WholeStageCodegenExec.scala:392)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8.apply(WholeStageCodegenExec.scala:389)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat};;;","19/Oct/17 19:10;srowen;I'm seeing that same problem consistently in a deployment [~clehene], though I also don't know whether it's resolved by the change above.
;;;","19/Oct/17 19:17;clehene;[~cloud_fan] Can you update the title and description? It helps, when finding the issue through Google, to get an accurate description within JIRA 

{quote}n UnsafeInMemorySorter, one record may take 32 bytes: 1 long for pointer, 1 long for key-prefix, and another 2 longs as the temporary buffer for radix sort.

In UnsafeExternalSorter, we set the DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD to be 1024 * 1024 * 1024 / 2, and hoping the max size of point array to be 8 GB. However this is wrong, 1024 * 1024 * 1024 / 2 * 32 is actually 16 GB, and if we grow the point array before reach this limitation, we may hit the max-page-size error.

This PR fixes this by making DEFAULT_NUM_ELEMENTS_FOR_SPILL_THRESHOLD 2 times smaller, and adding a safe check in UnsafeExternalSorter.growPointerArrayIfNecessary to avoid allocating a page larger than max page size.

{quote};;;","19/Oct/17 19:18;clehene;[~srowen] I'm hitting this while running some large jobs. I'm planning on patching and giving this a run today.;;;","30/Oct/17 17:07;cloud_fan;[~clehene] I think you hit a different issue. By looking at the stacktrace, seems your task failed because of running out of memory. There can be 2 possible reasons: 1) you have too many tasks running in one executor and the executor doesn't have enough memory. 2) a memory leak bug in Spark. Feel free to create a new ticket for it.;;;","16/Apr/18 08:28;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/21077;;;","08/May/18 18:33;tgraves;[~cloud_fan] the followup PR [https://github.com/apache/spark/pull/21077] didn't go into spark 2.3.0, this should have had its own Jira and we need to udpate the fix version.  Can you please fix so we properly track what version this is in.  Also does this need to be backported to 2.3.1?;;;","09/May/18 02:23;cloud_fan;The followup is just a code cleanup, I think we should not backport.

I agree that it should have its own JIRA, [~yuming] can you create a new JIRA for that PR? thanks!;;;","14/May/20 01:38;fanyunbojerry;[~clehene] Since it's 2020, have you solved the problem?

I‘m seeing the same one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BroadcastHashJoin producing wrong results,SPARK-20998,13077655,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mohitgargk,mohitgargk,06/Jun/17 12:13,09/Jun/17 10:30,14/Jul/23 06:30,09/Jun/17 10:29,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.0,,,,,SQL,,,,,0,,,,,,,,,"I have a hive table : _DistributionAttributes_, with 

*Schema*: 

root
 |-- distributionstatus: string (nullable = true)
 |-- enabledforselectionflag: boolean (nullable = true)
 |-- sourcedistributionid: integer (nullable = true)
 |-- rowstartdate: date (nullable = true)
 |-- rowenddate: date (nullable = true)
 |-- rowiscurrent: string (nullable = true)
 |-- dwcreatedate: timestamp (nullable = true)
 |-- dwlastupdatedate: timestamp (nullable = true)
 |-- appid: integer (nullable = true)
 |-- siteid: integer (nullable = true)
 |-- brandid: integer (nullable = true)


*DataFrame*

val df = spark.sql(""SELECT  s.sourcedistributionid as sid, t.sourcedistributionid as tid,  s.appid as sapp, t.appid as tapp,  s.brandid as sbrand, t.brandid as tbrand FROM DistributionAttributes t INNER JOIN DistributionAttributes s  ON t.sourcedistributionid=s.sourcedistributionid AND t.appid=s.appid  AND t.brandid=s.brandid"").


*Without BroadCastJoin* ( spark-shell --conf ""spark.sql.autoBroadcastJoinThreshold=-1"") : 

df.explain
== Physical Plan ==
*Project [sourcedistributionid#71 AS sid#0, sourcedistributionid#60 AS tid#1, appid#77 AS sapp#2, appid#66 AS tapp#3, brandid#79 AS sbrand#4, brandid#68 AS tbrand#5]
+- *SortMergeJoin [sourcedistributionid#60, appid#66, brandid#68], [sourcedistributionid#71, appid#77, brandid#79], Inner
   :- *Sort [sourcedistributionid#60 ASC, appid#66 ASC, brandid#68 ASC], false, 0
   :  +- Exchange hashpartitioning(sourcedistributionid#60, appid#66, brandid#68, 200)
   :     +- *Filter ((isnotnull(sourcedistributionid#60) && isnotnull(brandid#68)) && isnotnull(appid#66))
   :        +- HiveTableScan [sourcedistributionid#60, appid#66, brandid#68], MetastoreRelation  distributionattributes, t
   +- *Sort [sourcedistributionid#71 ASC, appid#77 ASC, brandid#79 ASC], false, 0
      +- Exchange hashpartitioning(sourcedistributionid#71, appid#77, brandid#79, 200)
         +- *Filter ((isnotnull(sourcedistributionid#71) && isnotnull(appid#77)) && isnotnull(brandid#79))
            +- HiveTableScan [sourcedistributionid#71, appid#77, brandid#79], MetastoreRelation  distributionattributes, s

df.show
|sid|tid|sapp|tapp|sbrand|tbrand|
| 22| 22|  61|  61|   614|   614|
| 29| 29|  65|  65|     0|     0|
| 30| 30|  12|  12|   121|   121|
| 10| 10|  73|  73|   731|   731|
| 24| 24|  61|  61|   611|   611|
| 35| 35|  65|  65|     0|     0|


*With BroadCastJoin* ( spark-shell )

df.explain

== Physical Plan ==
*Project [sourcedistributionid#136 AS sid#65, sourcedistributionid#125 AS tid#66, appid#142 AS sapp#67, appid#131 AS tapp#68, brandid#144 AS sbrand#69, brandid#133 AS tbrand#70]
+- *BroadcastHashJoin [sourcedistributionid#125, appid#131, brandid#133], [sourcedistributionid#136, appid#142, brandid#144], Inner, BuildRight
   :- *Filter ((isnotnull(brandid#133) && isnotnull(appid#131)) && isnotnull(sourcedistributionid#125))
   :  +- HiveTableScan [sourcedistributionid#125, appid#131, brandid#133], MetastoreRelation  distributionattributes, t
   +- BroadcastExchange HashedRelationBroadcastMode(List((shiftleft((shiftleft(cast(input[0, int, false] as bigint), 32) | (cast(input[1, int, false] as bigint) & 4294967295)), 32) | (cast(input[2, int, false] as bigint) & 4294967295))))
      +- *Filter ((isnotnull(brandid#144) && isnotnull(sourcedistributionid#136)) && isnotnull(appid#142))
         +- HiveTableScan [sourcedistributionid#136, appid#142, brandid#144], MetastoreRelation  distributionattributes, s

df.show
|sid|tid|sapp|tapp|sbrand|tbrand|
| 15| 22|  61|  61|   614|   614|
| 13| 22|  61|  61|   614|   614|
| 10| 22|  61|  61|   614|   614|
|  7| 22|  61|  61|   614|   614|
|  9| 22|  61|  61|   614|   614|
| 16| 22|  61|  61|   614|   614|",,anuj.thakwani@gmail.com,DjvuLee,iamhumanbeing,maropu,mohitgargk,viirya,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/17 10:39;mohitgargk;part-r-00000-e071fc92-9f0f-4ac9-acd6-75fe74d8b175.snappy.orc;https://issues.apache.org/jira/secure/attachment/12871812/part-r-00000-e071fc92-9f0f-4ac9-acd6-75fe74d8b175.snappy.orc",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 10:29:17 UTC 2017,,,,,,,,,,"0|i3fxb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/17 19:20;maropu;How about v2.1? The version has the same issue?;;;","07/Jun/17 06:51;viirya;Can you provide a sample data to reproduce this issue?;;;","07/Jun/17 10:10;anuj.thakwani@gmail.com;This could be duplicate of https://issues.apache.org/jira/browse/SPARK-17806;;;","07/Jun/17 10:40;mohitgargk;[~viirya] Attached an orc file which is giving this error. I used simply val df = spark.read.orc(""file:///mnt/mohit/data/*"") to read it.;;;","08/Jun/17 07:14;viirya;I've tried on current codebase and can't reproduce it. After comparing the query plans, I think [~anuj.thakwani@gmail.com] is correct. This should be the same issue of SPARK-17806.

SPARK-17806 is only in 2.0.2 and your affect version is 2.0.0. Can you try it on 2.0.2 if you could?;;;","09/Jun/17 10:28;mohitgargk;The issue seems to be fixed in . 2.1.0. I shall close this with Fix version = 2.1.0.;;;","09/Jun/17 10:29;mohitgargk;It was observed that this happens when there are more than two keys in ON clause of join. 
This has already been fixed in 2.1.0 onwards;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"spark-submit's --driver-cores marked as ""YARN-only"" but listed under ""Spark standalone with cluster deploy mode only""",SPARK-20997,13077639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,guoxiaolongzte,jlaskowski,jlaskowski,06/Jun/17 11:04,09/Jun/17 13:27,14/Jul/23 06:30,09/Jun/17 13:27,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Documentation,Spark Submit,,,,0,,,,,,,,,"Just noticed that {{spark-submit}} describes {{--driver-cores}} under:

* Spark standalone with cluster deploy mode only
* YARN-only

While I can understand ""only"" in ""Spark standalone with cluster deploy mode only"" to refer to cluster deploy mode (not the default client mode), but YARN-only baffles me which I think deserves a fix.
",,apachespark,guoxiaolongzte,jlaskowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 13:27:06 UTC 2017,,,,,,,,,,"0|i3fx7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/17 11:11;srowen;I suppose this should be removed from ""YARN-only"", and the previous section should be called ""Spark standalone or YARN with cluster deploy mode only"". The help message is getting hairy, but this seems like an improvement.;;;","07/Jun/17 05:30;guoxiaolongzte;Can i fix this jira? Because i found a similar problem here. I am revising these questions together.
[~srowen]
[~jlaskowski];;;","07/Jun/17 09:38;jlaskowski;Go ahead! Thanks [~guoxiaolongzte]!;;;","08/Jun/17 10:16;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/18241;;;","09/Jun/17 13:27;srowen;Issue resolved by pull request 18241
[https://github.com/apache/spark/pull/18241];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BROADCAST_TIMEOUT conf should be a timeoutConf,SPARK-20991,13077334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coyotehills,coyotehills,coyotehills,05/Jun/17 20:05,06/Jun/17 00:50,14/Jul/23 06:30,06/Jun/17 00:48,2.2.0,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,coyotehills,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 05 20:08:04 UTC 2017,,,,,,,,,,"0|i3fw3z:",9223372036854775807,,,,,,,,,,,,,2.2.1,,,,,,,,,,,"05/Jun/17 20:08;apachespark;User 'liufengdb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to start multiple workers on one host if external shuffle service is enabled in standalone mode,SPARK-20989,13077306,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,05/Jun/17 18:05,20/Jun/17 09:18,14/Jul/23 06:30,20/Jun/17 09:17,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Deploy,Spark Core,,,,0,,,,,,,,,"In standalone mode, if we enable external shuffle service by setting `spark.shuffle.service.enabled` to true, and then we try to start multiple workers on one host(by setting `SPARK_WORKER_INSTANCES=3` in spark-env.sh, and then run `sbin/start-slaves.sh`), we can only launch one worker on each host successfully and the rest of the workers fail to launch.

The reason is the port of external shuffle service if configed by `spark.shuffle.service.port`, so currently we could start no more than one external shuffle service on each host. In our case, each worker tries to start a external shuffle service, and only one of them successed doing this.",,apachespark,cloud_fan,jiangxb1987,vish741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 09:17:48 UTC 2017,,,,,,,,,,"0|i3fvxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/17 19:27;srowen;I'm not clear that this is intended to work this way, but if it's not, maybe the error could be clearer.;;;","13/Jun/17 15:29;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18290;;;","20/Jun/17 09:17;cloud_fan;Issue resolved by pull request 18290
[https://github.com/apache/spark/pull/18290];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV emits NPE when the number of tokens is less than given schema and corrupt column is given,SPARK-20978,13077167,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,05/Jun/17 05:09,12/Dec/22 17:51,14/Jul/23 06:30,05/Sep/17 15:22,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, if the number of tokens is less than the given schema, CSV datasource throws an NPE as below:

{code}
scala> spark.read.schema(""a string, b string, unparsed string"").option(""columnNameOfCorruptRecord"", ""unparsed"").csv(Seq(""a"").toDS).show()
17/06/05 13:59:26 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.NullPointerException
	at scala.collection.immutable.StringLike$class.stripLineEnd(StringLike.scala:89)
	at scala.collection.immutable.StringOps.stripLineEnd(StringOps.scala:29)
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$getCurrentInput(UnivocityParser.scala:56)
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$1.apply(UnivocityParser.scala:211)
	at org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$1.apply(UnivocityParser.scala:211)
	at org.apache.spark.sql.execution.datasources.FailureSafeParser$$anonfun$2.apply(FailureSafeParser.scala:50)
	at org.apache.spark.sql.execution.datasources.FailureSafeParser$$anonfun$2.apply(FailureSafeParser.scala:43)
	at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:64)
	at org.apache.spark.sql.DataFrameReader$$anonfun$11$$anonfun$apply$4.apply(DataFrameReader.scala:471)
	at org.apache.spark.sql.DataFrameReader$$anonfun$11$$anonfun$apply$4.apply(DataFrameReader.scala:471)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
{code}

If this is not given, it works as below:

{code}
scala> spark.read.schema(""a string, b string, unparsed string"").csv(Seq(""a"").toDS).show()
+---+----+--------+
|  a|   b|unparsed|
+---+----+--------+
|  a|null|    null|
+---+----+--------+
{code}",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 15:22:33 UTC 2017,,,,,,,,,,"0|i3fv2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/17 05:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18200;;;","04/Sep/17 01:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/19113;;;","05/Sep/17 15:22;cloud_fan;Issue resolved by pull request 19113
[https://github.com/apache/spark/pull/19113];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in CollectionAccumulator,SPARK-20977,13077164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jira.shegalov,sharkd,sharkd,05/Jun/17 03:44,22/Feb/21 06:58,14/Jul/23 06:30,21/Feb/21 02:59,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,3.1.1,3.2.0,,,,Spark Core,,,,,1,,,,,,,,,"{code:java}
17/06/03 13:39:31 ERROR Utils: Uncaught exception in thread heartbeat-receiver-event-loop-thread
java.lang.NullPointerException
	at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:464)
	at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:439)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6$$anonfun$7.apply(TaskSchedulerImpl.scala:408)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6$$anonfun$7.apply(TaskSchedulerImpl.scala:408)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6.apply(TaskSchedulerImpl.scala:408)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6.apply(TaskSchedulerImpl.scala:407)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
	at org.apache.spark.scheduler.TaskSchedulerImpl.executorHeartbeatReceived(TaskSchedulerImpl.scala:407)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2$$anonfun$run$2.apply$mcV$sp(HeartbeatReceiver.scala:129)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2.run(HeartbeatReceiver.scala:128)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}


Is that the bug of spark? Has anybody ever hit the problem?",,ajithshetty,apachespark,borice,howieyu,panga,sharkd,yumwang,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 21 02:59:25 UTC 2021,,,,,,,,,,"0|i3fv27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/17 09:30;srowen;Questions should go to the mailing list. There's no info about how you encountered this. I am not sure how it can happen, because _list can't be null and that seems to be the only thing that can trigger an NPE on this line. If you have more context about a modification or edge case that can possibly make this happen, add that information;;;","06/Jun/17 23:35;borice;I am seeing this problem as well.  

I have a scala SBT project with JavaAppPackaging using Spark 2.1.1.
I built my project with ""sbt stage"" and when I ran the resulting app, the log showed the same error reported here.

{noformat}
17/06/06 19:25:09 ERROR [heartbeat-receiver-event-loop-thread] o.a.s.u.Utils: Uncaught exception in thread heartbeat-receiver-even
t-loop-thread
java.lang.NullPointerException: null
        at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:464)
        at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:439)
        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6$$anonfun$7.apply(TaskSchedulerImpl.scala:408)
        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6$$anonfun$7.apply(TaskSchedulerImpl.scala:408)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.Iterator$class.foreach(Iterator.scala:742)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6.apply(TaskSchedulerImpl.scala:408)
        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$6.apply(TaskSchedulerImpl.scala:407)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:252)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:252)
        at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
        at org.apache.spark.scheduler.TaskSchedulerImpl.executorHeartbeatReceived(TaskSchedulerImpl.scala:407)
        at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2$$anonfun$run$2.apply$mcV$sp(HeartbeatReceiver.scala:129)
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
        at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2.run(HeartbeatReceiver.scala:128)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:748)
{noformat};;;","16/Jun/17 16:35;panga;I have the exactly same issue Spark 2.0.1 cluster under high load (master + 3 workers)

{code:java}
17/06/16 13:17:30 ERROR Utils: Uncaught exception in thread heartbeat-receiver-event-loop-thread
java.lang.NullPointerException
	at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:461)
	at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:438)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5$$anonfun$6.apply(TaskSchedulerImpl.scala:396)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5$$anonfun$6.apply(TaskSchedulerImpl.scala:396)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5.apply(TaskSchedulerImpl.scala:396)
	at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$5.apply(TaskSchedulerImpl.scala:395)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
	at org.apache.spark.scheduler.TaskSchedulerImpl.executorHeartbeatReceived(TaskSchedulerImpl.scala:395)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2$$anonfun$run$2.apply$mcV$sp(HeartbeatReceiver.scala:128)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1287)
	at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2.run(HeartbeatReceiver.scala:127)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}
;;;","28/Aug/18 07:41;howieyu;I use pyspark 2.3.1 also have this problem , but in different line

 

[2018-08-28 15:29:06,146][ERROR] Utils                    : Uncaught exception in thread heartbeat-receiver-event-loop-thread
java.lang.NullPointerException
    at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:477)
    at org.apache.spark.util.CollectionAccumulator.value(AccumulatorV2.scala:449)
    at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$8$$anonfun$9.apply(TaskSchedulerImpl.scala:449)
    at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$8$$anonfun$9.apply(TaskSchedulerImpl.scala:449)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.Iterator$class.foreach(Iterator.scala:893)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$8.apply(TaskSchedulerImpl.scala:449)
    at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$8.apply(TaskSchedulerImpl.scala:448)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)
    at org.apache.spark.scheduler.TaskSchedulerImpl.executorHeartbeatReceived(TaskSchedulerImpl.scala:448)
    at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2$$anonfun$run$2.apply$mcV$sp(HeartbeatReceiver.scala:129)
    at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1360)
    at org.apache.spark.HeartbeatReceiver$$anonfun$receiveAndReply$1$$anon$2.run(HeartbeatReceiver.scala:128)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748);;;","19/Feb/19 19:47;zsxwing;Reopening this issue as I believe I understand the cause. An accumulator is escaped before it's fully constructed in ""readObject"": https://github.com/apache/spark/blob/b19a28dea098c7d6188f8540429c50f42952d678/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala#L195

""The object should not be made visible to other threads, nor should the final fields be read, until all updates to the final fields of the object are complete."" (https://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.3);;;","10/Feb/21 01:57;apachespark;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/31540;;;","21/Feb/21 02:59;srowen;Issue resolved by pull request 31540
[https://github.com/apache/spark/pull/31540];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify Error Messages for FAILFAST mode. ,SPARK-20976,13077148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,04/Jun/17 20:48,08/Jun/17 19:10,14/Jul/23 06:30,08/Jun/17 19:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Previously, we indicate the job was terminated because of `FAILFAST` mode. 
{noformat}
Malformed line in FAILFAST mode: {""a"":{, b:3}
{noformat}

If possible, we should keep it.",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 08 19:10:56 UTC 2017,,,,,,,,,,"0|i3fuyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/17 20:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18196;;;","08/Jun/17 19:10;cloud_fan;Issue resolved by pull request 18196
[https://github.com/apache/spark/pull/18196];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
we should run REPL tests if SQL core has code changes,SPARK-20974,13077019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,02/Jun/17 23:18,03/Jun/17 05:01,14/Jul/23 06:30,03/Jun/17 05:01,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Build,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 23:21:05 UTC 2017,,,,,,,,,,"0|i3fu5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/17 23:21;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18191;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DESCRIBE showing 1 extra row of ""| # col_name  | data_type  | comment  |""",SPARK-20954,13076613,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,garrosc,garrosc,01/Jun/17 18:46,15/Jun/17 00:39,14/Jul/23 06:30,09/Jun/17 04:03,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"I am trying to do DESCRIBE on a table but seeing 1 extra row being auto-added to the result. You can see there is this 1 extra row with ""| # col_name  | data_type  | comment  |"" ; however, select and select count(*) only shows 1 row.

I searched online a long time and do not find any useful information.
Is this a bug?

hdp106m2:/usr/hdp/2.5.0.2-3/spark2 # ./bin/beeline
Beeline version 1.2.1.spark2 by Apache Hive
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: backward-delete-word
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
[INFO] Unable to bind key for unsupported operation: up-history
[INFO] Unable to bind key for unsupported operation: down-history
beeline> !connect jdbc:hive2://localhost:10016
Connecting to jdbc:hive2://localhost:10016
Enter username for jdbc:hive2://localhost:10016: hive
Enter password for jdbc:hive2://localhost:10016: ****
17/06/01 14:13:04 INFO Utils: Supplied authorities: localhost:10016
17/06/01 14:13:04 INFO Utils: Resolved authority: localhost:10016
17/06/01 14:13:04 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://localhost:10016
Connected to: Spark SQL (version 2.2.1-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ

0: jdbc:hive2://localhost:10016> describe garros.hivefloat;
+-------------+------------+----------+--+
|  col_name   | data_type  | comment  |
+-------------+------------+----------+--+
| # col_name  | data_type  | comment  |
| c1          | float      | NULL     |
+-------------+------------+----------+--+
2 rows selected (0.396 seconds)

0: jdbc:hive2://localhost:10016> select * from garros.hivefloat;
+---------------------+--+
|         c1          |
+---------------------+--+
| 123.99800109863281  |
+---------------------+--+
1 row selected (0.319 seconds)

0: jdbc:hive2://localhost:10016> select count(*) from garros.hivefloat;
+-----------+--+
| count(1)  |
+-----------+--+
| 1         |
+-----------+--+
1 row selected (0.783 seconds)

0: jdbc:hive2://localhost:10016> describe formatted garros.hiveint;
+-------------------------------+-------------------------------------------------------------+----------+--+
|           col_name            |                          data_type                          | comment  |
+-------------------------------+-------------------------------------------------------------+----------+--+
| # col_name                    | data_type                                                   | comment  |
| c1                            | int                                                         | NULL     |
|                               |                                                             |          |
| # Detailed Table Information  |                                                             |          |
| Database                      | garros                                                      |          |
| Table                         | hiveint                                                     |          |
| Owner                         | root                                                        |          |
| Created                       | Thu Feb 09 17:40:36 EST 2017                                |          |
| Last Access                   | Wed Dec 31 19:00:00 EST 1969                                |          |
| Type                          | MANAGED                                                     |          |
| Provider                      | hive                                                        |          |
| Properties                    | [serialization.format=1]                                    |          |
| Location                      | hdfs://HDP106/apps/hive/warehouse/garros.db/hiveint         |          |
| Serde Library                 | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe          |          |
| InputFormat                   | org.apache.hadoop.mapred.TextInputFormat                    |          |
| OutputFormat                  | org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat  |          |
| Partition Provider            | Catalog                                                     |          |
+-------------------------------+-------------------------------------------------------------+----------+--+
17 rows selected (0.304 seconds)
",,apachespark,cloud_fan,dongjoon,garrosc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20067,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 15 00:39:25 UTC 2017,,,,,,,,,,"0|i3frnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/17 05:48;dongjoon;Hi, [~garrosc]. This is due to recent incompatible change at SPARK-20067.
I'll make a PR to fix this because Spark 2.2.0 is not yet released.;;;","05/Jun/17 06:00;dongjoon;Hmm. Let me ask the committers first. The intention of that PR seems to be clear to unify into `DESCRIBE FORMATTED` format.;;;","05/Jun/17 06:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18203;;;","09/Jun/17 00:12;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18245;;;","09/Jun/17 04:03;cloud_fan;Issue resolved by pull request 18245
[https://github.com/apache/spark/pull/18245];;;","09/Jun/17 05:17;dongjoon;Hi, Wenchen.
I'm Dongjoon. :);;;","09/Jun/17 05:49;cloud_fan;oh sorry I misclicked...;;;","09/Jun/17 05:51;dongjoon;^^;;;","14/Jun/17 23:08;garrosc;Hi [~dongjoon]

I downloaded (spark-2.2.1-SNAPSHOT-bin-hadoop2.7.tgz	2017-06-14 09:44	194M) from https://people.apache.org/~pwendell/spark-nightly/spark-branch-2.2-bin/latest/

I still see that one extra line in DESCRIBE.
Does this latest tgz contains this JIRA fix?

Thanks
;;;","14/Jun/17 23:55;dongjoon;FYI, the following is the result on `branch-2.2`. I'm not sure how the snapshot is built.
{code}
~/s/spark-master:branch-2.2$ current_shell
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1-SNAPSHOT
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""show tables"").show
+--------+---------+-----------+
|database|tableName|isTemporary|
+--------+---------+-----------+
| default|        t|      false|
+--------+---------+-----------+


scala> sql(""desc t"").show
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|       a|      int|   null|
+--------+---------+-------+


scala> sql(""desc extended t"").show
+--------------------+--------------------+-------+
|            col_name|           data_type|comment|
+--------------------+--------------------+-------+
|                   a|                 int|   null|
...
{code};;;","14/Jun/17 23:58;garrosc;Hi [~dongjoon]

I see. Would you be able to tell me where I can get a latest build of 2.2.1 containing this fix?
If not, who else I can ask or where I can find this information?

Thanks
;;;","15/Jun/17 00:00;dongjoon;RC5 is coming very soon with this fix. :);;;","15/Jun/17 00:02;garrosc;Hi [~dongjoon]

I see. Do you mean spark-2.2.0-rc5 :) ?

Also, would you be able to find out if rc5 will contain fix for SPARK-12868?

Thanks!;;;","15/Jun/17 00:08;dongjoon;Yep. 2.2.0-RC5. It also includes SPARK-12868, too.;;;","15/Jun/17 00:14;garrosc;Hi [~dongjoon]

Thanks for your confirmation!
Do you know when RC5 will come out?

Also, I just noticed this behavior.

I am using (spark-2.2.1-SNAPSHOT-bin-hadoop2.7.tgz	2017-06-14 09:44	194M)
I have looked at your test result above using Scala.

I tried the same thing and I did not see the extra line in DESC.

{code:java}
scala> sql(""desc hiveint"").show
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|      c1|      int|   null|
+--------+---------+-------+
{code}

But when I tried with Beeline, it has that extra line again.

{code:java}
0: jdbc:hive2://localhost:10016> describe hiveint;
+-------------+------------+----------+--+
|  col_name   | data_type  | comment  |
+-------------+------------+----------+--+
| # col_name  | data_type  | comment  |
| c1          | int        | NULL     |
+-------------+------------+----------+--+
{code}

Any idea why please?
;;;","15/Jun/17 00:38;dongjoon;This is the same result over beeline on branch-2.2. I think you didn't stop your previous STS.
{code}
~/s/spark-master:branch-2.2$ bin/beeline -u jdbc:hive2://localhost:10000 -n hive -p password -e 'desc table t'
Connecting to jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 2.2.1-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| a         | int        | NULL     |
+-----------+------------+----------+--+
1 row selected (0.122 seconds)
Beeline version 1.2.1.spark2 by Apache Hive
Closing: 0: jdbc:hive2://localhost:10000
~/s/spark-master:branch-2.2$ bin/beeline -u jdbc:hive2://localhost:10000 -n hive -p password -e 'desc extended t'
Connecting to jdbc:hive2://localhost:10000
Connected to: Spark SQL (version 2.2.1-SNAPSHOT)
Driver: Hive JDBC (version 1.2.1.spark2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
+-------------------------------+--------------------------------------------------------------------+----------+--+
|           col_name            |                             data_type                              | comment  |
+-------------------------------+--------------------------------------------------------------------+----------+--+
| a                             | int                                                                | NULL     |
|                               |                                                                    |          |
{code};;;","15/Jun/17 00:39;dongjoon;FYI, Apache Spark STS uses 10000 as a port number.;;;",,,,,,,,,,,,,,,,,,,,,,
Encoding/decoding issue in PySpark pipe implementation,SPARK-20947,13076417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chaoslawful,chaoslawful,chaoslawful,01/Jun/17 07:36,12/Dec/22 18:11,14/Jul/23 06:30,22/Jan/18 01:46,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,2.4.0,,,,,PySpark,,,,,0,,,,,,,,,"Pipe action convert objects into strings using a way that was affected by the default encoding setting of Python environment.

Here is the related code fragment (L717-721@python/pyspark/rdd.py):
{code}
            def pipe_objs(out):
                for obj in iterator:
                    s = str(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
{code}

The `str(obj)` part implicitly convert `obj` to an unicode string, then encode it into a byte string using default encoding; On the other hand, the `s.encode('utf-8')` part implicitly decode `s` into an unicode string using default encoding and then encode it (AGAIN!) into a UTF-8 encoded byte string.

Typically the default encoding of Python environment would be 'ascii', which means passing  an unicode string containing characters beyond 'ascii' charset will raise UnicodeEncodeError exception at `str(obj)` and passing a byte string containing bytes greater than 128 will again raise UnicodeEncodeError exception at 's.encode('utf-8')`.

Changing `str(obj)` to `unicode(obj)` would eliminate these problems.

The following code snippet reproduces these errors:
{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.3
      /_/

Using Python version 2.7.12 (default, Jul 25 2016 15:06:45)
SparkContext available as sc, HiveContext available as sqlContext.
>>> sc.parallelize([u'\u6d4b\u8bd5']).pipe('cat').collect()
[Stage 0:>                                                          (0 + 4) / 4]Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/Users/wxz/Downloads/spark-1.6.3-bin-hadoop2.6/python/pyspark/rdd.py"", line 719, in pipe_objs
    s = str(obj).rstrip('\n') + '\n'
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)
>>>
>>> sc.parallelize([u'\u6d4b\u8bd5']).map(lambda x: x.encode('utf-8')).pipe('cat').collect()
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/local/Cellar/python/2.7.12/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/Users/wxz/Downloads/spark-1.6.3-bin-hadoop2.6/python/pyspark/rdd.py"", line 720, in pipe_objs
    out.write(s.encode('utf-8'))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe6 in position 0: ordinal not in range(128)
{code}
",,apachespark,chaoslawful,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/17 07:41;chaoslawful;fix-pipe-encoding-error.patch;https://issues.apache.org/jira/secure/attachment/12870750/fix-pipe-encoding-error.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 22 01:46:37 UTC 2018,,,,,,,,,,"0|i3fqgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/17 07:41;chaoslawful;Patch that fixed encode/decode error of pipe action;;;","12/Jun/17 10:21;apachespark;User 'chaoslawful' has created a pull request for this issue:
https://github.com/apache/spark/pull/18277;;;","22/Jan/18 01:46;gurwls223;Fixed in https://github.com/apache/spark/pull/18277;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElementException key not found in TaskSchedulerImpl,SPARK-20945,13076382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liupengcheng,liupengcheng,liupengcheng,01/Jun/17 03:31,17/May/20 17:46,14/Jul/23 06:30,05/Jun/17 09:23,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"We started spark ThriftServer, however when a job fails it caused the thriftserver also shutted down.
{quote}
2017-05-27,10:34:27,272 INFO org.apache.spark.scheduler.cluster.YarnScheduler: Cancelling stage 21
2017-05-27,10:34:27,274 ERROR org.apache.spark.scheduler.DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext
java.util.NoSuchElementException: key not found: 14100
at scala.collection.MapLike$class.default(MapLike.scala:228)
at scala.collection.AbstractMap.default(Map.scala:59)
at scala.collection.mutable.HashMap.apply(HashMap.scala:65)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:226)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:225)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:225)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:225)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:218)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:218)
at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:217)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:217)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1461)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1447)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1447)
at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1447)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
at scala.Option.foreach(Option.scala:257)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{quote}",,apachespark,joshrosen,liupengcheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Mon Jun 05 09:23:15 UTC 2017,,,,,,,,,,"0|i3fq8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/17 03:46;liupengcheng;I finally find out this bug may caused by some inconsistency in TaskSchedulerImpl. Code related is as following:
{code}
  override def cancelTasks(stageId: Int, interruptThread: Boolean): Unit = synchronized {
    logInfo(""Cancelling stage "" + stageId)
    taskSetsByStageIdAndAttempt.get(stageId).foreach { attempts =>
      attempts.foreach { case (_, tsm) =>
        // There are two possible cases here:
        // 1. The task set manager has been created and some tasks have been scheduled.
        //    In this case, send a kill signal to the executors to kill the task and then abort
        //    the stage.
        // 2. The task set manager has been created but no tasks has been scheduled. In this case,
        //    simply abort the stage.
        
          tsm.runningTasksSet.foreach{ tid =>
          val execId = taskIdToExecutorId(tid)
          backend.killTask(tid, execId, interruptThread)
         }
        tsm.abort(""Stage %s cancelled"".format(stageId))
        logInfo(""Stage %d was cancelled"".format(stageId))
      }
    }
  }
{code}

When Starting a new task, tid is inserted into tsm.runningTasksSet and taskIdToExecutorId, 
but when ExecutorLost happens, tid on the executor is cleaned up in taskIdToExecutorId 
with tid in tsm.runningTasksSet not cleaned until PendingLossReason got.
In this case, if another task fails 4 times and causes scheduler canceling stage, 
The above function cancelTasks may be triggered, and the exception is threw out.;;;","01/Jun/17 03:51;liupengcheng;https://github.com/apache/spark/pull/18171;;;","01/Jun/17 04:44;apachespark;User 'liupc' has created a pull request for this issue:
https://github.com/apache/spark/pull/18171;;;","05/Jun/17 09:23;srowen;Issue resolved by pull request 18171
[https://github.com/apache/spark/pull/18171];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subquery Reuse does not work,SPARK-20941,13076330,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,31/May/17 22:11,01/Jun/17 22:57,14/Jul/23 06:30,01/Jun/17 22:57,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"- Subquery reuse does not work.  
- It is sharing the same `SQLConf` (`spark.sql.exchange.reuse`) with the one for Exchange Reuse. 
- No test case covers the rule Subquery reuse. 
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 22:12:05 UTC 2017,,,,,,,,,,"0|i3fpx3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"31/May/17 22:12;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18169;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AccumulatorV2 should not throw IllegalAccessError,SPARK-20940,13076313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,31/May/17 21:13,01/Jun/17 00:28,14/Jul/23 06:30,01/Jun/17 00:28,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Spark Core,,,,,0,,,,,,,,,IllegalAccessError is a LinkageError which is a fatal error. We should use IllegalStateException instead.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 21:20:03 UTC 2017,,,,,,,,,,"0|i3fptb:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"31/May/17 21:20;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18168;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"A daemon thread, ""BatchedWriteAheadLog Writer"", left behind after terminating StreamingContext.",SPARK-20935,13076122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,chtyim,chtyim,31/May/17 08:14,12/Dec/22 18:10,14/Jul/23 06:30,11/Jun/17 08:55,1.6.3,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,DStreams,,,,,0,,,,,,,,,"With batched write ahead log on by default in driver (SPARK-11731), if there is no receiver based {{InputDStream}}, the ""BatchedWriteAheadLog Writer"" thread created by {{BatchedWriteAheadLog}} never get shutdown. 

The root cause is due to https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ReceiverTracker.scala#L168

that it never call {{ReceivedBlockTracker.stop()}} (which in turn call {{BatchedWriteAheadLog.close()}}) if there is no receiver based input.",,apachespark,chtyim,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 11 08:55:24 UTC 2017,,,,,,,,,,"0|i3fomv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/May/17 09:18;srowen;CC [~tdas] and [~hyukjin.kwon] who touched this code. It does look like it must be stop()-ed in all cases, because it's started in the constructor.  Seems OK to propose a PR, [~chtyim];;;","02/Jun/17 02:03;gurwls223;Thanks for pinging me. Could we just always stop() {{ReceivedBlockTracker}} and require {{WriteAheadLog.close()}} to be idempotent and make its implementations as so? I could propose a PR if it sounds okay instead.;;;","06/Jun/17 10:19;srowen;Sounds OK to me [~hyukjin.kwon] if you're up for it;;;","07/Jun/17 05:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18224;;;","11/Jun/17 08:55;srowen;Issue resolved by pull request 18224
[https://github.com/apache/spark/pull/18224];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LinearSVC should not use shared Param HasThresholds,SPARK-20929,13076048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,31/May/17 00:42,01/Jul/17 02:07,14/Jul/23 06:30,20/Jun/17 06:04,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"LinearSVC applies the Param 'threshold' to the rawPrediction, not the probability.  It has different semantics than the shared Param HasThreshold, so it should not use the shared Param.",,apachespark,iamhumanbeing,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 20 06:04:06 UTC 2017,,,,,,,,,,"0|i3fo6f:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"31/May/17 00:46;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/18151;;;","20/Jun/17 06:04;josephkb;Issue resolved by pull request 18151
[https://github.com/apache/spark/pull/18151];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exposure to Guava libraries by directly accessing tableRelationCache in SessionCatalog caused failures,SPARK-20926,13075950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rezasafi,rezasafi,rezasafi,30/May/17 19:31,03/Jul/17 11:15,14/Jul/23 06:30,05/Jun/17 20:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"Because of shading that we did for guava libraries, we see test failures whenever those components directly access tableRelationCache in SessionCatalog.
This can happen in any component that shaded guava library. Failures looks like this:
{noformat}
java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableRelationCache()Lcom/google/common/cache/Cache;
01:25:14   at org.apache.spark.sql.hive.test.TestHiveSparkSession.reset(TestHive.scala:492)
01:25:14   at org.apache.spark.sql.hive.test.TestHiveContext.reset(TestHive.scala:138)
01:25:14   at org.apache.spark.sql.hive.test.TestHiveSingleton$class.afterAll(TestHiveSingleton.scala:32)
01:25:14   at org.apache.spark.sql.hive.StatisticsSuite.afterAll(StatisticsSuite.scala:34)
01:25:14   at org.scalatest.BeforeAndAfterAll$class.afterAll(BeforeAndAfterAll.scala:213)
01:25:14   at org.apache.spark.SparkFunSuite.afterAll(SparkFunSuite.scala:31)
01:25:14   at org.scalatest.BeforeAndAfterAll$$anonfun$run$1.apply(BeforeAndAfterAll.scala:280)
01:25:14   at org.scalatest.BeforeAndAfterAll$$anonfun$run$1.apply(BeforeAndAfterAll.scala:278)
01:25:14   at org.scalatest.CompositeStatus.whenCompleted(Status.scala:377)
01:25:14   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:278)
{noformat}",,apachespark,rezasafi,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 06 16:55:20 UTC 2017,,,,,,,,,,"0|i3fnkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 19:32;rezasafi;I will post a pull request for this issue soon, by tonight at the latest.;;;","30/May/17 22:23;apachespark;User 'rezasafi' has created a pull request for this issue:
https://github.com/apache/spark/pull/18148;;;","05/Jun/17 20:22;vanzin;I chose not to put this in branch-2.2, but if someone runs into the issue during the test cycle it should be easy to do it later.;;;","06/Jun/17 16:55;vanzin;Turns out I didn't notice the PR was against 2.2 so it's there. Oh well, it's an internal change only anyway.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to call the function registered in the not-current database,SPARK-20924,13075898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,30/May/17 17:40,19/Aug/18 08:45,14/Jul/23 06:30,30/May/17 21:06,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"We are unable to call the function registered in the not-current database. 

{noformat}
sql(""CREATE DATABASE dAtABaSe1"")
sql(s""CREATE FUNCTION dAtABaSe1.test_avg AS '${classOf[GenericUDAFAverage].getName}'"")
sql(""SELECT dAtABaSe1.test_avg(1)"")
{noformat}

The above code returns an error:
{noformat}
Undefined function: 'dAtABaSe1.test_avg'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 7
{noformat}
",,apachespark,cloud_fan,rajeshhadoop,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 19 08:40:40 UTC 2018,,,,,,,,,,"0|i3fn93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 17:43;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18146;;;","30/May/17 21:06;cloud_fan;Issue resolved by pull request 18146
[https://github.com/apache/spark/pull/18146];;;","19/Aug/18 08:40;rajeshhadoop;[~cloud_fan]  / [~smilegator]

Looks like we can't backport this change to 2.1.0 as lot of dependency on improvement changes on Spark-2.2 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMetrics._updatedBlockStatuses uses a lot of memory,SPARK-20923,13075883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,30/May/17 16:44,23/Jun/17 01:25,14/Jul/23 06:30,23/Jun/17 01:20,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,releasenotes,,,,,,,,"The driver appears to use a ton of memory in certain cases to store the task metrics updated block status'.  For instance I had a user reading data form hive and caching it.  The # of tasks to read was around 62,000, they were using 1000 executors and it ended up caching a couple TB's of data.  The driver kept running out of memory. 

I investigated and it looks like there was 5GB of a 10GB heap being used up by the TaskMetrics._updatedBlockStatuses because there are a lot of blocks.

The updatedBlockStatuses was already removed from the task end event under SPARK-20084.  I don't see anything else that seems to be using this.  Anybody know if I missed something?

 If its not being used we should remove it, otherwise we need to figure out a better way of doing it so it doesn't use so much memory.",,anthony-truchet,apachespark,cloud_fan,joshrosen,rdblue,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20970,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 23 01:25:40 UTC 2017,,,,,,,,,,"0|i3fn5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 16:44;tgraves;[~rdblue]  with SPARK-20084, did you see anything using these updatedblockStatuses in TaskMetrics?;;;","30/May/17 18:05;rdblue;I didn't look at the code path up to writing history files. I just confirmed that nothing based on the history file actually used them. Sounds like if we can stop tracking this entirely, that would be great!;;;","30/May/17 18:52;tgraves;[~joshrosen] [~zsxwing] [~eseyfe] I think you have looked at this fairly recently, do you know if this is used by anything or anybody? I'm not finding it used anywhere in the code or UI but maybe I'm missing some obscure reference;;;","30/May/17 19:09;joshrosen;It doesn't seem to be used, as far as I can tell from a quick skim. The best way to confirm would probably be to start removing it, deleting things which depend on this as you go (e.g. the TaskMetrics getter method for accessing the current value) and see if you run into anything which looks like a non-test use. I'll be happy to review a patch to clean this up.;;;","30/May/17 20:22;tgraves;taking a quick look at the history of the _updatedBlockStatuses it looks like this used to be used for StorageStatusListener but it has been since changed to do this on the SparkListenerBlockUpdated event.  That BlockUpdated event is coming from the BlockManagerMaster.updateBlockInfo which is called by the executors.  So I'm not seeing anything use _updatedBlockStatuses.  I'll start to rip it out and see what I hit.;;;","31/May/17 14:47;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/18162;;;","23/Jun/17 01:25;cloud_fan;This patch changes the public behavior and we should mention it in the release notes. Basically users can track the status of updated blocks via {{SparkListenerTaskEnd}} event, but this feature was introduced for internal usage at the beginning and I'm wondering how many users are using this feature. After this patch we don't trach it anymore by default, users can still turn it on by setting {{spark.taskMetrics.trackUpdatedBlockStatuses}} to true.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsafe deserialization in Spark LauncherConnection,SPARK-20922,13075797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,adityasharad,adityasharad,30/May/17 10:58,14/May/18 08:04,14/Jul/23 06:30,01/Jun/17 21:46,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,2.3.0,,Spark Submit,,,,,0,security,,,,,,,,"The {{run()}} method of the class {{org.apache.spark.launcher.LauncherConnection}} performs unsafe deserialization of data received by its socket. This makes Spark applications launched programmatically using the {{SparkLauncher}} framework potentially vulnerable to remote code execution by an attacker with access to any user account on the local machine. Such an attacker could send a malicious serialized Java object to multiple ports on the local machine, and if this port matches the one (randomly) chosen by the Spark launcher, the malicious object will be deserialized. By making use of gadget chains in code present on the Spark application classpath, the deserialization process can lead to RCE or privilege escalation.

This vulnerability is identified by the “Unsafe deserialization” rule on lgtm.com:
https://lgtm.com/projects/g/apache/spark/snapshot/80fdc2c9d1693f5b3402a79ca4ec76f6e422ff13/files/launcher/src/main/java/org/apache/spark/launcher/LauncherConnection.java#V58 

Attached is a proof-of-concept exploit involving a simple {{SparkLauncher}}-based application and a known gadget chain in the Apache Commons Beanutils library referenced by Spark.
See the readme file for demonstration instructions.
",,adityasharad,apachespark,Fialkovsky,Pathreesyo,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 10:59;adityasharad;spark-deserialize-master.zip;https://issues.apache.org/jira/secure/attachment/12870374/spark-deserialize-master.zip",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 14 08:04:11 UTC 2018,,,,,,,,,,"0|i3fmmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 12:23;srowen;This is not the same as https://issues.apache.org/jira/browse/SPARK-11652 I take it.

[~vanzin] can maybe comment on the mechanics here, but it also sounds like the suggestion is that this gets around the shared secret because deserializing the message itself causes something to be executed.

Still, you have to be on the localhost to make the connection, right?
;;;","30/May/17 14:09;adityasharad;Yes, this is different from SPARK-11652, which focused on preventing a specific known gadget chain (found in Commons Collections). This issue involves the general problem of unconditionally deserializing untrusted data, and the proof-of-concept is simply an example of a gadget chain (in Commons Beanutils, and which cannot be addressed by updating the dependency) that works against the latest Spark dependencies.

I believe you are correct about the deserialization leading to code execution before the shared secret is established or checked.

Indeed, due to how the socket is opened, you must have access to the local machine to connect, but not necessarily to the same user that is running the Spark master or task.;;;","30/May/17 16:37;vanzin;Yeah, it's not as simple to exploit, but I guess we'll need custom serialization to avoid issues with 3rd-party libraries here... :-/;;;","31/May/17 17:59;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18166;;;","01/Jun/17 23:04;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18178;;;","05/Jun/17 05:25;adityasharad;I appreciate your quick response to this issue. I believe it would be appropriate to register a CVE ID -- is that something one of you would be willing to do?;;;","05/Jun/17 07:21;srowen;This is already publicly disclosed, which isn't a big deal because it's pretty limited in scope. Normally you'd discuss a CVE and disclosure after the fix on private@;;;","03/Aug/17 11:41;adityasharad;Apologies for the delay in getting back to you. I believe we first got in touch privately to report this, but in future we'll discuss the details and fix on private@ first if that fits better into your workflow.

The scope is indeed limited to attacks from local users and the issue is now publicly disclosed. However, I would argue neither of these points disqualifies the vulnerability reported here for the purposes of getting a CVE assigned.

Depending on the configuration and the intentions of an attacker, the repercussions of this vulnerability are potentially extremely severe despite the limited scope:
- The worst case is obviously when Spark runs as an administrative user.
- In the more common case where Spark runs under a user account that is also responsible for other services (like Hadoop, HDFS), the repercussions can be very severe. This is the case in the default Cloudera setup, for example. In that particular scenario, an attacker can cause a widespread outage by simply wiping all data that belongs to the 'hdfs' user. The repercussions reach far beyond Spark itself.
- In the 'best' case, Spark is set up to use a dedicated user account. Here we're looking at a DoS to Spark specifically, with a severe risk for data loss. An attacker can stop the service and wipe all of Spark's data.

We have seen significantly less severe vulnerabilities for which a CVE is assigned. The prime reasons for doing so are to advise users and to maintain a visible record of the issue that isn't project-specific, which I think would be appropriate in this case.

Please let me know if there's anything I can help with. I am willing to file separately for the CVE if that is easier, but I do not wish to do so without first having your agreement and finding out if Spark has a preferred CVE route. If you'd like to discuss this further off-list, please feel free to contact me on aditya@semmle.com.;;;","03/Aug/17 11:44;srowen;If you'd email a suggested CVE description to private@spark.apache.org, we can go through the motions of reporting it as one. The ASF process is: https://www.apache.org/security/ https://www.apache.org/security/projects.html;;;","05/Sep/17 18:09;srowen;This came up again today and our security folks also suggested this should be a CVE. I can work on this but feel free to supply text as a summary.;;;","07/Mar/18 17:06;Pathreesyo;Hi Guys,

I'm new to this vulnerability. Can someone share step by step procedure on how to remediate this vulnerability (Unsafe deserialization in Spark LauncherConnection). I've saw in this thread that there is an attached file (spark-deserialize-master.zip). Can someone share step by step on how to use this in our Linux machine? Btw, we're using CentOS.;;;","07/Mar/18 18:13;vanzin;You remediate it by upgrading to a version with the fix (see ""Fix Versions"").;;;","11/May/18 07:36;Fialkovsky;I can't update Spark to 2.2.

Will you make fix for Spark 1.6 branch ?;;;","11/May/18 16:38;vanzin;I think Spark 1.6 at this point is considered EOL by the community; there are no more planned releases for that line that I know of.;;;","11/May/18 18:11;vanzin;You should also be able to use just the spark-launcher library from a 2.x version to launch Spark 1.6 jobs, without having to update the rest of the Spark dependencies.;;;","14/May/18 08:04;Fialkovsky;Thanks for clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,
ForkJoinPool pools are leaked when writing hive tables with many partitions,SPARK-20920,13075759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,mrares,mrares,30/May/17 08:17,10/Jul/17 23:40,14/Jul/23 06:30,13/Jun/17 09:48,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"This bug is loosely related to SPARK-17396

In this case it happens when writing to a hive table with many, many, partitions (my table is partitioned by hour and stores data it gets from kafka in a spark streaming application):

df.repartition()
  .write
  .format(""orc"")
  .option(""path"", s""$tablesStoragePath/$tableName"")
  .mode(SaveMode.Append)
  .partitionBy(""dt"", ""hh"")
  .saveAsTable(tableName)

As this table grows beyond a certain size, ForkJoinPool pools start leaking. Upon examination (with a debugger) I found that the caller is AlterTableRecoverPartitionsCommand and the problem happens when `evalTaskSupport` is used (line 555). I have tried setting a very large threshold via `spark.rdd.parallelListingThreshold` and the problem went away.

My assumption is that the problem happens in this case and not in the one in SPARK-17396 due to the fact that AlterTableRecoverPartitionsCommand is a case class while UnionRDD is an object so multiple instances are not possible, therefore no leak.

Regards,
Rares",,apachespark,mrares,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 13 09:48:41 UTC 2017,,,,,,,,,,"0|i3fme7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 11:20;srowen;Duplicate of SPARK-20848, it seems, or others related to ForkJoinPool;;;","30/May/17 13:04;mrares;Yes, as per my comment, it's a related but distinct problem, I am hoping it's easily solvable by moving the ForkJoinPool into the companion object of the case class so that a single one is maintained;;;","30/May/17 14:36;srowen;Yeah, we can't use a ForkJoinPool like this. It has to be started, closed where used. It's a similar fix -- want to give it a try? See https://github.com/apache/spark/pull/18100;;;","06/Jun/17 10:35;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/18216;;;","13/Jun/17 09:48;srowen;Issue resolved by pull request 18216
[https://github.com/apache/spark/pull/18216];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FunctionIdentifier as function identifiers in FunctionRegistry,SPARK-20918,13075738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,30/May/17 05:02,03/Aug/21 07:14,14/Jul/23 06:30,09/Jun/17 17:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, the unquoted string of a function identifier is being used as the function identifier in the function registry. This could cause the incorrect the behavior when users use `.` in the function names. 

As an example, Spark can resolve a function like this
{code}
SELECT `d100.udf100`(`emp`.`name`) FROM `emp`;
{code}

Although the function name is wrapped with backticks, Spark still resolves it as database name + function name, which is wrong.",,apachespark,cloud_fan,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34971,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 17:16:52 UTC 2017,,,,,,,,,,"0|i3fm9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 05:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18142;;;","09/Jun/17 17:16;cloud_fan;Issue resolved by pull request 18142
[https://github.com/apache/spark/pull/18142];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error message for unaliased subqueries in FROM clause,SPARK-20916,13075727,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,joshrosen,joshrosen,30/May/17 00:20,30/May/17 13:29,14/Jul/23 06:30,30/May/17 13:29,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"The following query parses in branch-2.2, but doesn't parse correctly as of today's master:

{code}
SELECT x FROM (SELECT 1 AS x)
{code}

It still parses if you name the subquery in the FROM clause:

{code}
SELECT x FROM (SELECT 1 AS x) t
{code}

In master, this gives the following error:

{code}
scala> sql(""""""SELECT x FROM (SELECT 1 AS x)"""""")
org.apache.spark.sql.catalyst.parser.ParseException:
mismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 9)

== SQL ==
SELECT x FROM (SELECT 1 AS x)
---------^^^

  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)
  at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)
  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)
  ... 48 elided
{code}

It looks like this change is intentional due to SPARK-20690, but the error message that we give here isn't very clear. I think that we should improve it so as not to confuse users.",,apachespark,cloud_fan,joshrosen,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20690,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 30 13:29:11 UTC 2017,,,,,,,,,,"0|i3fm73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 00:49;joshrosen;It looks like this was caused by SPARK-20690 which changed the parser to reject unaliased subqueries in the FROM clause. However, the error message that we now give isn't very helpful. I think that we should modify the parser to throw a more clear error for such queries so that it's more clear to end users that this is a behavior change rather than a bug. /cc [~viirya] [~hvanhovell] [~smilegator]

;;;","30/May/17 01:14;viirya;I will look into this. Thanks.;;;","30/May/17 04:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18141;;;","30/May/17 13:29;cloud_fan;Issue resolved by pull request 18141
[https://github.com/apache/spark/pull/18141];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Javadoc contains code that is invalid,SPARK-20914,13075623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,cristian.teodor,cristian.teodor,29/May/17 14:22,03/Jul/17 11:15,14/Jul/23 06:30,08/Jun/17 09:57,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,Documentation,,,,,0,,,,,,,,,"i was looking over the [dataset|https://spark.apache.org/docs/2.1.1/api/java/org/apache/spark/sql/Dataset.html] and noticed the code on top that does not make sense in java.

{code}
 // To create Dataset<Row> using SparkSession
   Dataset<Row> people = spark.read().parquet(""..."");
   Dataset<Row> department = spark.read().parquet(""..."");

   people.filter(""age"".gt(30))
     .join(department, people.col(""deptId"").equalTo(department(""id"")))
     .groupBy(department.col(""name""), ""gender"")
     .agg(avg(people.col(""salary"")), max(people.col(""age"")));
{code}
invalid parts:
* ""age"".gt(30)
* department(""id"")


",,apachespark,cristian.teodor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 08 09:57:00 UTC 2017,,,,,,,,,,"0|i3fljz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 08:54;srowen;Agree, open a PR to fix them. Maybe do a search for similar instances while you are at it as this probably isn't the last copy paste error. ;;;","30/May/17 11:34;cristian.teodor;I'll look and update the ticket. Until now this was the only one found, but I just started a spark project so I’ll continue to work with the docs.;;;","02/Jun/17 19:19;srowen;It's OK if you don't see more like this just now, just open a PR for what you've got;;;","06/Jun/17 10:04;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/18215;;;","08/Jun/17 09:57;srowen;Issue resolved by pull request 18215
[https://github.com/apache/spark/pull/18215];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache Manager: Hint should be ignored in plan matching,SPARK-20908,13075410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,28/May/17 00:31,28/May/17 04:34,14/Jul/23 06:30,28/May/17 04:34,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"In Cache manager, the plan matching should ignore Hint. 

{noformat}
      val df1 = spark.range(10).join(broadcast(spark.range(10)))
      df1.cache()
      spark.range(10).join(spark.range(10)).explain()
{noformat}

The output plan of the above query shows that the second query is  not using the cached data of the first query.

{noformat}
BroadcastNestedLoopJoin BuildRight, Inner
:- *Range (0, 10, step=1, splits=2)
+- BroadcastExchange IdentityBroadcastMode
   +- *Range (0, 10, step=1, splits=2)
{noformat}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 28 00:36:02 UTC 2017,,,,,,,,,,"0|i3fk8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/May/17 00:36;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constrained Logistic Regression for SparkR,SPARK-20906,13075332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,27/May/17 07:28,24/Jan/18 07:16,14/Jul/23 06:30,22/Jun/17 03:43,2.2.0,2.2.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SparkR,,,,,0,,,,,,,,,PR https://github.com/apache/spark/pull/17715 Added Constrained Logistic Regression for ML. We should add it to SparkR.,,apachespark,felixcheung,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 24 07:16:04 UTC 2018,,,,,,,,,,"0|i3fjrb:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"27/May/17 07:32;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/18128;;;","21/Jan/18 20:15;felixcheung;[~wm624] would you like to add example of this in the API doc?

roxygen2 doc for spark.logit;;;","24/Jan/18 07:16;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/20380;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Task failures during shutdown cause problems with preempted executors,SPARK-20904,13075309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,27/May/17 00:59,23/Jul/17 15:24,14/Jul/23 06:30,23/Jul/17 15:24,1.6.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"Spark runs tasks in a thread pool that uses daemon threads in each executor. That means that when the JVM gets a signal to shut down, those tasks keep running.

Now when YARN preempts an executor, it sends a SIGTERM to the process, triggering the JVM shutdown. That causes shutdown hooks to run which may cause user code running in those tasks to fail, and report task failures to the driver. Those failures are then counted towards the maximum number of allowed failures, even though in this case we don't want that because the executor was preempted.

So we need a better way to handle that situation.",,apachespark,beettlle,cloud_fan,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 23 15:24:41 UTC 2017,,,,,,,,,,"0|i3fjm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/17 00:17;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18594;;;","23/Jul/17 15:24;cloud_fan;Issue resolved by pull request 18594
[https://github.com/apache/spark/pull/18594];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.blacklist.killBlacklistedExecutors doesn't work in YARN,SPARK-20898,13075207,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,tgraves,tgraves,26/May/17 16:37,03/Apr/19 16:01,14/Jul/23 06:30,26/Jun/17 16:17,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"I was trying out the new spark.blacklist.killBlacklistedExecutors on YARN but it doesn't appear to work.  Everytime I get:

17/05/26 16:28:12 WARN BlacklistTracker: Not attempting to kill blacklisted executor id 4 since allocation client is not defined

Even though dynamic allocation is on.  Taking a quick look, I think the way it creates the blacklisttracker and passes the allocation client is wrong. The scheduler backend is 
 not set yet so it never passes the allocation client to the blacklisttracker correctly.  Thus it will never kill.",,apachespark,irashid,jerryshao,jsoltren,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27272,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 01 02:51:04 UTC 2017,,,,,,,,,,"0|i3fizj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/May/17 06:43;jerryshao;[~tgraves], I addressed this issue in https://github.com/apache/spark/pull/17113 ;;;","01/Jun/17 02:51;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17113;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cached self-join should not fail,SPARK-20897,13075191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/May/17 15:06,27/May/17 23:18,14/Jul/23 06:30,27/May/17 23:18,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"code to reproduce this bug:
{code}
// force to plan sort merge join
spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", ""0"")
val df = Seq(1 -> ""a"").toDF(""i"", ""j"")
val df1 = df.as(""t1"")
val df2 = df.as(""t2"")
assert(df1.join(df2, $""t1.i"" === $""t2.i"").cache().count() == 1)
{code}",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 17:55:11 UTC 2017,,,,,,,,,,"0|i3fivz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"26/May/17 15:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18121;;;","26/May/17 17:55;marmbrus;Is this a regression?  If so, can you please make sure that its targeted at the 2.2.0 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the input parameter is float type for  ceil or floor ,the result is not we expected",SPARK-20876,13074715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,10110346,10110346,10110346,25/May/17 02:23,31/May/17 04:13,14/Jul/23 06:30,27/May/17 23:24,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"spark-sql>SELECT ceil(cast(12345.1233 as float));
spark-sql>12345
For this case, the result we expected is 12346

spark-sql>SELECT floor(cast(-12345.1233 as float));
spark-sql>-12345
For this case, the result we expected is  -12346",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 04:13:03 UTC 2017,,,,,,,,,,"0|i3ffy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/May/17 02:27;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18103;;;","31/May/17 04:13;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The ""examples"" project doesn't depend on Structured Streaming Kafka source",SPARK-20874,13074704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,25/May/17 00:43,25/May/17 17:50,14/Jul/23 06:30,25/May/17 17:50,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Examples,,,,,0,,,,,,,,,"Right now running `bin/run-example StructuredKafkaWordCount ...` will throw an error saying ""kafka"" source not found.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 25 00:45:03 UTC 2017,,,,,,,,,,"0|i3ffvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/May/17 00:45;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message for unsupported Column Type,SPARK-20873,13074624,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,RubenJanssen,RubenJanssen,RubenJanssen,24/May/17 19:53,29/Jun/17 19:06,14/Jul/23 06:30,26/May/17 22:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"For unsupported column type, we simply output the column type instead of the type name. 

{noformat}
java.lang.Exception: Unsupported type: org.apache.spark.sql.execution.columnar.ColumnTypeSuite$$anonfun$4$$anon$1@2205a05d
{noformat}

We should improve it by outputting its name.
",,apachespark,maropu,RubenJanssen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20565,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 29 19:06:03 UTC 2017,,,,,,,,,,"0|i3ffdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 20:15;apachespark;User 'setjet' has created a pull request for this issue:
https://github.com/apache/spark/pull/18097;;;","29/Jun/17 19:06;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleExchange.nodeName should handle null coordinator,SPARK-20872,13074603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,24/May/17 18:37,25/May/17 00:21,14/Jul/23 06:30,25/May/17 00:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,easyfix,,,,,,,,"A ShuffleExchange's coordinator can be null sometimes, and when we need to do a toString() on it, it'll go to ShuffleExchange.nodeName() and throw a MatchError there because of inexhaustive match -- the match only handles Some and None, but doesn't handle null.

An example of this issue is when trying to inspect a Catalyst physical operator on the Executor side in an IDE:
{code:none}
child = {WholeStageCodegenExec@13881} Method threw 'scala.MatchError' exception. Cannot evaluate org.apache.spark.sql.execution.WholeStageCodegenExec.toString()
{code}
where this WholeStageCodegenExec transitively references a ShuffleExchange.
On the Executor side, this ShuffleExchange instance is deserialized from the data sent over from the Driver, and because the coordinator field is marked transient, it's not carried over to the Executor, that's why it can be null upon inspection.",,apachespark,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9858,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 18:53:04 UTC 2017,,,,,,,,,,"0|i3ff9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 18:38;rednaxelafx;The said matching logic in ShuffleExchange.nodeName() is introduced from SPARK-9858.;;;","24/May/17 18:53;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/18095;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset map does not respect nullable field ,SPARK-20866,13074508,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,colinbreame,colinbreame,24/May/17 12:41,25/May/17 07:37,14/Jul/23 06:30,25/May/17 07:37,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The Dataset.map does not respect the nullable fields within the schema. 

*Test code:*
(run on spark-shell 2.1.0):

{code}
scala> case class Test(a: Int)
defined class Test

scala> val ds1 = (Test(10) :: Nil).toDS
ds1: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> val ds2 = ds1.map(x => Test(x.a))
ds2: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> ds1.schema == ds2.schema
res65: Boolean = false

scala> ds1.schema
res62: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,false))

scala> ds2.schema
res63: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,true))
{code}

*Expected*
The ds1 should equal ds2. i.e. the schema should be the same.

*Actual*
The schema is not equal - the StructField nullable property is true in ds2 and false in ds1.
",,colinbreame,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18284,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 25 07:35:55 UTC 2017,,,,,,,,,,"0|i3feo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 12:44;colinbreame;This might be similar to https://issues.apache.org/jira/browse/SPARK-14139.;;;","24/May/17 18:47;kiszk;I confirmed that it has been fixed in master branch. Am I correct?
If I am correct, we need to find the JIRA entry for this fix.;;;","25/May/17 07:35;maropu;SPARK-18284 [~kiszk] fixed also resolved this issue;
{code}

-- w/o the SPARK-18284 patch
scala> case class Test(a: Int)
defined class Test

scala> val ds1 = (Test(10) :: Nil).toDS
ds1: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> val ds2 = ds1.map(x => Test(x.a))
ds2: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> ds1.schema == ds2.schema
res0: Boolean = false

scala> ds1.schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,false))

scala> ds2.schema
res2: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,true))


-- w/ the SPARK-18284 patch
scala> case class Test(a: Int)
defined class Test

scala> val ds1 = (Test(10) :: Nil).toDS
ds1: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> val ds2 = ds1.map(x => Test(x.a))
ds2: org.apache.spark.sql.Dataset[Test] = [a: int]

scala> ds1.schema == ds2.schema
res0: Boolean = true

scala> ds1.schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,false))

scala> ds2.schema
res2: org.apache.spark.sql.types.StructType = StructType(StructField(a,IntegerType,false))
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogisticRegressionModel throws TypeError,SPARK-20862,13074377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bago.amirbekian,bago.amirbekian,bago.amirbekian,24/May/17 01:12,24/May/17 15:02,14/Jul/23 06:30,24/May/17 15:02,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,MLlib,PySpark,,,,0,,,,,,,,,"LogisticRegressionModel throws a TypeError using python3 and numpy 1.12.1:

**********************************************************************
File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 155, in __main__.LogisticRegressionModel
Failed example:
    mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
Exception raised:
    Traceback (most recent call last):
      File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/doctest.py"", line 1330, in __run
        compileflags, 1), test.globs)
      File ""<doctest __main__.LogisticRegressionModel[23]>"", line 1, in <module>
        mcm = LogisticRegressionWithLBFGS.train(data, iterations=10, numClasses=3)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 398, in train
        return _regression_train_wrapper(train, LogisticRegressionModel, data, initialWeights)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/regression.py"", line 216, in _regression_train_wrapper
        return modelClass(weights, intercept, numFeatures, numClasses)
      File ""/Users/bago/repos/spark/python/pyspark/mllib/classification.py"", line 176, in __init__
        self._dataWithBiasSize)
    TypeError: 'float' object cannot be interpreted as an integer
",,apachespark,bago.amirbekian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 01:21:03 UTC 2017,,,,,,,,,,"0|i3fdv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 01:21;apachespark;User 'MrBago' has created a pull request for this issue:
https://github.com/apache/spark/pull/18081;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dangling threads when reading parquet files in local mode,SPARK-20848,13074045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,pnpritchard,pnpritchard,22/May/17 23:36,24/May/17 23:18,14/Jul/23 06:30,24/May/17 16:37,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Input/Output,SQL,,,,0,,,,,,,,,"On each call to {{spark.read.parquet}}, a new ForkJoinPool is created. One of the threads in the pool is kept in the {{WAITING}} state, and never stopped, which leads to unbounded growth in number of threads.

This behavior is a regression from v2.1.0.

Reproducible example:
{code}
val spark = SparkSession
  .builder()
  .appName(""test"")
  .master(""local"")
  .getOrCreate()
while(true) {
  spark.read.parquet(""/path/to/file"")
  Thread.sleep(5000)
}
{code}
",,apachespark,pnpritchard,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/17 23:37;pnpritchard;Screen Shot 2017-05-22 at 4.13.52 PM.png;https://issues.apache.org/jira/secure/attachment/12869367/Screen+Shot+2017-05-22+at+4.13.52+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 23:18:03 UTC 2017,,,,,,,,,,"0|i3fbtb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/May/17 23:37;pnpritchard;Screen shot of JVisualVM thread visualization.;;;","23/May/17 09:38;srowen;CC [~viirya]  for https://github.com/apache/spark/pull/16474/files#diff-ee26d4c4be21e92e92a02e9f16dbc285R559 ?
It does look like this is missing a {{shutdown()}};;;","23/May/17 09:50;viirya;I am looking up this. Thanks [~sowen] for pinging me.;;;","23/May/17 13:07;viirya;It seems to me that to share the task support between parquet file reading is better than shutdowning after each reading?;;;","23/May/17 13:30;srowen;Yes, possibly. The main tradeoff is that concurrent read jobs share a pool of threads, and don't get their own. You don't need to spin up new threads, but also, will have a thread pool lying around for the whole app lifetime. No big deal. The main question is whether concurrent jobs were intended to limit their total concurrency on purpose by sharing a pool or not.;;;","23/May/17 14:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18073;;;","23/May/17 14:12;viirya;Ok. It seems better not to change the concurrency, I add a shutdown and a test case for it.;;;","24/May/17 23:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18100;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot gracefully kill drivers which take longer than 10 seconds to die,SPARK-20843,13073936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,michael,michael,22/May/17 17:14,27/May/17 05:26,14/Jul/23 06:30,27/May/17 05:26,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,,,,,0,regression,,,,,,,,"Commit https://github.com/apache/spark/commit/1c9a386c6b6812a3931f3fb0004249894a01f657 changed the behavior of driver process termination. Whereas before `Process.destroyForcibly` was never called, now it is called (on Java VM's supporting that API) if the driver process does not die within 10 seconds.

This prevents apps which take longer than 10 seconds to shutdown gracefully from shutting down gracefully. For example, streaming apps with a large batch duration (say, 30 seconds+) can take minutes to shutdown.",,ajbozarth,apachespark,marmbrus,michael,rxin,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13602,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 26 21:23:03 UTC 2017,,,,,,,,,,"0|i3fb53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/May/17 17:09;michael;[~rxin] I'd like to bump this to ""Critical"". This is really a disruptive, potentially dangerous change for spark streaming apps (among others). We could not tolerate this behavior in our production environment, and it caught us off guard in our prod migration to Spark 2.1.

I think this timeout should be configurable per-app (as a driver config param), but I couldn't find a way to do that. In our case, we modified our source build to set the timeout to `Int.MaxValue`, effectively reverting this change. Therefore, the best PR I could offer at this point is to effectively revert this change.

I have another concern that this behavior varies depending on the version of the underlying JDK. Specifically, this behavior will not manifest on Java 7 but will do so on Java 8+. IMO, users who upgrade their Java runtimes should not expect this kind of change in their Spark apps' behavior.

Thank you.;;;","25/May/17 17:12;rxin;cc [~joshrosen] and [~marmbrus];;;","26/May/17 18:25;marmbrus;I don't have much context here /cc [~zsxwing] and [~vanzin].  I does seem like this could at least be configurable.

I will say that I don't think its really safe to rely on clean shutdowns for correctness and I don't think this change affects structured streaming.

;;;","26/May/17 18:32;vanzin;I mostly followed [~zsxwing]'s review since he didn't seem to have issues with it (and he filed the original bug, SPARK-13602). My comments were mostly stylistic.

Given that it may cause problems it might be safer to revert this in 2.2 and fix it in master. Maybe [~bryanc] can chime in too since he worked on the change itself.;;;","26/May/17 18:47;michael;bq. I will say that I don't think its really safe to rely on clean shutdowns for correctness...

I agree, but we don't want a ""force kill"" to become the norm for an app shutdown. An unclean shutdown should be an exceptional situation that will be cause for an ops alert, an investigation and a validation that no data loss or corruption occurred (i.e. our failure mechanisms held).

Also, I would say that in some cases where an app integrates with other systems and cannot guarantee transactional or idempotent semantics in the event of a failure, an unclean shutdown *will* require cross-system validation and any necessary data synchronization or recovery.

Cheers.;;;","26/May/17 20:45;zsxwing;[~michael] Will a per-cluster config be enough for your usage? A per-app config requires more changes and it's too risky for 2.2 now.;;;","26/May/17 21:05;michael;bq. Will a per-cluster config be enough for your usage?

Yes.;;;","26/May/17 21:23;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18126;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone master should explicitly inform drivers of worker deaths and invalidate external shuffle service outputs,SPARK-20832,13073754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,joshrosen,joshrosen,22/May/17 03:19,17/May/20 17:47,14/Jul/23 06:30,22/Jun/17 12:49,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Deploy,Scheduler,Spark Core,,,1,,,,,,,,,"In SPARK-17370 (a patch authored by [~ekhliang] and reviewed by me), we added logic to the DAGScheduler to mark external shuffle service instances as unavailable upon task failure when the task failure reason was ""SlaveLost"" and this was known to be caused by worker death. If the Spark Master discovered that a worker was dead then it would notify any drivers with executors on those workers to mark those executors as dead. The linked patch simply piggybacked on this logic to have the executor death notification also imply worker death and to have worker-death-caused-executor-death imply shuffle file loss.

However, there are modes of external shuffle service loss which this mechanism does not detect, leaving the system prone race conditions. Consider the following:

* Spark standalone is configured to run an external shuffle service embedded in the Worker.
* Application has shuffle outputs and executors on Worker A.
* Stage depending on outputs of tasks that ran on Worker A starts.
* All executors on worker A are removed due to dying with exceptions, scaling-down via the dynamic allocation APIs, but _not_ due to worker death. Worker A is still healthy at this point.
* At this point the MapOutputTracker still records map output locations on Worker A's shuffle service. This is expected behavior. 
* Worker A dies at an instant where the application has no executors running on it.
* The Master knows that Worker A died but does not inform the driver (which had no executors on that worker at the time of its death).
* Some task from the running stage attempts to fetch map outputs from Worker A but these requests time out because Worker A's shuffle service isn't available.
* Due to other logic in the scheduler, these preventable FetchFailures don't wind up invaliding the now-invalid unavailable map output locations (this is a distinct bug / behavior which I'll discuss in a separate JIRA ticket).
* This behavior leads to several unsuccessful stage reattempts and ultimately to a job failure.

A simple way to address this would be to have the Master explicitly notify drivers of all Worker deaths, even if those drivers don't currently have executors. The Spark Standalone scheduler backend can receive the explicit WorkerLost message and can bubble up the right calls to the task scheduler and DAGScheduler to invalidate map output locations from the now-dead external shuffle service.

This relates to SPARK-20115 in the sense that both tickets aim to address issues where the external shuffle service is unavailable. The key difference is the mechanism for detection: SPARK-20115 marks the external shuffle service as unavailable whenever any fetch failure occurs from it, whereas the proposal here relies on more explicit signals. This JIRA ticket's proposal is scoped only to Spark Standalone mode. As a compromise, we might be able to consider ""all of a single shuffle's outputs lost on a single external shuffle service"" following a fetch failure (to be discussed in separate JIRA). ",,apachespark,cloud_fan,jiangxb1987,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17370,SPARK-17519,SPARK-20178,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 12:49:04 UTC 2017,,,,,,,,,,"0|i3fa0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/May/17 17:19;jiangxb1987;I'm working on this.;;;","20/Jun/17 13:25;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18362;;;","22/Jun/17 12:49;cloud_fan;Issue resolved by pull request 18362
[https://github.com/apache/spark/pull/18362];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unresolved operator when INSERT OVERWRITE data source tables with IF NOT EXISTS,SPARK-20831,13073726,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,21/May/17 18:28,22/May/17 14:26,14/Jul/23 06:30,22/May/17 14:26,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, we have a bug when we specify `IF NOT EXISTS` in `INSERT OVERWRITE` data source tables. For example, given a query:
{noformat}
INSERT OVERWRITE TABLE $tableName partition (b=2, c=3) IF NOT EXISTS SELECT 9, 10
{noformat}
we will get the following error:
{noformat}
unresolved operator 'InsertIntoTable Relation[a#425,d#426,b#427,c#428] parquet, Map(b -> Some(2), c -> Some(3)), true, true;;
'InsertIntoTable Relation[a#425,d#426,b#427,c#428] parquet, Map(b -> Some(2), c -> Some(3)), true, true
+- Project [cast(9#423 as int) AS a#429, cast(10#424 as int) AS d#430]
   +- Project [9 AS 9#423, 10 AS 10#424]
      +- OneRowRelation$
{noformat}",,apachespark,cloud_fan,nelsonc,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 22 14:26:18 UTC 2017,,,,,,,,,,"0|i3f9uf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"21/May/17 18:30;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18050;;;","22/May/17 14:26;cloud_fan;Issue resolved by pull request 18050
[https://github.com/apache/spark/pull/18050];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in RPackageUtils#checkManifestForR,SPARK-20815,13073602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jrshust,aash,aash,19/May/17 23:55,24/May/17 17:57,14/Jul/23 06:30,23/May/17 04:45,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SparkR,,,,,0,,,,,,,,,"Some jars don't have manifest files in them, such as in my case javax.inject-1.jar and value-2.2.1-annotations.jar

This causes the below NPE:

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.spark.deploy.RPackageUtils$.checkManifestForR(RPackageUtils.scala:95)
        at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1$$anonfun$apply$1.apply$mcV$sp(RPackageUtils.scala:180)
        at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1$$anonfun$apply$1.apply(RPackageUtils.scala:180)
        at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1$$anonfun$apply$1.apply(RPackageUtils.scala:180)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1322)
        at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:202)
        at org.apache.spark.deploy.RPackageUtils$$anonfun$checkAndBuildRPackage$1.apply(RPackageUtils.scala:175)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at org.apache.spark.deploy.RPackageUtils$.checkAndBuildRPackage(RPackageUtils.scala:175)
        at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:311)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:152)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:118)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}

due to RPackageUtils#checkManifestForR assuming {{jar.getManifest}} is non-null.

However per the JDK spec it can be null:

{noformat}
    /**
     * Returns the jar file manifest, or <code>null</code> if none.
     *
     * @return the jar file manifest, or <code>null</code> if none
     *
     * @throws IllegalStateException
     *         may be thrown if the jar file has been closed
     * @throws IOException  if an I/O error has occurred
     */
    public Manifest getManifest() throws IOException {
        return getManifestFromReference();
    }
{noformat}

This method should do a null check and return false if the manifest is null (meaning no R code in that jar)",,aash,apachespark,felixcheung,jrshust,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 17:57:56 UTC 2017,,,,,,,,,,"0|i3f92v:",9223372036854775807,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,"20/May/17 03:51;felixcheung;make sense to me. would you like to contribute the fix?
;;;","20/May/17 05:09;jrshust;I have a fix in the works, just adding a test case and running the full test suite now.;;;","20/May/17 07:37;apachespark;User 'jrshust' has created a pull request for this issue:
https://github.com/apache/spark/pull/18040;;;","23/May/17 04:48;felixcheung;[~srowen] could you help add jrshust to contributor list? I couldn't assign this JIRA to him. Thanks!;;;","23/May/17 09:06;srowen;[~felixcheung] you're an admin now, so you should be able to add at https://issues.apache.org/jira/plugins/servlet/project-config/SPARK/roles
However I'm not sure which JIRA user this is.;;;","24/May/17 17:57;felixcheung;Thanks Sean;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos scheduler does not respect spark.executor.extraClassPath configuration,SPARK-20814,13073553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,gpang,gpang,19/May/17 20:31,03/Jul/17 11:15,14/Jul/23 06:30,22/May/17 19:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Mesos,,,,,0,,,,,,,,,"When Spark executors are deployed on Mesos, the Mesos scheduler no longer respects the ""spark.executor.extraClassPath"" configuration parameter.

MesosCoarseGrainedSchedulerBackend used to use the environment variable ""SPARK_CLASSPATH"" to add the value of ""spark.executor.extraClassPath"" to the executor classpath. However, ""SPARK_CLASSPATH"" was deprecated, and was removed in this commit [https://github.com/apache/spark/commit/8f0490e22b4c7f1fdf381c70c5894d46b7f7e6fb#diff-387c5d0c916278495fc28420571adf9eL178].

This effectively broke the ability for users to specify ""spark.executor.extraClassPath"" for Spark executors deployed on Mesos.",,apachespark,gpang,laurentcoder,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 03 11:00:46 UTC 2017,,,,,,,,,,"0|i3f8rz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"19/May/17 20:35;vanzin;Hmm, this sucks, we should fix it for 2.2 (FYI [~marmbrus]).

Let me take a stab at fixing just the Mesos usage without re-introducing that variable.;;;","19/May/17 20:54;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18037;;;","03/Jul/17 10:46;laurentcoder;> Hmm, this sucks, we should fix it for 2.2 !
+1 ( for the whole spark-on-mesos community )
And this patch looks straightforward, without risk to break anything (not related to mesos), so why it didn't make it into 2.2.0 ?!
;;;","03/Jul/17 11:00;srowen;[~laurentcoder] there is no 2.2.0, yet. It is in the latest RC. Please look at the commits.
However [~marmbrus] it looks like the items marked Fix Version = 2.2.1 weren't changed to 2.2.0 after subsequent RCs. I'll update JIRAs like this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Web UI executor page tab search by status not working ,SPARK-20813,13073513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,yoonlee95,yoonlee95,yoonlee95,19/May/17 18:09,22/May/17 13:26,14/Jul/23 06:30,22/May/17 13:26,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Web UI,,,,,0,,,,,,,,,"When searching for status keywords such as active, dead or Blacklisted nothing is returned on the table.",,ajbozarth,apachespark,yoonlee95,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 22 13:26:19 UTC 2017,,,,,,,,,,"0|i3f8j3:",9223372036854775807,,,,,tgraves,,,,,,,,,,,,,,,,,,,"19/May/17 19:40;apachespark;User 'yoonlee95' has created a pull request for this issue:
https://github.com/apache/spark/pull/18036;;;","22/May/17 13:26;srowen;Resolved by https://github.com/apache/spark/pull/18036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GenerateUnsafeProjection should check if value is null before calling the getter,SPARK-20798,13073109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,18/May/17 12:11,19/May/17 11:20,14/Jul/23 06:30,19/May/17 11:20,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"GenerateUnsafeProjection.writeStructToBuffer() does not honor the assumption that one should first make sure the value is not null before calling the getter. This can lead to errors.

An example of generated code:
{noformat}
/* 059 */         final UTF8String fieldName = value.getUTF8String(0);
/* 060 */         if (value.isNullAt(0)) {
/* 061 */           rowWriter1.setNullAt(0);
/* 062 */         } else {
/* 063 */           rowWriter1.write(0, fieldName);
/* 064 */         }
{noformat}
",,ala.luszczak,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 18 13:31:02 UTC 2017,,,,,,,,,,"0|i3f61b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/May/17 13:31;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/18030;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALS with implicit feedback ignores negative values,SPARK-20790,13072912,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davidjeis,davidjeis,davidjeis,17/May/17 19:38,02/Jun/17 14:01,14/Jul/23 06:30,31/May/17 12:53,1.3.1,1.4.0,1.4.1,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,,,,,,,,,,,,2.2.0,,,,,ML,MLlib,,,,0,,,,,,,,,"The refactorization that was done in https://github.com/apache/spark/pull/5314/files introduced a bug, whereby for implicit feedback negative ratings just get ignored. Prior to that commit they were not ignored, but the absolute value was used as the confidence and the  preference was set to 0. The preservation of comments and absolute value indicate that this was unintentional.",,apachespark,davidjeis,shubhamc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 14:01:03 UTC 2017,,,,,,,,,,"0|i3f4tj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/May/17 20:24;srowen;This needs more detail. Where is the logic problem, what's a reproduction, etc?;;;","17/May/17 20:40;davidjeis;See https://github.com/apache/spark/pull/5314/files#diff-be65dd1d6adc53138156641b610fcadaR1118
ls.add is only called if rating > 0, which is not what was done prior to the commit, nor does it represent what is conveyed in the comment on the previous line.
To reproduce, you can run ALS with implicit feedback with a ratings matrix with any negative values and it will be identical to running with the same ratings matrix with the negative values zeroed out.;;;","17/May/17 21:07;apachespark;User 'davideis' has created a pull request for this issue:
https://github.com/apache/spark/pull/18022;;;","31/May/17 12:53;srowen;Issue resolved by pull request 18022
[https://github.com/apache/spark/pull/18022];;;","02/Jun/17 14:01;apachespark;User 'davideis' has created a pull request for this issue:
https://github.com/apache/spark/pull/18188;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the Executor task reaper's false alarm warning logs,SPARK-20788,13072891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,17/May/17 18:05,17/May/17 21:14,14/Jul/23 06:30,17/May/17 21:14,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Executor task reaper may fail to detect if a task is finished or not when a task is finishing but being killed at the same time.
",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 17 18:10:04 UTC 2017,,,,,,,,,,"0|i3f4ov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/May/17 18:10;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18021;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve ceil and floor handle the value which is not expected,SPARK-20786,13072760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,heary-cao,heary-cao,heary-cao,17/May/17 10:40,22/May/17 07:11,14/Jul/23 06:30,22/May/17 05:41,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"spark-sql>SELECT ceil(1234567890123456);
1234567890123456

spark-sql>SELECT ceil(12345678901234567);
12345678901234568

spark-sql>SELECT ceil(123456789012345678);
123456789012345680

when the length of the getText is greater than 16. long to double will be precision loss.

but mysql handle the value is ok.

mysql> SELECT ceil(1234567890123456);
+------------------------+
| ceil(1234567890123456) |
+------------------------+
|       1234567890123456 |
+------------------------+
1 row in set (0.00 sec)

mysql> SELECT ceil(12345678901234567);
+-------------------------+
| ceil(12345678901234567) |
+-------------------------+
|       12345678901234567 |
+-------------------------+
1 row in set (0.00 sec)

mysql> SELECT ceil(123456789012345678);
+--------------------------+
| ceil(123456789012345678) |
+--------------------------+
|       123456789012345678 |
+--------------------------+
1 row in set (0.00 sec)",,apachespark,heary-cao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 22 07:11:02 UTC 2017,,,,,,,,,,"0|i3f3vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/May/17 10:41;apachespark;User 'heary-cao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18016;;;","22/May/17 07:11;apachespark;User 'heary-cao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18057;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the location of Dockerfile in docker.properties.template is wrong,SPARK-20781,13072688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liuzhaokun,liuzhaokun,liuzhaokun,17/May/17 05:43,19/May/17 19:48,14/Jul/23 06:30,19/May/17 19:48,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Mesos,,,,,0,,,,,,,,,"the location of Dockerfile in docker.properties.template should be ""../external/docker/spark-mesos/Dockerfile""",,apachespark,liuzhaokun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 19 19:48:01 UTC 2017,,,,,,,,,,"0|i3f3fz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/May/17 05:47;apachespark;User 'liu-zhaokun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18013;;;","19/May/17 19:48;srowen;Issue resolved by pull request 18013
[https://github.com/apache/spark/pull/18013];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BroadcastExchangeExec doesn't cancel the Spark job if broadcasting a relation timeouts.,SPARK-20774,13072583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,zsxwing,zsxwing,16/May/17 20:32,15/May/19 21:50,14/Jul/23 06:30,15/May/19 21:47,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,SQL,,,,,0,,,,,,,,,"When broadcasting a table takes too long and triggers timeout, the SQL query will fail. However, the background Spark job is still running and it wastes resources.",,apachespark,codingcat,lyc,umesh9794@gmail.com,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27036,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 17:56:03 UTC 2017,,,,,,,,,,"0|i3f2sn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 17:56;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetWriteSupport.writeFields is quadratic in number of fields,SPARK-20773,13072560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tpoterba,tpoterba,tpoterba,16/May/17 19:24,19/May/17 12:19,14/Jul/23 06:30,19/May/17 12:18,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,easyfix,performance,,,,,,,"The writeFields method in ParquetWriteSupport uses Seq.apply(i) to select all elements. Since the fieldWriters object is a List, this is a quadratic operation.

See line 123: https://github.com/apache/spark/blob/ac1ab6b9db188ac54c745558d57dd0a031d0b162/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala",,apachespark,tpoterba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 16 19:41:03 UTC 2017,,,,,,,,,,"0|i3f2o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/May/17 19:41;apachespark;User 'tpoterba' has created a pull request for this issue:
https://github.com/apache/spark/pull/18005;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect documentation for using Jupyter notebook,SPARK-20769,13072481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,a1ray,a1ray,a1ray,16/May/17 14:56,17/May/17 09:07,14/Jul/23 06:30,17/May/17 09:06,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Documentation,,,,,0,,,,,,,,,"SPARK-13973 incorrectly removed the required PYSPARK_DRIVER_PYTHON_OPTS=""notebook"" from documentation to use pyspark with Jupyter notebook",,a1ray,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 17 09:06:34 UTC 2017,,,,,,,,,,"0|i3f273:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/May/17 15:01;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/18001;;;","17/May/17 09:06;srowen;Issue resolved by pull request 18001
[https://github.com/apache/spark/pull/18001];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The function of  `month` and `day` return a value which is not we expected,SPARK-20763,13072363,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,16/May/17 09:04,22/May/17 04:18,14/Jul/23 06:30,19/May/17 17:27,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"spark-sql>select month(""1582-09-28"");
spark-sql>10
For this case, the expected result is 9, but it is 10.

spark-sql>select day(""1582-04-18"");
spark-sql>28
For this case, the expected result is 18, but it is 28.",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 22 04:18:02 UTC 2017,,,,,,,,,,"0|i3f1gv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/May/17 09:07;srowen;This does not show an error.;;;","16/May/17 09:21;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/17997;;;","16/May/17 10:28;10110346;[~srowen] the function of `month` and ` day` return the value which is not we expected.;;;","22/May/17 01:10;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18053;;;","22/May/17 04:18;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18054;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
yarn-shuffle jar has references to unshaded guava and contains scala classes,SPARK-20756,13072283,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,mgrover,16/May/17 01:14,17/May/20 18:13,14/Jul/23 06:30,22/May/17 17:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,2.3.0,,Spark Core,YARN,,,,0,,,,,,,,,"There are 2 problems with yarn's shuffle jar currently:
1. It contains shaded guava but it contains references to unshaded classes.
{code}
# Guava is correctly relocated
>jar -tf common/network-yarn/target/scala-2.11/spark*yarn-shuffle.jar | grep guava | head
META-INF/maven/com.google.guava/
META-INF/maven/com.google.guava/guava/
META-INF/maven/com.google.guava/guava/pom.properties
META-INF/maven/com.google.guava/guava/pom.xml
org/spark_project/guava/
org/spark_project/guava/annotations/
org/spark_project/guava/annotations/Beta.class
org/spark_project/guava/annotations/GwtCompatible.class
org/spark_project/guava/annotations/GwtIncompatible.class
org/spark_project/guava/annotations/VisibleForTesting.class

# But, there are still references to unshaded guava
>javap -cp common/network-yarn/target/scala-2.11/spark*yarn-shuffle.jar -c org/apache/spark/network/yarn/YarnShuffleService | grep google
      57: invokestatic  #139                // Method com/google/common/collect/Lists.newArrayList:()Ljava/util/ArrayList;
{code}

2. There are references to scala classes in the uber jar:
{code}
jar -tf /opt/src/spark/common/network-yarn/target/scala-2.11/spark-*yarn-shuffle.jar | grep ""^scala""
scala/AnyVal.class
{code}

We should fix this.",,apachespark,mgrover,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 16 01:20:03 UTC 2017,,,,,,,,,,"0|i3f0z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/May/17 01:20;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/17990;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable cross join in TPCDSQueryBenchmark,SPARK-20735,13071816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,14/May/17 14:26,15/May/17 18:48,14/Jul/23 06:30,15/May/17 18:48,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,Tests,,,,0,,,,,,,,,"Since SPARK-17298, some queries (q28, q61, q77, q88, q90) fails with a message ""Use the CROSS JOIN syntax to allow cartesian products between these relations"".

This issue aims to enable the correct configuration in `TPCDSQueryBenchmark.scala`.",,apachespark,dongjoon,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 14 14:35:04 UTC 2017,,,,,,,,,,"0|i3ey3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/May/17 14:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17977;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partial aggregate should behave correctly for sameResult,SPARK-20725,13071551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,12/May/17 14:28,19/Jul/17 14:26,14/Jul/23 06:30,13/May/17 19:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 14 01:28:03 UTC 2017,,,,,,,,,,"0|i3ewgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/May/17 14:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17964;;;","14/May/17 01:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17975;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support LIMIT ALL,SPARK-20719,13071432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,12/May/17 04:37,12/May/17 22:26,14/Jul/23 06:30,12/May/17 22:26,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,`LIMIT ALL` is the same as omitting the `LIMIT` clause. It is supported by both PrestgreSQL and Presto. ,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 04:46:03 UTC 2017,,,,,,,,,,"0|i3evq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/May/17 04:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17960;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileSourceScanExec with different filter orders should be the same after canonicalization,SPARK-20718,13071426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,12/May/17 03:23,12/May/17 07:53,14/Jul/23 06:30,12/May/17 05:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Since `constraints` in `QueryPlan` is a set, the order of filters can differ. Usually this is ok because of canonicalization. However, in `FileSourceScanExec`, its data filters and partition filters are sequences, and their orders are not canonicalized. So `def sameResult` returns different results for different orders of data/partition filters. This leads to, e.g. different decision for `ReuseExchange`, and thus results in unstable performance.

The same issue exists in `HiveTableScanExec`.",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 07:06:03 UTC 2017,,,,,,,,,,"0|i3evov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/May/17 03:26;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17959;;;","12/May/17 07:06;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17962;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateStore.abort() should not throw further exception,SPARK-20716,13071416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,12/May/17 02:22,15/May/17 17:49,14/Jul/23 06:30,15/May/17 17:49,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,StateStore.abort() should do a best effort attempt to clean up temporary resources. It should not throw errors.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 03:24:03 UTC 2017,,,,,,,,,,"0|i3evmn:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"12/May/17 03:24;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/17958;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative task that got CommitDenied exception shows up as failed,SPARK-20713,13071192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nlyu2,tgraves,tgraves,11/May/17 14:43,12/Dec/22 18:10,14/Jul/23 06:30,03/Aug/17 18:19,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"When running speculative tasks you can end up getting a task failure on a speculative task (the other task succeeded) because that task got a CommitDenied exception when really it was ""killed"" by the driver. It is a race between when the driver kills and when the executor tries to commit.

I think ideally we should fix up the task state on this to be killed because the fact that this task failed doesn't matter since the other speculative task succeeded.  tasks showing up as failure confuse the user and could make other scheduler cases harder.   

This is somewhat related to SPARK-13343 where I think we should be correctly account for speculative tasks.  only one of the 2 tasks really succeeded and commited, and the other should be marked differently.",,apachespark,devaraj,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:23:20 UTC 2017,,,,,,,,,,"0|i3eu8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/May/17 10:07;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18070;;;","02/Aug/17 19:23;gurwls223;User 'nlyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18819;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make `addExclusionRules` up-to-date,SPARK-20708,13071078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/May/17 08:18,01/Jun/17 05:41,14/Jul/23 06:30,01/Jun/17 05:40,2.0.2,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Since SPARK-9263, `resolveMavenCoordinates` ignores Spark and Spark's dependencies by using `addExclusionRules`. This PR aims to make `addExclusionRules` up-to-date to neglect correctly because it fails to neglect some components like the following.

*mllib (correct)*
{code}
$ bin/spark-shell --packages org.apache.spark:spark-mllib_2.11:2.1.1
...
---------------------------------------------------------------------
|                  |            modules            ||   artifacts   |
|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
---------------------------------------------------------------------
|      default     |   0   |   0   |   0   |   0   ||   0   |   0   |
---------------------------------------------------------------------
{code}

*mllib-local (wrong)*
{code}
$ bin/spark-shell --packages org.apache.spark:spark-mllib-local_2.11:2.1.1
...
---------------------------------------------------------------------
|                  |            modules            ||   artifacts   |
|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
---------------------------------------------------------------------
|      default     |   15  |   2   |   2   |   0   ||   15  |   2   |
---------------------------------------------------------------------
{code}",,apachespark,brkyvz,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 01 05:41:01 UTC 2017,,,,,,,,,,"0|i3etjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 08:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17947;;;","01/Jun/17 05:41;brkyvz;Resolved by https://github.com/apache/spark/pull/17947;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-shell not overriding method/variable definition,SPARK-20706,13071055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mpetruska,raphsen,raphsen,11/May/17 06:53,12/Dec/22 18:10,14/Jul/23 06:30,06/Dec/17 00:08,2.0.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Shell,,,,,0,,,,,,,,,"


!screenshot-1.png!In the following example, the definition of myMethod is not correctly updated:

------------------------------
def myMethod()  = ""first definition""

val tmp = myMethod(); val out = tmp

println(out) // prints ""first definition""

def myMethod()  = ""second definition"" // override above myMethod

val tmp = myMethod(); val out = tmp 

println(out) // should be ""second definition"" but is ""first definition""
------------------------------


I'm using semicolon to force two statements to be compiled at the same time. It's also possible to reproduce the behavior using :paste

So if I-redefine myMethod, the implementation seems not to be updated in this case. I figured out that the second-last statement (val out = tmp) causes this behavior, if this is moved in a separate block, the code works just fine.

EDIT:

The same behavior can be seen when declaring variables :

------------------------------
val a = 1

val b = a; val c = b;

println(b) // prints ""1""

val a = 2 // override a

val b = a; val c = b;

println(b) // prints ""1"" instead of ""2""
------------------------------

Interestingly, if the second-last line ""val b = a; val c = b;"" is executed twice, then I get the expected result","Linux, Scala 2.11.8",apachespark,assaf.mendelson,mpetruska,raphsen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/17 06:11;raphsen;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12877237/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 06 00:08:49 UTC 2017,,,,,,,,,,"0|i3etef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 07:04;assaf.mendelson;Actually it seems like the use of ';' to do more than one command per line when reusing the val is causing it. For example:

val tmp = myMethod(); println(tmp)
val tmp = myMethod(); tmp+""bla""

also causes the issue but

val tmp = myMeothd(); {}

does not.
;;;","11/May/17 07:05;assaf.mendelson;I don't think the priority is right. This shouldn't be a major bug...;;;","11/May/17 08:25;raphsen;Well I'm using Spark-Notebook, there it's very anoying because it's quite common that cells are executed multiple times as you work on the code, and this bug causes the implementations not to be updated. I just added the semicolons to ""simulate"" the execution of a cell of code in Spark-Notebook. Is there another way to do it?;;;","11/May/17 08:30;raphsen;You can also reproduce this bug using :paste instead of using semicolon ;;;","11/May/17 09:28;srowen;I think this is more of a Scala shell issue than anything, though I can't reproduce it in Scala 2.12 (don't have 2.11 handy). Yes, it's taking the value of tmp from when the block starts executing for some reason. As you say, it doesn't happen in 'normal' code nor in compiled code, I'm guessing, so don't know how significant this is.;;;","12/May/17 11:15;gurwls223;(I can't reproduce this in Scala 2.11.6 too);;;","12/May/17 11:55;raphsen;also tested with scala console 2.11.8, this works fine. So I assume the bug is in spark-shell itself;;;","07/Jul/17 06:36;raphsen;[~sowen] I think this is quite significant, because when working with spark-shell / spark-notebook one often works on methods (and derefore executing blocks of code multiple times). In our team we already last quite some time because of this bug.;;;","04/Dec/17 15:12;mpetruska;This is a Scala repl bug, see: https://github.com/scala/bug/issues/9740. The fix for this made it into Scala 2.11.9.
Basically it affects ""class-based"" Scala-shells, which is used in Spark-shell.
Creating the PR for the fix.;;;","04/Dec/17 15:32;apachespark;User 'mpetruska' has created a pull request for this issue:
https://github.com/apache/spark/pull/19879;;;","06/Dec/17 00:08;srowen;Issue resolved by pull request 19879
[https://github.com/apache/spark/pull/19879];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The sort function can not be used in the master page when you use Firefox or Google Chrome.,SPARK-20705,13071052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,guoxiaolongzte,guoxiaolongzte,guoxiaolongzte,11/May/17 06:37,12/Dec/22 18:11,14/Jul/23 06:30,15/May/17 06:52,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Web UI,,,,,0,,,,,,,,,"When you open the master page, the console of Firefox or Google Chrome is wrong, when you use Firefox or Google Chrome.But The IE  is no problem.

My Firefox version is 48.0.2.
My Google Chrome version  is 49.0.2623.75 m.
",,ajbozarth,apachespark,guoxiaolongzte,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 11:08;guoxiaolongzte;error.png;https://issues.apache.org/jira/secure/attachment/12867547/error.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 06:52:29 UTC 2017,,,,,,,,,,"0|i3etdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 06:42;gurwls223;Please fill the description in the issue.;;;","11/May/17 06:51;guoxiaolongzte;I will work on it,thank you.;;;","11/May/17 11:09;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/17952;;;","15/May/17 06:52;srowen;Issue resolved by pull request 17952
[https://github.com/apache/spark/pull/17952];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRAN test should run single threaded,SPARK-20704,13071047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,11/May/17 05:19,12/May/17 16:23,14/Jul/23 06:30,12/May/17 16:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SparkR,,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 05:21:03 UTC 2017,,,,,,,,,,"0|i3etcn:",9223372036854775807,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,"11/May/17 05:21;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17945;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskContextImpl.markTaskCompleted should not hide the original error,SPARK-20702,13071005,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,11/May/17 00:26,12/May/17 17:47,14/Jul/23 06:30,12/May/17 17:47,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"If a TaskCompletionListener throws an error, TaskContextImpl.markTaskCompleted will hide the original error.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 00:35:03 UTC 2017,,,,,,,,,,"0|i3et3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 00:35;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17942;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InferFiltersFromConstraints stackoverflows for query (v2),SPARK-20700,13070965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,joshrosen,joshrosen,10/May/17 21:30,18/May/17 06:34,14/Jul/23 06:30,18/May/17 06:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Optimizer,SQL,,,,0,,,,,,,,,"The following (complicated) query eventually fails with a stack overflow during optimization:

{code}
CREATE TEMPORARY VIEW table_5(varchar0002_col_1, smallint_col_2, float_col_3, int_col_4, string_col_5, timestamp_col_6, string_col_7) AS VALUES
  ('68', CAST(NULL AS SMALLINT), CAST(244.90413 AS FLOAT), -137, '571', TIMESTAMP('2015-01-14 00:00:00.0'), '947'),
  ('82', CAST(213 AS SMALLINT), CAST(53.184647 AS FLOAT), -724, '-278', TIMESTAMP('1999-08-15 00:00:00.0'), '437'),
  ('-7', CAST(-15 AS SMALLINT), CAST(NULL AS FLOAT), -890, '778', TIMESTAMP('1991-05-23 00:00:00.0'), '630'),
  ('22', CAST(676 AS SMALLINT), CAST(385.27386 AS FLOAT), CAST(NULL AS INT), '-10', TIMESTAMP('1996-09-29 00:00:00.0'), '641'),
  ('16', CAST(430 AS SMALLINT), CAST(187.23717 AS FLOAT), 989, CAST(NULL AS STRING), TIMESTAMP('2024-04-21 00:00:00.0'), '-234'),
  ('83', CAST(760 AS SMALLINT), CAST(-695.45386 AS FLOAT), -970, '330', CAST(NULL AS TIMESTAMP), '-740'),
  ('68', CAST(-930 AS SMALLINT), CAST(NULL AS FLOAT), -915, '-766', CAST(NULL AS TIMESTAMP), CAST(NULL AS STRING)),
  ('48', CAST(692 AS SMALLINT), CAST(-220.59615 AS FLOAT), 940, '-514', CAST(NULL AS TIMESTAMP), '181'),
  ('21', CAST(44 AS SMALLINT), CAST(NULL AS FLOAT), -175, '761', TIMESTAMP('2016-06-30 00:00:00.0'), '487'),
  ('50', CAST(953 AS SMALLINT), CAST(837.2948 AS FLOAT), 705, CAST(NULL AS STRING), CAST(NULL AS TIMESTAMP), '-62');

CREATE VIEW bools(a, b) as values (1, true), (1, true), (1, null);

SELECT
AVG(-13) OVER (ORDER BY COUNT(t1.smallint_col_2) DESC ROWS 27 PRECEDING ) AS float_col,
COUNT(t1.smallint_col_2) AS int_col
FROM table_5 t1
INNER JOIN (
SELECT
(MIN(-83) OVER (PARTITION BY t2.a ORDER BY t2.a, (t1.int_col_4) * (t1.int_col_4) ROWS BETWEEN CURRENT ROW AND 15 FOLLOWING)) NOT IN (-222, 928) AS boolean_col,
t2.a,
(t1.int_col_4) * (t1.int_col_4) AS int_col
FROM table_5 t1
LEFT JOIN bools t2 ON (t2.a) = (t1.int_col_4)
WHERE
(t1.smallint_col_2) > (t1.smallint_col_2)
GROUP BY
t2.a,
(t1.int_col_4) * (t1.int_col_4)
HAVING
((t1.int_col_4) * (t1.int_col_4)) IN ((t1.int_col_4) * (t1.int_col_4), SUM(t1.int_col_4))
) t2 ON (((t2.int_col) = (t1.int_col_4)) AND ((t2.a) = (t1.int_col_4))) AND ((t2.a) = (t1.smallint_col_2));
{code}

(I haven't tried to minimize this failing case yet).

Based on sampled jstacks from the driver, it looks like the query might be repeatedly inferring filters from constraints and then pruning those filters.

Here's part of the stack at the point where it stackoverflows:

{code}
[... repeats ...]
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative$1.apply(Canonicalize.scala:50)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
        at scala.collection.immutable.List.flatMap(List.scala:344)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.org$apache$spark$sql$catalyst$expressions$Canonicalize$$gatherCommutative(Canonicalize.scala:50)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.orderCommutative(Canonicalize.scala:58)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.expressionReorder(Canonicalize.scala:63)
        at org.apache.spark.sql.catalyst.expressions.Canonicalize$.execute(Canonicalize.scala:36)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:158)
        - locked <0x00000007a298b940> (a org.apache.spark.sql.catalyst.expressions.Multiply)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:156)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:157)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:157)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
[...]
 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:157)
        - locked <0x00000007a28b7170> (a org.apache.spark.sql.catalyst.expressions.EqualNullSafe)
        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:156)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.add(ExpressionSet.scala:56)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:66)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus(ExpressionSet.scala:50)
        at scala.collection.SetLike$$anonfun$$plus$plus$1.apply(SetLike.scala:141)
        at scala.collection.SetLike$$anonfun$$plus$plus$1.apply(SetLike.scala:141)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.immutable.HashSet$HashSetCollision1.foreach(HashSet.scala:462)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:972)
        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
        at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
        at scala.collection.TraversableOnce$class.$div$colon(TraversableOnce.scala:151)
        at scala.collection.AbstractTraversable.$div$colon(Traversable.scala:104)
        at scala.collection.SetLike$class.$plus$plus(SetLike.scala:141)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.$plus$plus(ExpressionSet.scala:50)
        at scala.collection.SetLike$class.union(SetLike.scala:163)
        at org.apache.spark.sql.catalyst.expressions.ExpressionSet.union(ExpressionSet.scala:50)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.getRelevantConstraints(QueryPlan.scala:35)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7ebaa8> (a org.apache.spark.sql.catalyst.plans.logical.Join)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.plans.logical.Filter.validConstraints(basicLogicalOperators.scala:138)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7eba58> (a org.apache.spark.sql.catalyst.plans.logical.Filter)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.plans.logical.Aggregate.validConstraints(basicLogicalOperators.scala:571)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7eb958> (a org.apache.spark.sql.catalyst.plans.logical.Aggregate)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.plans.logical.Filter.validConstraints(basicLogicalOperators.scala:138)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7eb908> (a org.apache.spark.sql.catalyst.plans.logical.Filter)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.plans.logical.Project.validConstraints(basicLogicalOperators.scala:65)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7eb328> (a org.apache.spark.sql.catalyst.plans.logical.Project)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.plans.logical.Join.validConstraints(basicLogicalOperators.scala:320)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints$lzycompute(QueryPlan.scala:187)
        - locked <0x000000079a7eb2c0> (a org.apache.spark.sql.catalyst.plans.logical.Join)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.constraints(QueryPlan.scala:187)
        at org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$inferFilters$1.applyOrElse(Optimizer.scala:642)
        at org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints$$anonfun$inferFilters$1.applyOrElse(Optimizer.scala:629)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
        at org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints.inferFilters(Optimizer.scala:629)
        at org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints.apply(Optimizer.scala:623)
        at org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints.apply(Optimizer.scala:620)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
        at scala.collection.immutable.List.foldLeft(List.scala:84)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
        at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:79)
        - locked <0x0000000787ea2848> (a org.apache.spark.sql.execution.QueryExecution)
        at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:85)
        - locked <0x0000000787ea2848> (a org.apache.spark.sql.execution.QueryExecution)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:81)
        at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:90)
        - locked <0x0000000787ea2848> (a org.apache.spark.sql.execution.QueryExecution)
[...]
{code}

I suspect this is similar to SPARK-17733, another bug where {{InferFiltersFromConstraints}}, so I'll cc [~jiangxb1987] and [~sameerag] who worked on that earlier fix.",,apachespark,jiangxb1987,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17733,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 17 17:30:04 UTC 2017,,,,,,,,,,"0|i3esuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 10:18;jiangxb1987;I'm working on this, thank you![~joshrosen];;;","12/May/17 15:29;jiangxb1987;I've reproduced this case, will dive further into it this weekend.;;;","16/May/17 22:21;jiangxb1987;In the previous approach we used `aliasMap` to link an `Attribute` to the expression with potentially the form `f(a, b)`, but we only searched the `expressions` and `children.expressions` for this, which is not enough when an `Alias` may lies deep in the logical plan. In that case, we can't generate the valid equivalent constraint classes and thus we fail at preventing the recursive deductions.

I'll send a PR to fix this later today.;;;","17/May/17 17:30;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subqueries in FROM should have alias names,SPARK-20690,13070708,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,10/May/17 08:03,07/Jul/17 06:45,14/Jul/23 06:30,17/May/17 05:00,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"We add missing attributes into Filter in Analyzer. But we shouldn't do it through subqueries like this:

{code}
select 1 from  (select 1 from onerow t1 LIMIT 1) where  t1.c1=1
{code}

This query works in current codebase. However, the outside where clause shouldn't be able to refer t1.c1 attribute.

The root cause is we allow subqueries in FROM have no alias names previously, it is confusing and isn't supported by various databases such as MySQL, Postgres, Oracle. We shouldn't support it too.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20916,,,,,,,,,,,,SPARK-21335,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 08:33:05 UTC 2017,,,,,,,,,,"0|i3er9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 08:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17935;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
python doctest leaking bucketed table,SPARK-20689,13070694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,10/May/17 06:22,10/May/17 16:34,14/Jul/23 06:30,10/May/17 16:34,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"When trying to address build test failure in SPARK-20661 we discovered some tables are unexpectedly left behind causing R tests to fail. While we changed the R tests to be more resilient, we investigated further to see what was creating those tables.

It turns out pyspark doctest is calling saveAsTable without ever dropping them. Since we have separate python tests for bucketed table, and we don't check for result in doctest, there is really no need to run the doctest ",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 06:29:02 UTC 2017,,,,,,,,,,"0|i3er67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 06:29;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17932;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correctly check analysis for scalar sub-queries,SPARK-20688,13070683,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/May/17 04:58,11/May/17 19:58,14/Jul/23 06:30,10/May/17 11:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 11:34:50 UTC 2017,,,,,,,,,,"0|i3er3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 05:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17930;;;","10/May/17 11:34;cloud_fan;Issue resolved by pull request 17930
[https://github.com/apache/spark/pull/17930];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mllib.Matrices.fromBreeze may crash when converting from Breeze sparse matrix,SPARK-20687,13070680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,elghoto,elghoto,elghoto,10/May/17 04:24,22/May/17 09:29,14/Jul/23 06:30,22/May/17 09:28,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,MLlib,,,,,0,,,,,,,,,"Conversion of Breeze sparse matrices to Matrix is broken when matrices are product of certain operations. This problem I think is caused by the update method in Breeze CSCMatrix when they add provisional zeros to the data for efficiency.

This bug is serious and may affect at least BlockMatrix addition and substraction

http://stackoverflow.com/questions/33528555/error-thrown-when-using-blockmatrix-add/43883458#43883458

The following code, reproduces the bug (Check test(""breeze conversion bug""))

https://github.com/ghoto/spark/blob/test-bug/CSCMatrixBreeze/mllib/src/test/scala/org/apache/spark/mllib/linalg/MatricesSuite.scala

{code:title=MatricesSuite.scala|borderStyle=solid}

  test(""breeze conversion bug"") {
    // (2, 0, 0)
    // (2, 0, 0)
    val mat1Brz = Matrices.sparse(2, 3, Array(0, 2, 2, 2), Array(0, 1), Array(2, 2)).asBreeze
    // (2, 1E-15, 1E-15)
    // (2, 1E-15, 1E-15
    val mat2Brz = Matrices.sparse(2, 3, Array(0, 2, 4, 6), Array(0, 0, 0, 1, 1, 1), Array(2, 1E-15, 1E-15, 2, 1E-15, 1E-15)).asBreeze
    // The following shouldn't break
    val t01 = mat1Brz - mat1Brz
    val t02 = mat2Brz - mat2Brz
    val t02Brz = Matrices.fromBreeze(t02)
    val t01Brz = Matrices.fromBreeze(t01)

    val t1Brz = mat1Brz - mat2Brz
    val t2Brz = mat2Brz - mat1Brz
    // The following ones should break
    val t1 = Matrices.fromBreeze(t1Brz)
    val t2 = Matrices.fromBreeze(t2Brz)

  }

{code}",,apachespark,elghoto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Mon May 22 09:28:03 UTC 2017,,,,,,,,,,"0|i3er33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 04:28;elghoto;The bug on SPARK-11507 is caused by this conversion, and therefore the problem wasn't patched correctly.;;;","10/May/17 09:24;srowen;This doesn't say what the problem is. What goes wrong?
;;;","10/May/17 16:34;elghoto;When you try to do operations like addition or subtraction between 2 mllib.distributed.BlockMatrices that store in blocks sparse matrices, these are operated using breeze and then converted back to Matrices again. Sometimes this conversion back produces crashes, even though the resulting matrix is valid, because this method in Matrices.fromBreeze doesn't extract correctly the data hold in CSC breeze matrix.

Unfortunately, I'm not able to show some code with block matrices, but I can show you some backtrace. I manually debugged the crashes, and found the culprit, so that's why I posted in the description a quite more simplified snippet that reproduces the error.

The snippet that causes the crash in BlockMatrix lines 374-379

{code:title:BlockMatrix.scala:blockMap}
          } else if (b.isEmpty) {
            new MatrixBlock((blockRowIndex, blockColIndex), a.head)
          } else {
            val result = binMap(a.head.asBreeze, b.head.asBreeze)
            new MatrixBlock((blockRowIndex, blockColIndex), Matrices.fromBreeze(result)) // <--not able to get results
          }
{code}


The trace after the operation between 2 spark block matrices:

{code:text}
Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 34, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: The last value of colPtrs must equal the number of elements. values.length: 28, colPtrs.last: 15
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:590)
	at org.apache.spark.mllib.linalg.SparseMatrix.<init>(Matrices.scala:618)
	at org.apache.spark.mllib.linalg.Matrices$.fromBreeze(Matrices.scala:995)
	at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:378)
	at org.apache.spark.mllib.linalg.distributed.BlockMatrix$$anonfun$10.apply(BlockMatrix.scala:365)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)
	at scala.collection.AbstractIterator.fold(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1087)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1087)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2119)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2119)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}
;;;","10/May/17 18:30;elghoto;Proposing a patch in PR https://github.com/apache/spark/pull/17940;;;","11/May/17 17:21;apachespark;User 'ghoto' has created a pull request for this issue:
https://github.com/apache/spark/pull/17940;;;","22/May/17 09:28;srowen;Issue resolved by pull request 17940
[https://github.com/apache/spark/pull/17940];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropagateEmptyRelation incorrectly handles aggregate without grouping expressions,SPARK-20686,13070648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,10/May/17 00:47,11/May/17 19:59,14/Jul/23 06:30,10/May/17 06:42,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Optimizer,SQL,,,,0,correctness,,,,,,,,"The query

{code}
SELECT 1 FROM (SELECT COUNT(*) WHERE FALSE) t1
{code}

should return a single row of output because the subquery is an aggregate without a group-by and thus should return a single row. However, Spark incorrectly returns zero rows.

This is caused by SPARK-16208, a patch which added an optimizer rule to propagate EmptyRelation through operators. The logic for handling aggregates is wrong: it checks whether aggregate expressions are non-empty for deciding whether the output should be empty, whereas it should be checking grouping expressions instead:

An aggregate with non-empty group expression will return one output row per group. If the input to the grouped aggregate is empty then all groups will be empty and thus the output will be empty. It doesn't matter whether the SELECT statement includes aggregate expressions since that won't affect the number of output rows.

If the grouping expressions are empty, however, then the aggregate will always produce a single output row and thus we cannot propagate the EmptyRelation.

The current implementation is incorrect (since it returns a wrong answer) and also misses an optimization opportunity by not propagating EmptyRelation in the case where a grouped aggregate has aggregate expressions.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16208,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 00:55:03 UTC 2017,,,,,,,,,,"0|i3eqvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 00:55;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchPythonEvaluation UDF evaluator fails for case of single UDF with repeated argument,SPARK-20685,13070617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,09/May/17 22:14,11/May/17 19:58,14/Jul/23 06:30,10/May/17 23:52,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,PySpark,,,,,0,,,,,,,,,"There's a latent corner-case bug in PYSpark UDF evaluation where executing a stage with a single UDF that takes more than one argument _where that argument is repeated_ will crash at execution with a confusing error.

Here's a repro:

{code}
from pyspark.sql.types import *
spark.catalog.registerFunction(""add"", lambda x, y: x + y, IntegerType())
spark.sql(""SELECT add(1, 1)"").first()
{code}

This fails with

{code}
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/joshrosen/Documents/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 180, in main
    process()
  File ""/Users/joshrosen/Documents/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 175, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/Users/joshrosen/Documents/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 107, in <lambda>
    func = lambda _, it: map(mapper, it)
  File ""/Users/joshrosen/Documents/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 93, in <lambda>
    mapper = lambda a: udf(*a)
  File ""/Users/joshrosen/Documents/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 71, in <lambda>
    return lambda *a: f(*a)
TypeError: <lambda>() takes exactly 2 arguments (1 given)
{code}

The problem was introduced by SPARK-14267: there code there has a fast path for handling a ""batch UDF evaluation consisting of a single Python UDF, but that branch incorrectly assumes that a single UDF won't have repeated arguments and therefore skips the code for unpacking arguments from the input row (whose schema may not necessarily match the UDF inputs).

I have a simple fix for this which I'll submit now.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14267,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 22:23:03 UTC 2017,,,,,,,,,,"0|i3eqp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/May/17 22:23;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17927;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DataFram.Drop doesn't take effective, neither does error",SPARK-20681,13070468,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,wangxisea,wangxisea,09/May/17 15:53,11/Jul/17 14:08,14/Jul/23 06:30,11/Jul/17 14:08,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SQL,,,,,0,,,,,,,,,"I am running the following code trying to drop nested columns,  but it doesn't work, it doesn't return error either.

*I read the DF from this json:*
{'parent':{'child':{'grandchild':{'val':'1',' val_to_be_deleted':'0'}}}}
scala> spark.read.format(""json"").load(""c:/tmp/spark_issue.json"")
res0: org.apache.spark.sql.DataFrame = [father: struct<child: struct<grandchild: struct<val: bigint, val_to_be_deleted: bigint>>>]

*read the df:*
scala> res0.printSchema
root
|-- parent: struct (nullable = true)
|    |-- child: struct (nullable = true)
|    |    |-- grandchild: struct (nullable = true)
|    |    |    |-- val: long (nullable = true)
|    |    |    |-- val_to_be_deleted: long (nullable = true)


*drop the column (I tried different ways, ""quote"", `back-tick`, col(object) ...) column remains anyway:*
scala> res0.drop(col(""father.child.grandchild.val_to_be_deleted"")).printSchema
root
|-- father: struct (nullable = true)
|    |-- child: struct (nullable = true)
|    |    |-- grandchild: struct (nullable = true)
|    |    |    |-- val: long (nullable = true)
|    |    |    |-- val_to_be_deleted: long (nullable = true)


scala> res0.drop(""father.child.grandchild.val_to_be_deleted"").printSchema
root
|-- father: struct (nullable = true)
|    |-- child: struct (nullable = true)
|    |    |-- grandchild: struct (nullable = true)
|    |    |    |-- val: long (nullable = true)
|    |    |    |-- val_to_be_deleted: long (nullable = true)



Any help is appreciated.",,lyc,wangxisea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 23 11:55:11 UTC 2017,,,,,,,,,,"0|i3epsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/May/17 11:55;lyc;As said in spark source code, `drop` can only be used to drop top level columns. Why did you think this will work?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-sql do not support for void column datatype of view,SPARK-20680,13070377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,09/May/17 10:28,13/Aug/20 05:13,14/Jul/23 06:30,08/Jul/20 01:58,2.1.0,2.1.1,2.4.6,3.0.0,,,,,,,,,,,,,,,,,,,,,,,3.1.0,,,,,SQL,,,,,2,,,,,,,,,"Create a HIVE view:
{quote}
hive> create table bad as select 1 x, null z from dual;
{quote}

Because there's no type, Hive gives it the VOID type:
{quote}
hive> describe bad;
OK
x	int	
z	void
{quote}

In Spark2.0.x, the behaviour to read this view is normal:
{quote}
spark-sql> describe bad;
x       int     NULL
z       void    NULL
Time taken: 4.431 seconds, Fetched 2 row(s)
{quote}

But in Spark2.1.x, it failed with SparkException: Cannot recognize hive type string: void
{quote}
spark-sql> describe bad;
17/05/09 03:12:08 INFO execution.SparkSqlParser: Parsing command: describe bad
17/05/09 03:12:08 INFO parser.CatalystSqlParser: Parsing command: int
17/05/09 03:12:08 INFO parser.CatalystSqlParser: Parsing command: void
17/05/09 03:12:08 ERROR thriftserver.SparkSQLDriver: Failed in [describe bad]
org.apache.spark.SparkException: Cannot recognize hive type string: void
        at org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$fromHiveColumn(HiveClientImpl.scala:789)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:365)  
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11$$anonfun$7.apply(HiveClientImpl.scala:365)  
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.Iterator$class.foreach(Iterator.scala:893)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11.apply(HiveClientImpl.scala:365)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getTableOption$1$$anonfun$apply$11.apply(HiveClientImpl.scala:361)

Caused by: org.apache.spark.sql.catalyst.parser.ParseException:
DataType void() is not supported.(line 1, pos 0)

== SQL ==  
void       
^^^

        ... 61 more
org.apache.spark.SparkException: Cannot recognize hive type string: void


{quote}
",,apachespark,cltlfcjin,dongjoon,hvanhovell,jiangxb1987,KevinZwx,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 05:13:55 UTC 2020,,,,,,,,,,"0|i3ep8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/May/17 15:56;hvanhovell;[~jiangxb] Do you have time to work on this?;;;","11/May/17 09:52;jiangxb1987;[~hvanhovell]Sure, I'll look at this issue.;;;","11/May/17 12:04;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17953;;;","11/May/17 12:11;cltlfcjin;Hi [~jiangxb1987] and [~hvanhovell], I fixed with a very simple way. Could you help to review this?
Below is the testing after patched.
{quote}
spark-sql> describe bad;
x       int     NULL
z       null    NULL
Time taken: 0.486 seconds, Fetched 2 row(s)
{quote};;;","15/Jun/20 08:51;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28833;;;","15/Jun/20 08:52;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28833;;;","28/Jun/20 02:31;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28935;;;","28/Jun/20 02:31;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/28935;;;","08/Jul/20 01:58;dongjoon;Issue resolved by pull request 28833
[https://github.com/apache/spark/pull/28833];;;","08/Jul/20 13:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29041;;;","27/Jul/20 12:25;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29244;;;","13/Aug/20 05:13;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/29423;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test - SparkListenerBus randomly failing java.lang.IllegalAccessError,SPARK-20666,13070257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,felixcheung,felixcheung,09/May/17 04:10,15/May/17 17:52,14/Jul/23 06:30,15/May/17 16:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,Spark Core,SparkR,,,0,,,,,,,,,"seeing quite a bit of this on AppVeyor, aka Windows only,-> seems like in other test runs too, always only when running ML tests, it seems

{code}
Exception in thread ""SparkListenerBus"" java.lang.IllegalAccessError: Attempted to access garbage collected accumulator 159454
	at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:265)
	at org.apache.spark.util.AccumulatorContext$$anonfun$get$1.apply(AccumulatorV2.scala:261)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.util.AccumulatorContext$.get(AccumulatorV2.scala:261)
	at org.apache.spark.util.AccumulatorV2.name(AccumulatorV2.scala:88)
	at org.apache.spark.sql.execution.metric.SQLMetric.toInfo(SQLMetrics.scala:67)
	at org.apache.spark.sql.execution.ui.SQLListener$$anonfun$onTaskEnd$1.apply(SQLListener.scala:216)
	at org.apache.spark.sql.execution.ui.SQLListener$$anonfun$onTaskEnd$1.apply(SQLListener.scala:216)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.sql.execution.ui.SQLListener.onTaskEnd(SQLListener.scala:216)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:45)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus.doPostEvent(LiveListenerBus.scala:36)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:63)
	at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.scala:36)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(LiveListenerBus.scala:94)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(LiveListenerBus.scala:79)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:78)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1268)
	at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:77)
1
MLlib recommendation algorithms: Spark package found in SPARK_HOME: C:\projects\spark\bin\..

{code}

{code}
java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2015)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2044)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2063)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2088)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2923)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2474)
	at org.apache.spark.sql.Dataset$$anonfun$collect$1.apply(Dataset.scala:2474)
	at org.apache.spark.sql.Dataset$$anonfun$57.apply(Dataset.scala:2907)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2906)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2474)
	at org.apache.spark.sql.api.r.SQLUtils$.dfToCols(SQLUtils.scala:173)
	at org.apache.spark.sql.api.r.SQLUtils.dfToCols(SQLUtils.scala)
	at sun.reflect.GeneratedMethodAccessor104.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:40)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
1: expect_equivalent(expected_predictions, collect(predict(model, new_data))) at C:/projects/spark/R/lib/SparkR/tests/testthat/test_mllib_fpm.R:63
{code}",,apachespark,felixcheung,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 06:48:04 UTC 2017,,,,,,,,,,"0|i3eohz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"09/May/17 16:18;vanzin;This doesn't fail just on Windows. It's been failing in PR builders also (e.g. https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/76605/testReport).

In a private branch of mine I ran into this problem and have this workaround:
https://github.com/vanzin/spark/pull/22/files#diff-edd374dbb96bc16363b65dab1e554793R114

But that's not cleanly ""backportable"" at the moment, and I don't know if it's the right solution either.

(BTW my fix is in SQL, which also suffers from this, so it would not help these mllib tests...);;;","09/May/17 17:02;vanzin;[~cloud_fan] do you think your fix for SPARK-12837 could have caused this? I've only noticed this error recently, and that's the only recent change in accumulator code...

A quick look at it didn't flag anything, though.;;;","09/May/17 17:29;vanzin;I'm raising the severity because lots of PRs are failing because of these kinds of errors...;;;","10/May/17 06:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17931;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark-sql, ""Bround"" and ""Round"" function return NULL",SPARK-20665,13070253,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,10110346,10110346,10110346,09/May/17 02:49,24/May/17 01:25,14/Jul/23 06:30,12/May/17 03:44,2.0.0,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,SQL,,,,,0,,,,,,,,,">select bround(12.3, 2);
>NULL
For  this case, the expected result is 12.3, but it is null

""Round"" has the same problem:
>select round(12.3, 2);
>NULL
",,10110346,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 01:25:03 UTC 2017,,,,,,,,,,"0|i3eoh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/May/17 03:22;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/17906;;;","12/May/17 03:44;cloud_fan;Issue resolved by pull request 17906
[https://github.com/apache/spark/pull/17906];;;","24/May/17 01:25;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/18082;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR tableNames() test fails,SPARK-20661,13070161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,08/May/17 20:57,09/May/17 05:47,14/Jul/23 06:30,08/May/17 21:48,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,test,,,,,,,,"Due to prior state created by other test cases, testing {{tableNames()}} is failing in master.

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.7/2846/console",,apachespark,falaki,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 05:47:03 UTC 2017,,,,,,,,,,"0|i3enwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/17 21:00;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/17903;;;","08/May/17 21:48;yhuai;Issue resolved by pull request 17903
[https://github.com/apache/spark/pull/17903];;;","09/May/17 03:19;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17905;;;","09/May/17 05:47;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17909;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make rpc timeout and retry for shuffle registration configurable,SPARK-20640,13070071,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lyc,sitalkedia@gmail.com,sitalkedia@gmail.com,08/May/17 17:39,17/May/20 18:30,14/Jul/23 06:30,21/Jun/17 13:55,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Shuffle,Spark Core,,,,1,,,,,,,,,"Currently the shuffle service registration timeout and retry has been hardcoded (see https://github.com/sitalkedia/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java#L144 and https://github.com/sitalkedia/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L197). This works well for small workloads but under heavy workload when the shuffle service is busy transferring large amount of data we see significant delay in responding to the registration request, as a result we often see the executors fail to register with the shuffle service, eventually failing the job. We need to make these two parameters configurable.",,apachespark,cloud_fan,jincheng,KaiXu,laurentcoder,rchukh,satheesshc,sitalkedia@gmail.com,Tagar,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20956,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 11 23:59:17 UTC 2017,,,,,,,,,,"0|i3encn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/May/17 05:35;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18092;;;","21/Jun/17 13:55;cloud_fan;Issue resolved by pull request 18092
[https://github.com/apache/spark/pull/18092];;;","11/Dec/17 23:59;xuefuz;[~lyc], thanks for fixing this. I'm wondering if you have any recommendation on the values of the new configurations in a heavy workload environment. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileFormatWriter wrap the FetchFailedException which breaks job's failover,SPARK-20633,13069855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liushaohui,liushaohui,liushaohui,08/May/17 02:13,31/May/17 15:57,14/Jul/23 06:30,31/May/17 15:54,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"The task scheduler handles FetchFailedException separately for the task failover. But the FileFormatWriter wraps the FetchFailedException with SparkException. This causes the job cannot be recovered from the failure like a external shuffle server is down.

See the stacktrace:
{code}
2017-04-30,05:02:42,348 ERROR org.apache.spark.sql.execution.datasources.DefaultWriterContainer: Task attempt attempt_201704300443_0018_m_000096_1 aborted.
2017-04-30,05:02:42,392 ERROR org.apache.spark.executor.Executor: Exception in task 96.1 in stage 18.0 (TID 26538)
org.apache.spark.SparkException: Task failed while writing rows
  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
  at org.apache.spark.scheduler.Task.run(Task.scala:86)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.shuffle.FetchFailedException: java.lang.RuntimeException: Executor is not registered (appId=application_1491898760056_636981, execId=546)
  at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:319)
  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.handleMessage(ExternalShuffleBlockHandler.java:87)
  at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:74)
  at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:152)
  at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
  at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
  at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
  at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
  at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
  at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
  at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
{code}",,apachespark,irashid,KaiXu,liushaohui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19276,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 15:57:31 UTC 2017,,,,,,,,,,"0|i3em0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/17 02:15;apachespark;User 'lshmouse' has created a pull request for this issue:
https://github.com/apache/spark/pull/17893;;;","08/May/17 15:12;irashid;this is fixed by SPARK-19276 in 2.2.0.  its probably good to fix it in any case, just for cleaner error msgs.;;;","30/May/17 16:19;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/18145;;;","31/May/17 15:54;irashid;Issue resolved by pull request 18145
[https://github.com/apache/spark/pull/18145];;;","31/May/17 15:57;irashid;This is no longer strictly necessary after SPARK-19276, but it improves error messages and also will help avoid others stumbling across this in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogisticRegression._checkThresholdConsistency should use values not Params,SPARK-20631,13069816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,07/May/17 19:36,24/May/17 10:10,14/Jul/23 06:30,10/May/17 09:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,ML,PySpark,,,,0,,,,,,,,,"{{_checkThresholdConsistency}} incorrectly uses {{getParam}} in attempt to access {{threshold}} and {{thresholds}} values. Furthermore it calls it with {{Param}} instead of {{str}}:

{code}
>>> from pyspark.ml.classification import LogisticRegression
>>> lr = LogisticRegression(threshold=0.25, thresholds=[0.75, 0.25])
Traceback (most recent call last):
...
TypeError: getattr(): attribute name must be string
{code}

Finally exception message uses {{join}} without converting values to {{str}}.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11834,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 10:10:03 UTC 2017,,,,,,,,,,"0|i3elrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/May/17 19:49;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/17891;;;","24/May/17 10:10;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/18085;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread Dump link available in Executors tab irrespective of spark.ui.threadDumpsEnabled,SPARK-20630,13069763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ajbozarth,jlaskowski,jlaskowski,07/May/17 12:19,11/May/17 19:58,14/Jul/23 06:30,10/May/17 09:20,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Web UI,,,,,0,,,,,,,,,Irrespective of {{spark.ui.threadDumpsEnabled}} property web UI's Executors page displays *Thread Dump* column with an active link (that does nothing though).,,ajbozarth,apachespark,jlaskowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/17 12:22;jlaskowski;spark-webui-executors-threadDump.png;https://issues.apache.org/jira/secure/attachment/12866773/spark-webui-executors-threadDump.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 09:20:33 UTC 2017,,,,,,,,,,"0|i3elg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/17 11:03;srowen;Sounds good, yeah, if you can disable the column when this dump is not enabled.;;;","08/May/17 18:15;ajbozarth;If you want I can take a look at this, I'm not sure why it's no longer working. I wrote the js code that toggles the thread dump link a while back after the switch the jQuery DataTables;;;","08/May/17 18:25;jlaskowski;Go [~ajbozarth], go!;;;","08/May/17 21:47;apachespark;User 'ajbozarth' has created a pull request for this issue:
https://github.com/apache/spark/pull/17904;;;","10/May/17 09:20;srowen;Issue resolved by pull request 17904
[https://github.com/apache/spark/pull/17904];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SparkR test warning on Windows with timestamp time zone,SPARK-20626,13069731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,07/May/17 01:33,08/May/17 06:14,14/Jul/23 06:30,08/May/17 06:14,2.1.0,2.2.0,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SparkR,,,,,0,,,,,,,,,"Warnings -----------------------------------------------------------------------
1. infer types and check types (@test_sparkSQL.R#123) - unable to identify current timezone 'C':
please set environment variable 'TZ",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 07 23:12:03 UTC 2017,,,,,,,,,,"0|i3el93:",9223372036854775807,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,"07/May/17 23:12;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17892;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuleExecutor logDebug of batch results should show diff to start of batch,SPARK-20616,13069530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,05/May/17 17:34,05/May/17 22:31,14/Jul/23 06:30,05/May/17 22:31,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Due to a likely typo, the logDebug msg printing the diff of query plans shows a diff to the initial plan, not diff to the start of batch.",,apachespark,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 17:38:04 UTC 2017,,,,,,,,,,"0|i3ek0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/May/17 17:38;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/17875;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparseVector.argmax throws IndexOutOfBoundsException when the sparse vector has a size greater than zero but no elements defined.,SPARK-20615,13069489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mclean,mclean,mclean,05/May/17 15:17,11/May/17 20:00,14/Jul/23 06:30,09/May/17 08:48,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,ML,MLlib,,,,0,,,,,,,,,"org.apache.spark.ml.linalg.SparseVector.argmax throws an IndexOutOfRangeException when the vector size is greater than zero and no values are defined.  The toString() representation of such a vector is "" (100000,[],[])"".  This is because the argmax function tries to get the value at indexes(0) without checking the size of the array.

Code inspection reveals that the mllib version of SparseVector should have the same issue.",,apachespark,mclean,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 08:48:33 UTC 2017,,,,,,,,,,"0|i3ejrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/May/17 15:26;srowen;Agree, I think you just want to return 0 if numActives == 0 early in the method.;;;","05/May/17 15:29;mclean;Thank you.  I will submit a patch with tests.;;;","05/May/17 21:22;apachespark;User 'jonmclean' has created a pull request for this issue:
https://github.com/apache/spark/pull/17877;;;","09/May/17 08:48;srowen;Issue resolved by pull request 17877
[https://github.com/apache/spark/pull/17877];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double quotes in Windows batch script,SPARK-20613,13069415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jarrettmeyer,jarrettmeyer,jarrettmeyer,05/May/17 11:05,05/May/17 15:41,14/Jul/23 06:30,05/May/17 15:36,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,Windows,,,,,0,,,,,,,,,"This is a new issue in version 2.1.1. This problem was not present in 2.1.0.

In {{bin/spark-class2.cmd}}, both the line that sets the {{RUNNER}} and the like that invokes the {{RUNNER}} have quotes. This opens and closes the quote immediately, producing something like

{code}
RUNNER=""""C:\Program Files (x86)\Java\jre1.8.0_131\bin\java""""
       ab          c
{code}

The quote above {{a}} opens the quote. The quote above {{b}} closes the quote. This creates a space at position {{c}}, which is invalid syntax.",,apachespark,felixcheung,jarrettmeyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 15:36:51 UTC 2017,,,,,,,,,,"0|i3ejav:",9223372036854775807,,,,,,,,,,,,,2.1.2,2.2.0,2.3.0,,,,,,,,,"05/May/17 11:08;apachespark;User 'jarrettmeyer' has created a pull request for this issue:
https://github.com/apache/spark/pull/17861;;;","05/May/17 15:36;felixcheung;[~shivaram]could you add jarretmeyer to contributor list in JIRA so I could resolve this bug to him?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run the SortShuffleSuite unit tests have residual spark_* system directory,SPARK-20609,13069346,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,heary-cao,heary-cao,heary-cao,05/May/17 06:22,17/May/20 18:31,14/Jul/23 06:30,22/May/17 13:23,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Shuffle,Spark Core,Tests,,,0,,,,,,,,,"This PR solution to run the SortShuffleSuite unit tests have residual spark_* system directory
For example:
OS:Windows 7
After the running SortShuffleSuite unit tests, 
the system of TMP directory have '..\AppData\Local\Temp\spark-f64121f9-11b4-4ffd-a4f0-cfca66643503' not deleted.",,apachespark,heary-cao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 05 07:36:04 UTC 2017,,,,,,,,,,"0|i3eivj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/May/17 06:24;apachespark;User 'heary-cao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17869;;;","22/May/17 13:23;srowen;Issue resolved by pull request 17869
[https://github.com/apache/spark/pull/17869];;;","05/Jul/17 07:36;apachespark;User 'heary-cao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18537;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate not used AM and executor port configuration,SPARK-20605,13069320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,05/May/17 02:58,04/Mar/20 18:04,14/Jul/23 06:30,08/May/17 21:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Mesos,Spark Core,YARN,,,0,,,,,,,,,"After SPARK-10997, client mode Netty RpcEnv doesn't require to bind a port to start server, so port configurations are not used any more, here propose to remove these two configurations: ""spark.executor.port"" and ""spark.am.port"".",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10997,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 03:31:03 UTC 2017,,,,,,,,,,"0|i3eipr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/May/17 03:31;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17866;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The staging directory should be appended with "".hive-staging"" to avoid being deleted if we set hive.exec.stagingdir under the table directory without start with "".""",SPARK-20594,13069043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,04/May/17 07:10,12/May/17 18:25,14/Jul/23 06:30,12/May/17 18:25,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The staging directory should be appended with "".hive-staging"" to avoid being deleted when we set hive.exec.stagingdir under the table directory without start with "".""


spark-sql> set  hive.exec.stagingdir=./test;
spark-sql> insert overwrite table test_table1 select * from test_table;

we got error as follows:

2017-05-04 15:21:06,948 INFO org.apache.hadoop.hive.common.FileUtils: deleting  hdfs://nameservice/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1
2017-05-04 15:21:06,987 INFO org.apache.hadoop.fs.TrashPolicyDefault: Moved: 'hdfs://nameservice/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1' to trash at: hdfs://nameservice/user/mr/.Trash/Current/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1
2017-05-04 15:21:06,987 INFO org.apache.hadoop.hive.common.FileUtils: Moved to trash: hdfs://nameservice/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1
2017-05-04 15:21:07,001 ERROR org.apache.hadoop.hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
2017-05-04 15:21:07,007 INFO hive.ql.metadata.Hive: Replacing src:hdfs://nameservice/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1/-ext-10000/part-00000, dest: hdfs://nameservice/spark/ztb.db/test_table1/part-00000, Status:false
2017-05-04 15:21:07,024 ERROR org.apache.spark.sql.hive.thriftserver.SparkSQLDriver: Failed in [insert overwrite table test_table1 select * from test_table]
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:633)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:646)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:646)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:646)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadTable(HiveClientImpl.scala:645)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult$lzycompute(InsertIntoHiveTable.scala:290)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.sideEffectResult(InsertIntoHiveTable.scala:143)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.executeCollect(InsertIntoHiveTable.scala:308)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:186)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:331)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move file/directory from hdfs://nameservice/spark/ztb.db/test_table1/test_hive_2017-05-04_15-21-05_972_7582740597864081934-1/-ext-10000/part-00000 to hdfs://nameservice/spark/ztb.db/test_table1/part-00000
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2899)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1589)",,apachespark,glenn.strycker@gmail.com,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 04 07:26:02 UTC 2017,,,,,,,,,,"0|i3eh0f:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"04/May/17 07:26;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17858;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Succeeded tasks num not equal in job page and job detail page on spark web ui when speculative task(s) exist,SPARK-20591,13069012,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Jinhua Fu,Jinhua Fu,Jinhua Fu,04/May/17 03:31,22/May/17 12:59,14/Jul/23 06:30,22/May/17 12:58,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Web UI,,,,,0,,,,,,,,,"when spark.speculation is enabled,and there are some speculative tasks, then we can see succeeded tasks num include speculative tasks on the job page, which however not being included on the job detail page(job stages page).
When I consider some tasks may run a little slow by the job page's  succeeded tasks more than total tasks,which make me want to known which tasks and why,I have to check every stage to find the speculative tasks which is beacause speculative tasks not being included in the stage succeeded task num.
Can it be improved?

update two screenshots, succeeded task num is 557 on job page,but 550(by sum) on job detail page(stages),the extra 7 tasks are speculative tasks. ",,ajbozarth,apachespark,discipleforteen,iamhumanbeing,Jinhua Fu,zzr1000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/17 09:36;Jinhua Fu;job detail page(stages).png;https://issues.apache.org/jira/secure/attachment/12866368/job+detail+page%28stages%29.png","04/May/17 09:32;Jinhua Fu;job page.png;https://issues.apache.org/jira/secure/attachment/12866364/job+page.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 22 12:58:55 UTC 2017,,,,,,,,,,"0|i3egtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/May/17 09:15;srowen;Provide an example please? at least screen shots.;;;","04/May/17 09:39;srowen;I agree, it's not consistent. I'm not sure what the nature of the problem is, why they're counting differently.;;;","04/May/17 12:57;Jinhua Fu;Does it need modify and may I take this PR?;;;","08/May/17 08:18;discipleforteen;i think that's better if it can be consistent;;;","09/May/17 13:38;apachespark;User 'fjh100456' has created a pull request for this issue:
https://github.com/apache/spark/pull/17923;;;","22/May/17 12:58;srowen;Issue resolved by pull request 17923
[https://github.com/apache/spark/pull/17923];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map default input data source formats to inlined classes,SPARK-20590,13068961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,sameerag,sameerag,03/May/17 22:16,12/Dec/22 17:34,14/Jul/23 06:30,10/May/17 05:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"One of the common usability problems around reading data in spark (particularly CSV) is that there can often be a conflict between different readers in the classpath.

As an example, if someone launches a 2.x spark shell with the spark-csv package in the classpath, Spark currently fails in an extremely unfriendly way

{code}
./bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0
scala> val df = spark.read.csv(""/foo/bar.csv"")
java.lang.RuntimeException: Multiple sources found for csv (org.apache.spark.sql.execution.datasources.csv.CSVFileFormat, com.databricks.spark.csv.DefaultSource15), please specify the fully qualified class name.
  at scala.sys.package$.error(package.scala:27)
  at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:574)
  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:85)
  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:85)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:295)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:412)
  ... 48 elided
{code}

This JIRA proposes a simple way of fixing this error by always mapping default input data source formats to inlined classes (that exist in Spark).

{code}
./bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0
scala> val df = spark.read.csv(""/foo/bar.csv"")
df: org.apache.spark.sql.DataFrame = [_c0: string]
{code}",,apachespark,cloud_fan,felixcheung,sameerag,shawnxiangyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 09:43:55 UTC 2021,,,,,,,,,,"0|i3egi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/17 22:21;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/17847;;;","09/May/17 07:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17916;;;","10/May/17 05:47;cloud_fan;Issue resolved by pull request 17916
[https://github.com/apache/spark/pull/17916];;;","11/May/17 04:18;felixcheung;When the user explicitly specifies the package to use, shouldn't that take priority over the internal one?
say if there is a better csv implementation exists as a spark package, then right now there is no way to use it.
;;;","11/May/17 04:25;cloud_fan;We only prefer internal data source if the given name is a short name like ""csv"", ""json"", etc. Using full name still works.;;;","17/Mar/21 09:43;shawnxiangyu;[~cloud_fan], I tried to use the full name as, it does not work. Any idea?  (more detailed explanation of the problem is here: https://stackoverflow.com/questions/66664181/spark-multiple-sources-found-for-text)
{code:java}
DataFrameReader read = spark.read()；
JavaRDD<String> stringJavaRDD = read.format(""org.apache.spark.sql.execution.datasources.text.TextFileFormat"").textFile(inputPath).javaRDD();

{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add deterministic to ScalaUDF,SPARK-20586,13068836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,03/May/17 16:53,13/Dec/17 14:12,14/Jul/23 06:30,26/Jul/17 00:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"https://hive.apache.org/javadocs/r2.0.1/api/org/apache/hadoop/hive/ql/udf/UDFType.html

Like Hive UDFType, we should allow users to add the extra flags for ScalaUDF too. {{stateful}}/{{impliesOrder}} are not applicable to ScalaUDF. Thus, we only add the following two flags. 

- deterministic: Certain optimizations should not be applied if UDF is not deterministic. Deterministic UDF returns same result each time it is invoked with a particular input. This determinism just needs to hold within the context of a query.
",,apachespark,bago.amirbekian,maropu,smilegator,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15282,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 21 19:50:03 UTC 2017,,,,,,,,,,"0|i3efqf:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"03/May/17 22:35;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17848;;;","21/Nov/17 19:39;bago.amirbekian;Is there some documentation somewhere about the right way to use this `deterministic` flag?

I bring it up because in `spark.ml` sometimes we will raise errors in a udf when a dataFrame contains invalid data. This can cause bad behavior if the optimizer re-orders the udf with other operations so we’re marking these udfs as `nonDeterministic` (https://github.com/apache/spark/pull/19662) but that somehow seems wrong. The issue isn’t that the udfs are non-deterministic, they are deterministic and always raise on the same inputs.

I guess my question is
1) Is this correct usage of the non-deterministic flag or are we simply abusing it when we should come up with a more specific solution?
2) If this is correct usage could we rename the flag or document this type of usage somewhere?;;;","21/Nov/17 19:50;bago.amirbekian;Also a follow up questions, are the performance implications to using the `deterministic` flag we should try and avoid by restructuring the ml UDFs to avoid raising within the UDF.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Bucketizer to handle non-Double column,SPARK-20574,13068648,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,03/May/17 05:59,05/May/17 02:30,14/Jul/23 06:30,05/May/17 02:30,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"Bucketizer currently requires input column to be Double, but the logic should work on any numeric data types. Many practical problems have integer/float data types, and it could get very tedious to manually cast them into Double before calling bucketizer. This transformer could be extended to handle all numeric types.  

The example below shows failure of Bucketizer on integer data. 
{code}
val splits = Array(-3.0, 0.0, 3.0)
val data: Array[Int] = Array(-2, -1, 0, 1, 2)
val expectedBuckets = Array(0.0, 0.0, 1.0, 1.0, 1.0)
val dataFrame = data.zip(expectedBuckets).toSeq.toDF(""feature"", ""expected"")
val bucketizer = new Bucketizer()
  .setInputCol(""feature"")
  .setOutputCol(""result"")
  .setSplits(splits)
bucketizer.transform(dataFrame)  

java.lang.IllegalArgumentException: requirement failed: Column feature must be of type DoubleType but was actually IntegerType.
{code}

",,actuaryzhang,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 06:05:39 UTC 2017,,,,,,,,,,"0|i3eekn:",9223372036854775807,,,,,yanboliang,,,,,,,,,,,,,,,,,,,"03/May/17 06:05;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17840;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The main version number on docs/latest/index.html,SPARK-20570,13068619,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,liucht-inspur,liucht-inspur,03/May/17 02:00,04/May/17 11:24,14/Jul/23 06:30,03/May/17 17:31,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,,,,,Documentation,,,,,0,,,,,,,,,"On the spark.apache.org home page, when I click the menu  Latest Release (Spark 2.1.1) under the documentation menu ,the next page latest appear with display 2.1.0 lable in the upper left corner of the page",,liucht-inspur,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-14082,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 04 11:22:12 UTC 2017,,,,,,,,,,"0|i3eee7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/17 09:16;srowen;Indeed it seems like the 2.1.1 docs didn't deploy: http://spark.apache.org/docs/
CC [~marmbrus];;;","03/May/17 17:20;marmbrus;Hmmm, I did push them, and they show up on the [asf git website|https://git-wip-us.apache.org/repos/asf?p=spark-website.git;a=commit;h=d4f0c34ac33001169452a276acfebb7b8cc58c0f].  They don't seem to appear on github though.  I guess I'll open an INFRA ticket.;;;","03/May/17 17:26;srowen;Oh, probably another git sync hiccup. It may 'flush' if you push an empty commit to the repo. I can try that in a sec.;;;","03/May/17 17:31;srowen;Oh, [~marmbrus] already did that, and it looks like it's OK now: http://spark.apache.org/docs/latest/;;;","04/May/17 11:22;liucht-inspur;Version problem solved and looks like good running,thanks Sean Owen and Michael Armbrust ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeReplaceable functions accept invalid third parameter,SPARK-20569,13068618,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,cloud_fan,10110346,10110346,03/May/17 01:56,11/May/17 19:57,14/Jul/23 06:30,11/May/17 07:42,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,">select  Nvl(null,'1',3);
>3

The function of ""Nvl"" has Only two  input parameters,so, when input three parameters, i think it should notice that:""Error in query: Invalid number of arguments for function nvl"".

Such as ""nvl2"", ""nullIf"",""IfNull"",these have a similar problem",,10110346,apachespark,cloud_fan,iamhumanbeing,marmbrus,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 18:04:02 UTC 2017,,,,,,,,,,"0|i3eedz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/May/17 09:10;umesh9794@gmail.com;Those functions seem to support additional parameter because of following like definition: 

{code}
case class Nvl(left: Expression, right: Expression, child: Expression) extends RuntimeReplaceable {
{code};;;","03/May/17 09:30;10110346;yes，it  supports additional parameter.
 I think it is best to  change this class into only two parameters.
Because  three parameters for this class has no sense,but i do not know how to modify it;;;","03/May/17 09:36;umesh9794@gmail.com;I believe [~marmbrus] is the best person to answer the question about current implementation. ;;;","03/May/17 10:18;10110346;ok,thanks  [~umesh9794@gmail.com];;;","03/May/17 18:51;marmbrus;[~rxin] this does seem like a bug.;;;","05/May/17 15:35;cloud_fan;yea this is a bug, I'm working on a fix;;;","05/May/17 18:04;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17876;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to bind when using explode and collect_set in streaming,SPARK-20567,13068599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,03/May/17 00:12,03/May/17 05:45,14/Jul/23 06:30,03/May/17 05:45,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Here is a small test case:
{code}
  test(""count distinct"") {
    val inputData = MemoryStream[(Int, Seq[Int])]

    val aggregated =
      inputData.toDF()
        .select($""*"", explode($""_2"") as 'value)
        .groupBy($""_1"")
        .agg(size(collect_set($""value"")))
        .as[(Int, Int)]

    testStream(aggregated, Update)(
      AddData(inputData, (1, Seq(1, 2))),
      CheckLastBatch((1, 2))
    )
  }
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 00:17:03 UTC 2017,,,,,,,,,,"0|i3ee9r:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"03/May/17 00:17;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/17838;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error message for unsupported JDBC types,SPARK-20565,13068547,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,02/May/17 20:56,25/May/17 05:34,14/Jul/23 06:30,25/May/17 05:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"For unsupported data types, we simply output the type number instead of the type name. 

{noformat}
java.sql.SQLException: Unsupported type 2014
{noformat}

We should improve it by outputting its name.
",,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20873,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 25 05:34:07 UTC 2017,,,,,,,,,,"0|i3edy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/May/17 05:34;smilegator;Fixed in https://github.com/apache/spark/pull/17835;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clear InheritableThreadLocal variables in SparkContext when stopping it,SPARK-20558,13068431,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,02/May/17 13:42,11/May/17 20:02,14/Jul/23 06:30,03/May/17 02:12,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Spark Core,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 13:58:04 UTC 2017,,,,,,,,,,"0|i3ed8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/May/17 13:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17833;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect handling of Oracle's decimal types via JDBC,SPARK-20555,13068377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gfeher,gfeher,gfeher,02/May/17 10:19,09/Dec/17 00:30,14/Jul/23 06:30,24/Jun/17 05:11,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,2,,,,,,,,,"When querying an Oracle database, Spark maps some Oracle numeric data types to incorrect Catalyst data types:
1. DECIMAL(1) becomes BooleanType
In Orcale, a DECIMAL(1) can have values from -9 to 9.
In Spark now, values larger than 1 become the boolean value true.
2. DECIMAL(3,2) becomes IntegerType
In Oracle, a DECIMAL(2) can have values like 1.23
In Spark now, digits after the decimal point are dropped.
3. DECIMAL(10) becomes IntegerType
In Oracle, a DECIMAL(10) can have the value 9999999999 (ten nines), which is more than 2^31
Spark throws an exception: ""java.sql.SQLException: Numeric Overflow""

I think the best solution is to always keep Oracle's decimal types. (In theory we could introduce a FloatType in some case of #2, and fix #3 by only introducing IntegerType for DECIMAL(9). But in my opinion, that would end up complicated and error-prone.)

Note: I think the above problems were introduced as part of  https://github.com/apache/spark/pull/14377
The main purpose of that PR seems to be converting Spark types to correct Oracle types, and that part seems good to me. But it also adds the inverse conversions. As it turns out in the above examples, that is not possible.",,apachespark,asukhenko,georg.kf.heiler@gmail.com,gfeher,shankarkool,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 09 00:30:02 UTC 2017,,,,,,,,,,"0|i3ecwf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/May/17 10:26;apachespark;User 'gaborfeher' has created a pull request for this issue:
https://github.com/apache/spark/pull/17830;;;","16/May/17 13:34;gfeher;Hi,

Maybe it was not clear from the title, but this issue is causing data (precision) loss or crashes when reading from Oracle Databases via Spark. I also have a patch!;;;","23/Jun/17 17:55;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18408;;;","07/Dec/17 16:47;shankarkool;[~gfeher]
May i know if first issue has been resolved?

""1. DECIMAL(1) becomes BooleanType
In Orcale, a DECIMAL(1) can have values from -9 to 9.""

I am using the spark 2.2.0 but still i am getting Boolean ""false"" when source is having NUMBER(1) as 0.  I want it as 0 without customschema Could you please advise?;;;","09/Dec/17 00:30;gfeher;This issues was fixed, so in theory, you should not be getting Boolean type in Spark if the database has a NUMBER(1) column.

I checked the code and it seems to be that the fix is still there in Spark 2.2.0:
Fix: https://github.com/apache/spark/pull/18408/files
Spark 2.2.0 fixed code: https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/jdbc/OracleDialect.scala#L33
Spark 2.2.0 tests: https://github.com/apache/spark/blob/v2.2.0/external/docker-integration-tests/src/test/scala/org/apache/spark/sql/jdbc/OracleIntegrationSuite.scala#L108

I suggest you the following: double-check that you have a clean Spark 2.2.0 setup, i.e. there are no remnants of earlier Spark versions loaded with your program. If you have confirmed that, then create an as simple as possible step by step guide how to observe your problem, so that another person can do it easily. Perhaps the best would be to demonstrate your problem in a cleanly installed Spark shell.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.CharConversionException: Invalid UTF-32 in JsonToStructs,SPARK-20549,13068261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,01/May/17 22:33,16/Jan/18 20:55,14/Jul/23 06:30,02/May/17 06:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,The same fix for SPARK-16548 needs to be applied for JsonToStructs,,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23094,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 01 23:48:04 UTC 2017,,,,,,,,,,"0|i3ec6n:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"01/May/17 23:48;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/17826;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test:  ReplSuite.newProductSeqEncoder with REPL defined class,SPARK-20548,13068249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sameerag,sameerag,sameerag,01/May/17 21:45,11/May/17 20:00,14/Jul/23 06:30,09/May/17 16:12,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{{newProductSeqEncoder with REPL defined class}} in {{ReplSuite}} has been failing in-deterministically : https://spark-tests.appspot.com/failed-tests over the last few days.

https://spark.test.databricks.com/job/spark-master-test-sbt-hadoop-2.7/176/testReport/junit/org.apache.spark.repl/ReplSuite/newProductSeqEncoder_with_REPL_defined_class/history/",,apachespark,cloud_fan,hvanhovell,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 16:12:23 UTC 2017,,,,,,,,,,"0|i3ec3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/May/17 21:52;sameerag;I'm going to disable this test in the short term;;;","01/May/17 21:54;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/17823;;;","02/May/17 01:57;hvanhovell;I jumped the gun on this one, the test has been disabled. This has not been fixed yet.;;;","02/May/17 14:02;cloud_fan;I'll re-enable this test after https://github.com/apache/spark/pull/17833 is merged;;;","03/May/17 18:10;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17844;;;","09/May/17 16:12;cloud_fan;Issue resolved by pull request 17844
[https://github.com/apache/spark/pull/17844];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorClassLoader's findClass may not work correctly when a task is cancelled.,SPARK-20547,13068237,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,01/May/17 20:57,04/Aug/19 13:42,14/Jul/23 06:30,28/May/19 19:57,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Spark Shell,,,,,0,,,,,,,,,"ExecutorClassLoader's findClass may throw some transient exception. For example, when a task is cancelled, if ExecutorClassLoader is running, you may see InterruptedException or IOException, even if this class can be loaded. Then the result of findClass will be cached by JVM, and later when the same class is being loaded (note: in this case, this class may be still loadable), it will just throw NoClassDefFoundError.

We should make ExecutorClassLoader retry on transient exceptions.",,devaraj,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 02 18:09:38 UTC 2017,,,,,,,,,,"0|i3ec1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/May/17 23:35;zsxwing;This is a reproducer: https://github.com/zsxwing/spark/commit/993b6d24d935cf489fdf643ef4266c239028d6e4;;;","02/May/17 18:09;zsxwing;Did some investigation using the reproducer. Looks like it’s not a class loader issue. It’s because the class initialization result will be cached. My current proposal is recreating a new class loader if a task fails.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-class gets syntax error in posix mode,SPARK-20546,13068233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jyu0,jyu0,jyu0,01/May/17 20:45,11/May/17 20:01,14/Jul/23 06:30,05/May/17 10:37,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Deploy,,,,,0,,,,,,,,,"spark-class gets the following error when running in posix mode:
{code}
spark-class: line 78: syntax error near unexpected token `<'
spark-class: line 78: `done < <(build_command ""$@"")'
{code}
\\
It appears to be complaining about the process substitution: 
{code}
CMD=()
while IFS= read -d '' -r ARG; do
  CMD+=(""$ARG"")
done < <(build_command ""$@"")
{code}
\\
This can be reproduced by first turning on allexport then posix mode:
{code}set -a -o posix {code}
then run something like spark-shell which calls spark-class.

\\
The simplest fix is probably to always turn off posix mode in spark-class before the while loop.
\\
This was previously reported in [SPARK-8417|https://issues.apache.org/jira/browse/SPARK-8417] which closed with cannot reproduce. ",,apachespark,jyu0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 10:37:26 UTC 2017,,,,,,,,,,"0|i3ec0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/May/17 22:24;srowen;Are there downsides to turning off posix mode?
It seems reasonable if it makes this case work and doesn't affect other behavior.;;;","02/May/17 13:27;jyu0;Given the current code relies on posix mode being off, turning it off explicitly shouldn't affect other behavior, especially since the change is confined to just the spark-class subshell. ;;;","03/May/17 08:26;srowen;Ok, seems fine to confine it to just this script with a comment about the reason. ;;;","04/May/17 01:12;apachespark;User 'jyu00' has created a pull request for this issue:
https://github.com/apache/spark/pull/17852;;;","05/May/17 10:37;srowen;Issue resolved by pull request 17852
[https://github.com/apache/spark/pull/17852];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R should skip long running or non-essential tests when running on CRAN,SPARK-20543,13068080,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,01/May/17 06:03,06/May/17 04:40,14/Jul/23 06:30,04/May/17 04:55,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SparkR,,,,,0,,,,,,,,,This is actually recommended in the CRAN policies,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 06 04:40:03 UTC 2017,,,,,,,,,,"0|i3eb2f:",9223372036854775807,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,"01/May/17 06:07;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17817;;;","06/May/17 04:40;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17878;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR SS should support awaitTermination without timeout,SPARK-20541,13068052,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,01/May/17 00:05,01/May/17 06:27,14/Jul/23 06:30,01/May/17 06:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SparkR,Structured Streaming,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 01 00:13:02 UTC 2017,,,,,,,,,,"0|i3eaw7:",9223372036854775807,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,"01/May/17 00:13;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17815;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation constantly requests and kills executors,SPARK-20540,13068046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdblue,rdblue,rdblue,30/Apr/17 22:23,01/May/17 21:49,14/Jul/23 06:30,01/May/17 21:49,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"We are seeing some strange behavior with dynamic allocation, where in some cases the driver will get into a state where it constantly kills idle executors while requesting new executors. This happens at the end of a stage when all tasks are assigned and never stops even when there are no tasks to run.

From the YarnAllocator logs, it looks like the allocator is getting lots of requests from the driver, even though the timeout between requests should be 5s:

{code:title=Yarn allocator logs}
17/04/20 19:52:05 INFO dispatcher-event-loop-49 YarnAllocator: Driver requested a total number of 227 executor(s).
17/04/20 19:52:05 INFO dispatcher-event-loop-30 YarnAllocator: Driver requested a total number of 213 executor(s).
17/04/20 19:52:05 INFO Reporter YarnAllocator: Will request 1 executor containers, each with 2 cores and 7168 MB memory including 2048 MB overhead
17/04/20 19:52:05 INFO Reporter YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/04/20 19:52:05 INFO Reporter YarnAllocator: Submitted container request (host: Any, capability: &lt;memory:7168, vCores:2&gt;)
spark://CoarseGrainedScheduler@100.74.39.143:10895,  executorHostname: ip-100-74-34-230.ec2.internal
spark://CoarseGrainedScheduler@100.74.39.143:10895,  executorHostname: ip-100-74-47-57.ec2.internal
17/04/20 19:52:05 INFO Reporter YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
17/04/20 19:52:05 INFO dispatcher-event-loop-11 YarnAllocator: Driver requested a total number of 195 executor(s).
17/04/20 19:52:05 INFO dispatcher-event-loop-55 YarnAllocator: Driver requested a total number of 174 executor(s).
17/04/20 19:52:05 INFO Reporter YarnAllocator: Will request 2 executor containers, each with 2 cores and 7168 MB memory including 2048 MB overhead
17/04/20 19:52:05 INFO Reporter YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/04/20 19:52:05 INFO Reporter YarnAllocator: Submitted container request (host: Any, capability: &lt;memory:7168, vCores:2&gt;)
17/04/20 19:52:05 INFO Reporter YarnAllocator: Submitted container request (host: Any, capability: &lt;memory:7168, vCores:2&gt;)
17/04/20 19:52:05 INFO Reporter YarnAllocator: Received 4 containers from YARN, launching executors on 4 of them.
{code}

I think the allocator cancels what requests it can, but is getting containers that have already been requested and the executors keep growing because of requests from the driver. Here are 5 seconds from the log:

{code}
17/04/20 19:52:30 INFO dispatcher-event-loop-22 YarnAllocator: Driver requested a total number of 185 executor(s).
17/04/20 19:52:30 INFO dispatcher-event-loop-48 YarnAllocator: Driver requested a total number of 193 executor(s).
17/04/20 19:52:30 INFO dispatcher-event-loop-24 YarnAllocator: Driver requested a total number of 192 executor(s).
17/04/20 19:52:30 INFO dispatcher-event-loop-60 YarnAllocator: Driver requested a total number of 195 executor(s).
17/04/20 19:52:30 INFO dispatcher-event-loop-53 YarnAllocator: Driver requested a total number of 205 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-19 YarnAllocator: Driver requested a total number of 202 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-17 YarnAllocator: Driver requested a total number of 232 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-45 YarnAllocator: Driver requested a total number of 243 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-19 YarnAllocator: Driver requested a total number of 254 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-42 YarnAllocator: Driver requested a total number of 263 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-20 YarnAllocator: Driver requested a total number of 271 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-35 YarnAllocator: Driver requested a total number of 280 executor(s).
17/04/20 19:52:31 INFO dispatcher-event-loop-61 YarnAllocator: Driver requested a total number of 289 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-22 YarnAllocator: Driver requested a total number of 305 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-28 YarnAllocator: Driver requested a total number of 310 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-0 YarnAllocator: Driver requested a total number of 313 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-28 YarnAllocator: Driver requested a total number of 315 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-40 YarnAllocator: Driver requested a total number of 316 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-13 YarnAllocator: Driver requested a total number of 317 executor(s).
17/04/20 19:52:32 INFO dispatcher-event-loop-35 YarnAllocator: Driver requested a total number of 311 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-40 YarnAllocator: Driver requested a total number of 308 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-4 YarnAllocator: Driver requested a total number of 301 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-23 YarnAllocator: Driver requested a total number of 294 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-46 YarnAllocator: Driver requested a total number of 287 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-8 YarnAllocator: Driver requested a total number of 285 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-63 YarnAllocator: Driver requested a total number of 283 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-35 YarnAllocator: Driver requested a total number of 281 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-63 YarnAllocator: Driver requested a total number of 278 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-3 YarnAllocator: Driver requested a total number of 277 executor(s).
17/04/20 19:52:33 INFO dispatcher-event-loop-38 YarnAllocator: Driver requested a total number of 276 executor(s).
17/04/20 19:52:34 INFO dispatcher-event-loop-51 YarnAllocator: Driver requested a total number of 273 executor(s).
17/04/20 19:52:34 INFO dispatcher-event-loop-31 YarnAllocator: Driver requested a total number of 271 executor(s).
17/04/20 19:52:34 INFO dispatcher-event-loop-44 YarnAllocator: Driver requested a total number of 270 executor(s).
{code}",,apachespark,rdblue,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 30 22:37:04 UTC 2017,,,,,,,,,,"0|i3eauv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/17 22:37;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/17813;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffHeapColumnVector reallocation may not copy existing data,SPARK-20537,13067988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,30/Apr/17 10:16,11/May/17 20:02,14/Jul/23 06:30,02/May/17 05:58,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"As SPARK-20474 revealed, reallocation in {{OnHeapColumnVector}} may copy a part of the original storage.
{{OffHeapColumnVector}} reallocation also copies to the new storage data up to {{elementsAppended}}. This variable is only updated when using the ColumnVector.appendX API, while ColumnVector.putX is more commonly used.",,apachespark,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 30 10:32:02 UTC 2017,,,,,,,,,,"0|i3eahz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/17 10:32;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17811;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outer generators skip missing records if used alone,SPARK-20534,13067943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,29/Apr/17 17:15,11/May/17 20:03,14/Jul/23 06:30,01/May/17 16:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Example data:

{code}
val df = Seq(
  (1, Some(""a"" :: ""b"" :: ""c"" :: Nil)), 
  (2, None), 
  (3, Some(""a"" :: Nil)
)).toDF(""k"", ""vs"")
{code}

Correct behavior if there are other expressions:

{code}
df.select($""k"", explode_outer($""vs"")).show
// +---+----+
// |  k| col|
// +---+----+
// |  1|   a|
// |  1|   b|
// |  1|   c|
// |  2|null|
// |  3|   a|
// +---+----+


df.select($""k"", posexplode_outer($""vs"")).show
// +---+----+----+
// |  k| pos| col|
// +---+----+----+
// |  1|   0|   a|
// |  1|   1|   b|
// |  1|   2|   c|
// |  2|null|null|
// |  3|   0|   a|
// +---+----+----+
{code}

Incorrect behavior if used alone:

{code}

df.select(explode_outer($""vs"")).show
// +---+
// |col|
// +---+
// |  a|
// |  b|
// |  c|
// |  a|
// +---+


df.select(posexplode_outer($""vs"")).show
// +---+---+
// |pos|col|
// +---+---+
// |  0|  a|
// |  1|  b|
// |  2|  c|
// |  0|  a|
// +---+---+
{code}",master 814a61a867ded965433c944c90961df529ac83ab,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 29 22:20:03 UTC 2017,,,,,,,,,,"0|i3ea7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/17 22:20;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17810;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Worker should not use the received Master address,SPARK-20529,13067848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,28/Apr/17 23:16,16/May/17 17:38,14/Jul/23 06:30,16/May/17 17:38,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Right now when worker connects to master, master will send its address to the worker. Then worker will save this address and use it to reconnect in case of failure.

However, sometimes, this address is not correct. If there is a proxy between master and worker, the address master sent is not the address of proxy.",,apachespark,codingcat,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20531,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 01 18:02:03 UTC 2017,,,,,,,,,,"0|i3e9mv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/May/17 18:02;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download link in history server UI is not correct,SPARK-20517,13067570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,28/Apr/17 02:14,08/Mar/19 04:45,14/Jul/23 06:30,01/May/17 17:27,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"The download link in history server UI is concatenated with:

{code}
      <td><a href=""{{uiroot}}/api/v1/applications/{{id}}/{{num}}/logs"" class=""btn btn-info btn-mini"">Download</a></td>
{code}

Here {{num}} filed represents number of attempts, this is not equal to REST APIs. In the REST API, if attempt id is not existed, then {{num}} field should be empty, otherwise this {{num}} field should actually be {{attemptId}}.

This will lead to the issue of ""no such app"", rather than correctly download the event log.",,apachespark,flightc,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 08 04:45:22 UTC 2019,,,,,,,,,,"0|i3e7x3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/17 02:38;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17795;;;","08/Mar/19 04:45;flightc;Is this fix going to be merged into any official 2.2 release? Cheers!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Jetty to 9.3.11.v20160721,SPARK-20514,13067530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,mgrover,27/Apr/17 21:31,28/Apr/17 21:07,14/Jul/23 06:30,28/Apr/17 21:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Currently, we are using Jetty version 9.2.16.v20160414.

However, Hadoop 3, uses [9.3.11.v20160721|https://github.com/apache/hadoop/blob/release-3.0.0-alpha2-RC0/hadoop-project/pom.xml#L38] (Jetty upgrade was brought in by HADOOP-10075).

Currently, when you try to build Spark with Hadoop 3, due to this incompatibilities in jetty versions used by Hadoop and Spark, compilation fails with:
{code}
[ERROR] source/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala:31: error: object gzip is not a member of package org.eclipse.jetty.servlets
[ERROR] import org.eclipse.jetty.servlets.gzip.GzipHandler
[ERROR]                                   ^
[ERROR] source/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala:238: error: not found: type GzipHandler
[ERROR]       val gzipHandler = new GzipHandler
[ERROR]                             ^
[ERROR] two errors found
{code}

So, it'd be good to upgrade Jetty to get us closer to working with Hadoop 3.",,apachespark,mgrover,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 21:35:03 UTC 2017,,,,,,,,,,"0|i3e7o7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/17 21:35;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/17790;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaWriter Uses Unanalyzed Logical Plan,SPARK-20496,13067472,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bill_chambers,bill_chambers,bill_chambers,27/Apr/17 18:56,28/Apr/17 17:21,14/Jul/23 06:30,28/Apr/17 17:21,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"Right now we use the unanalyzed logical plan for writing to Kafka, we should use the analyzed plan.

https://github.com/apache/spark/blob/master/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWriter.scala#L50",,apachespark,bill_chambers,brkyvz,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 28 17:21:24 UTC 2017,,,,,,,,,,"0|i3e7bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/17 18:57;apachespark;User 'anabranch' has created a pull request for this issue:
https://github.com/apache/spark/pull/17787;;;","28/Apr/17 01:31;apachespark;User 'anabranch' has created a pull request for this issue:
https://github.com/apache/spark/pull/17792;;;","28/Apr/17 14:51;apachespark;User 'anabranch' has created a pull request for this issue:
https://github.com/apache/spark/pull/17804;;;","28/Apr/17 14:54;bill_chambers;This should probably be backported too.;;;","28/Apr/17 17:21;brkyvz;Resolved with https://github.com/apache/spark/pull/17804;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos Coarse mode may starve other Mesos frameworks if max cores is not a multiple of executor cores,SPARK-20483,13067192,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dgshep,dgshep,dgshep,26/Apr/17 22:44,11/May/17 20:03,14/Jul/23 06:30,27/Apr/17 18:06,2.0.2,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Mesos,,,,,0,,,,,,,,,"if {{spark.cores.max = 10}} for example and {{spark.executor.cores = 4}}, 2 executors will get launched thus {{totalCoresAcquired = 8}}. All future Mesos offers will not get tasks launched because {{sc.conf.getInt(""spark.executor.cores"", ...) + totalCoresAcquired <= maxCores}} will always evaluate to false.  However, in {{handleMatchedOffers}} we check if {{totalCoresAcquired >= maxCores}} to determine if we should decline the offer ""for a configurable amount of time to avoid starving other frameworks"", and this will always evaluate to false in the above scenario. This leaves the framework in a state of limbo where it will never launch any new executors, but only decline offers for the Mesos default of 5 seconds, thus starving other frameworks of offers.",,apachespark,dbtsai,dgshep,gengmao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19702,SPARK-12554,MESOS-6112,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 20:12:04 UTC 2017,,,,,,,,,,"0|i3e5l3:",9223372036854775807,,,,,dbtsai,,,,,,,,,,,,,,,,,,,"27/Apr/17 17:07;apachespark;User 'dgshep' has created a pull request for this issue:
https://github.com/apache/spark/pull/17786;;;","27/Apr/17 18:06;dbtsai;Issue resolved by pull request 17786
[https://github.com/apache/spark/pull/17786];;;","27/Apr/17 20:12;apachespark;User 'dgshep' has created a pull request for this issue:
https://github.com/apache/spark/pull/17788;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolving Casts is too strict on having time zone set,SPARK-20482,13067190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,26/Apr/17 22:31,11/May/17 20:03,14/Jul/23 06:30,27/Apr/17 19:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20329,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 23:11:03 UTC 2017,,,,,,,,,,"0|i3e5kn:",9223372036854775807,,,,,hvanhovell,,,,,,,,2.2.0,,,,,,,,,,,"26/Apr/17 23:11;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/17777;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception between ""create table as"" and ""get_json_object""",SPARK-20476,13067081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,cenyuhai,cenyuhai,26/Apr/17 15:14,11/May/17 20:03,14/Jul/23 06:30,28/Apr/17 06:18,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"I encounter this problem when I want to create a table as select xxxx, get_json_object from xxx;
It is wrong.
{code}
create table spark_json_object as
select get_json_object(deliver_geojson,'$.xxxx')
from dw.dw_prd_order where dt='2017-04-24' limit 10;
{code}
It is ok.
{code}
create table spark_json_object as
select *
from dw.dw_prd_order where dt='2017-04-24' limit 10;
{code}
It is ok
{code}
select get_json_object(deliver_geojson,'$.xxxx')
from dw.dw_prd_order where dt='2017-04-24' limit 10;
{code}

{code}
17/04/26 23:12:56 ERROR [hive.log(397) -- main]: error in initSerDe: org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe: columns has 2 elements while columns.types has 1 elements!
org.apache.hadoop.hive.serde2.SerDeException: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe: columns has 2 elements while columns.types has 1 elements!
        at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.extractColumnInfo(LazySerDeParameters.java:146)
        at org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.<init>(LazySerDeParameters.java:85)
        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.initialize(LazySimpleSerDe.java:125)
        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:53)
        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:521)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:391)
        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:276)
        at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:197)
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:699)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:455)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:455)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:455)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:309)
        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:256)
        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:255)
        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:298)
        at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:454)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:237)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:199)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:199)
        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
        at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:199)
        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:248)
        at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.metastoreRelation$lzycompute$1(CreateHiveTableAsSelectCommand.scala:72)
        at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.metastoreRelation$1(CreateHiveTableAsSelectCommand.scala:48)
        at org.apache.spark.sql.hive.execution.CreateHiveTableAsSelectCommand.run(CreateHiveTableAsSelectCommand.scala:91)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:179)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:593)
        at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:63)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:742)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:186)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:211)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}


",,apachespark,cenyuhai,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 06:29:03 UTC 2017,,,,,,,,,,"0|i3e4wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 16:32;smilegator;This sounds a bug to me. Let me double check it;;;","26/Apr/17 17:14;smilegator;You can bypass it by 
{noformat}
create table spark_json_object as
select get_json_object(deliver_geojson,'$.xxxx') as col1
from dw.dw_prd_order where dt='2017-04-24' limit 10;
{noformat}

Will fix it later.;;;","26/Apr/17 19:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17776;;;","27/Apr/17 06:29;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17781;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OnHeapColumnVector realocation may not copy existing data,SPARK-20474,13067008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michal@databricks.com,michal.db,michal.db,26/Apr/17 10:25,26/Apr/17 19:47,14/Jul/23 06:30,26/Apr/17 19:47,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"OnHeapColumnVector reallocation copies to the new storage data up to 'elementsAppended'. This variable is only updated when using the ColumnVector.appendX API, while ColumnVector.putX is more commonly used.",,apachespark,michal.db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 10:59:03 UTC 2017,,,,,,,,,,"0|i3e4g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 10:59;apachespark;User 'michal-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/17773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnVector.Array is missing accessors for some types,SPARK-20473,13067006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michal@databricks.com,michal.db,michal.db,26/Apr/17 10:17,26/Apr/17 18:21,14/Jul/23 06:30,26/Apr/17 18:21,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"ColumnVector implementations originally did not support some Catalyst types (float, short, and boolean). Now that they do, those types should be also added to the ColumnVector.Array.",,apachespark,michal.db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 10:58:02 UTC 2017,,,,,,,,,,"0|i3e4fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 10:58;apachespark;User 'michal-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/17772;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove AggregateBenchmark testsuite warning: Two level hashmap is disabled but vectorized hashmap is enabled.,SPARK-20471,13067000,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,heary-cao,heary-cao,heary-cao,26/Apr/17 09:33,11/May/17 20:03,14/Jul/23 06:30,28/Apr/17 21:49,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Tests,,,,,0,,,,,,,,,"remove  AggregateBenchmark testsuite warning:
such as '14:26:33.220 WARN org.apache.spark.sql.execution.aggregate.HashAggregateExec: Two level hashmap is disabled but vectorized hashmap is enabled.'

unit tests: AggregateBenchmark 
Modify the 'ignore function for 'test funtion",,apachespark,heary-cao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 09:35:02 UTC 2017,,,,,,,,,,"0|i3e4ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 09:35;apachespark;User 'heary-cao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17771;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HadoopRDD#addLocalConfiguration throws NPE,SPARK-20466,13066960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stakiar,kellyzly,kellyzly,26/Apr/17 07:13,17/May/20 18:13,14/Jul/23 06:30,03/Oct/17 23:57,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.3,2.2.1,2.3.0,,,Spark Core,YARN,,,,1,,,,,,,,,"in spark2.0.2, it throws NPE
{code}
  17/04/23 08:19:55 ERROR executor.Executor: Exception in task 439.0 in stage 16.0 (TID 986)$ 
java.lang.NullPointerException$
^Iat org.apache.spark.rdd.HadoopRDD$.addLocalConfiguration(HadoopRDD.scala:373)$
^Iat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:243)$
^Iat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)$
^Iat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)$
^Iat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)$
^Iat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)$
^Iat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)$
^Iat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)$
^Iat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)$
^Iat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)$
^Iat org.apache.spark.scheduler.Task.run(Task.scala:86)$
^Iat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)$
^Iat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)$
^Iat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)$
^Iat java.lang.Thread.run(Thread.java:745)$
{code}

suggestion to add some code to avoid NPE

{code} 

   /** Add Hadoop configuration specific to a single partition and attempt. */
  def addLocalConfiguration(jobTrackerId: String, jobId: Int, splitId: Int, attemptId: Int,
                            conf: JobConf) {
    val jobID = new JobID(jobTrackerId, jobId)
    val taId = new TaskAttemptID(new TaskID(jobID, TaskType.MAP, splitId), attemptId)
    if ( conf != null){
    conf.set(""mapred.tip.id"", taId.getTaskID.toString)
    conf.set(""mapred.task.id"", taId.toString)
    conf.setBoolean(""mapred.task.is.map"", true)
    conf.setInt(""mapred.task.partition"", splitId)
    conf.set(""mapred.job.id"", jobID.toString)
   }
  }


{code}",,apachespark,KaiXu,kellyzly,stakiar,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 07:15;kellyzly;NPE_log;https://issues.apache.org/jira/secure/attachment/12865082/NPE_log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 02 21:04:03 UTC 2017,,,,,,,,,,"0|i3e45j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/17 07:15;kellyzly;NPE_log describes the detailed info.;;;","21/Jun/17 04:45;yumwang;[~kellyzly] How to reproduce it?;;;","21/Jun/17 04:53;kellyzly;[~q79969786]: found the NPE when runing Hive on Spark on TPCx-BB. But i did not remember to run which query to find this exception. But it is better to add a judge( if conf != null ) in [code|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L374];;;","21/Jun/17 10:57;srowen;Hm, I think the question is how the JobConf is ever null here. I think adding a null check here would only be a band-aid, or at least, something that would need to be taken care of consistently across many more classes.;;;","18/Sep/17 21:32;stakiar;I just hit this issue in Hive-on-Spark when running some TPC-DS queries. It seems to be intermittent, re-tries of the task succeed. I have a very similar stack trace:

{code}
java.lang.NullPointerException
        at org.apache.spark.rdd.HadoopRDD$.addLocalConfiguration(HadoopRDD.scala:364)
        at org.apache.spark.rdd.HadoopRDD$$anon$1.&lt;init&gt;(HadoopRDD.scala:238)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}

The {{JobConf}} object can be {{null}} if {{HadoopRDD#getJobConf}} returns {{null}}. Looks like there is a race condition in {{#getJobConf}} [here|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L160]. The method {{HadoopRDD.containsCachedMetadata}} looks into an internal metadata cache - {{SparkEnv#hadoopJobMetadata}}. This cache uses soft references, so the JVM may reclaim entries from the map whenever there is some GC pressure. In which case, any get request on the key will return a {{null}}. The race condition is that the {{#getJobConf}} method first checks if the cache contains the key, and then retrieves it. In between the {{containsKey}} and {{get}} its possible the the key is GCed by the JVM. This would cause {{#getJobConf}} to return {{null}}.

The fix should be pretty simple, don't use the {{containsKey(key)}} method on the cache, just run a {{get(key)}} and check if it returns {{null}} or not.

Happy to create a PR if other agrees with my analysis.;;;","19/Sep/17 02:17;kellyzly;[~stakiar]:  
this exception happened on which query of tpcds?  I found in another benchmark test(TPCx-BB)
{quote}
This cache uses soft references, so the JVM may reclaim entries from the map whenever there is some GC pressure. In which case, any get request on the key will return a null. The race condition is that the #getJobConf method first checks if the cache contains the key, and then retrieves. In between the containsKey and get its possible the the key is GCed by the JVM. 
{quote}
this exception is because {{HadoopRDD.containsCachedMetadata(jobConfCacheKey)}} returns soft reference and it will return {{null}} when GC happens? If it changes to 
{code}
 else if ( HadoopRDD.getCachedMetadata(jobConfCacheKey) != null) {
        logDebug(""Re-using cached JobConf"")
        HadoopRDD.getCachedMetadata(jobConfCacheKey).asInstanceOf[JobConf]
      }
{code}
 HadoopRDD.getCachedMetadata(jobConfCacheKey) will not return null if GC happens?



;;;","20/Sep/17 04:51;stakiar;I can't remember exactly which query, I was running a chain of about 10 TPC-DS queries, all in the same HoS session. It was a 1 TB Parquet dataset though.

The exception is because {{HadoopRDD.containsCachedMetadata}} can return {{true}}, but then a future call to {{HadoopRDD.getCachedMetadata}} on the same key, can return {{null}}. This can happen if the JVM decided to GC some of the entires in the metadata cache (which it can since [soft references|https://docs.oracle.com/javase/7/docs/api/java/lang/ref/SoftReference.html] are used).

We would have to change it to something like:

{code}
} else {
    Object conf = HadoopRDD.getCachedMetadata(jobConfCacheKey)
    if (conf != null) {
        logDebug(""Re-using cached JobConf"")
        HadoopRDD.getCachedMetadata(jobConfCacheKey).asInstanceOf[JobConf]
    }
}
{code}

I'm not sure how to write it exactly in Scala, but something like that. Once you create {{Object conf}} and point it to the result of {{HadoopRDD.getCachedMetdata(jobConfCacheKey)}}, you then have a hard reference to the object and it can no longer be GCd.;;;","25/Sep/17 18:08;stakiar;[~srowen], [~vanzin] does my analysis of this bug make sense? If you agree this is a bug, I can make a PR to fix it.;;;","25/Sep/17 18:19;vanzin;Explanation makes sense, but your proposed solution is also race-prone (two calls to {{getCachedMetadata}}). You want something like:

{code}
Option(HadoopRDD.getCachedMetadata(jobConfCacheKey)).getOrElse( /* recover from when there's no cached metadata */
{code}
;;;","25/Sep/17 18:37;stakiar;[~vanzin] thanks for taking a look. Yes, you are right. I'll start working on a PR.;;;","02/Oct/17 21:04;apachespark;User 'sahilTakiar' has created a pull request for this issue:
https://github.com/apache/spark/pull/19413;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a job group and an informative description for streaming queries,SPARK-20464,13066867,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kunalkhamar,kunalkhamar,kunalkhamar,25/Apr/17 22:22,01/May/17 18:38,14/Jul/23 06:30,01/May/17 18:38,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,,,apachespark,kunalkhamar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 25 22:27:03 UTC 2017,,,,,,,,,,"0|i3e3kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/17 22:27;apachespark;User 'kunalkhamar' has created a pull request for this issue:
https://github.com/apache/spark/pull/17765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CachedKafkaConsumer may hang forever when it's interrupted,SPARK-20461,13066777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zsxwing,zsxwing,25/Apr/17 18:19,27/Apr/17 20:55,14/Jul/23 06:30,27/Apr/17 20:55,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,CachedKafkaConsumer may hang forever when it's interrupted because of KAFKA-1894,,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1894,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 20:55:38 UTC 2017,,,,,,,,,,"0|i3e30v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/17 18:24;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17761;;;","27/Apr/17 20:55;tdas;Issue resolved by pull request 17761
[https://github.com/apache/spark/pull/17761];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcUtils throws IllegalStateException: Cause already initialized after getting SQLException,SPARK-20459,13066718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,jyu0,jyu0,25/Apr/17 15:14,11/May/17 20:02,14/Jul/23 06:30,02/May/17 00:02,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Testing some failure scenarios, and JdbcUtils throws an IllegalStateException instead of the expected SQLException:
{code}
scala> org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils.saveTable(prodtbl,url3,""DB2.D_ITEM_INFO"",prop1) 
17/04/03 17:19:35 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)    
java.lang.IllegalStateException: Cause already initialized                      
.at java.lang.Throwable.setCause(Throwable.java:365)                            
.at java.lang.Throwable.initCause(Throwable.java:341)                           
.at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:241)
.at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:300)
.at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:299)
.at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
.at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
.at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899) 
.at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)  
.at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)          
.at org.apache.spark.scheduler.Task.run(Task.scala:86)                          
.at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) 
.at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153
.at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628
.at java.lang.Thread.run(Thread.java:785)                                       
{code}

The code in JdbcUtils.savePartition has 
{code}
    } catch {
      case e: SQLException =>
        val cause = e.getNextException
        if (cause != null && e.getCause != cause) {
          if (e.getCause == null) {
            e.initCause(cause)
          } else {
            e.addSuppressed(cause)
          }
        }
{code}

According to Throwable Java doc, {{initCause()}} throws an {{IllegalStateException}} ""if this throwable was created with Throwable(Throwable) or Throwable(String,Throwable), or this method has already been called on this throwable"". The code does check whether {{cause}} is {{null}} before initializing it. However, {{getCause()}} ""returns the cause of this throwable or null if the cause is nonexistent or unknown."" In other words, {{null}} is returned if {{cause}} already exists (which would result in {{IllegalStateException}}) but is unknown. ",,apachespark,jyu0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 28 08:17:03 UTC 2017,,,,,,,,,,"0|i3e2nr:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"25/Apr/17 15:52;srowen;Ugh, so there's no actual way to detect whether the exception has no cause initialized? I guess it's OK here to catch the IllegalStateException and resort to addSuppressed if it fails. Go ahead.;;;","28/Apr/17 08:17;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17800;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump master branch version to 2.3.0-SNAPSHOT,SPARK-20453,13066510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,24/Apr/17 22:56,25/Apr/17 04:48,14/Jul/23 06:30,25/Apr/17 04:48,2.3.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Build,,,,,0,,,,,,,,,,,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 24 23:05:02 UTC 2017,,,,,,,,,,"0|i3e1dj:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"24/Apr/17 23:05;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancel a batch Kafka query and rerun the same DataFrame may cause ConcurrentModificationException,SPARK-20452,13066503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,24/Apr/17 22:41,27/Apr/17 20:59,14/Jul/23 06:30,27/Apr/17 20:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,Cancel a batch Kafka query and rerun the same DataFrame may cause ConcurrentModificationException because it may launch two tasks sharing the same group id.,,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 20:59:08 UTC 2017,,,,,,,,,,"0|i3e1bz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/17 22:45;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17752;;;","27/Apr/17 20:59;tdas;Issue resolved by pull request 17752
[https://github.com/apache/spark/pull/17752];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filter out nested mapType datatypes from sort order in randomSplit,SPARK-20451,13066484,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sameerag,sameerag,sameerag,24/Apr/17 21:05,25/Apr/17 05:08,14/Jul/23 06:30,25/Apr/17 05:08,2.0.0,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,2.3.0,,SQL,,,,,0,,,,,,,,,"In {{randomSplit}}, It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its constituent partitions each time a split is materialized which could result in overlapping splits.

To prevent this, we explicitly sort each input partition to make the ordering deterministic. Given that MapTypes cannot be sorted they should be explicitly pruned out from the sort order. Additionally, if the resulting sort order is empty, we then materialize the dataset to guarantee determinism.",,apachespark,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 24 21:11:03 UTC 2017,,,,,,,,,,"0|i3e17r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/17 21:11;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/17751;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unexpected first-query schema inference cost with 2.1.1 RC,SPARK-20450,13066456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,24/Apr/17 19:25,24/Apr/17 23:23,14/Jul/23 06:30,24/Apr/17 23:23,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,,,,,SQL,,,,,0,,,,,,,,,"https://issues.apache.org/jira/browse/SPARK-19611 fixes a regression from 2.0 where Spark silently fails to read case-sensitive fields missing a case-sensitive schema in the table properties. The fix is to detect this situation, infer the schema, and write the case-sensitive schema into the metastore.

However this can incur an unexpected performance hit the first time such a problematic table is queried (and there is a high false-positive rate here since most tables don't actually have case-sensitive fields).",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19611,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 24 19:40:23 UTC 2017,,,,,,,,,,"0|i3e11j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/17 19:33;srowen;[~ekhliang]: don't set Blocker.
Is this actually a new issue? or a question of modifying the default config introduced in the last change for 2.1 only?;;;","24/Apr/17 19:36;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/17749;;;","24/Apr/17 19:40;ekhliang;I'm not sure what you mean by new issue, but it's only in the latest 2.1.1 RC afaik. It was not present in the original 2.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Within the same streaming query, one StreamingRelation should only be transformed to one StreamingExecutionRelation",SPARK-20441,13066153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,lwlin,lwlin,23/Apr/17 10:11,25/Dec/18 02:21,14/Jul/23 06:30,03/May/17 15:57,2.1.0,2.1.1,2.1.2,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Within the same streaming query, when one StreamingRelation is referred multiple times -- e.g. df.union(df) -- we should transform it only to one StreamingExecutionRelation, instead of two or more different  StreamingExecutionRelations.",,apachespark,brkyvz,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20432,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 15:57:08 UTC 2017,,,,,,,,,,"0|i3dz6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/17 12:31;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17735;;;","03/May/17 15:57;brkyvz;Resolved with https://github.com/apache/spark/pull/17735;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalog.listTables() depends on all libraries used to create tables,SPARK-20439,13066110,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,22/Apr/17 22:19,26/Apr/17 03:40,14/Jul/23 06:30,24/Apr/17 09:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,2.3.0,,,SQL,,,,,0,,,,,,,,,"spark.catalog.listTables() and getTable

You may get an error on the table serde library:
java.lang.RuntimeException: java.lang.ClassNotFoundException: com.amazon.emr.kinesis.hive.KinesisHiveInputFormat

Or if the database contains any table (e.g., index) with a table type that is not accessible by Spark SQL, it will fail the whole listTable API.
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 25 16:43:03 UTC 2017,,,,,,,,,,"0|i3dywv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"22/Apr/17 22:32;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17730;;;","25/Apr/17 16:43;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17760;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"More thorough redaction of sensitive information from logs/UI, more unit tests",SPARK-20435,13066043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,mgrover,22/Apr/17 00:34,27/Apr/17 05:16,14/Jul/23 06:30,27/Apr/17 00:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"SPARK-18535 and SPARK-19720 were works to redact sensitive information (e.g. hadoop credential provider password, AWS access/secret keys) from event logs + YARN logs + UI and from the console output, respectively.

While some unit tests were added along with these changes - they asserted when a sensitive key was found, that redaction took place for that key. They didn't assert globally that when running a full-fledged Spark app (whether or YARN or locally), that sensitive information was not present in any of the logs or UI. Such a test would also prevent regressions from happening in the future if someone unknowingly adds extra logging that publishes out sensitive information to disk or UI.

Consequently, it was found that in some Java configurations, sensitive information was still being leaked in the event logs under the {{SparkListenerEnvironmentUpdate}} event, like so:
{code}
""sun.java.command"":""org.apache.spark.deploy.SparkSubmit ... --conf spark.executorEnv.HADOOP_CREDSTORE_PASSWORD=secret_password ...
{code}

""secret_password"" should have been redacted.

Moreover, previously redaction logic was only checking if the key matched the secret regex pattern, it'd redact it's value. That worked for most cases. However, in the above case, the key (sun.java.command) doesn't tell much, so the value needs to be searched. So the check needs to be expanded to match against values as well.",,apachespark,mgrover,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 00:09:40 UTC 2017,,,,,,,,,,"0|i3dyhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/17 00:39;vanzin;{noformat}
""sun.java.command"":""org.apache.spark.deploy.SparkSubmit ... --conf spark.executorEnv.HADOOP_CREDSTORE_PASSWORD=
{noformat}

If someone is typing passwords in the process's command line, they have bigger problems than the password showing up in the logs... (a.k.a. ""ps ax"");;;","22/Apr/17 00:41;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/17725;;;","22/Apr/17 04:13;mgrover;{quote}
If someone is typing passwords in the process's command line, they have bigger problems than the password showing up in the logs... (a.k.a. ""ps ax"")
{quote}
Thanks for your comment, Marcelo. Providing passwords that way is supported by Spark, terminal sessions finish, and ps ax works for users with appropriate privileges. Log files, on the other hand, touch disks that may not be encrypted, that may be blindly shared over unencrypted channels (say for debugging), so this is still a good thing to do, in my opinion.;;;","22/Apr/17 17:57;vanzin;bq. Providing passwords that way is supported by Spark

That's not an argument. You can type secrets in any command line and that doesn't make it OK.

bq. and ps ax works for users with appropriate privileges

""ps ax"" works for all users. Try it for yourself.

{noformat}
vanzin@vanzin-t460p:/work/apache/spark-prs$ sudo sleep 100000
{noformat}

{noformat}
vanzin@vanzin-t460p:/tmp$ ps axu | grep sleep
root     32583  0.0  0.0  73264  4524 pts/5    S+   10:55   0:00 sudo sleep 100000
root     32584  0.0  0.0   7296   760 pts/5    S+   10:55   0:00 sleep 100000
vanzin   32586  0.0  0.0  14232  1072 pts/2    S+   10:56   0:00 grep --color=auto sleep
{noformat}

I'm not saying redacting from logs is useless, but I'm saying that a user that is providing secrets in the command line is giving up any security, and redaction won't save him.;;;","24/Apr/17 19:23;mgrover;bq. I'm not saying redacting from logs is useless, but I'm saying that a user that is providing secrets in the command line is giving up any security, and redaction won't save him.
Thanks for the ps ax explanation. I appreciated your input and agree that redacting from logs is not useless.

The way it is there are 2 ways to supply passwords:
1. The user copies over the entire conf (say from /etc/spark/conf to $USER/custom-conf). And, then updates the spark-defaults.conf with the appropriate properties containing the password. And, runs Spark jobs with this custom configuration. The benefit is that without any change in Spark today, they can run the jobs and the password won't be leaked anywhere. However, the disadvantage is it is hard to keep the custom configuration in sync given the lack of an overlay style config today in Spark. Moreover, the password is being written by the user to possibly unencrypted disk in the custom configuration.
2. Supply the password over command line to spark-submit. The advantage is that there's no custom configuration to be maintained, there's no password being persisted to a file by the user. However, during the duration of the job, the password is visible through output of commands like 'ps ax' and with the current version of Spark, the password shows up in HDFS, in the event logs and anything derived from them. And, the latter may not be secure. This change is to make this case less worse by redacting passwords from HDFS event logs. Furthermore, as a benefit, we get to add some unit tests that make sure none of the redaction functionality regresses in the future.

I think both the above methods have their pros and cons and I think it's best for us to document both ways and let the users choose which method they prefer. This change makes #2 slight less worse and I think it's worth doing. 
Your points make sense, but it seems still worth making #2 less worse. And, if you agree, I'd really appreciate your review of the PR. Thanks!;;;","24/Apr/17 21:24;vanzin;bq. The user copies over the entire conf (say from /etc/spark/conf to $USER/custom-conf). And, then updates the spark-defaults.conf with the appropriate properties containing the password.

While that can be automated with a short script, it's unnecessarily awkward. I've seen the idea around of having a user-specific config file appended to the default configuration automatically by Spark, but haven't seen anybody actually implement that. It would solve this problem in a cleaner manner.

bq. Supply the password over command line to spark-submit.

I hope this is not documented as a recommended way of doing this anywhere, because it's just not secure. Worst case, the user should be setting passwords via env variables (as is allowed for S3 credentials, for example, using {{AWS_ACCESS_KEY_ID}} / {{AWS_SECRET_ACCESS_KEY}}).;;;","27/Apr/17 00:09;mgrover;Thanks Marcelo!


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw an NullPointerException in range when wholeStage is off,SPARK-20430,13065874,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,21/Apr/17 13:21,22/Apr/17 16:43,14/Jul/23 06:30,22/Apr/17 16:43,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"I hit an exception below in master;
{code}
sql(""SET spark.sql.codegen.wholeStage=false"")
sql(""SELECT * FROM range(1)"").show

17/04/20 17:11:05 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
        at org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:54)
        at org.apache.spark.sql.execution.RangeExec.numSlices(basicPhysicalOperators.scala:343)
        at org.apache.spark.sql.execution.RangeExec$$anonfun$20.apply(basicPhysicalOperators.scala:506)
        at org.apache.spark.sql.execution.RangeExec$$anonfun$20.apply(basicPhysicalOperators.scala:505)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:320)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
{code}",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 21 13:28:04 UTC 2017,,,,,,,,,,"0|i3dxgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/17 13:28;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with Spark interpreting Oracle datatype NUMBER,SPARK-20427,13065800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,alextornado,alextornado,21/Apr/17 08:04,20/Jun/21 12:16,14/Jul/23 06:30,13/Sep/17 23:34,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"In Oracle exists data type NUMBER. When defining a filed in a table of type NUMBER the field has two components, precision and scale.
For example, NUMBER(p,s) has precision p and scale s. 
Precision can range from 1 to 38.
Scale can range from -84 to 127.
When reading such a filed Spark can create numbers with precision exceeding 38. In our case it has created fields with precision 44,
calculated as sum of the precision (in our case 34 digits) and the scale (10):

""...java.lang.IllegalArgumentException: requirement failed: Decimal precision 44 exceeds max precision 38..."".

The result was, that a data frame was read from a table on one schema but could not be inserted in the identical table on other schema.",,abhijitcaps,alextornado,apachespark,khwunchai,Kyrdan,ogonchar,ORichard,sgejun,smilegator,sobusiak,sunayansaikia,umesh9794@gmail.com,yumwang,yxzhang,zwu.net@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20921,,,,,,,SPARK-22002,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 20 12:02:31 UTC 2021,,,,,,,,,,"0|i3dx07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/17 08:06;srowen;Are you describing an error from Oracle?;;;","21/Apr/17 08:09;alextornado;No, I describe error from Spark.;;;","21/Apr/17 08:10;srowen;OK, please update the description, because you say it's related to an Oracle precision limit. This needs a reproduction, if possible.;;;","21/Apr/17 08:14;alextornado;Here is the full stacktrace of the error we have encountered:
java.lang.IllegalArgumentException: requirement failed: Decimal precision 44 exceeds max precision 38
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:113)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:426)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3$$anonfun$9.apply(JdbcUtils.scala:337)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3$$anonfun$9.apply(JdbcUtils.scala:337)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$nullSafeConvert(JdbcUtils.scala:438)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3.apply(JdbcUtils.scala:337)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3.apply(JdbcUtils.scala:335)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:286)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:268)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:578)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:925)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:925)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745);;;","21/Apr/17 08:40;srowen;What does it have to do with Oracle?
What code produces this decimal?;;;","21/Apr/17 10:20;ogonchar;Hi Sean,
What we've spotted is that by default Oracle's NUMBER type is translated as Decimal(38,10). And if value has no zeroes - Spark still adds those.
Therefore imagine you have number which is 30 digits long, e.g. = 12312321321321312312312312123. When Spark SQL (in Java) reads such value from Oracle's NUMBER column - it perceives it as 12312321321321312312312312123,0000000000 (10 zeroes). Then somehow precision is counted as 30 + 10 = 40, which is higher than specified in Decimal(38,10). Even though zeros shouldn't be counted as precision (see http://stackoverflow.com/questions/35435691/bigdecimal-precision-and-scale).
That is an issue. And the only solution we have found so far is to change Column's type to INTEGER (so that it interprets it as Decimal(38.0)). But unfortunately, we do not have a control over all of tables.

Hope it clarifies;;;","25/Apr/17 18:55;smilegator;cc [~tsuresh] Are you interested in this? ;;;","11/Jun/17 02:27;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/18266;;;","07/Sep/17 11:33;sobusiak;You can have very big and very small numbers at the same time in Oracle's NUMBER if precision and scale is not specified.
Oracle documentation says:
{quote}
The following numbers can be stored in a NUMBER column:
* Positive numbers in the range 1 x 10^-130 to 9.99...9 x 10^125 with up to 38 significant digits
* Negative numbers from -1 x 10^-130 to 9.99...99 x 10^125 with up to 38 significant digits
{quote}

As was already noted before, currently Spark throws an exception for very big numbers (like ""Decimal precision 61 exceeds max precision 38"" for 1E+50).
What was not noted is that it also truncates very small numbers to 0 (like 1E-50).

As far as I understand you cannot fit all these numbers at the same time in {{DecimalType}} whatever precision and scale you set. I believe the default Spark type for Oracle's NUMBER should be {{DoubleType}}.

Last but not least, this issue is not Oracle-specific! I have confirmed that the very same problems occur for NUMERIC of PostgreSQL. BTW, PostgreSQL documentation states explicitly:
{quote}
Specifying NUMERIC without any precision or scale creates a column in which numeric values of any precision and scale can be stored, up to the implementation limit on precision. A column of this kind will not coerce input values to any particular scale, whereas numeric columns with a declared scale will coerce input values to that scale.
{quote}
So basically NUMBER/NUMERIC *without* precision and scale is very different from NUMBER/NUMERIC *with* precision and scale.;;;","07/Sep/17 13:44;sobusiak;As far as Oracle is concerned this is the code to blame:

{code:java}
private case object OracleDialect extends JdbcDialect {

  override def canHandle(url: String): Boolean = url.startsWith(""jdbc:oracle"")

  override def getCatalystType(
      sqlType: Int, typeName: String, size: Int, md: MetadataBuilder): Option[DataType] = {
    if (sqlType == Types.NUMERIC) {
      val scale = if (null != md) md.build().getLong(""scale"") else 0L
      size match {
        // Handle NUMBER fields that have no precision/scale in special way
        // because JDBC ResultSetMetaData converts this to 0 precision and -127 scale
        // For more details, please see
        // https://github.com/apache/spark/pull/8780#issuecomment-145598968
        // and
        // https://github.com/apache/spark/pull/8780#issuecomment-144541760
        case 0 => Option(DecimalType(DecimalType.MAX_PRECISION, 10))
        // Handle FLOAT fields in a special way because JDBC ResultSetMetaData converts
        // this to NUMERIC with -127 scale
        // Not sure if there is a more robust way to identify the field as a float (or other
        // numeric types that do not specify a scale.
        case _ if scale == -127L => Option(DecimalType(DecimalType.MAX_PRECISION, 10))
        case _ => None
      }
    } else {
      None
    }
  }
{code}
;;;","01/Jul/18 07:51;ORichard;I'm still getting the same problem even in newest version!.;;;","01/Jul/18 08:25;yumwang;[~ORichard]. Please try to use {{customSchema}} to specifying the custom data types of the read schema.  
https://github.com/apache/spark/blob/v2.3.1/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala#L197


;;;","01/Oct/19 15:48;zwu.net@gmail.com;Some one asked me this problem months ago and I found a solution for him , but I forgot the solution when another one in my team asked me again yesterday. I had to spend several hours on this since her query was quite complex.  For a record and my own reference, I would like to put the solution  here (inspired by [~sobusiak]  and [~yumwang] ):  Add the customSchema option after the read() that specifies all potential trouble makers as Double types.  It can probably resolve most cases in real applications.  Surely, this is supposed that one does not particularly concern about the exact significant digits in his/her applications.
{code:java}
.read()
.option(""customSchema"", ""col1 Double, col2 Double"") //where col1, col2... are columns that could cause the trouble.
    
{code}
 Also, some may think their issues come from the .write() operation, but the issues are in fact from the .read() operation. The col1, col2...column names are not necessarily from the original tables. They could be the calculated fields for output in the queries.   One could mistakenly bark at a wrong place to try to fix the issues. 

 ;;;","08/Mar/20 18:04;sunayansaikia;[~yumwang] 
Seems this fix broke the way we could get the column name with the _'name'_  key via the MetadataBuiler map inside getCatalystType()
 Is there a way I could get the column name now while I'm overriding the getCatalystType() method?

Please check the Java code below for which things broke.
 public Option<DataType> getCatalystType(int sqlJdbcType, String typeName, int size, MetadataBuilder md) {
 String columnName = String.valueOf(md.getMap().get(""name"").get());;;;","06/Apr/20 08:06;sunayansaikia;[~yumwang]: Hi - sorry to bug again but any solution to the things I asked above? I'm kind of blocked for one of my use cases.;;;","14/May/20 14:49;Kyrdan;Hey guys, 
 I encountered an issue related to precision issues.

Now the code expects the Decimal type we need to have in JDBC metadata precision and scale. 

[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala#L402-L414]

 

I found out that in the OracleDB it is valid to have Decimal without these data. When I do a query read metadata for such column I'm getting DATA_PRECISION = Null, and DATA_SCALE = Null.

Then when I run the `spark-sql` I'm getting such error:
{code:java}
java.lang.IllegalArgumentException: requirement failed: Decimal precision 45 exceeds max precision 38
        at scala.Predef$.require(Predef.scala:224)
        at org.apache.spark.sql.types.Decimal.set(Decimal.scala:114)
        at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:465)
        at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3$$anonfun$12.apply(JdbcUtils.scala:407)
{code}
Do you have a work around how spark-sql can work with such cases?

 

UPDATE:

Solved with the custom scheme.;;;","23/Jul/20 07:38;abhijitcaps;[~Kyrdan]

Can you explain your implementation ?

Have you used custom JdbcDialect  or OracleDialect ?;;;","20/Jun/21 12:02;sgejun;When you use custom scheme, problem solved according to my test. 

But spark's max precision 38 seems still have some gap with oracle. For oracle, I believe there could be 39-40 for 10g version.

From oracle document.

_p is the precision, or the maximum number of significant decimal digits, where the most significant digit is the left-most nonzero digit, and the least significant digit is the right-most known digit. Oracle guarantees the portability of numbers with precision of up to 20 base-100 digits, which is equivalent to 39 or 40 decimal digits depending on the position of the decimal point._;;;",,,,,,,,,,,,,,,,,,,,,
NullPointerException in places expecting non-optional partitionSpec.,SPARK-20412,13065565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,20/Apr/17 15:02,21/Apr/17 14:23,14/Jul/23 06:30,21/Apr/17 14:23,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"A number of commands expect a partition specification without empty values, e.g. {{SHOW PARTITIONS}}.
But then running {{SHOW PARTITIONS tbl (colStr='foo', colInt)}} throws it in an unfriendly way:
{code}
java.lang.NullPointerException
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireNonEmptyValueInPartitionSpec$1$$anonfun$apply$1.apply(SessionCatalog.scala:927)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireNonEmptyValueInPartitionSpec$1$$anonfun$apply$1.apply(SessionCatalog.scala:927)
  at scala.collection.Iterator$class.exists(Iterator.scala:919)
  at scala.collection.AbstractIterator.exists(Iterator.scala:1336)
  at scala.collection.IterableLike$class.exists(IterableLike.scala:77)
  at scala.collection.AbstractIterable.exists(Iterable.scala:54)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireNonEmptyValueInPartitionSpec$1.apply(SessionCatalog.scala:927)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireNonEmptyValueInPartitionSpec$1.apply(SessionCatalog.scala:926)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireNonEmptyValueInPartitionSpec(SessionCatalog.scala:926)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$listPartitionNames$1.apply(SessionCatalog.scala:882)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog$$anonfun$listPartitionNames$1.apply(SessionCatalog.scala:880)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionNames(SessionCatalog.scala:880)
  at org.apache.spark.sql.execution.command.ShowPartitionsCommand.run(tables.scala:817)
{code}

where {{requireNonEmptyValueInPartitionSpec}} does not expect a NULL there.

It seems that {{visitNonOptionalPartitionSpec}} could throw {{ParseException}} instead of putting in {{null}}, but I'm not sure if there are any implications for other commands using non-optional partitionSpec.",,apachespark,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 15:26:03 UTC 2017,,,,,,,,,,"0|i3dvjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/17 15:26;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/17707;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetQuerySuite 'Enabling/disabling ignoreCorruptFiles' flaky test,SPARK-20407,13065472,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,20/Apr/17 10:21,23/Apr/17 05:11,14/Jul/23 06:30,20/Apr/17 16:51,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Tests,,,,,0,,,,,,,,,"ParquetQuerySuite test ""Enabling/disabling ignoreCorruptFiles"" can sometimes fail. This is caused by the fact that when one task fails, the driver call returns and test code continues, but there might still be tasks running that will be killed at the next killing point.

There are 2 specific issues created by this:
1. Files can be closed some time after the test finishes, so DebugFilesystem.assertNoOpenStreams fails. One solution for this is to change SharedSqlContext and call assertNoOpenStreams inside eventually {}

2. ParquetFileReader constructor from apache parquet 1.8.2 can leak a stream at line 538. This happens when the next line throws an exception. So, the constructor fails and Spark doesn't have any way to close the file.
This happens in this test because the test deletes the temporary directory at the end (but while tasks might still be running). Deleting the directory causes the constructor to fail.
The solution for this could be to Thread.sleep at the end of the test or to somehow wait for all tasks to be definitely killed before finishing the test",,apachespark,bograd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 21 14:22:03 UTC 2017,,,,,,,,,,"0|i3duzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/17 11:06;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17701;;;","21/Apr/17 14:21;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17718;;;","21/Apr/17 14:22;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17720;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.withNewExecutionId should be private,SPARK-20405,13065429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,20/Apr/17 07:35,20/Apr/17 12:31,14/Jul/23 06:30,20/Apr/17 12:31,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Dataset.withNewExecutionId is only used in Dataset itself and should be private.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 07:40:03 UTC 2017,,,,,,,,,,"0|i3dupr:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"20/Apr/17 07:40;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17699;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"It is wrong to the instructions of some functions,such as  boolean，tinyint，smallint，int，bigint，float，double，decimal，date，timestamp，binary，string",SPARK-20403,13065421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,10110346,10110346,10110346,20/Apr/17 07:13,25/May/17 00:33,14/Jul/23 06:30,25/May/17 00:33,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Documentation,SQL,,,,0,,,,,,,,,"spark-sql>desc function boolean;
Function: boolean
Class: org.apache.spark.sql.catalyst.expressions.Cast
Usage: boolean(expr AS type) - Casts the value `expr` to the target data type `type`.

spark-sql>desc function int;
Function: int
Class: org.apache.spark.sql.catalyst.expressions.Cast
Usage: int(expr AS type) - Casts the value `expr` to the target data type `type`.",,10110346,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 07:24:03 UTC 2017,,,,,,,,,,"0|i3dunz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/17 07:24;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/17698;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't use same regex pattern between 1.6 and 2.x due to unescaped sql string in parser,SPARK-20399,13065393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,20/Apr/17 04:22,21/May/17 12:23,14/Jul/23 06:30,12/May/17 03:16,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The new SQL parser is introduced into Spark 2.0. Seems it bring an issue regarding the regex pattern string.

The following codes can reproduce it:
{code}
val data = Seq(""\u0020\u0021\u0023"", ""abc"")
val df = data.toDF()

// 1st usage: works in 1.6
// Let parser parse pattern string
val rlike1 = df.filter(""value rlike '^\\x20[\\x20-\\x23]+$'"")
// 2nd usage: works in 1.6, 2.x
// Call Column.rlike so the pattern string is a literal which doesn't go through parser
val rlike2 = df.filter($""value"".rlike(""^\\x20[\\x20-\\x23]+$""))

// In 2.x, we need add backslashes to make regex pattern parsed correctly
val rlike3 = df.filter(""value rlike '^\\\\x20[\\\\x20-\\\\x23]+$'"")
{code}

Due to unescaping SQL String in parser, the first usage working in 1.6 can't work in 2.0. To make it work, we need to add additional backslashes.

It is quite weird that we can't use the same regex pattern string in the 2 usages. I think we should not unescape regex pattern string.
",,apachespark,dbtsai,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 21 12:23:03 UTC 2017,,,,,,,,,,"0|i3duhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/17 04:23;viirya;I already have the fix for this.

I am not sure if this is a regression we want to fix so I've not submitted the PR yet.

cc [~dbtsai] and [~hvanhovell];;;","20/Apr/17 06:50;dbtsai;Thanks [~viirya]. We ran into this bugs while migrating from Spark 1.6 to Spark 2.0, and this breaks all our production jobs. I'll argue that the behavior should not be changed in the unescaped sql string in parser. Thanks.;;;","24/Apr/17 00:53;viirya;[~hvanhovell] Do you think this is a regression we should fix?;;;","24/Apr/17 03:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17736;;;","07/May/17 05:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17887;;;","21/May/17 12:23;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18048;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range() operator should include cancellation reason when killed,SPARK-20398,13065337,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,19/Apr/17 23:08,20/Apr/17 02:54,14/Jul/23 06:30,20/Apr/17 02:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"https://issues.apache.org/jira/browse/SPARK-19820 adds a reason field for why tasks were killed. However, for backwards compatibility it left the old TaskKilledException constructor which defaults to ""unknown reason"".

The range() operator should use the constructor that fills in the reason rather than dropping it on task kill.",,apachespark,ekhliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 23:09:02 UTC 2017,,,,,,,,,,"0|i3du5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/17 23:09;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/17692;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strengthen Spark to prevent XSS vulnerabilities,SPARK-20393,13065113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nmarion,nmarion,nmarion,19/Apr/17 14:28,19/Oct/17 17:59,14/Jul/23 06:30,10/May/17 10:55,1.5.2,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Web UI,,,,,0,security,,,,,,,,"Using IBM Security AppScan Standard, we discovered several easy to recreate MHTML cross site scripting vulnerabilities in the Apache Spark Web GUI application and these vulnerabilities were found to exist in Spark version 1.5.2 and 2.0.2, the two levels we initially tested. Cross-site scripting attack is not really an attack on the Spark server as much as an attack on the end user, taking advantage of their trust in the Spark server to get them to click on a URL like the ones in the examples below.  So whether the user could or could not change lots of stuff on the Spark server is not the key point.  It is an attack on the user themselves.  If they click the link the script could run in their browser and comprise their device.  Once the browser is compromised it could submit Spark requests but it also might not.

https://blogs.technet.microsoft.com/srd/2011/01/28/more-information-about-the-mhtml-script-injection-vulnerability/

{quote}
Request: GET /app/?appId=Content-Type:%20multipart/related;%20boundary=_AppScan%0d%0a--
_AppScan%0d%0aContent-Location:foo%0d%0aContent-Transfer-
Encoding:base64%0d%0a%0d%0aPGh0bWw%2bPHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw%2b%0d%0a
HTTP/1.1

Excerpt from response: <div class=""row-fluid"">No running application with ID Content-Type: multipart/related;
boundary=_AppScan
--_AppScan
Content-Location:foo
Content-Transfer-Encoding:base64
PGh0bWw+PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw+
</div>

Result: In the above payload the BASE64 data decodes as:
<html><script>alert(""XSS"")</script></html>


Request: GET /history/app-20161012202114-0038/stages/stage?id=1&attempt=0&task.sort=Content-
Type:%20multipart/related;%20boundary=_AppScan%0d%0a--_AppScan%0d%0aContent-
Location:foo%0d%0aContent-Transfer-
Encoding:base64%0d%0a%0d%0aPGh0bWw%2bPHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw%2b%0d%0a&tas
k.pageSize=100 HTTP/1.1

Excerpt from response: Content-Type: multipart/related;
boundary=_AppScan
--_AppScan
Content-Location:foo
Content-Transfer-Encoding:base64
PGh0bWw+PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw+

Result: In the above payload the BASE64 data decodes as:
<html><script>alert(""XSS"")</script></html>


Request: GET /log?appId=app-20170113131903-0000&executorId=0&logType=Content-
Type:%20multipart/related;%20boundary=_AppScan%0d%0a--_AppScan%0d%0aContent-
Location:foo%0d%0aContent-Transfer-
Encoding:base64%0d%0a%0d%0aPGh0bWw%2bPHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw%2b%0d%0a&byt
eLength=0 HTTP/1.1

Excerpt from response: ==== Bytes 0-0 of 0 of /u/nmarion/Spark_2.0.2.0/Spark-DK/work/app-20170113131903-0000/0/Content-
Type: multipart/related; boundary=_AppScan
--_AppScan
Content-Location:foo
Content-Transfer-Encoding:base64
PGh0bWw+PHNjcmlwdD5hbGVydCgiWFNTIik8L3NjcmlwdD48L2h0bWw+

Result: In the above payload the BASE64 data decodes as:
<html><script>alert(""XSS"")</script></html>
{quote}

security@apache was notified and recommended a PR.",,ajbozarth,apachespark,aroberts,jyu0,nmarion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 19 17:59:04 UTC 2017,,,,,,,,,,"0|i3dtcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/17 14:34;apachespark;User 'n-marion' has created a pull request for this issue:
https://github.com/apache/spark/pull/17686;;;","10/May/17 10:55;srowen;Issue resolved by pull request 17686
[https://github.com/apache/spark/pull/17686];;;","18/Oct/17 19:50;apachespark;User 'ambauma' has created a pull request for this issue:
https://github.com/apache/spark/pull/19528;;;","19/Oct/17 17:59;apachespark;User 'ambauma' has created a pull request for this issue:
https://github.com/apache/spark/pull/19538;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The log info ""Added %s in memory on %s (size: %s, free: %s)""  in function ""org.apache.spark.storage.BlockManagerInfo.updateBlockInfo"" is not accurate if the block exists on the slave already",SPARK-20386,13065021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,eaton,eaton,eaton,19/Apr/17 08:41,22/Apr/17 11:54,14/Jul/23 06:30,22/Apr/17 11:30,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"The log info""Added %s in memory on %s (size: %s, free: %s)""  in function ""org.apache.spark.storage.BlockManagerInfo.updateBlockInfo"" is not accurate if the block exists on the slave already;
the current code is:
if (storageLevel.useMemory) {
        blockStatus = BlockStatus(storageLevel, memSize = memSize, diskSize = 0)
        _blocks.put(blockId, blockStatus)
        _remainingMem -= memSize
        logInfo(""Added %s in memory on %s (size: %s, free: %s)"".format(
          blockId, blockManagerId.hostPort, Utils.bytesToString(memSize),
          Utils.bytesToString(_remainingMem)))
      }

If  the block exists on the slave already, the added memory should be memSize - originalMemSize, the originalMemSize is _blocks.get(blockId).memSize",,apachespark,eaton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 22 11:30:00 UTC 2017,,,,,,,,,,"0|i3dsrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/17 10:30;apachespark;User 'eatoncys' has created a pull request for this issue:
https://github.com/apache/spark/pull/17683;;;","22/Apr/17 11:30;srowen;Issue resolved by pull request 17683
[https://github.com/apache/spark/pull/17683];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectHashAggregateExec is missing numOutputRows,SPARK-20381,13064958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,19/Apr/17 02:49,05/May/17 16:54,14/Jul/23 06:30,05/May/17 16:54,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,Add SQL metrics of numOutputRows for ObjectHashAggregateExec.,,apachespark,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 03:07:03 UTC 2017,,,,,,,,,,"0|i3dsef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/17 03:07;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17678;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe table not showing updated table comment after alter operation,SPARK-20380,13064947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,S71955,S71955,S71955,19/Apr/17 01:20,08/May/17 06:18,14/Jul/23 06:30,08/May/17 06:18,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When user alter the table properties and adds/updates table comment. table comment which is now directly part of CatalogTable instance is not getting updated and old table comment was shown

Proposal for solution
To handle this issue while updating the table properties map with newly added/modified properties in CatalogTable
instance also update the comment parameter in CatalogTable with the newly added/modified comment. already a PR is added for this issue

https://github.com/apache/spark/pull/17649",,apachespark,S71955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 01:28:03 UTC 2017,,,,,,,,,,"0|i3dsbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/17 01:28;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/17649;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix JavaStructuredSessionization example,SPARK-20377,13064893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,18/Apr/17 21:54,18/Apr/17 23:06,14/Jul/23 06:30,18/Apr/17 23:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,Extra accessors in java bean class causes incorrect encoder generation. This led to SPARK-20374 which is a duplicate of  SPARK-18598. This JIRA fixes the example by removing extra accessor getDurationMs().,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 18 23:06:00 UTC 2017,,,,,,,,,,"0|i3drzz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"18/Apr/17 22:01;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/17676;;;","18/Apr/17 23:06;tdas;Issue resolved by pull request 17676
[https://github.com/apache/spark/pull/17676];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch queries with 'Dataset/DataFrame.withWatermark()` does not execute,SPARK-20373,13064859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,tdas,tdas,18/Apr/17 19:56,11/May/17 20:00,14/Jul/23 06:30,11/May/17 20:00,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Any Dataset/DataFrame batch query with the operation `withWatermark` does not execute because the batch planner does not have any rule to explicitly handle the EventTimeWatermark logical plan. The right solution is to simply remove the plan node, as the watermark should not affect any batch query in any way.

{code}
from pyspark.sql.functions import *

eventsDF = spark.createDataFrame([(""2016-03-11 09:00:07"", ""dev1"", 123)]).toDF(""eventTime"", ""deviceId"", ""signal"").select(col(""eventTime"").cast(""timestamp"").alias(""eventTime""), ""deviceId"", ""signal"")

windowedCountsDF = \
  eventsDF \
    .withWatermark(""eventTime"", ""10 minutes"") \
    .groupBy(
      ""deviceId"",
      window(""eventTime"", ""5 minutes"")) \
    .count()

windowedCountsDF.collect()
{code}

This throws as an error 
{code}
java.lang.AssertionError: assertion failed: No plan for EventTimeWatermark eventTime#3762657: timestamp, interval 10 minutes
+- Project [cast(_1#3762643 as timestamp) AS eventTime#3762657, _2#3762644 AS deviceId#3762651]
   +- LogicalRDD [_1#3762643, _2#3762644, _3#3762645L]

	at scala.Predef$.assert(Predef.scala:170)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:90)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:90)
{code}",,apachespark,carsonwang,codingcat,Lubo Zhang,lwlin,tdas,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20672,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 08 05:40:03 UTC 2017,,,,,,,,,,"0|i3drsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/May/17 05:40;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark silently escapes partition column names,SPARK-20367,13064729,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,juliuszsompolski,juliuszsompolski,juliuszsompolski,18/Apr/17 14:31,12/Dec/22 18:11,14/Jul/23 06:30,21/Apr/17 01:58,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"CSV files can have arbitrary column names:
{code}
scala> spark.range(1).select(col(""id"").as(""Column?""), col(""id"")).write.option(""header"", true).csv(""/tmp/foo"")
scala> spark.read.option(""header"", true).csv(""/tmp/foo"").schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(Column?,StringType,true), StructField(id,StringType,true))
{code}
However, once a column with characters like ""?"" in the name gets used in a partitioning column, the column name gets silently escaped, and reading the schema information back renders the column name with ""?"" turned into ""%3F"":
{code}
scala> spark.range(1).select(col(""id"").as(""Column?""), col(""id"")).write.partitionBy(""Column?"").option(""header"", true).csv(""/tmp/bar"")
scala> spark.read.option(""header"", true).csv(""/tmp/bar"").schema
res3: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(Column%3F,IntegerType,true))
{code}
The same happens for other formats, but I encountered it working with CSV, since these more often contain ugly schemas... 

Not sure if it's a bug or a feature, but it might be more intuitive to fail queries with invalid characters in the partitioning column name, rather than silently escaping the name?",,apachespark,juliuszsompolski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 11:49:02 UTC 2017,,,,,,,,,,"0|i3dqzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/17 14:55;gurwls223;I guess probably this is not a CSV datasource specific behaviour. Would you be able to test other sources such as text or json?;;;","18/Apr/17 15:03;gurwls223;Actually, I did while trying to reproduce this :)

{code}
scala> spark.range(1).select(col(""id"").as(""Column?""), col(""id"")).write.partitionBy(""Column?"").parquet(""/tmp/bar00"")

scala> spark.read.parquet(""/tmp/bar00"").schema
res16: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,true), StructField(Column%3F,IntegerType,true))
{code}

{code}
scala> spark.range(1).select(col(""id"").as(""Column?""), col(""id"")).write.partitionBy(""Column?"").json(""/tmp/bar0"")

scala> spark.read.json(""/tmp/bar0"").schema
res13: org.apache.spark.sql.types.StructType = StructType(StructField(id,LongType,true), StructField(Column%3F,IntegerType,true))
{code};;;","18/Apr/17 15:04;juliuszsompolski;Hi [~hyukjin.kwon]. I tested also with parquet, and it also happens there.
{quote}
The same happens for other formats, but I encountered it working with CSV, since these more often contain ugly schemas...
{quote};;;","18/Apr/17 15:05;gurwls223;Doh. I rushed reading ...;;;","20/Apr/17 11:49;apachespark;User 'juliuszsompolski' has created a pull request for this issue:
https://github.com/apache/spark/pull/17703;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not so accurate classpath format for AM and Containers,SPARK-20365,13064599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jerryshao,jerryshao,18/Apr/17 07:16,17/May/20 18:14,14/Jul/23 06:30,01/Jun/17 21:42,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"In Spark on YARN, when configuring ""spark.yarn.jars"" with local jars (jars started with ""local"" scheme), we will get inaccurate classpath for AM and containers. This is because we don't remove ""local"" scheme when concatenating classpath. It is OK to run because classpath is separated with "":"" and java treat ""local"" as a separate jar. But we could improve it to remove the scheme.

{code}
java.class.path = /tmp/hadoop-sshao/nm-local-dir/usercache/sshao/appcache/application_1492057593145_0009/container_1492057593145_0009_01_000003:/tmp/hadoop-sshao/nm-local-dir/usercache/sshao/appcache/application_1492057593145_0009/container_1492057593145_0009_01_000003/__spark_conf__:/tmp/hadoop-sshao/nm-local-dir/usercache/sshao/appcache/application_1492057593145_0009/container_1492057593145_0009_01_000003/__spark_libs__/*:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/activation-1.1.1.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/antlr-2.7.7.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/antlr-runtime-3.4.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/antlr4-runtime-4.5.3.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/aopalliance-1.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/aopalliance-repackaged-2.4.0-b34.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/apache-log4j-extras-1.2.17.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/apacheds-i18n-2.0.0-M15.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/apacheds-kerberos-codec-2.0.0-M15.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/api-asn1-api-1.0.0-M20.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/api-util-1.0.0-M20.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/arpack_combined_all-0.1.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/avro-1.7.7.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/avro-ipc-1.7.7-tests.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/avro-ipc-1.7.7.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/avro-mapred-1.7.7-hadoop2.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/base64-2.3.8.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/bcprov-jdk15on-1.51.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/bonecp-0.8.0.RELEASE.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/breeze-macros_2.11-0.12.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/breeze_2.11-0.12.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/calcite-avatica-1.2.0-incubating.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/calcite-core-1.2.0-incubating.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/calcite-linq4j-1.2.0-incubating.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/cglib-2.2.1-v20090111.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/chill-java-0.8.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/chill_2.11-0.8.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-beanutils-1.7.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-beanutils-core-1.8.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-cli-1.2.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-codec-1.10.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-collections-3.2.2.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-compiler-3.0.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-compress-1.4.1.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-configuration-1.6.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-crypto-1.0.0.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-dbcp-1.4.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-digester-1.8.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-httpclient-3.1.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-io-2.4.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-lang-2.6.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-lang3-3.5.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-logging-1.2.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-math3-3.4.1.jar:local:///Users/sshao/projects/apache-spark/assembly/target/scala-2.11/jars/commons-net-3.1.jar
{code}",,apachespark,devaraj,jerryshao,lyc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 05:42:17 UTC 2017,,,,,,,,,,"0|i3dq6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/May/17 08:11;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18129;;;","02/Jun/17 05:42;lyc;Thanks for reviewing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet predicate pushdown on columns with dots return empty results,SPARK-20364,13064588,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,18/Apr/17 06:05,12/Dec/22 18:11,14/Jul/23 06:30,18/May/17 17:53,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, if there are dots in the column name, predicate pushdown seems being failed in Parquet.

**With dots**

{code}
val path = ""/tmp/abcde""
Seq(Some(1), None).toDF(""col.dots"").write.parquet(path)
spark.read.parquet(path).where(""`col.dots` IS NOT NULL"").show()
{code}

{code}
+--------+
|col.dots|
+--------+
+--------+
{code}

**Without dots**

{code}
val path = ""/tmp/abcde2""
Seq(Some(1), None).toDF(""coldots"").write.parquet(path)
spark.read.parquet(path).where(""`coldots` IS NOT NULL"").show()
{code}

{code}
+-------+
|coldots|
+-------+
|      1|
+-------+
{code}

It seems dot in the column names via {{FilterApi}} tries to separate the field name with dot ({{ColumnPath}} with multiple column paths) whereas the actual column name is {{col.dots}}. (See [FilterApi.java#L71 |https://github.com/apache/parquet-mr/blob/apache-parquet-1.8.2/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/FilterApi.java#L71] and it calls [ColumnPath.java#L44|https://github.com/apache/parquet-mr/blob/apache-parquet-1.8.2/parquet-common/src/main/java/org/apache/parquet/hadoop/metadata/ColumnPath.java#L44].

I just tried to come up with ways to resolve it and I came up with two as below:

One is simply to don't push down filters when there are dots in column names so that it reads all and filters in Spark-side.

The other way creates Spark's {{FilterApi}} for those columns (it seems final) to get always use single column path it in Spark-side (this seems hacky) as we are not pushing down nested columns currently. So, it looks we can get a field name via {{ColumnPath.get}} not {{ColumnPath.fromDotString}} in this way.

I just made a rough version of the latter. 

{code}
private[parquet] object ParquetColumns {
  def intColumn(columnPath: String): Column[Integer] with SupportsLtGt = {
    new Column[Integer] (ColumnPath.get(columnPath), classOf[Integer]) with SupportsLtGt
  }

  def longColumn(columnPath: String): Column[java.lang.Long] with SupportsLtGt = {
    new Column[java.lang.Long] (
      ColumnPath.get(columnPath), classOf[java.lang.Long]) with SupportsLtGt
  }

  def floatColumn(columnPath: String): Column[java.lang.Float] with SupportsLtGt = {
    new Column[java.lang.Float] (
      ColumnPath.get(columnPath), classOf[java.lang.Float]) with SupportsLtGt
  }

  def doubleColumn(columnPath: String): Column[java.lang.Double] with SupportsLtGt = {
    new Column[java.lang.Double] (
      ColumnPath.get(columnPath), classOf[java.lang.Double]) with SupportsLtGt
  }

  def booleanColumn(columnPath: String): Column[java.lang.Boolean] with SupportsEqNotEq = {
    new Column[java.lang.Boolean] (
      ColumnPath.get(columnPath), classOf[java.lang.Boolean]) with SupportsEqNotEq
  }

  def binaryColumn(columnPath: String): Column[Binary] with SupportsLtGt = {
    new Column[Binary] (ColumnPath.get(columnPath), classOf[Binary]) with SupportsLtGt
  }
}
{code}

Both sound not the best. Please let me know if anyone has a better idea.
",,aash,apachespark,codingcat,dongjoon,kiszk,myali,robert3005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PARQUET-389,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 16 13:53:03 UTC 2017,,,,,,,,,,"0|i3dq47:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"18/Apr/17 06:08;gurwls223;[~aash], [~robert3005] who found this issue in https://github.com/apache/spark/pull/17667 

and [~lian cheng] who might have a better idea and I think can confirm if the investigation here is correct and decide the way to resolve it.;;;","18/Apr/17 09:36;robert3005;Looks like parquet doesn't differentiate between column named ""a.b"" and field ""b"" in column ""a"". We could always treat them the same but that might lead to some surprises down the road. Ideally parquet would let us construct filter given exact column name or dot separated accessors. I think what [~aash] proposes is roughly how we should fix it with caveat that eventually this should live in parquet imho.;;;","18/Apr/17 23:53;aash;Thanks for the investigation [~hyukjin.kwon]!

The proposal you put up (sorry Rob, wasn't me) looks like a good direction.  I'm not sure it will work as written though, since I'd expect the IntColumn/LongColumn/etc classes from Parquet to be expected in other places, an instanceof for example.  As an alternate suggestion, I'm wondering if we could call the package-protected constructors of the {{IntColumn}} class from Spark-owned class in that package with the result from the {{ColumnPath.get()}} method, rather than its {{.fromDotString()}} method.  Then Spark can use its better knowledge of whether the field is a column-with-dot or a field-in-struct and assemble the ColumnPath correctly.

And I do think that this should eventually live in Parquet.  Maybe there's a ticket already open there for this?

Does that make sense?;;;","19/Apr/17 00:06;gurwls223;Ah, actually, I ran some of tests already with the latter proposal (of course :)) and took a look related code. In terms of whether it works or not, I am positive.

Yes, I think that makes sense in a way but I guess that is an option too as I guess calling protected ones sounds not the best too and rather an workaround similarly. We could adds it in the JIRA as the third option.

Actually, I think both this way and the way I suggested as the latter use Parquet APIs as working around. So, I guess none of both is particually better.

The issue here looks to me about setting dot in the filter API. I wonder if there is no way to set this in the column path via filter API. I will investigate a bit more, try both ways and open a PR after picking up one if no one has an aswer about this for few days.

I actually searched a bit ahead and I failed to find a JIRA in Parquet.

;;;","19/Apr/17 07:17;gurwls223;Probably, let me open a PR first at least to show/check if the latter suggestion works.;;;","19/Apr/17 07:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17680;;;","16/May/17 13:53;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/18000;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalyst EliminateOuterJoin optimization can cause NPE,SPARK-20359,13064469,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koertkuipers,koert,koert,17/Apr/17 19:55,27/Apr/17 08:31,14/Jul/23 06:30,19/Apr/17 07:58,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,2.3.0,,,SQL,,,,,0,,,,,,,,,"we were running in to an NPE in one of our UDFs for spark sql.
 
now this particular function indeed could not handle nulls, but this was by design since null input was never allowed (and we would want it to blow up if there was a null as input).

we realized the issue was not in our data when we added filters for nulls and the NPE still happened. then we also saw the NPE when just doing dataframe.explain instead of running our job.

turns out the issue is in EliminateOuterJoin.canFilterOutNull where a row with all nulls ifs fed into the expression as a test. its the line:
val v = boundE.eval(emptyRow)

i believe it is a bug to assume the expression can always handle nulls.

for example this fails:
{noformat}
val df1 = Seq(""a"", ""b"", ""c"").toDF(""x"")
  .withColumn(""y"", udf{ (x: String) => x.substring(0, 1) + ""!"" }.apply($""x""))
val df2 = Seq(""a"", ""b"").toDF(""x1"")
df1
  .join(df2, df1(""x"") === df2(""x1""), ""left_outer"")
  .filter($""x1"".isNotNull || !$""y"".isin(""a!""))
  .count
{noformat}

",spark master at commit 35e5ae4f81176af52569c465520a703529893b50 (Sun Apr 16),apachespark,koert,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20312,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 17 20:01:03 UTC 2017,,,,,,,,,,"0|i3dpdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/17 20:01;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/17660;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors failing stage on interrupted exception thrown by cancelled tasks,SPARK-20358,13064461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,17/Apr/17 19:13,14/Jul/17 07:31,14/Jul/23 06:30,20/Apr/17 16:55,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,https://issues.apache.org/jira/browse/SPARK-20217 introduced a regression where now interrupted exceptions will cause a task to fail on cancellation. This is because NonFatal(e) does not match if e is an InterrupedException.,,apachespark,ekhliang,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19293,,,,,,,,,,,,,,SPARK-20217,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 20 16:55:48 UTC 2017,,,,,,,,,,"0|i3dpbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/17 19:17;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/17659;;;","20/Apr/17 16:55;yhuai;Issue resolved by pull request 17659
[https://github.com/apache/spark/pull/17659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark sql group by returns incorrect results after join + distinct transformations,SPARK-20356,13064425,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,ckipers,ckipers,17/Apr/17 16:30,14/May/19 06:17,14/Jul/23 06:30,19/Apr/17 08:03,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,2,correctness,,,,,,,,"I'm experiencing a bug with the head version of spark as of 4/17/2017. After joining to dataframes, renaming a column and invoking distinct, the results of the aggregation is incorrect after caching the dataframe. The following code snippet consistently reproduces the error.

from pyspark.sql import SparkSession
import pyspark.sql.functions as sf
import pandas as pd

spark = SparkSession.builder.master(""local"").appName(""Word Count"").getOrCreate()

mapping_sdf = spark.createDataFrame(pd.DataFrame([
    {""ITEM"": ""a"", ""GROUP"": 1},
    {""ITEM"": ""b"", ""GROUP"": 1},
    {""ITEM"": ""c"", ""GROUP"": 2}
]))

items_sdf = spark.createDataFrame(pd.DataFrame([
    {""ITEM"": ""a"", ""ID"": 1},
    {""ITEM"": ""b"", ""ID"": 2},
    {""ITEM"": ""c"", ""ID"": 3}
]))

mapped_sdf = \
    items_sdf.join(mapping_sdf, on='ITEM').select(""ID"", sf.col(""GROUP"").alias('ITEM')).distinct()

print(mapped_sdf.groupBy(""ITEM"").count().count())  # Prints 2, correct
mapped_sdf.cache()
print(mapped_sdf.groupBy(""ITEM"").count().count())  # Prints 3, incorrect

The next code snippet is almost the same after the first except I don't call distinct on the dataframe. This snippet performs as expected:

mapped_sdf = \
    items_sdf.join(mapping_sdf, on='ITEM').select(""ID"", sf.col(""GROUP"").alias('ITEM'))

print(mapped_sdf.groupBy(""ITEM"").count().count())  # Prints 2, correct
mapped_sdf.cache()
print(mapped_sdf.groupBy(""ITEM"").count().count())  # Prints 2, correct

I don't experience this bug with spark 2.1 or event earlier versions for 2.2","Linux mint 18
Python 3.5",apachespark,ckipers,dkbiswal,emtl97,hvanhovell,jobevers,maropu,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 03:31:02 UTC 2017,,,,,,,,,,"0|i3dp3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/17 11:52;emtl97;really quite dangerous bug;;;","18/Apr/17 14:18;hvanhovell;Here is a reproduction in scala:
{noformat}
val df1 = Seq((""a"", 1), (""b"", 1), (""c"", 2)).toDF(""item"", ""group"")
val df2 = Seq((""a"", 1), (""b"", 2), (""c"", 3)).toDF(""item"", ""id"")
val df3 = df1.join(df2, Seq(""item"")).select($""id"", $""group"".as(""item"")).distinct()

df3.unpersist()
val agg_without_cache = df3.groupBy($""item"").count()
agg_without_cache.show()

df3.cache()
val agg_with_cache = df3.groupBy($""item"").count()
agg_with_cache.show()
{noformat};;;","18/Apr/17 17:17;jobevers;Commit 5ed397baa758c29c54a853d3f8fee0ad44e97c14 doesn't seem to have this issue.;;;","18/Apr/17 20:45;dkbiswal;[~viirya] [~hvanhovell] [~cloud_fan] [~smilegator]
I took a quick look and it seems the issue started happening after this [pr|https://github.com/apache/spark/pull/17175]. We are
changing the output partitioning information of InMemoryTableScanExec as part of the fix ( id, item -> item, item) causing a missing shuffle in the operators
above InMemoryTableScan. Changing to use the child's output partitioning like before fixes the issue. 

I am a little new to this code :-) And this is what i have found so far. Hope this helps.
;;;","19/Apr/17 00:28;viirya;[~dkbiswal] Thanks for pinging me. I will look into this.;;;","19/Apr/17 02:02;viirya;[~hvanhovell] I can't reproduce it with your example code. They both output:
{code}
+----+-----+
|item|count|
+----+-----+
|   1|    2|
|   2|    1|
+----+-----+
{code}

Am I missing something?;;;","19/Apr/17 02:30;viirya;I think I found the reason of the issue. I am working on it.;;;","19/Apr/17 02:31;dkbiswal;[~viirya] Did you try from spark-shell or from one of our query suites ? I could reproduce it from spark-shell fine. From our query suites i had to force the number of shuffle partitions to reproduce it.
{code}
test(""cache defect"") {
    withSQLConf(""spark.sql.shuffle.partitions"" -> ""200"") {
      val df1 = Seq((""a"", 1), (""b"", 1), (""c"", 2)).toDF(""item"", ""group"")
      val df2 = Seq((""a"", 1), (""b"", 2), (""c"", 3)).toDF(""item"", ""id"")
      val df3 = df1.join(df2, Seq(""item"")).select($""id"", $""group"".as(""item"")).distinct()

      df3.explain(true)

      df3.unpersist()
      val agg_without_cache = df3.groupBy($""item"").count()
      agg_without_cache.show()

      df3.cache()
      val agg_with_cache = df3.groupBy($""item"").count()
      agg_with_cache.explain(true)
      agg_with_cache.show()
    }
  }
{code};;;","19/Apr/17 02:34;viirya;[~dkbiswal] Yeah, right. Thanks. We need to force the partition to reproduce this.;;;","19/Apr/17 03:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17679;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When I request access to the 'http: //ip:port/api/v1/applications' link, return 'sparkUser' is empty in REST API.",SPARK-20354,13064351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,guoxiaolongzte,guoxiaolongzte,guoxiaolongzte,17/Apr/17 07:17,18/Apr/17 17:03,14/Jul/23 06:30,18/Apr/17 17:03,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"When I request access to the 'http: //ip:port/api/v1/applications' link, get the json. I need the 'sparkUser' field specific value because my Spark big data management platform needs to filter through this field which user submits the application to facilitate my administration and query, but the current return of the json string is empty, causing me this Function can not be achieved, that is, I do not know who the specific application is submitted by this REST Api.

return json:
[ {
  ""id"" : ""app-20170417152053-0000"",
  ""name"" : ""KafkaWordCount"",
  ""attempts"" : [ {
    ""startTime"" : ""2017-04-17T07:20:51.395GMT"",
    ""endTime"" : ""1969-12-31T23:59:59.999GMT"",
    ""lastUpdated"" : ""2017-04-17T07:20:51.395GMT"",
    ""duration"" : 0,
    ""sparkUser"" : """",
    ""completed"" : false,
    ""endTimeEpoch"" : -1,
    ""startTimeEpoch"" : 1492413651395,
    ""lastUpdatedEpoch"" : 1492413651395
  } ]
} ]",,apachespark,guoxiaolongzte,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 17 07:38:02 UTC 2017,,,,,,,,,,"0|i3donj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/17 07:38;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/17656;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ListFunctions returns duplicate functions after using persistent functions,SPARK-20349,13064260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,16/Apr/17 07:32,18/Apr/17 00:57,14/Jul/23 06:30,17/Apr/17 16:51,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The session catalog caches some persistent functions in the FunctionRegistry, so there can be duplicates. Our Catalog API listFunctions does not handle it.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 17 20:26:03 UTC 2017,,,,,,,,,,"0|i3do3b:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"16/Apr/17 07:44;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17646;;;","17/Apr/17 20:26;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix STS error handling logic on HiveSQLException,SPARK-20345,13064225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Apr/17 18:58,03/Jul/17 11:15,14/Jul/23 06:30,12/Jun/17 21:06,1.6.3,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"[SPARK-5100|https://github.com/apache/spark/commit/343d3bfafd449a0371feb6a88f78e07302fa7143] added Spark Thrift Server UI and the following logic to handle exceptions on case `Throwable`.
{code}
        HiveThriftServer2.listener.onStatementError(
          statementId, e.getMessage, SparkUtils.exceptionString(e))
{code}

However, there occurred a missed case after implementing [SPARK-6964|https://github.com/apache/spark/commit/eb19d3f75cbd002f7e72ce02017a8de67f562792]'s `Support Cancellation in the Thrift Server` by adding case `HiveSQLException` before case `Throwable`.

Logically, we had better add `HiveThriftServer2.listener.onStatementError` on case `HiveSQLException`, too.
{code}
      case e: HiveSQLException =>
        if (getStatus().getState() == OperationState.CANCELED) {
          return
        } else {
          setState(OperationState.ERROR)
          throw e
        }
      // Actually do need to catch Throwable as some failures don't inherit from Exception and
      // HiveServer will silently swallow them.
      case e: Throwable =>
        val currentState = getStatus().getState()
        logError(s""Error executing query, currentState $currentState, "", e)
        setState(OperationState.ERROR)
        HiveThriftServer2.listener.onStatementError(
          statementId, e.getMessage, SparkUtils.exceptionString(e))
        throw new HiveSQLException(e.toString)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 15 19:06:05 UTC 2017,,,,,,,,,,"0|i3dnvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/17 19:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17643;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SBT master build for Hadoop 2.6 in Jenkins fails due to Avro version resolution ,SPARK-20343,13064178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,15/Apr/17 00:24,12/Dec/22 18:11,14/Jul/23 06:30,16/Apr/17 13:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Build,,,,,0,,,,,,,,,"Please refer https://github.com/apache/spark/pull/17477#issuecomment-293942637

{quote}
[error] /home/jenkins/workspace/spark-master-test-sbt-hadoop-2.6/core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala:123: value createDatumWriter is not a member of org.apache.avro.generic.GenericData
[error]     writerCache.getOrElseUpdate(schema, GenericData.get.createDatumWriter(schema))
[error]     
{quote}

https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-sbt-hadoop-2.6/2770/consoleFull

It seems sbt has a different resolution for Avro differently with Maven in some cases. ",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 14:35:33 UTC 2017,,,,,,,,,,"0|i3dnl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/17 00:24;gurwls223;I don't know the {{Priority}} in this case. Please correct this if anyone knows.;;;","15/Apr/17 00:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17642;;;","16/Apr/17 16:10;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17651;;;","18/Apr/17 10:51;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17669;;;","18/Apr/17 14:53;gurwls223;Please let me know if anyone is able to reproduce this. I am having hard time to reproduce this..;;;","19/Apr/17 14:35;srowen;Resolved by https://github.com/apache/spark/pull/17669;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler sends SparkListenerTaskEnd before updating task's accumulators,SPARK-20342,13064162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,14/Apr/17 22:17,08/Jul/17 16:26,14/Jul/23 06:30,08/Jul/17 16:25,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,Spark Core,,,,,0,,,,,,,,,"Hit this on 2.2, but probably has been there forever. This is similar in spirit to SPARK-20205.

Event is sent here, around L1154:

{code}
    listenerBus.post(SparkListenerTaskEnd(
       stageId, task.stageAttemptId, taskType, event.reason, event.taskInfo, taskMetrics))
{code}

Accumulators are updated later, around L1173:

{code}
    val stage = stageIdToStage(task.stageId)
    event.reason match {
      case Success =>
        task match {
          case rt: ResultTask[_, _] =>
            // Cast to ResultStage here because it's part of the ResultTask
            // TODO Refactor this out to a function that accepts a ResultStage
            val resultStage = stage.asInstanceOf[ResultStage]
            resultStage.activeJob match {
              case Some(job) =>
                if (!job.finished(rt.outputId)) {
                  updateAccumulators(event)
{code}

Same thing applies here; UI shows correct info because it's pointing at the mutable {{TaskInfo}} structure. But the event log, for example, may record the wrong information.",,apachespark,bograd,cloud_fan,devaraj,lwlin,vanzin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18085,,,,,,,SPARK-21009,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 08 16:25:42 UTC 2017,,,,,,,,,,"0|i3dnhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/17 15:32;bograd;This code fails because of this issue:

{code}
test(""test"") {
    val foundMetrics = mutable.Set.empty[String]
    spark.sparkContext.addSparkListener(new SparkListener {
      override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
        taskEnd.taskInfo.accumulables.foreach { a =>
          if (a.name.isDefined) {
            foundMetrics.add(a.name.get)
          }
        }
      }
    })
    for (iter <- 0 until 100) {
      foundMetrics.clear()
      println(s""iter = $iter"")
      spark.range(10).groupBy().agg(""id"" -> ""sum"").collect
      spark.sparkContext.listenerBus.waitUntilEmpty(3000)
      assert(foundMetrics.size > 0)
    }
  }
{code};;;","07/Jun/17 16:50;vanzin;I have a fix for this, might as well use a variant of your test code for it...;;;","07/Jun/17 17:26;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18233;;;","22/Jun/17 21:51;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18393;;;","08/Jul/17 16:25;cloud_fan;Issue resolved by pull request 18393
[https://github.com/apache/spark/pull/18393];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support BigIngeger values > 19 precision,SPARK-20341,13064137,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,pzaczkiewicz,pzaczkiewicz,14/Apr/17 19:49,21/Apr/17 14:31,14/Jul/23 06:30,21/Apr/17 14:31,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"If you create a {{Dataset\[scala.math.BigInt\]}}, then you can't have a precision > 19.
{code}
scala> case class BigIntWrapper(value:scala.math.BigInt)
defined class BigIntWrapper

scala> val longDf = spark.createDataset(BigIntWrapper(scala.math.BigInt(""10000000000000000002""))::Nil)
17/04/14 19:45:15 main INFO CodeGenerator: Code generated in 211.949738 ms
java.lang.RuntimeException: Error while encoding: java.lang.IllegalArgumentException: requirement failed: BigInteger 10000000000000000002 too large for decimal
staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, BigIntWrapper, true], top level non-flat input object).value, true) AS value#16
+- staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, BigIntWrapper, true], top level non-flat input object).value, true)
   +- assertnotnull(input[0, BigIntWrapper, true], top level non-flat input object).value
      +- assertnotnull(input[0, BigIntWrapper, true], top level non-flat input object)
         +- input[0, BigIntWrapper, true]

  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:280)
  at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:421)
  at org.apache.spark.sql.SparkSession$$anonfun$3.apply(SparkSession.scala:421)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.SparkSession.createDataset(SparkSession.scala:421)
  ... 54 elided
Caused by: java.lang.IllegalArgumentException: requirement failed: BigInteger 10000000000000000002 too large for decimal
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.sql.types.Decimal.set(Decimal.scala:137)
  at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:419)
  at org.apache.spark.sql.types.Decimal.apply(Decimal.scala)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:277)
  ... 62 more
{code}",,apachespark,kiszk,pzaczkiewicz,umesh9794@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 13:52:06 UTC 2017,,,,,,,,,,"0|i3dnbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/17 18:22;kiszk;Master branch also throws an exception.;;;","18/Apr/17 19:31;kiszk;This exception is thrown since {{BigInt}} and {{BigInteger}} does not have {{precision}} method. Current Spark runtime assumes that the value is fit into 64-bit.
I will create a PR by using [this approach|http://stackoverflow.com/questions/15677518/how-to-obtain-the-precision-of-biginteger-in-java] to get a precision for {{BigInt}}.;;;","19/Apr/17 13:00;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17684;;;","19/Apr/17 13:52;pzaczkiewicz;Thanks! I saw {{scala.math.BigInt}} and {{java.math.BigInteger}} when looking through {{ScalaReflection.schemaFor}}. I couldn't use a {{Dataset\[java.math.BigDecimal\]}} because that is always precision(38,18). I figured that getting the {{BigInt}} variants to work would be easier than getting {{BigDecimal}} to work with a non-default precision.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spaces in spark.eventLog.dir are not correctly handled,SPARK-20338,13064054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,14/Apr/17 10:29,22/Jun/17 03:25,14/Jul/23 06:30,16/Jun/17 17:35,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"set spark.eventLog.dir=/home/mr/event log and submit an app ,we got error as follows:
017-04-14 17:28:40,378 INFO org.apache.spark.SparkContext: Successfully stopped SparkContext
Exception in thread ""main"" ExitCodeException exitCode=1: chmod: cannot access `/home/mr/event%20log/app-20170414172839-0000.inprogress': No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:561)
	at org.apache.hadoop.util.Shell.run(Shell.java:478)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:738)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:831)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:814)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:712)
	at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:506)
	at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:125)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2258)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$9.apply(SparkSession.scala:879)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$9.apply(SparkSession.scala:871)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:871)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:288)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:137)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
",,apachespark,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 03:25:03 UTC 2017,,,,,,,,,,"0|i3dmtz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/17 10:35;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17638;;;","13/Jun/17 06:44;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/18285;;;","22/Jun/17 03:25;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Children expressions of Hive UDF impacts the determinism of Hive UDF,SPARK-20335,13063997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,14/Apr/17 05:17,17/Apr/17 08:01,14/Jul/23 06:30,16/Apr/17 04:11,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
  /**
   * Certain optimizations should not be applied if UDF is not deterministic.
   * Deterministic UDF returns same result each time it is invoked with a
   * particular input. This determinism just needs to hold within the context of
   * a query.
   *
   * @return true if the UDF is deterministic
   */
  boolean deterministic() default true;
{noformat}

Based on the definition o UDFType, when Hive UDF's children are non-deterministic, Hive UDF is also non-deterministic.
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 16 20:43:05 UTC 2017,,,,,,,,,,"0|i3dmhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/17 05:19;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17635;;;","16/Apr/17 20:43;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17652;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix HashPartitioner in DAGSchedulerSuite,SPARK-20333,13063992,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,14/Apr/17 03:55,30/May/17 19:03,14/Jul/23 06:30,30/May/17 19:03,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"In test 
""don't submit stage until its dependencies map outputs are registered (SPARK-5259)""
""run trivial shuffle with out-of-band executor failure and retry""
""reduce tasks should be placed locally with map output""
HashPartitioner should be compatible with num of child RDD's partitions. ",,apachespark,darion,irashid,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 30 19:03:00 UTC 2017,,,,,,,,,,"0|i3dmg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/17 04:03;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/17634;;;","30/May/17 19:03;irashid;Issue resolved by pull request 17634
[https://github.com/apache/spark/pull/17634];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolution error when HAVING clause uses GROUP BY expression that involves implicit type coercion,SPARK-20329,13063946,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,joshrosen,joshrosen,13/Apr/17 22:03,26/Apr/17 23:01,14/Jul/23 06:30,21/Apr/17 02:07,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,SQL,,,,,0,,,,,,,,,"The following example runs without error on Spark 2.0.x and 2.1.x but fails in the current Spark master:

{code}
create temporary view foo (a, b) as values (cast(1 as bigint), 2), (cast(3 as bigint), 4);

select a + b from foo group by a + b having (a + b) > 1 
{code}

The error is

{code}
Error in SQL statement: AnalysisException: cannot resolve '`a`' given input columns: [(a + CAST(b AS BIGINT))]; line 1 pos 45;
'Filter (('a + 'b) > 1)
+- Aggregate [(a#249243L + cast(b#249244 as bigint))], [(a#249243L + cast(b#249244 as bigint)) AS (a + CAST(b AS BIGINT))#249246L]
   +- SubqueryAlias foo
      +- Project [col1#249241L AS a#249243L, col2#249242 AS b#249244]
         +- LocalRelation [col1#249241L, col2#249242]
{code}

I think what's happening here is that the implicit cast is breaking things: if we change the types so that both columns are integers then the analysis error disappears. Similarly, adding explicit casts, as in

{code}
select a + cast(b as bigint) from foo group by a + cast(b as bigint) having (a + cast(b as bigint)) > 1 
{code}

works so I'm pretty sure that the resolution problem is being introduced when the casts are automatically added by the type coercion rule.",,apachespark,joshrosen,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20482,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 14 20:25:03 UTC 2017,,,,,,,,,,"0|i3dm5z:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"14/Apr/17 20:25;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17641;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
query optimizer calls udf with null values when it doesn't expect them,SPARK-20312,13063531,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kitbellew,kitbellew,12/Apr/17 15:47,27/Apr/17 08:33,14/Jul/23 06:30,27/Apr/17 08:33,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,2.3.0,,,SQL,,,,,0,,,,,,,,,"When optimizing an outer join, spark passes an empty row to both sides to see if nulls would be ignored (side comment: for half-outer joins it subsequently ignores the assessment on the dominant side).

For some reason, a condition such as {{xx IS NOT NULL && udf(xx) IS NOT NULL}} might result in checking the right side first, and an exception if the udf doesn't expect a null input (given the left side first).

A example is SIMILAR to the following (see actual query plans separately):

{noformat}
def func(value: Any): Int = ... // return AnyVal which probably causes a IS NOT NULL added filter on the result

val df1 = sparkSession
  .table(...)
  .select(""col1"", ""col2"") // LongType both
val df11 = df1
  .filter(df1(""col1"").isNotNull)
  .withColumn(""col3"", functions.udf(func)(df1(""col1""))
  .repartition(df1(""col2""))
  .sortWithinPartitions(df1(""col2""))

val df2 = ... // load other data containing col2, similarly repartition and sort

val df3 =
  df1.join(df2, Seq(""col2""), ""left_outer"")
{noformat}
",,kiszk,kitbellew,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20359,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 08:30:40 UTC 2017,,,,,,,,,,"0|i3djlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/17 15:49;kitbellew;Query plans are as follows:

{noformat}
== Parsed Logical Plan ==
'InsertIntoTable 'UnresolvedRelation `database`.`table`, OverwriteOptions(true,Map()), false
+- Filter (((isnotnull(col2#10L) && isnotnull(col1#11L)) && isnotnull(version#59)) && isnotnull(col5#45))
   +- Project [col4#0L, col3#5L, col2#10L, col1#11L, version#59, col5#45]
      +- Project [col4#0L, col3#5L, col2#10L, col1#11L, col5#45, 2016-11-11 AS version#59]
         +- Repartition 8, true
            +- Project [col4#0L, col3#5L, col2#10L, col1#11L, UDF(col1#11L) AS col5#45]
               +- Project [col4#0L, col3#5L, col2#10L, col1#11L]
                  +- Project [col2#10L, col1#11L, col4#0L, col3#5L]
                     +- Join LeftOuter, (col2#10L = col2#6L)
                        :- Sort [col2#10L ASC NULLS FIRST], false
                        :  +- RepartitionByExpression [col2#10L]
                        :     +- Filter isnotnull(col1#11L)
                        :        +- Filter isnotnull(col2#10L)
                        :           +- LocalRelation [col2#10L, col1#11L]
                        +- Project [col4#0L, col3#5L, col2#6L]
                           +- Project [col3#5L, col2#6L, col4#0L]
                              +- Join LeftOuter, (col3#5L = col3#1L)
                                 :- Sort [col3#5L ASC NULLS FIRST], false
                                 :  +- RepartitionByExpression [col3#5L]
                                 :     +- Filter isnotnull(col2#6L)
                                 :        +- Filter isnotnull(col3#5L)
                                 :           +- LocalRelation [col3#5L, col2#6L]
                                 +- Sort [col3#1L ASC NULLS FIRST], false
                                    +- RepartitionByExpression [col3#1L]
                                       +- Filter isnotnull(col3#1L)
                                          +- Filter isnotnull(col4#0L)
                                             +- LocalRelation [col4#0L, col3#1L]

== Analyzed Logical Plan ==
InsertIntoTable MetastoreRelation database, table, Map(version -> None, col5 -> None), OverwriteOptions(true,Map()), false
+- Filter (((isnotnull(col2#10L) && isnotnull(col1#11L)) && isnotnull(version#59)) && isnotnull(col5#45))
   +- Project [col4#0L, col3#5L, col2#10L, col1#11L, version#59, col5#45]
      +- Project [col4#0L, col3#5L, col2#10L, col1#11L, col5#45, 2016-11-11 AS version#59]
         +- Repartition 8, true
            +- Project [col4#0L, col3#5L, col2#10L, col1#11L, UDF(col1#11L) AS col5#45]
               +- Project [col4#0L, col3#5L, col2#10L, col1#11L]
                  +- Project [col2#10L, col1#11L, col4#0L, col3#5L]
                     +- Join LeftOuter, (col2#10L = col2#6L)
                        :- Sort [col2#10L ASC NULLS FIRST], false
                        :  +- RepartitionByExpression [col2#10L]
                        :     +- Filter isnotnull(col1#11L)
                        :        +- Filter isnotnull(col2#10L)
                        :           +- LocalRelation [col2#10L, col1#11L]
                        +- Project [col4#0L, col3#5L, col2#6L]
                           +- Project [col3#5L, col2#6L, col4#0L]
                              +- Join LeftOuter, (col3#5L = col3#1L)
                                 :- Sort [col3#5L ASC NULLS FIRST], false
                                 :  +- RepartitionByExpression [col3#5L]
                                 :     +- Filter isnotnull(col2#6L)
                                 :        +- Filter isnotnull(col3#5L)
                                 :           +- LocalRelation [col3#5L, col2#6L]
                                 +- Sort [col3#1L ASC NULLS FIRST], false
                                    +- RepartitionByExpression [col3#1L]
                                       +- Filter isnotnull(col3#1L)
                                          +- Filter isnotnull(col4#0L)
                                             +- LocalRelation [col4#0L, col3#1L]

== Optimized Logical Plan ==
InsertIntoTable MetastoreRelation database, table, Map(version -> None, col5 -> None), OverwriteOptions(true,Map()), false
+- Project [col4#0L, col3#5L, col2#10L, col1#11L, 2016-11-11 AS version#59, col5#45]
   +- Repartition 8, true
      +- Project [col4#0L, col3#5L, col2#10L, col1#11L, UDF(col1#11L) AS col5#45]
         +- Join LeftOuter, (col2#10L = col2#6L)
            :- Sort [col2#10L ASC NULLS FIRST], false
            :  +- RepartitionByExpression [col2#10L]
            :     +- Filter ((isnotnull(col2#10L) && isnotnull(col1#11L)) && isnotnull(UDF(col1#11L)))
            :        +- LocalRelation [col2#10L, col1#11L]
            +- Project [col4#0L, col3#5L, col2#6L]
               +- Join LeftOuter, (col3#5L = col3#1L)
                  :- Sort [col3#5L ASC NULLS FIRST], false
                  :  +- RepartitionByExpression [col3#5L]
                  :     +- Filter (isnotnull(col3#5L) && isnotnull(col2#6L))
                  :        +- LocalRelation [col3#5L, col2#6L]
                  +- Sort [col3#1L ASC NULLS FIRST], false
                     +- RepartitionByExpression [col3#1L]
                        +- Filter (isnotnull(col4#0L) && isnotnull(col3#1L))
                           +- LocalRelation [col4#0L, col3#1L]

== Physical Plan ==
InsertIntoHiveTable MetastoreRelation database, table, Map(version -> None, col5 -> None), true, false
+- *Project [col4#0L, col3#5L, col2#10L, col1#11L, 2016-11-11 AS version#59, col5#45]
   +- Exchange RoundRobinPartitioning(8)
      +- *Project [col4#0L, col3#5L, col2#10L, col1#11L, UDF(col1#11L) AS col5#45]
         +- *BroadcastHashJoin [col2#10L], [col2#6L], LeftOuter, BuildRight
            :- *Sort [col2#10L ASC NULLS FIRST], false, 0
            :  +- Exchange hashpartitioning(col2#10L, 200)
            :     +- *Filter ((isnotnull(col2#10L) && isnotnull(col1#11L)) && isnotnull(UDF(col1#11L)))
            :        +- LocalTableScan [col2#10L, col1#11L]
            +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, bigint, true]))
               +- *Project [col4#0L, col3#5L, col2#6L]
                  +- *BroadcastHashJoin [col3#5L], [col3#1L], LeftOuter, BuildRight
                     :- *Sort [col3#5L ASC NULLS FIRST], false, 0
                     :  +- Exchange hashpartitioning(col3#5L, 200)
                     :     +- *Filter (isnotnull(col3#5L) && isnotnull(col2#6L))
                     :        +- LocalTableScan [col3#5L, col2#6L]
                     +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, false]))
                        +- *Sort [col3#1L ASC NULLS FIRST], false, 0
                           +- Exchange hashpartitioning(col3#1L, 200)
                              +- *Filter (isnotnull(col4#0L) && isnotnull(col3#1L))
                                 +- LocalTableScan [col4#0L, col3#1L]
{noformat}
;;;","12/Apr/17 15:52;kitbellew;During query optimization, one of the subtrees becomes:

{noformat}
Project [col4#0L, col3#5L, col2#10L, col1#11L, 2016-11-11 AS version#59, col5#45]
+- Repartition 8, true
   +- Project [col4#0L, col3#5L, col2#10L, col1#11L, UDF(col1#11L) AS col5#45]
      +- Filter isnotnull(UDF(col1#11L))
         +- Join LeftOuter, (col2#10L = col2#6L)
            :- Sort [col2#10L ASC NULLS FIRST], false
            :  +- RepartitionByExpression [col2#10L]
            :     +- Filter (isnotnull(col2#10L) && isnotnull(col1#11L))
            :        +- LocalRelation [col2#10L, col1#11L]
            +- Project [col4#0L, col3#5L, col2#6L]
               +- Join LeftOuter, (col3#5L = col3#1L)
                  :- Sort [col3#5L ASC NULLS FIRST], false
                  :  +- RepartitionByExpression [col3#5L]
                  :     +- Filter (isnotnull(col3#5L) && isnotnull(col2#6L))
                  :        +- LocalRelation [col3#5L, col2#6L]
                  +- Sort [col3#1L ASC NULLS FIRST], false
                     +- RepartitionByExpression [col3#1L]
                        +- Filter (isnotnull(col4#0L) && isnotnull(col3#1L))
                           +- LocalRelation [col4#0L, col3#1L]
{noformat}

And that causes evaluation of the UDF in the {{org.apache.spark.sql.catalyst.optimizer.EliminateOuterJoin}} class, before the filter of the parameter value.;;;","18/Apr/17 02:02;maropu;I made the query a bit simpler and tried though, I couldn't reproduce this in the master;
{code}
val testUdf = udf { (s: String) => s }

val df1 = Seq((1L, null), (2L, """")).toDF(""a"", ""b"")
  .filter($""b"".isNotNull)
  .withColumn(""c"", testUdf($""b""))
  .repartition($""a"")
  .sortWithinPartitions($""a"")

val df2 = Seq((1L, """")).toDF(""a"", ""b"")
  .filter($""b"".isNotNull)
  .repartition($""a"")
  .sortWithinPartitions($""a"")

df1.join(df2, ""a"" :: Nil, ""left_outer"").explain(true)

== Analyzed Logical Plan ==
a: bigint, b: string, c: string, b: string
Project [a#509L, b#510, c#515, b#528]
+- Join LeftOuter, (a#509L = a#527L)
   :- Sort [a#509L ASC NULLS FIRST], false
   :  +- RepartitionByExpression [a#509L], 200
   :     +- Project [a#509L, b#510, UDF(b#510) AS c#515]
   :        +- Filter isnotnull(b#510)
   :           +- Project [_1#506L AS a#509L, _2#507 AS b#510]
   :              +- LocalRelation [_1#506L, _2#507]
   +- Sort [a#527L ASC NULLS FIRST], false
      +- RepartitionByExpression [a#527L], 200
         +- Filter isnotnull(b#528)
            +- Project [_1#524L AS a#527L, _2#525 AS b#528]
               +- LocalRelation [_1#524L, _2#525]

== Optimized Logical Plan ==
Project [a#509L, b#510, c#515, b#528]
+- Join LeftOuter, (a#509L = a#527L)
   :- Sort [a#509L ASC NULLS FIRST], false
   :  +- RepartitionByExpression [a#509L], 200
   :     +- Project [_1#506L AS a#509L, _2#507 AS b#510, UDF(_2#507) AS c#515]
   :        +- Filter isnotnull(_2#507)
   :           +- LocalRelation [_1#506L, _2#507]
   +- Sort [a#527L ASC NULLS FIRST], false
      +- RepartitionByExpression [a#527L], 200
         +- Project [_1#524L AS a#527L, _2#525 AS b#528]
            +- Filter isnotnull(_2#525)
               +- LocalRelation [_1#524L, _2#525]
{code}

Do I miss anything? If yes, could you give me more?;;;","24/Apr/17 19:09;kitbellew;[~maropu]: making the query a bit simpler might make the logic work differently. Try this:

{noformat}
    case class C12(c1: Long, c2: Long)
    case class C23(c2: Long, c3: Long)
    case class C34(c3: Long, c4: Long)

    import sparkSession.implicits._

    val udf = functions.udf { (x: java.lang.Long) => x.toString }

    val df12n: DataFrame = sparkSession.emptyDataset[C12].toDF()
    val df23n: DataFrame = sparkSession.emptyDataset[C23].toDF()
    val df34n: DataFrame = sparkSession.emptyDataset[C34].toDF()

    val df12 = df12n
      .filter(df12n(""c1"").isNotNull)
      .filter(df12n(""c2"").isNotNull)
      .repartition(df12n(""c2""))
      .sortWithinPartitions(df12n(""c2""))

    val df23 = df23n
      .filter(df23n(""c2"").isNotNull)
      .filter(df23n(""c3"").isNotNull)
      .repartition(df23n(""c2""))
      .sortWithinPartitions(df23n(""c2""))

    val df34 = df34n
      .filter(df34n(""c3"").isNotNull)
      .filter(df34n(""c4"").isNotNull)
      .repartition(df34n(""c3""))
      .sortWithinPartitions(df34n(""c3""))

    val df13: DataFrame = df23
      .join(df12, Seq(""c2""), ""left_outer"")
      .select(df12(""c1""), df23(""c2""), df23(""c3""))

    val df14: DataFrame = df34
      .join(df13, Seq(""c3""), ""left_outer"")
      .select(df13(""c1""), df13(""c2""), df34(""c3""), df34(""c4""))

    val df: DataFrame = df14
      .withColumn(""c0"", udf(df14(""c4"")))
      .repartition(sparkSession.sparkContext.defaultParallelism)

    val result = df
      .filter(functions.col(""c3"").isNotNull)
      .filter(functions.col(""c4"").isNotNull)
      .filter(functions.col(""c0"").isNotNull)
      .collect()
{noformat}
;;;","27/Apr/17 08:29;maropu;I checked there was no issue in the current master. IIUC this issue has been fixed in SPARK-20359. This fix has already been applied into branch-2.1;;;","27/Apr/17 08:30;maropu;I'll close this because the issue has already been fixed. If any issue, feel free to reopen this. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL ""range(N) as alias"" or ""range(N) alias"" doesn't work",SPARK-20311,13063514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,juliuszsompolski,juliuszsompolski,12/Apr/17 14:40,11/May/17 10:10,14/Jul/23 06:30,11/May/17 10:10,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"`select * from range(10) as A;` or `select * from range(10) A;`
does not work.
As a workaround, a subquery has to be used:
`select * from (select * from range(10)) as A;`",,apachespark,cloud_fan,juliuszsompolski,lwlin,maropu,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 11 10:10:24 UTC 2017,,,,,,,,,,"0|i3djhz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/17 01:10;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17666;;;","09/May/17 12:24;cloud_fan;Issue resolved by pull request 17666
[https://github.com/apache/spark/pull/17666];;;","09/May/17 21:50;yhuai;It introduced a regression (https://github.com/apache/spark/pull/17666#issuecomment-300309896). I have reverted the change.;;;","10/May/17 00:46;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17928;;;","11/May/17 10:10;cloud_fan;Issue resolved by pull request 17928
[https://github.com/apache/spark/pull/17928];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spelling mistake: charactor,SPARK-20298,13063274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,bdwyer,bdwyer,bdwyer,11/Apr/17 18:08,12/Apr/17 08:26,14/Jul/23 06:30,12/Apr/17 08:26,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"""charactor"" should be ""character""
{code}
R/pkg/R/DataFrame.R:2821:              stop(""path should be charactor, NULL or omitted."")
R/pkg/R/DataFrame.R:2828:              stop(""mode should be charactor or omitted. It is 'error' by default."")
R/pkg/R/DataFrame.R:3043:              stop(""value should be an integer, numeric, charactor or named list."")
R/pkg/R/DataFrame.R:3055:                  stop(""Each item in value should be an integer, numeric or charactor."")
R/pkg/R/DataFrame.R:3601:              stop(""outputMode should be charactor or omitted."")
R/pkg/R/SQLContext.R:609:    stop(""path should be charactor, NULL or omitted."")
R/pkg/inst/tests/testthat/test_sparkSQL.R:2929:               ""path should be charactor, NULL or omitted."")
R/pkg/inst/tests/testthat/test_sparkSQL.R:2931:               ""mode should be charactor or omitted. It is 'error' by default."")
R/pkg/inst/tests/testthat/test_sparkSQL.R:2950:               ""path should be charactor, NULL or omitted."")
{code}",,apachespark,bdwyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 12 08:26:40 UTC 2017,,,,,,,,,,"0|i3di0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/17 18:08;bdwyer;I can work on this;;;","11/Apr/17 20:01;srowen;Go ahead, though this is not generally worth a JIRA;;;","11/Apr/17 20:30;apachespark;User 'bdwyer2' has created a pull request for this issue:
https://github.com/apache/spark/pull/17611;;;","12/Apr/17 08:26;srowen;Resolved by https://github.com/apache/spark/pull/17611;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NaNvl(FloatType, NullType) should not be cast to NaNvl(DoubleType, DoubleType) ",SPARK-20291,13063126,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dbtsai,dbtsai,dbtsai,11/Apr/17 09:03,12/Apr/17 16:09,14/Jul/23 06:30,12/Apr/17 03:19,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"`NaNvl(float value, null)` will be converted into `NaNvl(float value, Cast(null, DoubleType))` and finally `NaNvl(Cast(float value, DoubleType), Cast(null, DoubleType))`.

This will cause mismatching in the output type when the input type is float.

By adding extra rule in TypeCoercion can resolve this issue.",,apachespark,cloud_fan,dbtsai,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 12 06:41:05 UTC 2017,,,,,,,,,,"0|i3dh3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/17 09:11;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17606;;;","12/Apr/17 03:19;cloud_fan;Issue resolved by pull request 17606
[https://github.com/apache/spark/pull/17606];;;","12/Apr/17 06:41;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17618;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Improve BasicSchedulerIntegrationSuite ""multi-stage job""",SPARK-20288,13063086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,11/Apr/17 06:11,31/May/17 15:47,14/Jul/23 06:30,31/May/17 15:46,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"ShuffleId is determined before job submitted. But it's hard to predict stageId by shuffleId.
Stage is created in DAGScheduler(
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L381), but the order is n
ot determined.
I added a log(println(s""Creating ShufflMapStage-$id on shuffle-${shuffleDep.shuffleId}"")) after (https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L331), when testing BasicSchedulerIntegrationSuite:""multi-stage job"". It will print:
Creating ShufflMapStage-0 on shuffle-0
Creating ShufflMapStage-1 on shuffle-2
Creating ShufflMapStage-2 on shuffle-1
Creating ShufflMapStage-3 on shuffle-3
or
Creating ShufflMapStage-0 on shuffle-1
Creating ShufflMapStage-1 on shuffle-3
Creating ShufflMapStage-2 on shuffle-0
Creating ShufflMapStage-3 on shuffle-2

So It might be better to avoid generating the MapStatus by stageId.",,apachespark,irashid,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 15:46:47 UTC 2017,,,,,,,,,,"0|i3dguv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/17 06:22;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/17603;;;","31/May/17 15:46;irashid;Issue resolved by pull request 17603
[https://github.com/apache/spark/pull/17603];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dynamicAllocation.executorIdleTimeout is ignored after unpersist,SPARK-20286,13063025,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,mpasa,mpasa,10/Apr/17 22:16,14/Jul/19 22:42,14/Jul/23 06:30,05/Jun/19 13:10,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Spark Core,,,,,0,,,,,,,,,"With dynamic allocation enabled, it seems that executors with cached data which are unpersisted are still being killed using the {{dynamicAllocation.cachedExecutorIdleTimeout}} configuration, instead of {{dynamicAllocation.executorIdleTimeout}}. Assuming the default configuration ({{dynamicAllocation.cachedExecutorIdleTimeout = Infinity}}), an executor with unpersisted data won't be released until the job ends.

*How to reproduce*
- Set different values for {{dynamicAllocation.executorIdleTimeout}} and {{dynamicAllocation.cachedExecutorIdleTimeout}}
- Load a file into a RDD and persist it
- Execute an action on the RDD (like a count) so some executors are activated.
- When the action has finished, unpersist the RDD
- The application UI removes correctly the persisted data from the *Storage* tab, but if you look in the *Executors* tab, you will find that the executors remain *active* until ({{dynamicAllocation.cachedExecutorIdleTimeout}} is reached.",,aaruna,apachespark,asukhenko,Dhruve Ashar,glenn.strycker@gmail.com,hzfeiwang,irashid,jerryshao,mpasa,Tagar,toopt4,umesh9794@gmail.com,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24786,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 05 13:10:19 UTC 2019,,,,,,,,,,"0|i3dghz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/17 03:09;jerryshao;So the fix is that if there's no RDD get persisted, executors idle time should be changed to {{dynamicAllocation.executorIdleTimeout}}.;;;","12/Apr/17 05:34;mpasa;Yes. I suppose this is only checked when an executors passes from active to idle status (maybe when the last action has finished), and it's not changed when unpersist is called. But this is problematic, for example, if the job is interactive. Imagine you have a Zeppelin notebook or a Spark shell. The same driver could have different purposes and the session could be opened a long time. Once you have called a single persist, all these executors will have to wait {{dynamicAllocation.cachedExecutorIdleTimeout}} (which is Infinity by default) or the user executing another action (executors will become active again and then idle, but this time without cached data).

I don't know the solution. Maybe the best approach is to change from cachedExecutorIdleTimeout to executorIdleTimeout on all cached executors when the last RDD has been unpersisted, and then restart the time counter (unpersist will then count as an action).;;;","12/Apr/17 05:38;jerryshao;bq. Maybe the best approach is to change from cachedExecutorIdleTimeout to executorIdleTimeout on all cached executors when the last RDD has been unpersisted, and then restart the time counter (unpersist will then count as an action).

Yes, I think this is a feasible solution. I can help out it if you're not familiar with that code.;;;","12/Apr/17 08:41;mpasa;I'm zero familiar with the code, but I'll try to send a merge request when I have time to look it up. I'll be grateful for any kind of help. Thank you.;;;","15/Apr/17 11:39;umesh9794@gmail.com;While looking at ExecutorAllocationManager.onExecutorIdle, there is a condition which checks whether executor has CachedBlocks or not , if it has cached blocks then it uses cachedExecutorIdleTimeoutS and if no cached blocks it uses executorIdleTimeoutS.

Still not sure why even after calling unpersist it is behaving like this. One possibility: there might be some cached data on executors which is not reported to the BlockManager and it is causing executor to follow cachedExecutorIdleTimeout instead of executorIdleTimeout. 
Need some thoughts though cc: [~joshrosen], [~rxin].;;;","15/Apr/17 12:47;mpasa;My supposition is that {{onExecutorIdle}} is only called when a task ends, so it's already idle when you call {{unpersist}}. I'm not sure how to test this though. Also, it will be great if the UI can show an ""idle"" status for the executors. Currently, they're shown as ""Active"" until they're killed and then shown as ""Dead"".;;;","18/Apr/17 07:16;umesh9794@gmail.com;Yep, +1 to the UI changes. However, I tested the behaviour by following the steps mentioned by you and this seems to be fixed in 2.1.0. 

{code}
spark2-shell  --conf spark.dynamicAllocation.executorIdleTimeout=7s --conf spark.dynamicAllocation.cachedExecutorIdleTimeout=20s --conf spark.dynamicAllocation.minExecutors=2

scala> sc.setLogLevel(""INFO"")

scala> val rdd=sc.textFile(""/tmp/config.txt"")
..
17/04/17 23:46:58 INFO spark.SparkContext: Created broadcast 0 from textFile at <console>:24
rdd: org.apache.spark.rdd.RDD[String] = /tmp/config.txt MapPartitionsRDD[1] at textFile at <console>:24
..
..
scala> rdd.collect
17/04/17 23:47:10 INFO mapred.FileInputFormat: Total input paths to process : 1
17/04/17 23:47:10 INFO spark.SparkContext: Starting job: collect at <console>:27
..
..
17/04/17 23:47:13 INFO scheduler.DAGScheduler: Job 0 finished: collect at <console>:27, took 3.457215 s
..
scala> 17/04/17 23:47:20 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 1
{code}

As expected executors remove request was received above after 7 secs. Then I tested behaviour for persist: 

{code}
scala> rdd.persist
res2: rdd.type = /tmp/config.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> rdd.count
17/04/17 23:47:45 INFO spark.SparkContext: Starting job: count at <console>:27
..
17/04/17 23:47:45 INFO scheduler.DAGScheduler: Job 1 finished: count at <console>:27, took 0.293053 s
..
scala> 17/04/17 23:48:05 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 1
{code}

In this case also request for removing the executor was received after 20 secs. Afterwords I tested unpersist: 

{code}
scala> rdd.unpersist(true)
17/04/17 23:50:22 INFO rdd.MapPartitionsRDD: Removing RDD 1 from persistence list
17/04/17 23:50:22 INFO storage.BlockManager: Removing RDD 1
res7: rdd.type = /tmp/config.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> rdd.count
17/04/17 23:50:31 INFO spark.SparkContext: Starting job: count at <console>:27
..
17/04/17 23:50:31 INFO scheduler.DAGScheduler: Job 3 finished: count at <console>:27, took 0.219764 s
res8: Long = 100
..
scala> 17/04/17 23:50:38 INFO spark.ExecutorAllocationManager: Request to remove executorIds: 1
{code}

This time remove request was received after 7 secs. ;;;","18/Apr/17 07:35;mpasa;Thank you! I'll check it again and close the issue if I cannot reproduce the problem in 2.1.0.;;;","06/Aug/18 20:45;apachespark;User 'dhruve' has created a pull request for this issue:
https://github.com/apache/spark/pull/22015;;;","17/Feb/19 10:14;toopt4;gentle ping;;;","05/Jun/19 13:10;irashid;Issue resolved by pull request 24704
[https://github.com/apache/spark/pull/24704];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: pyspark.streaming.tests.BasicOperationTests.test_cogroup,SPARK-20285,13062995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,10/Apr/17 19:46,10/Apr/17 21:07,14/Jul/23 06:30,10/Apr/17 21:07,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Tests,,,,,0,,,,,,,,,"Saw the following failure locally:

{code}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/python/pyspark/streaming/tests.py"", line 351, in test_cogroup
    self._test_func(input, func, expected, sort=True, input2=input2)
  File ""/home/jenkins/workspace/python/pyspark/streaming/tests.py"", line 162, in _test_func
    self.assertEqual(expected, result)
AssertionError: Lists differ: [[(1, ([1], [2])), (2, ([1], [... != []

First list contains 3 additional elements.
First extra element 0:
[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))]

+ []
- [[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))],
-  [(1, ([1, 1, 1], [])), (2, ([1], [])), (4, ([], [1]))],
-  [('', ([1, 1], [1, 2])), ('a', ([1, 1], [1, 1])), ('b', ([1], [1]))]]
{code}

It also happened on Jenkins: http://spark-tests.appspot.com/builds/spark-branch-2.1-test-sbt-hadoop-2.7/120",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 19:49:03 UTC 2017,,,,,,,,,,"0|i3dgbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 19:49;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SharedInMemoryCache Weigher integer overflow,SPARK-20280,13062873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,10/Apr/17 11:49,10/Apr/17 19:57,14/Jul/23 06:30,10/Apr/17 19:57,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"in FileStatusCache.scala:
{code}
    .weigher(new Weigher[(ClientId, Path), Array[FileStatus]] {
      override def weigh(key: (ClientId, Path), value: Array[FileStatus]): Int = {
        (SizeEstimator.estimate(key) + SizeEstimator.estimate(value)).toInt
      }})
{code}

Weigher.weigh returns Int but the size of an Array[FileStatus] could be bigger than Int.maxValue. Then, a negative value is returned, leading to this exception:

{code}
* [info]   java.lang.IllegalStateException: Weights must be non-negative
* [info]   at com.google.common.base.Preconditions.checkState(Preconditions.java:149)
* [info]   at com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2223)
* [info]   at com.google.common.cache.LocalCache$Segment.put(LocalCache.java:2944)
* [info]   at com.google.common.cache.LocalCache.put(LocalCache.java:4212)
* [info]   at com.google.common.cache.LocalCache$LocalManualCache.put(LocalCache.java:4804)
* [info]   at org.apache.spark.sql.execution.datasources.SharedInMemoryCache$$anon$3.putLeafFiles(FileStatusCache.scala:131)

{code}",,apachespark,bograd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 11:59:03 UTC 2017,,,,,,,,,,"0|i3dfk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 11:58;srowen;I guess cap it at {{Int.MaxValue}}?;;;","10/Apr/17 11:59;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17591;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable 'multiple_dots_linter' lint rule that is against project's code style,SPARK-20278,13062837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,10/Apr/17 09:34,12/Dec/22 17:51,14/Jul/23 06:30,16/Apr/17 18:28,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"Currently, multi-dot separated variables in R is not allowed. For example,

{code}
 setMethod(""from_json"", signature(x = ""Column"", schema = ""structType""),
-          function(x, schema, asJsonArray = FALSE, ...) {
+          function(x, schema, as.json.array = FALSE, ...) {
             if (asJsonArray) {
               jschema <- callJStatic(""org.apache.spark.sql.types.DataTypes"",
                                      ""createArrayType"",
{code}

produces an error as below:

{code}
R/functions.R:2462:31: style: Words within variable and function names should be separated by '_' rather than '.'.
          function(x, schema, as.json.array = FALSE, ...) {
                              ^~~~~~~~~~~~~
{code}

This seems against https://google.github.io/styleguide/Rguide.xml#identifiers which says

{quote}
 The preferred form for variable names is all lower case letters and words separated with dots
{quote}

This looks because lintr https://github.com/jimhester/lintr follows http://r-pkgs.had.co.nz/style.html as written in the README.md. Few cases seems not following Google's one.

Per SPARK-6813, we follow Google's R Style Guide with few exceptions https://google.github.io/styleguide/Rguide.xml. This is also merged into Spark's website - https://github.com/apache/spark-website/pull/43

Also, we have no limit on function name. This rule also looks affecting to the name of functions as written in the README.md.

{quote}
multiple_dots_linter: check that function and variable names are separated by _ rather than ..
{quote}
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 09:42:02 UTC 2017,,,,,,,,,,"0|i3dfc7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"10/Apr/17 09:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17590;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer page shows incorrect complete date of inprogress apps,SPARK-20275,13062804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,10/Apr/17 07:48,31/May/17 03:26,14/Jul/23 06:30,31/May/17 03:26,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"The HistoryServer's incomplete page shows in-progress application's completed date as {{1969-12-31 23:59:59}}, which is not meaningful and could be improved.

!https://issues.apache.org/jira/secure/attachment/12862656/screenshot-1.png!

So instead of showing this date, here proposed to not display this column since it is not required for in-progress applications.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 07:49;jerryshao;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12862656/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 08:12:03 UTC 2017,,,,,,,,,,"0|i3df4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 08:12;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17588;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support compatible array element type in encoder,SPARK-20274,13062796,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Apr/17 06:52,11/Apr/17 12:21,14/Jul/23 06:30,11/Apr/17 12:21,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,darion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 11 12:21:58 UTC 2017,,,,,,,,,,"0|i3df33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 06:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17587;;;","11/Apr/17 12:21;cloud_fan;Issue resolved by pull request 17587
[https://github.com/apache/spark/pull/17587];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow Non-deterministic Filter push-down into Join Conditions,SPARK-20273,13062787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,10/Apr/17 05:58,10/Apr/17 16:18,14/Jul/23 06:30,10/Apr/17 16:18,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{noformat}
sql(""SELECT t1.b, rand(0) as r FROM cachedData, cachedData t1 GROUP BY t1.b having r > 0.5"").show()
{noformat}

We will get the following error:
{noformat}
Job aborted due to stage failure: Task 1 in stage 4.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4.0 (TID 8, localhost, executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition$1.apply(BroadcastNestedLoopJoinExec.scala:87)
	at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition$1.apply(BroadcastNestedLoopJoinExec.scala:87)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
{noformat}

Filters could be pushed down to the join conditions by the optimizer rule {{PushPredicateThroughJoin}}. However, we block users to add non-deterministics conditions by the analyzer (For details, see the PR https://github.com/apache/spark/pull/7535). 

We should not push down non-deterministic conditions; otherwise, we should allow users to do it by explicitly initialize the non-deterministic expressions
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 06:04:03 UTC 2017,,,,,,,,,,"0|i3df13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 06:04;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17585;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
na.fill will change the values in long or integer when the default value is in double,SPARK-20270,13062707,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dbtsai,dbtsai,dbtsai,09/Apr/17 07:57,11/Apr/17 00:16,14/Jul/23 06:30,10/Apr/17 05:17,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"This bug was partially addressed in SPARK-18555, but the root cause isn't completely solved. This bug is pretty critical since it changes the member id in Long in our application if the member id can not be represented by Double losslessly when the member id is very big. 

Here is an example how this happens, with
{code}
      Seq[(java.lang.Long, java.lang.Double)]((null, 3.14), (9123146099426677101L, null),
        (9123146560113991650L, 1.6), (null, null)).toDF(""a"", ""b"").na.fill(0.2),
{code}
the logical plan will be
{code}
== Analyzed Logical Plan ==
a: bigint, b: double
Project [cast(coalesce(cast(a#232L as double), cast(0.2 as double)) as bigint) AS a#240L, cast(coalesce(nanvl(b#233, cast(null as double)), 0.2) as double) AS b#241]
+- Project [_1#229L AS a#232L, _2#230 AS b#233]
   +- LocalRelation [_1#229L, _2#230]
{code}.
Note that even the value is not null, Spark will cast the Long into Double first. Then if it's not null, Spark will cast it back to Long which results in losing precision. 

The behavior should be that the original value should not be changed if it's not null, but Spark will change the value which is wrong.

With the PR, the logical plan will be 
{code}
== Analyzed Logical Plan ==
a: bigint, b: double
Project [coalesce(a#232L, cast(0.2 as bigint)) AS a#240L, coalesce(nanvl(b#233, cast(null as double)), cast(0.2 as double)) AS b#241]
+- Project [_1#229L AS a#232L, _2#230 AS b#233]
   +- LocalRelation [_1#229L, _2#230]
{code}
which behaves correctly without changing the original Long values.",,apachespark,dbtsai,fabboe,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 05:17:06 UTC 2017,,,,,,,,,,"0|i3dejb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/17 08:05;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17577;;;","10/Apr/17 05:17;dbtsai;Issue resolved by pull request 17577
[https://github.com/apache/spark/pull/17577];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
asm should be non-test dependency in sql/core,SPARK-20264,13062617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,08/Apr/17 05:46,10/Apr/17 03:32,14/Jul/23 06:30,10/Apr/17 03:32,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Build,SQL,,,,0,,,,,,,,,"sq/core module currently declares asm as a test scope dependency. Transitively it should actually be a normal dependency since the actual core module defines it. This occasionally confuses IntelliJ.
",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 08 05:48:04 UTC 2017,,,,,,,,,,"0|i3ddzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/17 05:48;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17574;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertNotNull should throw NullPointerException,SPARK-20262,13062596,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,08/Apr/17 00:16,08/Apr/17 04:33,14/Jul/23 06:30,08/Apr/17 04:33,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 08 00:18:03 UTC 2017,,,,,,,,,,"0|i3ddun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/17 00:18;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17573;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLUtils parseLibSVMRecord has incorrect string interpolation for error message,SPARK-20260,13062572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,v_krishna,v_krishna,v_krishna,07/Apr/17 21:51,09/Apr/17 18:41,14/Jul/23 06:30,09/Apr/17 18:39,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,ML,,,,,0,,,,,,,,,"There is missing string interpolation for the error message, which causes it to not actually display the line that failed. See https://github.com/apache/spark/pull/17572/files for a trivial fix. ",,apachespark,v_krishna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 09 18:39:41 UTC 2017,,,,,,,,,,"0|i3ddpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/17 21:52;apachespark;User 'vijaykramesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/17572;;;","08/Apr/17 07:30;srowen;OK. this is the kind of thing that doesn't even need a JIRA;;;","09/Apr/17 18:39;srowen;Issue resolved by pull request 17572
[https://github.com/apache/spark/pull/17572];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR logistic regression example did not converge in programming guide,SPARK-20258,13062529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,07/Apr/17 18:44,07/Apr/17 20:33,14/Jul/23 06:30,07/Apr/17 20:33,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"SparkR logistic regression example did not converge in programming guide. All estimates are essentially zero:

{code}
training2 <- read.df(""data/mllib/sample_binary_classification_data.txt"", source = ""libsvm"")
df_list2 <- randomSplit(training2, c(7,3), 2)
binomialDF <- df_list2[[1]]
binomialTestDF <- df_list2[[2]]
binomialGLM <- spark.glm(binomialDF, label ~ features, family = ""binomial"")


17/04/07 11:42:03 WARN WeightedLeastSquares: Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.

> summary(binomialGLM)

Deviance Residuals: 
(Note: These are approximate quantiles with relative error <= 0.01)
        Min           1Q       Median           3Q          Max  
-2.4828e-06  -2.4063e-06   2.2778e-06   2.4350e-06   2.7722e-06  

Coefficients:
                 Estimate
(Intercept)    9.0255e+00
features_0     0.0000e+00
features_1     0.0000e+00
features_2     0.0000e+00
features_3     0.0000e+00
features_4     0.0000e+00
features_5     0.0000e+00
features_6     0.0000e+00
features_7     0.0000e+00
{code}",,actuaryzhang,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 18:47:12 UTC 2017,,,,,,,,,,"0|i3ddfr:",9223372036854775807,,,,,felixcheung,,,,,,,,,,,,,,,,,,,"07/Apr/17 18:47;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17571;;;","07/Apr/17 18:47;felixcheung;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to start SparkContext/SparkSession with Hive support enabled when user does not have read/write privilege to Hive metastore warehouse dir,SPARK-20256,13062513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,xwu0226,xwu0226,07/Apr/17 17:52,05/Jul/17 02:30,14/Jul/23 06:30,04/Jul/17 16:50,2.1.0,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.1,2.3.0,,,SQL,,,,,1,,,,,,,,,"In a cluster setup with production Hive running, when the user wants to run spark-shell using the production Hive metastore, hive-site.xml is copied to SPARK_HOME/conf. So when spark-shell is being started, it tries to check database existence of ""default"" database from Hive metastore. Yet, since this user may not have READ/WRITE access to the configured Hive warehouse directory done by Hive itself, such permission error will prevent spark-shell or any spark application with Hive support enabled from starting at all. 

Example error:
{code}To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':
  at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:981)
  at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)
  at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)
  at org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)
  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
  at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:878)
  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:95)
  ... 47 elided
Caused by: java.lang.reflect.InvocationTargetException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.security.AccessControlException: Permission denied: user=notebook, access=READ, inode=""/apps/hive/warehouse"":hive:hadoop:drwxrwx---
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1686)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:8238)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:1933)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1455)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1697)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)
);
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
  at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)
  ... 58 more
Caused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.security.AccessControlException: Permission denied: user=notebook, access=READ, inode=""/apps/hive/warehouse"":hive:hadoop:drwxrwx---
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1686)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:8238)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:1933)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1455)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1697)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)
);
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)
  at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:169)
  at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:96)
  at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
  at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)
  at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)
  at org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)
  at org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)
  ... 63 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.security.AccessControlException: Permission denied: user=notebook, access=READ, inode=""/apps/hive/warehouse"":hive:hadoop:drwxrwx---
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1686)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:8238)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:1933)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1455)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1697)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)
)
  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1305)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getDatabaseOption$1.apply(HiveClientImpl.scala:336)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getDatabaseOption$1.apply(HiveClientImpl.scala:336)
  at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:279)
  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:226)
  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:225)
  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:268)
  at org.apache.spark.sql.hive.client.HiveClientImpl.getDatabaseOption(HiveClientImpl.scala:335)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:170)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:170)
  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:170)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:95)
  ... 72 more
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: java.security.AccessControlException: Permission denied: user=notebook, access=READ, inode=""/apps/hive/warehouse"":hive:hadoop:drwxrwx---
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:219)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1686)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:8238)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:1933)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1455)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1697)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)

  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme.read(ThriftHiveMetastore.java:15345)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result$get_database_resultStandardScheme.read(ThriftHiveMetastore.java:15313)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_database_result.read(ThriftHiveMetastore.java:15244)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:654)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:641)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1158)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
  at com.sun.proxy.$Proxy19.getDatabase(Unknown Source)
  at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1301)
  ... 83 more{code}

The root cause of this is a regression introduced by SPARK-18050, which tries to avoid annoying error message from Hive complaining about default database already exists. An if-condition was added to check ""default"" database existence before calling createDatabase triggers this permission error. 

I think this is a bit of critical because this error prevent spark from being used in Hive support enabled mode. ",,apachespark,darose,dongjoon,felixcheung,wzheng,xwu0226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20946,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 04 17:42:06 UTC 2017,,,,,,,,,,"0|i3ddc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/17 17:59;xwu0226;I am working on a fix and creating simulated test cases for this issue. ;;;","11/Apr/17 20:51;dongjoon;Hi, [~xwu0226].
Is there any progress?;;;","11/Apr/17 21:16;xwu0226;Yes. I am working on it. 
My proposal is to revert the SPARK-18050 change, then add a try-catch over externalCatalog.createDatabase(...)  and log the error of existing default database from Hive into DEBUG log. 

I am trying to create a unit-test case to simulate the permission issue, which I have some difficulty. 

;;;","01/Jul/17 21:46;dongjoon;Hi, [~xwu0226].
Are you still preparing a unit test case?;;;","02/Jul/17 00:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18501;;;","04/Jul/17 17:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/18530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK-19716 generates unnecessary data conversion for Dataset with primitive array,SPARK-20254,13062470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,07/Apr/17 16:02,29/Jan/18 06:23,14/Jul/23 06:30,19/Apr/17 02:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Since {{unresolvedmapobjects}} is newly introduced by SPARK-19716, the current implementation generates {{mapobjects()}} at {{DeserializeToObject}} in {{Analyzed Logical Plan}}. This {{mapObject()}} introduces Java code to store an array into {{GenericArrayData}}.
cc: [~cloud_fan]
 
{code}
val ds = sparkContext.parallelize(Seq(Array(1.1, 2.2)), 1).toDS.cache
ds.count
val ds2 = ds.map(e => e)
ds2.explain(true)
ds2.show
{code}

Plans before SPARK-19716
{code}
== Parsed Logical Plan ==
'SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- 'MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- 'DeserializeToObject unresolveddeserializer(upcast(getcolumnbyordinal(0, ArrayType(DoubleType,false)), ArrayType(DoubleType,false), - root class: ""scala.Array"").toDoubleArray), obj#23: [D
      +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
         +- ExternalRDD [obj#1]

== Analyzed Logical Plan ==
value: array<double>
SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- DeserializeToObject cast(value#2 as array<double>).toDoubleArray, obj#23: [D
      +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
         +- ExternalRDD [obj#1]

== Optimized Logical Plan ==
SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- DeserializeToObject value#2.toDoubleArray, obj#23: [D
      +- InMemoryRelation [value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
            +- *SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
               +- Scan ExternalRDDScan[obj#1]

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- *MapElements <function1>, obj#24: [D
   +- *DeserializeToObject value#2.toDoubleArray, obj#23: [D
      +- *InMemoryTableScan [value#2]
            +- InMemoryRelation [value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
                  +- *SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
                     +- Scan ExternalRDDScan[obj#1]

{code}

Plans after SPARK-19716
{code}
== Parsed Logical Plan ==
'SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- 'MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- 'DeserializeToObject unresolveddeserializer(unresolvedmapobjects(<function1>, getcolumnbyordinal(0, ArrayType(DoubleType,false)), None).toDoubleArray), obj#23: [D
      +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
         +- ExternalRDD [obj#1]

== Analyzed Logical Plan ==
value: array<double>
SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- DeserializeToObject mapobjects(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, assertnotnull(lambdavariable(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, true), - array element class: ""scala.Double"", - root class: ""scala.Array""), value#2, None, MapObjects_builderValue4).toDoubleArray, obj#23: [D
      +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
         +- ExternalRDD [obj#1]

== Optimized Logical Plan ==
SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- MapElements <function1>, class [D, [StructField(value,ArrayType(DoubleType,false),true)], obj#24: [D
   +- DeserializeToObject mapobjects(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, assertnotnull(lambdavariable(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, true), - array element class: ""scala.Double"", - root class: ""scala.Array""), value#2, None, MapObjects_builderValue4).toDoubleArray, obj#23: [D
      +- InMemoryRelation [value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
            +- *SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
               +- Scan ExternalRDDScan[obj#1]

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#25]
+- *MapElements <function1>, obj#24: [D
   +- *DeserializeToObject mapobjects(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, assertnotnull(lambdavariable(MapObjects_loopValue4, MapObjects_loopIsNull4, DoubleType, true), - array element class: ""scala.Double"", - root class: ""scala.Array""), value#2, None, MapObjects_builderValue4).toDoubleArray, obj#23: [D
      +- InMemoryTableScan [value#2]
            +- InMemoryRelation [value#2], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
                  +- *SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(DoubleType,false), fromPrimitiveArray, input[0, [D, true], true) AS value#2]
                     +- Scan ExternalRDDScan[obj#1]
{{java}}

{{java}}
...
/* 056 */       ArrayData deserializetoobject_value1 = null;
/* 057 */
/* 058 */       if (!inputadapter_isNull) {
/* 059 */         int deserializetoobject_dataLength = inputadapter_value.numElements();
/* 060 */
/* 061 */         Double[] deserializetoobject_convertedArray = null;
/* 062 */         deserializetoobject_convertedArray = new Double[deserializetoobject_dataLength];
/* 063 */
/* 064 */         int deserializetoobject_loopIndex = 0;
/* 065 */         while (deserializetoobject_loopIndex < deserializetoobject_dataLength) {
/* 066 */           MapObjects_loopValue2 = (double) (inputadapter_value.getDouble(deserializetoobject_loopIndex));
/* 067 */           MapObjects_loopIsNull2 = inputadapter_value.isNullAt(deserializetoobject_loopIndex);
/* 068 */
/* 069 */           if (MapObjects_loopIsNull2) {
/* 070 */             throw new RuntimeException(((java.lang.String) references[0]));
/* 071 */           }
/* 072 */           if (false) {
/* 073 */             deserializetoobject_convertedArray[deserializetoobject_loopIndex] = null;
/* 074 */           } else {
/* 075 */             deserializetoobject_convertedArray[deserializetoobject_loopIndex] = MapObjects_loopValue2;
/* 076 */           }
/* 077 */
/* 078 */           deserializetoobject_loopIndex += 1;
/* 079 */         }
/* 080 */
/* 081 */         deserializetoobject_value1 = new org.apache.spark.sql.catalyst.util.GenericArrayData(deserializetoobject_convertedArray); /*###*/
/* 082 */       }
...
{code}",,apachespark,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 19 02:59:33 UTC 2017,,,,,,,,,,"0|i3dd2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/17 17:07;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17568;;;","07/Apr/17 17:36;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17569;;;","19/Apr/17 02:59;cloud_fan;Issue resolved by pull request 17568
[https://github.com/apache/spark/pull/17568];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improper OOM error when a task been killed while spilling data,SPARK-20250,13062385,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,wellfengzhu,wellfengzhu,07/Apr/17 09:59,25/May/17 08:00,14/Jul/23 06:30,25/May/17 07:54,1.6.1,1.6.2,1.6.3,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Spark Core,,,,,0,,,,,,,,,"    When a task is calling spill() but it receives a killing request from driver (e.g., speculative task), the TaskMemoryManager will throw an OOM exception. 
    Then the executor takes it as UncaughtException, which will be handled by SparkUncaughtExceptionHandler and the executor will consequently be shutdown. 
    However, this error may lead to the whole application failure due to the ""max number of executor failures (30) reached"". 
    In our production environment, we have encountered a lot of such cases. 
\\
{noformat}
17/04/05 06:41:27 INFO sort.UnsafeExternalSorter: Thread 115 spilling sort data of 928.0 MB to disk (1 time so far)
17/04/05 06:41:27 INFO sort.UnsafeSorterSpillWriter: Spill file:/data/usercache/application_1482394966158_87487271/blockmgr-85c25fa8-06b4/32/temp_local_b731
17/04/05 06:41:27 INFO sort.UnsafeSorterSpillWriter: Write numRecords:2097152
17/04/05 06:41:30 INFO executor.Executor: Executor is trying to kill task 16.0 in stage 3.0 (TID 857)
17/04/05 06:41:30 ERROR memory.TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43a122ed
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)
	at org.apache.spark.storage.DiskBlockObjectWriter.updateBytesWritten(DiskBlockObjectWriter.scala:228)
	at org.apache.spark.storage.DiskBlockObjectWriter.recordWritten(DiskBlockObjectWriter.scala:207)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:139)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:196)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:170)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:244)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:302)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:346)
        ................................................................
17/04/05 06:41:30 INFO sort.UnsafeExternalSorter: Thread 115 spilling sort data of 928.0 MB to disk (2  times so far)
17/04/05 06:41:30 INFO sort.UnsafeSorterSpillWriter: Spill file:/data/usercache/appcache/application_1482394966158_87487271/blockmgr-573312a3-bd46-4c5c-9293-1021cc34c77
17/04/05 06:41:30 INFO sort.UnsafeSorterSpillWriter: Write numRecords:2097152
17/04/05 06:41:31 ERROR memory.TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43a122ed
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)
	at org.apache.spark.storage.DiskBlockObjectWriter.updateBytesWritten(DiskBlockObjectWriter.scala:228)
	at org.apache.spark.storage.DiskBlockObjectWriter.recordWritten(DiskBlockObjectWriter.scala:207)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:139)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:196)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:170)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:244)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
        .........................................................................
17/04/05 06:41:31 WARN memory.TaskMemoryManager: leak 32.0 KB memory from org.apache.spark.shuffle.sort.ShuffleExternalSorter@513661a6
17/04/05 06:41:31 ERROR executor.Executor: Managed memory leak detected; size = 26010016 bytes, TID = 857
17/04/05 06:41:31 ERROR executor.Executor: Exception in task 16.0 in stage 3.0 (TID 857)
java.lang.OutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43a122ed : null
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:178)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:244)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
        ......................................................
17/04/05 06:41:31 ERROR util.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@43a122ed : null
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:178)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:244)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:164)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:358)
	..........................................
17/04/05 06:41:31 INFO storage.DiskBlockManager: Shutdown hook called
17/04/05 06:41:31 INFO util.ShutdownHookManager: Shutdown hook called
{noformat}",,apachespark,ianoc,lwlin,umesh9794@gmail.com,wellfengzhu,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 24 15:18:04 UTC 2017,,,,,,,,,,"0|i3dcjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/May/17 15:18;apachespark;User 'ConeyLiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/18090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should check determinism when pushing predicates down through aggregation,SPARK-20246,13062285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,weiluo_ren123,weiluo_ren123,06/Apr/17 23:06,08/Apr/17 03:57,14/Jul/23 06:30,08/Apr/17 03:57,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,SQL,,,,,0,correctness,,,,,,,,"{code}import org.apache.spark.sql.functions._
spark.range(1,1000).distinct.withColumn(""random"", rand()).filter(col(""random"") > 0.3).orderBy(""random"").show{code}

gives wrong result.

 In the optimized logical plan, it shows that the filter with the non-deterministic predicate is pushed beneath the aggregate operator, which should not happen.

cc [~lian cheng]",,apachespark,codingcat,lian cheng,viirya,weiluo_ren123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 07 07:52:03 UTC 2017,,,,,,,,,,"0|i3dbxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/17 23:12;lian cheng;[This line|https://github.com/apache/spark/blob/a4491626ed8169f0162a0dfb78736c9b9e7fb434/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L795] should be the root cause. We didn't check determinism of the predicates before pushing them down.

The same thing also applies when pushing predicates through union and window operators.

cc [~cloud_fan];;;","07/Apr/17 05:23;viirya;Seems we have checked determinism in [here|https://github.com/apache/spark/blob/a4491626ed8169f0162a0dfb78736c9b9e7fb434/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L805].;;;","07/Apr/17 05:47;viirya;We should also check determinism of the [replaced expression|https://github.com/apache/spark/blob/a4491626ed8169f0162a0dfb78736c9b9e7fb434/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L810].;;;","07/Apr/17 06:01;viirya;For union and window, we don't have the replacement of expression. So I think they should be safe.;;;","07/Apr/17 06:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17559;;;","07/Apr/17 07:52;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17562;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect input size in UI with pyspark,SPARK-20244,13062134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,asukhenko,asukhenko,06/Apr/17 15:06,01/Jun/17 05:37,14/Jul/23 06:30,01/Jun/17 05:37,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Web UI,,,,,0,,,,,,,,,"In Spark UI (Details for Stage) Input Size is  64.0 KB when running in PySparkShell. 
Also it is incorrect in Tasks table:
64.0 KB / 132120575 in pyspark
252.0 MB / 132120575 in spark-shell

I will attach screenshots.

Reproduce steps:
Run this  to generate big file (press Ctrl+C after 5-6 seconds)
$ yes > /tmp/yes.txt
$ hadoop fs -copyFromLocal /tmp/yes.txt /tmp/
$ ./bin/pyspark
{code}
Python 2.7.5 (default, Nov  6 2016, 00:28:07) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.5 (default, Nov  6 2016 00:28:07)
SparkSession available as 'spark'.{code}
>>> a = sc.textFile(""/tmp/yes.txt"")
>>> a.count()


Open Spark UI and check Stage 0.",,ajbozarth,apachespark,asukhenko,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/17 15:13;asukhenko;pyspark_incorrect_inputsize.png;https://issues.apache.org/jira/secure/attachment/12862302/pyspark_incorrect_inputsize.png","06/Apr/17 15:13;asukhenko;sparkshell_correct_inputsize.png;https://issues.apache.org/jira/secure/attachment/12862301/sparkshell_correct_inputsize.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 12 06:34:03 UTC 2017,,,,,,,,,,"0|i3db07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/17 15:13;asukhenko;Spark-shell and pyspark UI screenshots.;;;","11/Apr/17 14:09;jerryshao;This actually is not a UI problem, it is FileSystem thread local statistics problem, because PythonRDD will create another thread to read data, so the readBytes getting from another thread will be error. But there's no problem if using spark-shell, since everything is processed in one thread.

This is a general problem if the child RDD's computation creates another thread to handle parent's RDD (HadoopRDD)'s iterator. I tried several different ways to handle this problem, but still have some small issues. The multi-thread processing inside the RDD make the fix quite complex. ;;;","12/Apr/17 06:34;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17617;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DebugFilesystem.assertNoOpenStreams thread race,SPARK-20243,13062130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,06/Apr/17 14:47,14/Apr/17 13:51,14/Jul/23 06:30,10/Apr/17 15:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Tests,,,,,0,,,,,,,,,"Introduced by SPARK-19946.

DebugFilesystem.assertNoOpenStreams gets the size of the openStreams ConcurrentHashMap and then later, if the size was > 0, accesses the first element in openStreams.values. But, the ConcurrentHashMap might be cleared by another thread between getting its size and accessing it, resulting in an exception when trying to call .head on an empty collection.
",,apachespark,bograd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 10 12:12:04 UTC 2017,,,,,,,,,,"0|i3dazb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/17 08:15;srowen;Agree. I think this implementation should just synchronize access to a simple mutable.HashMap. It's test-only and contention won't be significant as it's only accessed when a stream is open. ;;;","10/Apr/17 12:12;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17592;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve HistoryServer ACL mechanism,SPARK-20239,13062027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,06/Apr/17 07:40,25/Apr/17 22:22,14/Jul/23 06:30,25/Apr/17 01:19,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Spark Core,,,,,0,,,,,,,,,"Current SHS (Spark History Server) two different ACLs. 

* ACL of base URL, it is controlled by ""spark.acls.enabled"" or ""spark.ui.acls.enabled"", and with this enabled, only user configured with ""spark.admin.acls"" (or group) or ""spark.ui.view.acls"" (or group), or the user who started SHS could list all the applications, otherwise none of them can be listed. This will also affect REST APIs which listing the summary of all apps and one app.

* Per application ACL. This is controlled by ""spark.history.ui.acls.enabled"". With this enabled only history admin user and user/group who ran this app can access the details of this app. 

With this two ACLs, we may encounter several unexpected behaviors:

1. if base URL's ACL is enabled but user A has no view permission. User ""A"" cannot see the app list but could still access details of it's own app.
2. if ACLs of base URL is disabled. Then user ""A"" could see the summary of all the apps, even some didn't run by user ""A"", but cannot access the details.
3. history admin ACL has no permission to list all apps if this admin user is not added to base URL's ACL.

The unexpected behaviors is mainly because we have two different ACLs, ideally we should have only one to manage all.

So to improve SHS's ACL mechanism, we should:

* Unify two different ACLs into one, and always honor this one (both in base URL and app details).
* User could partially list and display apps which ran by him according to the ACLs in event log.",,apachespark,jerryshao,Sephiroth-Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 25 02:37:03 UTC 2017,,,,,,,,,,"0|i3dacf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/17 03:17;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17582;;;","25/Apr/17 02:37;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17755;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in tpcds q77.sql,SPARK-20223,13061674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,05/Apr/17 09:30,05/Apr/17 17:23,14/Jul/23 06:30,05/Apr/17 17:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,SQL,,,,,0,,,,,,,,,,,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 09:33:04 UTC 2017,,,,,,,,,,"0|i3d85z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/17 09:33;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17538;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor should not fail stage if killed task throws non-interrupted exception,SPARK-20217,13061578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ekhliang,ekhliang,ekhliang,04/Apr/17 23:59,11/May/17 14:03,14/Jul/23 06:30,06/Apr/17 02:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"This is reproducible as follows. Run the following, and then use SparkContext.killTaskAttempt to kill one of the tasks. The entire stage will fail since we threw a RuntimeException instead of InterruptedException.

We should probably unconditionally return TaskKilled instead of TaskFailed if the task was killed by the driver, regardless of the actual exception thrown.

{code}
spark.range(100).repartition(100).foreach { i =>
  try {
    Thread.sleep(10000000)
  } catch {
    case t: InterruptedException =>
      throw new RuntimeException(t)
  }
}
{code}

Based on the code in TaskSetManager, I think this also affects kills of speculative tasks. However, since the number of speculated tasks is few, and usually you need to fail a task a few times before the stage is cancelled, probably no-one noticed this in production.",,apachespark,codingcat,ekhliang,xuefuz,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19354,,,,,,,,,,,,,,SPARK-20358,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 02:37:56 UTC 2017,,,,,,,,,,"0|i3d7kn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/17 00:10;apachespark;User 'ericl' has created a pull request for this issue:
https://github.com/apache/spark/pull/17531;;;","06/Apr/17 02:37;yhuai;Issue resolved by pull request 17531
[https://github.com/apache/spark/pull/17531];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Install pandoc on machine(s) used for packaging,SPARK-20216,13061562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,holden,holden,holden,04/Apr/17 22:44,05/Apr/17 18:09,14/Jul/23 06:30,05/Apr/17 18:09,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Project Infra,PySpark,,,,0,,,,,,,,,"For Python packaging having pandoc is required to have a reasonable package doc string. Which ever machine(s) are used for packaging should have both pandoc and pypandoc installed on them.
cc [~joshrosen] who I know was doing something related to this",,holden,marmbrus,Simon_poortman@icloud.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 18:09:37 UTC 2017,,,,,,,,,,"0|i3d7h3:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"04/Apr/17 23:20;marmbrus;I think it all runs on https://amplab.cs.berkeley.edu/jenkins/computer/amp-jenkins-worker-01/;;;","05/Apr/17 18:09;holden;Thanks [~marmbrus] :) So looking at that host it seems like pandoc is installed but pypandoc is installed for the wrong version of Python (namely its installed for Python 2.6, but the packaging is running in conda's Python 2.7.;;;","05/Apr/17 18:09;holden;This has been fixed with install pypandoc into the conda env and verified by running the Python package sdist.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark linalg _convert_to_vector should check for sorted indices,SPARK-20214,13061551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,josephkb,josephkb,04/Apr/17 22:03,06/Apr/17 00:59,14/Jul/23 06:30,06/Apr/17 00:51,1.5.2,1.6.3,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,ML,MLlib,PySpark,Tests,,0,,,,,,,,,"I've seen a few failures of this line: https://github.com/apache/spark/blame/402bf2a50ddd4039ff9f376b641bd18fffa54171/python/pyspark/mllib/tests.py#L847

It converts a scipy.sparse.lil_matrix to a dok_matrix and then to a pyspark.mllib.linalg.Vector.  The failure happens in the conversion to a vector and indicates that the dok_matrix is not returning its values in sorted order. (Actually, the failure is in _convert_to_vector, which converts the dok_matrix to a csc_matrix and then passes the CSC data to the MLlib Vector constructor.) Here's the stack trace:
{code}
Traceback (most recent call last):
  File ""/home/jenkins/workspace/python/pyspark/mllib/tests.py"", line 847, in test_serialize
    self.assertEqual(sv, _convert_to_vector(lil.todok()))
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 78, in _convert_to_vector
    return SparseVector(l.shape[0], csc.indices, csc.data)
  File ""/home/jenkins/workspace/python/pyspark/mllib/linalg/__init__.py"", line 556, in __init__
    % (self.indices[i], self.indices[i + 1]))
TypeError: Indices 3 and 1 are not strictly increasing
{code}

This seems like a bug in _convert_to_vector, where we really should check {{csc_matrix.has_sorted_indices}} first.

I haven't seen this bug in pyspark.ml.linalg, but it probably exists there too.",,apachespark,josephkb,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 00:51:08 UTC 2017,,,,,,,,,,"0|i3d7en:",9223372036854775807,,,,,josephkb,,,,,,,,2.0.3,2.1.2,2.2.0,,,,,,,,,"05/Apr/17 03:23;viirya;Confirmed that dok_matrix.tocsc() won't guarantee sorted indices:

{code}
>>> from scipy.sparse import lil_matrix
>>> lil = lil_matrix((4, 1))
>>> lil[1, 0] = 1
>>> lil[3, 0] = 2
>>> dok = lil.todok()
>>> csc = dok.tocsc()
>>> csc.has_sorted_indices
0
>>> csc.indices
array([3, 1], dtype=int32)
{code}

I checked the source codes of scipy. The only way to guarantee it is {{csc_matrix.tocsr()}} and {{csr_matrix.tocsc()}}.
;;;","05/Apr/17 03:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17532;;;","06/Apr/17 00:51;josephkb;Issue resolved by pull request 17532
[https://github.com/apache/spark/pull/17532];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrameWriter operations do not show up in SQL tab,SPARK-20213,13061550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,rdblue,rdblue,04/Apr/17 21:57,02/Mar/20 19:41,14/Jul/23 06:30,31/May/17 03:13,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,Web UI,,,,0,,,,,,,,,"In 1.6.1, {{DataFrame}} writes started using {{DataFrameWriter}} actions like {{insertInto}} would show up in the SQL tab. In 2.0.0 and later, they no longer do. The problem is that 2.0.0 and later no longer wrap execution with {{SQLExecution.withNewExecutionId}}, which emits {{SparkListenerSQLExecutionStart}}.

Here are the relevant parts of the stack traces:
{code:title=Spark 1.6.1}
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:130)
org.apache.spark.sql.execution.QueryExecution$$anonfun$toRdd$1.apply(QueryExecution.scala:56)
org.apache.spark.sql.execution.QueryExecution$$anonfun$toRdd$1.apply(QueryExecution.scala:56)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)
org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:56) => holding Monitor(org.apache.spark.sql.hive.HiveContext$QueryExecution@424773807})
org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:55)
org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:196)
{code}

{code:title=Spark 2.0.0}
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86) => holding Monitor(org.apache.spark.sql.execution.QueryExecution@490977924})
org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
org.apache.spark.sql.DataFrameWriter.insertInto(DataFrameWriter.scala:301)
{code}

I think this was introduced by [54d23599|https://github.com/apache/spark/commit/54d23599]. The fix should be to add withNewExecutionId to https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L610",,ajbozarth,apachespark,cloud_fan,kiszk,rdblue,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20635,SPARK-20635,,,,,SPARK-26682,,,,SPARK-22977,,,,,,,,,,,,,,"04/May/17 00:00;zsxwing;Screen Shot 2017-05-03 at 5.00.19 PM.png;https://issues.apache.org/jira/secure/attachment/12866298/Screen+Shot+2017-05-03+at+5.00.19+PM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 26 04:46:03 UTC 2017,,,,,,,,,,"0|i3d7ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/17 17:33;apachespark;User 'rdblue' has created a pull request for this issue:
https://github.com/apache/spark/pull/17540;;;","04/May/17 00:00;zsxwing;I tested the master branch, and I can see ""insertInto"" in SQL tab.  

!Screen Shot 2017-05-03 at 5.00.19 PM.png|width=300!

Could you clarify the issue? It would be great if you can provide a reproducer.;;;","04/May/17 00:05;rdblue;[~zsxwing], the PR adds a method that causes tests to fail if they aren't wrapped. If you remove the additional high-level calls to withNewExecutionId that I added, you can see all the test failures.;;;","22/May/17 20:00;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18064;;;","31/May/17 03:13;cloud_fan;Issue resolved by pull request 18064
[https://github.com/apache/spark/pull/18064];;;","26/Jun/17 04:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/18419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`1 > 0.0001` throws Decimal scale (0) cannot be greater than precision (-2) exception,SPARK-20211,13061450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,stanzhai,stanzhai,04/Apr/17 14:53,12/Dec/22 18:11,14/Jul/23 06:30,14/Jun/17 11:19,2.0.0,2.0.1,2.0.2,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,SQL,,,,,1,correctness,,,,,,,,"The following SQL:
{code}
select 1 > 0.0001 from tb
{code}
throws Decimal scale (0) cannot be greater than precision (-2) exception in Spark 2.x.

`floor(0.0001)` and `ceil(0.0001)` have the same problem in Spark 1.6.x and Spark 2.x.",,1028344078@qq.com,apachespark,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 06:07:03 UTC 2017,,,,,,,,,,"0|i3d6s7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/17 15:14;gurwls223;Please refer http://spark.apache.org/contributing.html

{quote}
Critical: a large minority of users are missing important functionality without this, and/or a workaround is difficult
{quote}

workaround as below:

{code}
scala> sql(""select double(1) > double(0.0001)"").show()
+--------------------------------------------+
|(CAST(1 AS DOUBLE) > CAST(0.0001 AS DOUBLE))|
+--------------------------------------------+
|                                        true|
+--------------------------------------------+
{code}

Also, it states,

{quote}
Priority. Set to Major or below; higher priorities are generally reserved for committers to set
{quote};;;","04/Apr/17 15:19;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17529;;;","04/Apr/17 15:51;stanzhai;A workaround is difficult for me, because of all of my SQL are generated by a high-level system, I cannot cast all columns as double.
FLOOR and CEIL are frequently used functions, and not all users will give a feedback to the community when encounter this problem.
We should pay attention to the correctness of the SQL.;;;","08/Jun/17 23:38;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18244;;;","14/Jun/17 06:07;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/18297;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Execute next trigger immediately if previous batch took longer than trigger interval,SPARK-20209,13061350,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,04/Apr/17 06:51,04/May/17 00:35,14/Jul/23 06:30,05/Apr/17 06:17,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"For large trigger intervals (e.g. 10 minutes), if a batch takes 11 minutes, then it will wait for 9 mins before starting the next batch. This does not make sense. The processing time based trigger policy should be to do process batches as fast as possible, but no faster than 1 in every trigger interval. If batches are taking longer than trigger interval anyways, then no point waiting extra trigger interval.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20063,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 06:17:02 UTC 2017,,,,,,,,,,"0|i3d65z:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"04/Apr/17 06:54;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/17525;;;","05/Apr/17 06:17;tdas;Issue resolved by pull request 17525
[https://github.com/apache/spark/pull/17525];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler posts SparkListenerStageSubmitted before updating stage,SPARK-20205,13061257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,03/Apr/17 21:08,24/May/17 23:57,14/Jul/23 06:30,24/May/17 23:57,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"Probably affects other versions, haven't checked.

The code that submits the event to the bus is around line 991:

{code}
    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)
    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
{code}

Later in the same method, the stage information is updated (around line 1057):

{code}
    if (tasks.size > 0) {
      logInfo(s""Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 "" +
        s""tasks are for partitions ${tasks.take(15).map(_.partitionId)})"")
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
{code}

That means an event handler might get a stage submitted event with an unset submission time.",,apachespark,codingcat,lwlin,mridulm80,vanzin,xuefuz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18085,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 20:25:03 UTC 2017,,,,,,,,,,"0|i3d5lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/17 23:33;mridulm80;This is nasty ! This means submissionTime will always be unset ?
Btw, is it possible for submissionTime to be set - but to an incorrect value ?;;;","03/Apr/17 23:42;vanzin;bq. This is nasty ! This means submissionTime will always be unset ?

Well, it's a little more complicated than that.

The UI code currently ""self heals"", because it just keeps a pointer to the {{StageInfo}} object which is modified by the scheduler later. So eventually the UI sees the value.

But the event log, for example, might not have the submission time.

bq. Btw, is it possible for submissionTime to be set - but to an incorrect value ?

I wouldn't say incorrect; at worst it's gonna be slightly inaccurate.;;;","03/Apr/17 23:48;mridulm80;bq. I wouldn't say incorrect; at worst it's gonna be slightly inaccurate.

I was referring to the case where we are persisting to event log or consuming events to externally persist them.
In this context, will we always have unspecified submissionTime  or is there case where submissionTime  is pointing to some incorrect/spurious value (if this is always in the codepath after makeNewStageAttempt; then it should be fine).

Essentially, is the workaround for existing spark versions to simply set submissionTime to current time if it is None for SparkListenerStageSubmitted sufficient ? Will it miss some corner case ? (value is set but is incorrect ?);;;","03/Apr/17 23:54;vanzin;bq. I was referring to the case where we are persisting to event log or consuming events to externally persist them.

I see. In that case I believe it will always be unset. For live listeners, current time is a good enough approximation, but for the history server, for example, that's not an option (since {{SparkListenerStageSubmitted}} does not have a {{time}} field).;;;","04/Apr/17 00:14;mridulm80;For history server that will fail - good point.
Atleast for custom listeners, users can workaround until next release by using current time (in there code when field submissionTime  is None).
Thanks for clarifying [~vanzin] !;;;","09/May/17 20:25;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17925;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove references to org.spark-project.hive,SPARK-20202,13061083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,omalley,omalley,03/Apr/17 10:23,12/Dec/22 18:11,14/Jul/23 06:30,05/Oct/20 22:30,1.6.4,2.0.3,2.1.1,2.2.3,2.3.4,2.4.4,3.0.0,3.1.0,,,,,,,,,,,,,,,,,,,3.1.0,,,,,Build,SQL,,,,7,,,,,,,,,Spark can't continue to depend on their fork of Hive and must move to standard Hive versions.,,apachespark,asukhenko,aweise,Chopinxb,csun,cwsteinbach,diederik,dlavati,dongjoon,dongwook,dougb,ekoifman,felixcheung,Ferd,gates,holden,jerryshao,jonathak,joshrosen,junzhang,kgyrtkirk,krisden,lirui,marmbrus,maropu,medb,mlnick,mridulm80,omalley,pclay,randallwhitman,rdblue,rxin,sershe,stevel@apache.org,Steven Rand,Tagar,tgraves,toopt4,viirya,vish741,wzheng,xinxianyin,yumwang,zhaoyunjiong,,,,,,,,,,,SPARK-30034,,,,,,,,,,,,,,SPARK-33072,SPARK-33082,SPARK-27733,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 03:01:46 UTC 2020,,,,,,,,,,"0|i3d4in:",9223372036854775807,,,,,,,,,,,,,3.1.0,,,,,,,,,,,"03/Apr/17 10:29;srowen;I see wide agreement on that. One question I have is, is including Hive this way merely a really-not-nice-to-have or actually not allowed? I think the question is whether sources are available, right? because releases can't have binary-only parts. I plead ignorance, I have never myself paid much attention to this integration. 

If it's not then this sounds like something has to change for releases beyond 2.1.1 and this can be targeted as a Blocker accordingly.

Does this depend on refactoring or changes in Hive? IIRC the problem was hive-exec being an uber-jar, but it's been a long time since I read any of that discussion.;;;","03/Apr/17 11:15;omalley;As an Apache member, the Spark project can't release binary artifacts that aren't made from its Apache code base. So either, the Spark project needs to use Hive's release artifacts or it needs to formally fork Hive and move the fork into its git repository at Apache and rename it away from org.apache.hive to org.apache.spark. The current path is not allowed.

Hive is in the middle of rolling releases and thus this is a good time to make requests. The old uber jar (hive-exec) is already released separately with the classifier ""core."" It looks like we are using the same protobuf (2.5.0) and kryo (3.0.3) versions.;;;","03/Apr/17 11:23;srowen;Agree. I think the logic was that Spark had released its own source/binary version of Hive, and then used that in Spark. I don't think anybody believes that's a good solution in the long term; it was a work-around for hive-exec's packaging IIRC. Once whatever that is is resolved this can go away, but I defer to those who know the issue better on the details.

What I'm not clear on is whether the current org.spark-hive situation is streeetching the source/binary policy so far that it breaks, enough that no more releases can happen without it. Best to make it go away ASAP anyway. But I don't know if changes in Hive 2.5 help integration with Hive 1.x. It may require either temporarily blessing the fork, or more jar surgery to un-uberize the hive-exec jar or something.;;;","03/Apr/17 11:25;omalley;I should also say here that the Hive community is willing to help. We are in the process of rolling releases so if Spark needs a change,  we can work together to get this done.;;;","03/Apr/17 13:16;omalley;It is against Apache policy to release binaries that aren't part of your project.;;;","03/Apr/17 13:37;srowen;Alrighty, you can leave the status for now, but generally committers set Blocker. I'm not entirely clear this blocks a release, not yet.

You're absolutely right, but, the hive fork with binaries and source is part of this project. At least, that's the idea. For example, this is notionally voted on and released with each Spark release, but the binary/source of this fork project isn't separately, explicitly, voted on and separately released. I think that should occur for avoidance of doubt, that this is a blessed artifact of the Spark project. Would this answer your process and policy concerns about the release? It's not pretty but I think that's within the law.

Of course, it's no answer in the long term. The goal is to not have to use the fork at all. If Hive packaging changes are already in place to make it unnecessary, great (is that all there is to it, everyone?) I don't know if that presents a solution for earlier versions of Hive. This fork thing may persist in existing branches, but it has to at least be released and used in a proper way. This may need fixes right now.;;;","04/Apr/17 14:23;srowen;[~marmbrus] as release manager of the moment, I suggest we actually formally vote on release the org.spark-project.hive artifact, as I'm not clear we ever did formally. That much seems like a must-have. I don't know that it requires re-releasing the artifacts, but at least having a meaningful review of what it is, and agreeing (or not) that it's what the PMC wants to release, would I believe resolve doubts about the legitimacy of that artifact.

There's still more to do no doubt, to get rid of the fork. This might include seeing if Hive 1.2.x can provide an un-uberized artifact.;;;","05/Apr/17 13:41;stevel@apache.org;# the ugliness need to inset the spark thrift stuff under the hive thrift stuff is obsolete, can be cut entirely.
# with the shading of kryo not needed, an unshaded hive *may* work. I forget which troublespots there were last time, probably the usual suspects: jackson, guava, etc.
# Hive 1.2.x refuses to work with Hadoop 3; it considers that an unsupported version. For basic client-side testing, you can build Hadoop 3 with a fake version (e..g {{mvn install -DskipShade -Ddeclared.hadoop.version=2.11}}, but as hadoop version is something which NN/DNs care about, not something that's really going to work in real systems. Presumably later hive versions will address that.

If hive take over ownership of the spark 1.2.1-spark branch, this could be done first simply by pulling the spark branch into the Hive repo as a branch, defining the artifact naming properly and releasing it. If that is done, before any release of that 1.2.x branch is done, there's a couple of outstanding PRs to pull in (groovy version for security reasons, ... ).. A quick import & re-release would be the fast way to get this out as an asf-approved binary;;;","05/Apr/17 18:22;rdblue;+1 for a release of the Spark fork from the Hive community. While the reasons for the fork appear to be fixed in the latest version, there's a lot of work to do to get Spark on a newer Hive version. And for patch releases like 2.0.3 and 2.1.1, I don't think updating Hive is an option.

I'm also all for getting master on a real Hive release. A release of alternate Hive binaries was inappropriate. I think that if a third-party organization had done the same, it would be entirely reasonable to treat it as a trademark violation and ask them to stop.

http://www.apache.org/foundation/marks/faq/#products;;;","05/Apr/17 21:48;rxin;Yes this is really important. The proper way to do this is to publish a proper version of Hive with the right dependency declared (rather than including all the dependencies in a uber jar). Looks like there are broad support to do this. I'm going to create a JIRA ticket on Hive and add a dependency on this.

This ticket will depend on that.;;;","05/Apr/17 22:05;rxin;I've created a ticket on the Hive side to publish 1.2.x: https://issues.apache.org/jira/browse/HIVE-16391

Until that is resolved, I also wonder if there are other things we should do. For example, vote on the current fork to rectify it?

;;;","10/Apr/17 14:05;stevel@apache.org;One thing I do recall as trouble here was that ivy resolution was different from mvns, and fixing up all the transitives was a troublespot. Patches here need to be tested against SBT and maven —as jenkins only does SBT, the mvn builds will have to be manual. I don't remember which specific dependency was the problem.;;;","12/Apr/17 00:54;holden;Would it possible make sense to untarget this from the maintenance releases (1.6.X, 2.0.X, 2.1.X) and instead focus on the future versions?;;;","12/Apr/17 00:56;rxin;There are no currently targeted version, are there?
;;;","12/Apr/17 16:51;holden;Oh right, sorry I was misreading the intent of Affects Version/s.;;;","05/Mar/18 14:15;yumwang;How about upgrade Hive directly to 2.3.2. In fact, I've completed the initial work and have been running for a few days.
 [https://github.com/apache/spark/pull/20659];;;","01/Jun/18 16:46;felixcheung;Prefer newer Hive also;;;","04/Jun/18 03:44;jerryshao;What is our plan to to fix this issue, are we going to use new Hive version, or we are still stick to 1.2?

If we're still stick to 1.2, [~stevel@apache.org] and I will take this issue and make the ball rolling in Hive community.;;;","04/Jun/18 17:25;stevel@apache.org;I think you could split things into two

# a modified hive 1.2.1.x for hadoop 3, with a new package name in the maven builds. (joy, profiles!) and some work with the hive team to get this officially published by them. Strength: easy for people to backport into shipping 2.2, 2.3 builds just by changing the POM

# the bigger move to Hive 2. This will be the best for future, but is bound to have more surprises. There's even the possibility that the hive team might have to make some changes too, which isn't impossible if the timelines line up.;;;","05/Jun/18 03:00;jerryshao;OK, for the 1st, I've already started working on it locally. Looks like it is not a big change, only some POM changes are enough, I will submit a patch to Hive community.;;;","23/Jun/18 15:52;gurwls223;Hi all, what do you guys think about replacing it to Hive 2.3.x in the near future (like Spark 3.0.0) given SPARK-23710, and keeping the fork for now?

Looks [~q79969786] completed the initial try at SPARK-23710 and now it sounds pretty much feasible as an option now although it sounds there are still some investigations; however, I believe that we can focus on getting through if we have the explicit plan here. I think we are mostly all positive on this option as a final goal anyway but I felt like we need to make sure on this.
If the above can be set as the goal for this JIRA to get rid of the fork completely, \*I personally think\* Hive side also can focus on landing other fixes to the more resent versions without diverting the efforts to maintain an old branch.

Until then, I think we could probably consider keeping the fork for now and landing some minor fixes if there're some strong reasons for it. For example, Hadoop 3 support is blocked by one liner fix in the fork. \*I personally think\* it is the easiest way to land this fix into the fork. I believe this is pretty reasonable.

What do you guys think about this?

;;;","25/Jun/18 03:52;gurwls223;[~owen.omalley] and [~rxin], what do you think about the suggestion above? I tried to check all other contexts hard at all my best and ^ was my current conclusion to get through this issue mostly smoothly and easily. ;;;","29/Jun/18 07:45;gurwls223;Would you guys please give some thought on this when you guys are available?;;;","03/Jul/18 17:19;gurwls223;kindly ping [~owen.omalley] and [~rxin]. I would like to make a progress further on this since it's blocked for a while but it's pretty important to make up for this affair. ;;;","11/Jul/18 02:45;gurwls223;Hey [~owen.omalley] and [~rxin], I know I see many sensitive things for example the policy stuff frankly; however, this one needs some input from you guys before proceeding further ...;;;","11/Jul/18 17:46;rxin;If you want to try and put together a PR that actually does it, that could work too. But note that it's a lot of work to upgrade execution Hive. Probably 10X more work than Hive publishing the exec jar.

 ;;;","11/Jul/18 17:59;gurwls223;I was thinking we target it for 3.0.0 (otherwise 4.0.0 might make sense ... ). It might be a lot of work indeed but I believe this is what we should do as a final goal which we should do anyway sometime ...

Untill then, I wanted to propose to keep the fork as a temporary solution until 3.0.0 and we target to upgrade it to 2.3.x in 3.0.0 as the goal and target version ... Branch-2.4 will be cut out soon and I think we would go for 3.0.0 for the next release if I am not mistaken.;;;","11/Jul/18 18:01;rxin;Yea you can try and see how difficult it is.

 ;;;","11/Jul/18 18:16;gurwls223;[~rxin], there was an initial try above already though which at least made the regression tests we wrote so far passed. I talked with [~q79969786] before and she's willing to finish this. For this, I need more supports from you and other guys to go this way ..

I get your point too on the other hand. So, do you think we should rather not explicitly target it since it's pretty difficult and we should better let Hive publish 1.2.x first rather then keeping the fork since it's unclear if we make it in 3.0.0?;;;","12/Jul/18 02:18;gurwls223;I am asking this to set the goal for this JIRA as of the current status and make a progress on this. I left some comments here because to me it looked [~q79969786]'s try is kind of a new fact arrived here to be considered.

If publishing Hive is still preferred to get through here for any reason, I will help go with Saisai's patch in HIVE-16391. If keeping the fork and upgrade could be set as a goal for now, I will try to help go with Yumming's way and make a fix to the fork.

Which one do you prefer?;;;","12/Jul/18 06:04;felixcheung;How like will there be a hive release? HIVE-16391 is still open?

Stay with hive 1.2 will slowly become a big problem for us within a few months...;;;","15/Jul/18 04:29;gurwls223;I think we are unclear about how we are going to deal with this and it's been left open for a while ..

[~rxin], do you maybe have some preference in [my comment above|https://issues.apache.org/jira/browse/SPARK-20202?focusedCommentId=16541034&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16541034]?

1. Go with Saisai's patch in HIVE-16391
  - Publishing Hive 1.2.x could be easier but will give some overhead to Hive side (e.g., maintaining the old branches, for example, backports).
  - If I understood correctly, we have less problems (e.g., policy stuff) if we go publishing Hive 1.2.x HIVE-16391

2. Target the upgrade with [~q79969786]'s fix, and add some fixes to our current fork when there's strong reasons
  - It is difficult but [~q79969786] made and completed an initial try about the upgrade. It still need some further investigation (e.g., see [SPARK-23710|https://issues.apache.org/jira/browse/SPARK-23710]) but the try made the regression tests passed at least. She's willing to finish this.
  - If we miss the Hive upgrade to 2.3.x in Spark 3.0.0, we should probably target 4.0.0 with upper version of Hive, which I guess make this upgrade even harder.
  - Looks we implicitly agree upon this should be the final goal in the long term.

See also [~stevel@apache.org]'s [comment above|https://issues.apache.org/jira/browse/SPARK-20202?focusedCommentId=16500560&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16500560].

I am re-raising and giving some refreshes here because I personally see:

- Few facts arrived here since the JIRA was open. So, it looked to me it might be better we consider the possible options again.
- Looks we are quite unclear on this about how we should get through this to me.
- To me, I am sure we need to share and feel in the same way for this JIRA and, it looks I need some more supports from you guys before we go ahead because it'd be a kind of not easily revertible changes. 
- Branch-2.4 will be cut out soon and we will go for Spark 3.0.0 if I am not mistaken.

I know there are many sensitive things going on here; however, please kindly consider and give some inputs. I am sure we all feel that we should resolve this.
Lastly, FWIW, I am doing this on my own rather individually if it matters to anyone in any case.;;;","19/Nov/19 22:55;dongjoon;Hi, All.
I set the target version to `3.1.0`. Please join the discussion if you have any concerns.
- https://lists.apache.org/thread.html/eca4e55c717f35f41c029e227fa9be0a7ee2c8a6f378fcce8f9fd4ff@%3Cdev.spark.apache.org%3E;;;","03/Oct/20 22:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29936;;;","03/Oct/20 22:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/29936;;;","05/Oct/20 22:30;dongjoon;Issue resolved by pull request 29936
[https://github.com/apache/spark/pull/29936];;;","08/Oct/20 03:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29973;;;","08/Oct/20 03:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29973;;;","08/Oct/20 03:01;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/29973;;;"
Flaky Test: org.apache.spark.rdd.LocalCheckpointSuite,SPARK-20200,13061047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,ueshin,ueshin,03/Apr/17 07:23,15/Jun/17 16:09,14/Jul/23 06:30,15/Jun/17 16:09,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.2,2.2.0,,,Spark Core,,,,,0,flaky-test,,,,,,,,"This test failed recently here:
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.6/2909/testReport/junit/org.apache.spark.rdd/LocalCheckpointSuite/missing_checkpoint_block_fails_with_informative_message/

Dashboard
https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.rdd.LocalCheckpointSuite&test_name=missing+checkpoint+block+fails+with+informative+message

Error Message
{code}
Collect should have failed if local checkpoint block is removed...
{code}

{code}
org.scalatest.exceptions.TestFailedException: Collect should have failed if local checkpoint block is removed...
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:495)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
      at org.scalatest.Assertions$class.fail(Assertions.scala:1328)
      at org.scalatest.FunSuite.fail(FunSuite.scala:1555)
      at org.apache.spark.rdd.LocalCheckpointSuite$$anonfun$16.apply$mcV$sp(LocalCheckpointSuite.scala:173)
      at org.apache.spark.rdd.LocalCheckpointSuite$$anonfun$16.apply(LocalCheckpointSuite.scala:155)
      at org.apache.spark.rdd.LocalCheckpointSuite$$anonfun$16.apply(LocalCheckpointSuite.scala:155)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
      at org.apache.spark.rdd.LocalCheckpointSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(LocalCheckpointSuite.scala:27)
      at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
      at org.apache.spark.rdd.LocalCheckpointSuite.runTest(LocalCheckpointSuite.scala:27)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$class.run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
      at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
      at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$class.run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
{code}
",,apachespark,cloud_fan,ueshin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 15 16:09:17 UTC 2017,,,,,,,,,,"0|i3d4an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/May/17 01:21;yumwang;Can you check it again? it works for me.
{code}
build/sbt  ""test-only org.apache.spark.rdd.LocalCheckpointSuite""
{code};;;","15/Jun/17 09:00;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18314;;;","15/Jun/17 16:09;cloud_fan;Issue resolved by pull request 18314
[https://github.com/apache/spark/pull/18314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the inconsistency in table/function name conventions in SparkSession.Catalog APIs,SPARK-20198,13061035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,03/Apr/17 06:51,04/Apr/17 10:59,14/Jul/23 06:30,04/Apr/17 10:59,2.0.2,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Observed by @felixcheung , in `SparkSession`.`Catalog` APIs, we have different conventions/rules for table/function identifiers/names. Most APIs accept the qualified name (i.e., `databaseName`.`tableName` or `databaseName`.`functionName`). However, the following five APIs do not accept it.  
- def listColumns(tableName: String): Dataset[Column]
- def getTable(tableName: String): Table 
- def getFunction(functionName: String): Function
- def tableExists(tableName: String): Boolean
- def functionExists(functionName: String): Boolean

It is desirable to make them more consistent with the other Catalog APIs, updates the function/API comments and adds the `@params` to clarify the inputs we allow.
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20188,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 10:59:22 UTC 2017,,,,,,,,,,"0|i3d47z:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"03/Apr/17 06:53;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17518;;;","04/Apr/17 10:59;cloud_fan;Issue resolved by pull request 17518
[https://github.com/apache/spark/pull/17518];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRAN check fail with package installation ,SPARK-20197,13060997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,02/Apr/17 20:09,07/Apr/17 18:19,14/Jul/23 06:30,03/Apr/17 02:47,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,," Failed -------------------------------------------------------------------------
  1. Failure: No extra files are created in SPARK_HOME by starting session and making calls (@test_sparkSQL.R#2858)
  length(sparkRFilesBefore) > 0 isn't true.

",,apachespark,felixcheung,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20257,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 03 02:46:30 UTC 2017,,,,,,,,,,"0|i3d3zj:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"02/Apr/17 22:42;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17513;;;","02/Apr/17 22:43;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17514;;;","02/Apr/17 22:45;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17515;;;","02/Apr/17 23:07;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17516;;;","03/Apr/17 02:46;felixcheung;merged https://github.com/apache/spark/pull/17515 to branch-2.1
will need to follow up on master separately.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python to add catalog API for refreshByPath,SPARK-20196,13060995,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,02/Apr/17 19:33,06/Apr/17 16:13,14/Jul/23 06:30,06/Apr/17 16:13,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,SQL,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 02 19:35:02 UTC 2017,,,,,,,,,,"0|i3d3z3:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"02/Apr/17 19:35;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17512;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR to add createTable catalog API and deprecate createExternalTable,SPARK-20195,13060994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,02/Apr/17 19:01,06/Apr/17 16:15,14/Jul/23 06:30,06/Apr/17 16:15,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,SQL,,,,0,,,,,,,,,"Only naming differences for clarity, functionality is already supported with and without path parameter.",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 02 19:08:02 UTC 2017,,,,,,,,,,"0|i3d3yv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"02/Apr/17 19:08;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17511;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RackResolver not correctly being overridden in YARN tests,SPARK-20191,13060944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,02/Apr/17 00:27,17/May/20 18:14,14/Jul/23 06:30,04/Apr/17 18:48,2.0.3,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"YARN tests currently try to override YARN's RackResolver, but that class self-initializes the first time it's called, storing state in static variables and ignoring any further config params that might override the initial behavior.

So we need a better solution for Spark tests, so that tests such as {{LocalityPlacementStrategySuite}} don't flood the DNS server with requests (making the test really slow in certain environments).",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 02 00:31:03 UTC 2017,,,,,,,,,,"0|i3d3nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/17 00:31;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17508;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'/applications/[app-id]/jobs' in rest api,status should be [running|succeeded|failed|unknown]",SPARK-20190,13060943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,guoxiaolongzte,guoxiaolongzte,guoxiaolongzte,02/Apr/17 00:06,04/Apr/17 08:56,14/Jul/23 06:30,04/Apr/17 08:56,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Documentation,,,,,0,,,,,,,,,"'/applications/[app-id]/jobs' in rest api.status should be'[running|succeeded|failed|unknown]'.
now status is '[complete|succeeded|failed]'.

but '/applications/[app-id]/jobs?status=complete' the server return 'HTTP ERROR 404'.

Added '?status=running' and '?status=unknown'.

code ：
public enum JobExecutionStatus {
  RUNNING,
  SUCCEEDED,
  FAILED,
  UNKNOWN;

",,apachespark,guoxiaolongzte,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 08:56:41 UTC 2017,,,,,,,,,,"0|i3d3nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/17 00:22;apachespark;User 'guoxiaolongzte' has created a pull request for this issue:
https://github.com/apache/spark/pull/17507;;;","04/Apr/17 08:56;srowen;Issue resolved by pull request 17507
[https://github.com/apache/spark/pull/17507];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw NullPointerException when HiveThriftServer2 is shutdown,SPARK-20173,13060628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,31/Mar/17 09:34,02/Apr/17 14:41,14/Jul/23 06:30,02/Apr/17 14:40,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Throw NullPointerException when HiveThriftServer2 is shutdown:

========================================================
2017-03-30 11:52:56,355 ERROR Utils: Uncaught exception in thread Thread-2
java.lang.NullPointerException
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$$anonfun$main$1.apply$mcV$sp(HiveThriftServer2.scala:85)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:215)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2017-03-30 11:52:56,357 INFO ShutdownHookManager: Shutdown hook called",,apachespark,ouyangxc.zte,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 02 14:40:53 UTC 2017,,,,,,,,,,"0|i3d1pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/17 09:37;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17493;;;","31/Mar/17 09:49;ouyangxc.zte;+1;;;","31/Mar/17 11:02;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17496;;;","02/Apr/17 14:40;srowen;Issue resolved by pull request 17496
[https://github.com/apache/spark/pull/17496];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event log without read permission should be filtered out before actually reading it,SPARK-20172,13060627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,31/Mar/17 09:28,20/Apr/17 23:02,14/Jul/23 06:30,20/Apr/17 23:02,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"In the current Spark's HistoryServer, we expected to check file permission when listing all the files, and filter out this files with no read permission. That was not worked because we actually doesn't check the access permission, so we defer this permission check until reading files, that is not necessary and the exception is printed out in every 10 seconds by default.

So to avoid this problem we should add a access check logic in listing files.",,apachespark,jerryshao,wzheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 31 09:48:02 UTC 2017,,,,,,,,,,"0|i3d1pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/17 09:48;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17495;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnalysisException not tolerant of null query plan,SPARK-20164,13060497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kunalkhamar,kunalkhamar,kunalkhamar,30/Mar/17 21:10,31/Mar/17 22:51,14/Jul/23 06:30,31/Mar/17 16:23,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"The query plan in an AnalysisException may be null when an AnalysisException object is serialized and then deserialized, since plan is marked @transient. Or when someone throws an AnalysisException with a null query plan (which should not happen).
def getMessage is not tolerant of this and throws a NullPointerException, leading to loss of information about the original exception.
The fix is to add a null check in getMessage.",,apachespark,kunalkhamar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 21:14:03 UTC 2017,,,,,,,,,,"0|i3d0wf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/17 21:14;apachespark;User 'kunalkhamar' has created a pull request for this issue:
https://github.com/apache/spark/pull/17486;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support complete Catalog API in R,SPARK-20159,13060426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,30/Mar/17 16:23,02/Apr/17 19:00,14/Jul/23 06:30,02/Apr/17 19:00,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"As an user, I'd like to have access to catalog API to manage and view databases, tables, functions and metadata",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 16:29:03 UTC 2017,,,,,,,,,,"0|i3d0gn:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"30/Mar/17 16:29;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Java String toLowerCase ""Turkish locale bug"" causes Spark problems",SPARK-20156,13060297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,serkan_tas,serkan_tas,30/Mar/17 08:30,08/Nov/18 10:04,14/Jul/23 06:30,10/Apr/17 19:12,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Shell,,,,,0,,,,,,,,,"If the regional setting of the operation system is Turkish, the famous java locale problem occurs (https://jira.atlassian.com/browse/CONF-5931 or https://issues.apache.org/jira/browse/AVRO-1493). 
e.g : 

""SERDEINFO"" lowers to ""serdeınfo""
""uniquetable"" uppers to ""UNİQUETABLE""

work around : 
add -Duser.country=US -Duser.language=en to the end of the line 
SPARK_SUBMIT_OPTS=""$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true""

in spark-shell.sh
","Ubunutu 16.04
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_121)",apachespark,serkan_tas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20361,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/17 08:39;serkan_tas;sprk_shell.txt;https://issues.apache.org/jira/secure/attachment/12861198/sprk_shell.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 10:04:50 UTC 2018,,,,,,,,,,"0|i3cznz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/17 08:35;serkan_tas;console output before setting locale;;;","30/Mar/17 09:36;srowen;Yeah, the problem is that people call {{toLowerCase}} or {{toUpperCase}} without a {{Locale.ENGLISH}} argument and it makes things locale-dependent that definitely should not be. Really, every single instance of these calls should have such an argument. ;;;","04/Apr/17 08:49;srowen;I retitled this; please refer to things like http://mattryall.net/blog/2009/02/the-infamous-turkish-locale-bug for back-story on this particular issue. 

I believe the best change is to make all case-changing operations use Locale.ROOT.;;;","04/Apr/17 08:53;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17527;;;","10/Apr/17 19:12;srowen;Issue resolved by pull request 17527
[https://github.com/apache/spark/pull/17527];;;","10/Apr/17 19:43;serkan_tas;Thank you Sean, it was more than i expected i think.;;;","17/Apr/17 01:41;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17655;;;","08/Nov/18 10:04;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/22975;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cloning SessionState does not clone streaming query listeners,SPARK-20147,13060187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kunalkhamar,kunalkhamar,kunalkhamar,29/Mar/17 22:17,02/Jun/17 21:17,14/Jul/23 06:30,02/Jun/17 21:17,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Cloning session should clone StreamingQueryListeners registered on the StreamingQueryListenerBus.
Similar to SPARK-20048, https://github.com/apache/spark/pull/17379",,codingcat,kunalkhamar,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 02 21:17:21 UTC 2017,,,,,,,,,,"0|i3cyzj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"02/Jun/17 21:17;marmbrus;Fixed by https://github.com/apache/spark/pull/17379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""SELECT * FROM range(1)"" works, but ""SELECT * FROM RANGE(1)"" doesn't",SPARK-20145,13060058,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,samelamin,juliuszsompolski,juliuszsompolski,29/Mar/17 14:47,04/Apr/17 00:17,14/Jul/23 06:30,04/Apr/17 00:17,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Executed at clean tip of the master branch, with all default settings:

scala> spark.sql(""SELECT * FROM range(1)"")
res1: org.apache.spark.sql.DataFrame = [id: bigint]

scala> spark.sql(""SELECT * FROM RANGE(1)"")
org.apache.spark.sql.AnalysisException: could not resolve `RANGE` to a table-valued function; line 1 pos 14
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.ResolveTableValuedFunctions$$anonfun$apply$1.applyOrElse(ResolveTableValuedFunctions.scala:126)
  at org.apache.spark.sql.catalyst.analysis.ResolveTableValuedFunctions$$anonfun$apply$1.applyOrElse(ResolveTableValuedFunctions.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)
...

I believe it should be case insensitive?",,apachespark,bomeng,juliuszsompolski,samelamin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 22:10:03 UTC 2017,,,,,,,,,,"0|i3cy6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/17 19:09;bomeng;From the current code, I can see builtinFunctions is using the exact match for looking up (""range"" as a key is all lowercase). ;;;","29/Mar/17 19:46;samelamin;if no one is picking this up, id love to take it;;;","30/Mar/17 17:13;juliuszsompolski;[~samelamin] sure, go ahead :-).;;;","30/Mar/17 18:43;samelamin;Perfect. Can you assign it  to me please?;;;","30/Mar/17 18:48;bomeng;You do not need to be assigned. Just go ahead provide your solution in PR. ;;;","30/Mar/17 22:10;apachespark;User 'samelamin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17487;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove hardcoded kinesis retry wait and max retries,SPARK-20140,13060014,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yash360@gmail.com,yash360@gmail.com,yash360@gmail.com,29/Mar/17 12:22,03/Jul/17 11:15,14/Jul/23 06:30,16/May/17 22:11,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,DStreams,,,,,0,kinesis,recovery,,,,,,,"The pull requests proposes to remove the hardcoded values for Amazon Kinesis - MIN_RETRY_WAIT_TIME_MS, MAX_RETRIES.

This change is critical for kinesis checkpoint recovery when the kinesis backed rdd is huge.
Following happens in a typical kinesis recovery :
- kinesis throttles large number of requests while recovering
- retries in case of throttling are not able to recover due to the small wait period
- kinesis throttles per second, the wait period should be configurable for recovery

The patch picks the spark kinesis configs from:
- spark.streaming.kinesis.retry.wait.time
- spark.streaming.kinesis.retry.max.attempts",,apachespark,brkyvz,yash360@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 16 22:10:56 UTC 2017,,,,,,,,,,"0|i3cxx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/17 12:23;yash360@gmail.com;Proposing : https://github.com/apache/spark/pull/17467
Please review.;;;","29/Mar/17 13:17;apachespark;User 'yssharma' has created a pull request for this issue:
https://github.com/apache/spark/pull/17467;;;","16/May/17 22:10;brkyvz;resolved by https://github.com/apache/spark/pull/17467;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test: o.a.s.streaming.StreamingContextSuite.SPARK-18560 Receiver data should be deserialized properly,SPARK-20131,13059814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,ueshin,ueshin,28/Mar/17 18:48,13/Apr/17 00:44,14/Jul/23 06:30,13/Apr/17 00:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,DStreams,,,,,0,flaky-test,,,,,,,,"This test failed recently here:
https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/2861/testReport/junit/org.apache.spark.streaming/StreamingContextSuite/SPARK_18560_Receiver_data_should_be_deserialized_properly_/

Dashboard
https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.streaming.StreamingContextSuite&test_name=SPARK-18560+Receiver+data+should+be+deserialized+properly.

Error Message
{code}
latch.await(60L, SECONDS) was false
{code}
{code}
org.scalatest.exceptions.TestFailedException: latch.await(60L, SECONDS) was false
      at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
      at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
      at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$43.apply$mcV$sp(StreamingContextSuite.scala:837)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$43.apply(StreamingContextSuite.scala:810)
      at org.apache.spark.streaming.StreamingContextSuite$$anonfun$43.apply(StreamingContextSuite.scala:810)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
      at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingContextSuite.scala:44)
      at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
      at org.apache.spark.streaming.StreamingContextSuite.runTest(StreamingContextSuite.scala:44)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$class.run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
      at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
      at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingContextSuite.scala:44)
      at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
      at org.apache.spark.streaming.StreamingContextSuite.run(StreamingContextSuite.scala:44)
      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$class.run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
{code}",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 11 18:55:03 UTC 2017,,,,,,,,,,"0|i3cwov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/17 04:47;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17463;;;","11/Apr/17 18:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17610;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset of type option of map does not work,SPARK-20125,13059666,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,28/Mar/17 08:17,28/Mar/17 22:14,14/Jul/23 06:30,28/Mar/17 22:14,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"A simple reproduce:
{code}
case class ABC(m: Option[scala.collection.Map[Int, Int]])
val ds = Seq(ABC(Some(Map(1 -> 1)))).toDS() 
ds.collect()
{code}",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 08:25:03 UTC 2017,,,,,,,,,,"0|i3cvrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/17 08:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17454;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"$SPARK_HOME variable might have spaces in it(e.g. $SPARK_HOME=/home/spark build/spark), then build spark failed.",SPARK-20123,13059649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,28/Mar/17 07:07,02/Apr/17 14:31,14/Jul/23 06:30,02/Apr/17 14:31,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Build,,,,,0,,,,,,,,,"If $SPARK_HOME or $FWDIR variable contains spaces, then use ""./dev/make-distribution.sh --name custom-spark --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn"" build spark will failed.",,apachespark,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 02 14:31:28 UTC 2017,,,,,,,,,,"0|i3cvo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/17 07:23;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17452;;;","02/Apr/17 14:31;srowen;Issue resolved by pull request 17452
[https://github.com/apache/spark/pull/17452];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test: org.apache.spark.sql.execution. DataSourceScanExecRedactionSuite,SPARK-20119,13059619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,28/Mar/17 04:56,28/Mar/17 07:38,14/Jul/23 06:30,28/Mar/17 07:38,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,Failed in https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-maven-hadoop-2.6/2895/testReport/org.apache.spark.sql.execution/DataSourceScanExecRedactionSuite/treeString_is_redacted/,,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 04:58:13 UTC 2017,,,,,,,,,,"0|i3cvhj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"28/Mar/17 04:58;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17448;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
codegen bug surfaced by GraphFrames issue 165,SPARK-20111,13059534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,josephkb,josephkb,27/Mar/17 20:37,27/Mar/17 21:59,14/Jul/23 06:30,27/Mar/17 21:59,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"In GraphFrames, test {{test(""named edges"")}} in {{PatternMatchSuite}} surfaces a SQL codegen bug.
This is described in https://github.com/graphframes/graphframes/issues/165

Summary
* The unit test does a simple motif query on a graph.  Essentially, this means taking 2 DataFrames, doing a few joins, selecting 2 columns, and collecting the (tiny) DataFrame.
* The test runs, but codegen fails.  See the linked GraphFrames issue for the stacktrace.

To reproduce this:
* Check out GraphFrames https://github.com/graphframes/graphframes
* Run {{sbt assembly}} to compile it and run tests

Copying [~felixcheung]'s comment from the GraphFrames issue 165:
{quote}
Seems like codegen bug; it looks like at least 2 issues:
1. At L472, inputadapter_value is not defined within scope
2. inputadapter_value is an InternalRow, for this statement to work
{{bhj_primitiveA = inputadapter_value;}}
it should be
{{bhj_primitiveA = inputadapter_value.getLong(0);}}
{quote}",,hvanhovell,josephkb,timhunter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19512,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 21:58:40 UTC 2017,,,,,,,,,,"0|i3cuyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/17 20:43;timhunter;As Spark SQL is making more and more forays into code generation, I have been wondering if it would make sense to start adopting practical compiler technologies, such as generating first an intermediate representation, instead of doing string manipulation as we currently do. This is of course much beyond the scope of this particular ticket.;;;","27/Mar/17 21:41;hvanhovell;[~josephkb] this might be already fixed in the latest master/2.1 (see SPARK-19512). Did you try this on the latest master?;;;","27/Mar/17 21:58;josephkb;Yep, you're right.  Closing now.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix two minor build script issues blocking 2.1.1 RC + master snapshot builds,SPARK-20102,13059243,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,26/Mar/17 21:15,27/Mar/17 17:25,14/Jul/23 06:30,27/Mar/17 17:25,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Build,,,,,0,,,,,,,,,"The master snapshot publisher builds are currently broken due to two minor build issues:

1. For unknown reasons, the LFTP {{mkdir -p}} command started throwing errors when the remote FTP directory already exists. To work around this, we should update the script to ignore errors.
2. The PySpark setup.py file references a non-existent module, causing Python packaging to fail.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 17:25:25 UTC 2017,,,,,,,,,,"0|i3ct67:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"26/Mar/17 21:21;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17437;;;","27/Mar/17 17:25;joshrosen;Fixed for 2.1.1 and master.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consolidate SessionState construction,SPARK-20100,13059210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,26/Mar/17 09:20,28/Mar/17 02:08,14/Jul/23 06:30,28/Mar/17 02:08,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The current SessionState initialization path is quite complex. A part of the creation is done in the SessionState companion objects, a part of the creation is one inside the SessionState class, and a part is done by passing functions.

The proposal is to consolidate the SessionState initialization into a builder class. This SessionState will not do any initialization and just becomes a place holder for the various Spark SQL internals. The advantages of this approach are the following:
- SessionState initialization is less dispersed. The builder should be a one stop shop.
- This provides us with a start for removing the HiveSessionState. Removing the hive session state would also require us to move resource loading into a separate class, and to (re)move metadata hive.
- It is easier to customize the Spark Session. You just need to create a custom version of the builder. I will add hooks to make this easier. Opening up these API's will happen at a later point.",,apachespark,cloud_fan,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 02:08:08 UTC 2017,,,,,,,,,,"0|i3csyv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"26/Mar/17 10:08;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17433;;;","28/Mar/17 02:08;cloud_fan;Issue resolved by pull request 17433
[https://github.com/apache/spark/pull/17433];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataType's typeName method returns with 'StructF' in case of StructField,SPARK-20098,13059141,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,szalai1,szalai1,szalai1,25/Mar/17 14:00,12/Dec/22 18:10,14/Jul/23 06:30,10/Sep/17 08:48,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,PySpark,,,,,0,,,,,,,,,"Currently, if you want to get the name of a DateType and the DateType is a `StructField`, you get `StructF`. 

http://spark.apache.org/docs/2.1.0/api/python/_modules/pyspark/sql/types.html ",,apachespark,szalai1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 10 08:52:02 UTC 2017,,,,,,,,,,"0|i3csjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/17 18:17;apachespark;User 'szalai1' has created a pull request for this issue:
https://github.com/apache/spark/pull/17435;;;","10/Sep/17 08:48;gurwls223;Issue resolved by pull request 17435
[https://github.com/apache/spark/pull/17435];;;","10/Sep/17 08:52;gurwls223;[~jerryshao], I am sorry for asking this again but would you mind if i ask set the role for this user - https://issues.apache.org/jira/secure/ViewProfile.jspa?name=szalai1 and assign this user to this JIRA?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose the real queue name not null while using --verbose,SPARK-20096,13059130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,25/Mar/17 11:43,30/Mar/17 15:11,14/Jul/23 06:30,30/Mar/17 15:11,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Submit,,,,,0,,,,,,,,,"while submit apps with -v or --verbose， we can print the right queue name, but if we set a queue name with `spark.yarn.queue` by --conf or in the spark-default.conf, we just got `null` for the queue in Parsed arguments.
{code}
bin/spark-shell -v --conf spark.yarn.queue=thequeue
Using properties file: /home/hadoop/spark-2.1.0-bin-apache-hdp2.7.3/conf/spark-defaults.conf
....
Adding default property: spark.yarn.queue=default
Parsed arguments:
  master                  yarn
  deployMode              client
  ...
  queue                   null
  ....
  verbose                 true
Spark properties used, including those specified through
 --conf and those from the properties file /home/hadoop/spark-2.1.0-bin-apache-hdp2.7.3/conf/spark-defaults.conf:
  spark.yarn.queue -> thequeue
  ....
{code}",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 30 15:11:18 UTC 2017,,,,,,,,,,"0|i3csh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/17 11:45;srowen;got null where? This needs more detail.;;;","25/Mar/17 11:53;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/17430;;;","25/Mar/17 12:29;Qin Yao;[~sowen] example added;;;","30/Mar/17 15:11;srowen;Issue resolved by pull request 17430
[https://github.com/apache/spark/pull/17430];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should Prevent push down of IN subquery to Join operator,SPARK-20094,13059115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,25/Mar/17 07:32,28/Mar/17 11:43,14/Jul/23 06:30,28/Mar/17 11:43,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"ReorderJoin collects all predicates and try to put them into join condition when creating ordered join. If a predicate with an IN subquery is in a join condition instead of a filter condition, `RewritePredicateSubquery.rewriteExistentialExpr` would fail to convert the subquery to an ExistenceJoin, and thus result in error.

For example, tpcds q45 fails due to the above reason:
{noformat}
spark-sql> explain codegen
         > SELECT
         >   ca_zip,
         >   ca_city,
         >   sum(ws_sales_price)
         > FROM web_sales, customer, customer_address, date_dim, item
         > WHERE ws_bill_customer_sk = c_customer_sk
         >   AND c_current_addr_sk = ca_address_sk
         >   AND ws_item_sk = i_item_sk
         >   AND (substr(ca_zip, 1, 5) IN
         >   ('85669', '86197', '88274', '83405', '86475', '85392', '85460', '80348', '81792')
         >   OR
         >   i_item_id IN (SELECT i_item_id
         >   FROM item
         >   WHERE i_item_sk IN (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)
         >   )
         > )
         >   AND ws_sold_date_sk = d_date_sk
         >   AND d_qoy = 2 AND d_year = 2001
         > GROUP BY ca_zip, ca_city
         > ORDER BY ca_zip, ca_city
         > LIMIT 100;
17/03/25 15:27:02 ERROR SparkSQLDriver: Failed in [explain codegen              
SELECT
  ca_zip,
  ca_city,
  sum(ws_sales_price)
FROM web_sales, customer, customer_address, date_dim, item
WHERE ws_bill_customer_sk = c_customer_sk
  AND c_current_addr_sk = ca_address_sk
  AND ws_item_sk = i_item_sk
  AND (substr(ca_zip, 1, 5) IN
  ('85669', '86197', '88274', '83405', '86475', '85392', '85460', '80348', '81792')
  OR
  i_item_id IN (SELECT i_item_id
  FROM item
  WHERE i_item_sk IN (2, 3, 5, 7, 11, 13, 17, 19, 23, 29)
  )
)
  AND ws_sold_date_sk = d_date_sk
  AND d_qoy = 2 AND d_year = 2001
GROUP BY ca_zip, ca_city
ORDER BY ca_zip, ca_city
LIMIT 100]
java.lang.UnsupportedOperationException: Cannot evaluate expression: list#1 []
	at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:224)
	at org.apache.spark.sql.catalyst.expressions.ListQuery.doGenCode(subquery.scala:262)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.catalyst.expressions.In$$anonfun$3.apply(predicates.scala:199)
	at org.apache.spark.sql.catalyst.expressions.In$$anonfun$3.apply(predicates.scala:199)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.expressions.In.doGenCode(predicates.scala:199)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.catalyst.expressions.Or.doGenCode(predicates.scala:379)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.getJoinCondition(BroadcastHashJoinExec.scala:174)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:199)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:215)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.consume(SortMergeJoinExec.scala:36)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:601)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:329)
	at org.apache.spark.sql.execution.debug.package$$anonfun$codegenString$3.apply(package.scala:66)
	at org.apache.spark.sql.execution.debug.package$$anonfun$codegenString$3.apply(package.scala:62)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.sql.execution.debug.package$.codegenString(package.scala:62)
	at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:116)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:117)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
java.lang.UnsupportedOperationException: Cannot evaluate expression: list#1 []
	at org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:224)
	at org.apache.spark.sql.catalyst.expressions.ListQuery.doGenCode(subquery.scala:262)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.catalyst.expressions.In$$anonfun$3.apply(predicates.scala:199)
	at org.apache.spark.sql.catalyst.expressions.In$$anonfun$3.apply(predicates.scala:199)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.sql.catalyst.expressions.In.doGenCode(predicates.scala:199)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.catalyst.expressions.Or.doGenCode(predicates.scala:379)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.getJoinCondition(BroadcastHashJoinExec.scala:174)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:199)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:215)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:68)
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.consume(SortMergeJoinExec.scala:36)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.doProduce(SortMergeJoinExec.scala:601)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.produce(SortMergeJoinExec.scala:36)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:46)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:36)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:600)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:329)
	at org.apache.spark.sql.execution.debug.package$$anonfun$codegenString$3.apply(package.scala:66)
	at org.apache.spark.sql.execution.debug.package$$anonfun$codegenString$3.apply(package.scala:62)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at org.apache.spark.sql.execution.debug.package$.codegenString(package.scala:62)
	at org.apache.spark.sql.execution.command.ExplainCommand.run(commands.scala:116)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:335)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:247)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:117)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{noformat}",,apachespark,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 25 07:37:02 UTC 2017,,,,,,,,,,"0|i3csdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/17 07:37;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17428;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not create new SparkContext in SparkR createSparkContext,SPARK-20088,13059086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,25/Mar/17 00:31,27/Mar/17 15:54,14/Jul/23 06:30,27/Mar/17 15:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"In the implementation of {{createSparkContext}}, we are calling 
{code}
 new JavaSparkContext()
{code}",,apachespark,falaki,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 27 15:54:39 UTC 2017,,,,,,,,,,"0|i3cs7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/17 00:35;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/17423;;;","27/Mar/17 15:54;mengxr;Issue resolved by pull request 17423
[https://github.com/apache/spark/pull/17423];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
issue with pyspark 2.1.0 window function,SPARK-20086,13059042,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,mandarup,mandarup,24/Mar/17 21:31,13/Nov/18 18:30,14/Jul/23 06:30,26/Mar/17 20:48,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,,,,,1,,,,,,,,,"original  post at
[stackoverflow | http://stackoverflow.com/questions/43007433/pyspark-2-1-0-error-when-working-with-window-function]

I get error when working with pyspark window function. here is some example code:

{code:title=borderStyle=solid}
    import pyspark
    import pyspark.sql.functions as sf
    import pyspark.sql.types as sparktypes
    from pyspark.sql import window
    
    sc = pyspark.SparkContext()
    sqlc = pyspark.SQLContext(sc)
    rdd = sc.parallelize([(1, 2.0), (1, 3.0), (1, 1.), (1, -2.), (1, -1.)])
    df = sqlc.createDataFrame(rdd, [""x"", ""AmtPaid""])
    df.show()

{code}

gives:


    |  x|AmtPaid|
    |  1|    2.0|
    |  1|    3.0|
    |  1|    1.0|
    |  1|   -2.0|
    |  1|   -1.0|


next, compute cumulative sum

{code:title=test.py|borderStyle=solid}
    win_spec_max = (window.Window
                    .partitionBy(['x'])
                    .rowsBetween(window.Window.unboundedPreceding, 0)))
    df = df.withColumn('AmtPaidCumSum',
                       sf.sum(sf.col('AmtPaid')).over(win_spec_max))
    df.show()
{code}

gives,


    |  x|AmtPaid|AmtPaidCumSum|
    |  1|    2.0|          2.0|
    |  1|    3.0|          5.0|
    |  1|    1.0|          6.0|
    |  1|   -2.0|          4.0|
    |  1|   -1.0|          3.0|

next, compute cumulative max,

{code}
    df = df.withColumn('AmtPaidCumSumMax', sf.max(sf.col('AmtPaidCumSum')).over(win_spec_max))

    df.show()
{code}

gives error log

{noformat}
     Py4JJavaError: An error occurred while calling o2609.showString.


with traceback:


    Py4JJavaErrorTraceback (most recent call last)
    <ipython-input-215-3106d06b6e49> in <module>()
    ----> 1 df.show()

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         """"""
        317         if isinstance(truncate, bool) and truncate:
    --> 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -> 1133             answer, self.gateway_client, self.target_id, self.name)
       1134 
       1135         for temp_arg in temp_args:

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---> 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     ""An error occurred while calling {0}{1}{2}.\n"".
    --> 319                     format(target_id, ""."", name), value)
        320             else:
        321                 raise Py4JError(
{noformat}

but interestingly enough, if i introduce another change before sencond window operation, say inserting a column then it does not give that error:

{code}
    df = df.withColumn('MaxBound', sf.lit(6.))
    df.show()
{code}


    |  x|AmtPaid|AmtPaidCumSum|MaxBound|
    |  1|    2.0|          2.0|     6.0|
    |  1|    3.0|          5.0|     6.0|
    |  1|    1.0|          6.0|     6.0|
    |  1|   -2.0|          4.0|     6.0|
    |  1|   -1.0|          3.0|     6.0|

{code}
    #then apply the second window operations
    df = df.withColumn('AmtPaidCumSumMax', sf.max(sf.col('AmtPaidCumSum')).over(win_spec_max))
    df.show()
{code}

    |  x|AmtPaid|AmtPaidCumSum|MaxBound|AmtPaidCumSumMax|
    |  1|    2.0|          2.0|     6.0|             2.0|
    |  1|    3.0|          5.0|     6.0|             5.0|
    |  1|    1.0|          6.0|     6.0|             6.0|
    |  1|   -2.0|          4.0|     6.0|             6.0|
    |  1|   -1.0|          3.0|     6.0|             6.0|

I do not understand this behaviour

well, so far so good, but then I try another operation then again get similar error:

{code}
    def _udf_compare_cumsum_sll(x):
        if x['AmtPaidCumSumMax'] >= x['MaxBound']:
            output = 0
        else:
            output = x['AmtPaid']
        return output

    udf_compare_cumsum_sll = sf.udf(_udf_compare_cumsum_sll, sparktypes.FloatType())
    df = df.withColumn('AmtPaidAdjusted', udf_compare_cumsum_sll(sf.struct([df[x] for x in df.columns])))
    df.show()
{code}

gives,

{noformat}
    Py4JJavaErrorTraceback (most recent call last)
    <ipython-input-18-3106d06b6e49> in <module>()
    ----> 1 df.show()

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         """"""
        317         if isinstance(truncate, bool) and truncate:
    --> 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -> 1133             answer, self.gateway_client, self.target_id, self.name)
       1134 
       1135         for temp_arg in temp_args:

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---> 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     ""An error occurred while calling {0}{1}{2}.\n"".
    --> 319                     format(target_id, ""."", name), value)
        320             else:
        321                 raise Py4JError(

    Py4JJavaError: An error occurred while calling o91.showString.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 645, localhost, executor driver): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: AmtPaidCumSum#10
{noformat}    	

I wonder if someone could reproduce this behaviour ...


here is complete log ..

{noformat}
    Py4JJavaErrorTraceback (most recent call last)
    <ipython-input-69-3106d06b6e49> in <module>()
    ----> 1 df.show()

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc in show(self, n, truncate)
        316         """"""
        317         if isinstance(truncate, bool) and truncate:
    --> 318             print(self._jdf.showString(n, 20))
        319         else:
        320             print(self._jdf.showString(n, int(truncate)))

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/java_gateway.pyc in __call__(self, *args)
       1131         answer = self.gateway_client.send_command(command)
       1132         return_value = get_return_value(
    -> 1133             answer, self.gateway_client, self.target_id, self.name)
       1134
       1135         for temp_arg in temp_args:

    /Users/<>/spark-2.1.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw)
         61     def deco(*a, **kw):
         62         try:
    ---> 63             return f(*a, **kw)
         64         except py4j.protocol.Py4JJavaError as e:
         65             s = e.java_exception.toString()

    /Users/<>/.virtualenvs/<>/lib/python2.7/site-packages/py4j/protocol.pyc in get_return_value(answer, gateway_client, target_id, name)
        317                 raise Py4JJavaError(
        318                     ""An error occurred while calling {0}{1}{2}.\n"".
    --> 319                     format(target_id, ""."", name), value)
        320             else:
        321                 raise Py4JError(

    Py4JJavaError: An error occurred while calling o703.showString.
    : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 1817, localhost, executor driver): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: AmtPaidCumSum#2076
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:360)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.immutable.List.foreach(List.scala:381)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.immutable.List.map(List.scala:285)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.bind(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
    	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:353)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:203)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:202)
    	at org.apache.spark.sql.execution.window.AggregateProcessor$.apply(AggregateProcessor.scala:98)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2.org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1(WindowExec.scala:198)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:225)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:222)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.<init>(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:290)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:289)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:99)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	at java.lang.Thread.run(Thread.java:745)
    Caused by: java.lang.RuntimeException: Couldn't find AmtPaidCumSum#2076 in [sum#2299,max#2300,x#2066L,AmtPaid#2067]
    	at scala.sys.package$.error(package.scala:27)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
    	... 62 more

    Driver stacktrace:
    	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
    	at scala.Option.foreach(Option.scala:257)
    	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
    	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
    	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
    	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
    	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
    	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
    	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
    	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
    	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
    	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
    	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)
    	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)
    	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)
    	at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
    	at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
    	at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
    	at sun.reflect.GeneratedMethodAccessor83.invoke(Unknown Source)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    	at java.lang.reflect.Method.invoke(Method.java:498)
    	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    	at py4j.Gateway.invoke(Gateway.java:280)
    	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    	at py4j.commands.CallCommand.execute(CallCommand.java:79)
    	at py4j.GatewayConnection.run(GatewayConnection.java:214)
    	at java.lang.Thread.run(Thread.java:745)
    Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: null
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:288)
    	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:287)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:360)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.immutable.List.foreach(List.scala:381)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.immutable.List.map(List.scala:285)
    	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:358)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:293)
    	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:277)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$bind$1.apply(GenerateMutableProjection.scala:38)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.bind(GenerateMutableProjection.scala:38)
    	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
    	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:353)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:203)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1$1.apply(WindowExec.scala:202)
    	at org.apache.spark.sql.execution.window.AggregateProcessor$.apply(AggregateProcessor.scala:98)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2.org$apache$spark$sql$execution$window$WindowExec$$anonfun$$processor$1(WindowExec.scala:198)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:225)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6.apply(WindowExec.scala:222)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1$$anonfun$16.apply(WindowExec.scala:318)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.<init>(WindowExec.scala:318)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:290)
    	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:289)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
    	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
    	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    	at org.apache.spark.scheduler.Task.run(Task.scala:99)
    	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
    	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    	... 1 more
    Caused by: java.lang.RuntimeException: Couldn't find AmtPaidCumSum#2076 in [sum#2299,max#2300,x#2066L,AmtPaid#2067]
    	at scala.sys.package$.error(package.scala:27)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
    	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
    	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
    	... 62 more
{noformat}",,apachespark,dongjoon,jacshen,mandarup,mscheifer,Tagar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26041,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 22:23:40 UTC 2017,,,,,,,,,,"0|i3crxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/17 07:58;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17432;;;","14/Jul/17 19:31;mscheifer;Is there a way I can work around this if I'm stuck on Spark 2.1.0 ?

Thanks,
Matthew;;;","02/Aug/17 22:23;jacshen;will there be a fix for 2.1.0?

Regards,
Jacky;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re registration of AM hangs spark cluster in yarn-client mode,SPARK-20079,13058789,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,gq,gq,24/Mar/17 02:17,17/May/20 18:14,14/Jul/23 06:30,01/Aug/17 17:12,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"The ExecutorAllocationManager.reset method is called when re-registering AM, which sets the ExecutorAllocationManager.initializing field true. When this field is true, the Driver does not start a new executor from the AM request. The following two cases will cause the field to False

1. A executor idle for some time.
2. There are new stages to be submitted


After the a stage was submitted, the AM was killed and restart ,the above two cases will not appear.

1. When AM is killed, the yarn will kill all running containers. All execuotr will be lost and no executor will be idle.
2. No surviving executor, resulting in the current stage will never be completed, DAG will not submit a new stage.


Reproduction steps:

1. Start cluster

{noformat}
echo -e ""sc.parallelize(1 to 2000).foreach(_ => Thread.sleep(1000))"" | ./bin/spark-shell  --master yarn-client --executor-cores 1 --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.maxExecutors=2
{noformat}

2.  Kill the AM process when a stage is scheduled. ",,apachespark,gq,jerryshao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 18 00:21:03 UTC 2017,,,,,,,,,,"0|i3cqdb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/17 06:34;jerryshao;What is the specific symptom you met? I believe there's bunch of corner cases regarding RPC back and forth in yarn-client + AM reattempt scenario, and sometimes these scenarios are quite hard to fix, so usually I would suggest to set max attempt to 1 in yarn client mode.;;;","30/Mar/17 14:28;gq;I met the problem when doing reliability test. After the restart AM, in more than ten hours it has not applied for a new executor.;;;","30/Mar/17 14:35;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/17480;;;","06/May/17 13:32;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/17882;;;","18/Jul/17 00:21;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/18663;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redact datasource explain output,SPARK-20070,13058551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,23/Mar/17 10:37,14/Jul/17 14:20,14/Jul/23 06:30,24/Mar/17 22:53,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"When calling explain on a datasource, the output can contain sensitive information. We should provide an admin/user to redact such information.",,apachespark,darabos,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 14 14:20:07 UTC 2017,,,,,,,,,,"0|i3cowf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"23/Mar/17 12:12;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17397;;;","24/Mar/17 23:52;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17420;;;","28/Mar/17 02:25;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17448;;;","14/Jul/17 14:20;darabos;I think this change has broken our application. Your input on https://issues.apache.org/jira/browse/SPARK-21418 would be greatly appreciated. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty output files created for aggregation query in append mode,SPARK-20065,13058444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,sfiorito,sfiorito,23/Mar/17 02:45,12/Dec/22 18:10,14/Jul/23 06:30,19/Jul/17 06:27,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"I've got a Kafka topic which I'm querying, running a windowed aggregation, with a 30 second watermark, 10 second trigger, writing out to Parquet with append output mode.

Every 10 second trigger generates a file, regardless of whether there was any data for that trigger, or whether any records were actually finalized by the watermark.

Is this expected behavior or should it not write out these empty files?

{code}
val df = spark.readStream.format(""kafka"")....

val query = df
  .withWatermark(""timestamp"", ""30 seconds"")
  .groupBy(window($""timestamp"", ""10 seconds""))
  .count()
  .select(date_format($""window.start"", ""HH:mm:ss"").as(""time""), $""count"")

query
  .writeStream
  .format(""parquet"")
  .option(""checkpointLocation"", aggChk)
  .trigger(ProcessingTime(""10 seconds""))
  .outputMode(""append"")
  .start(aggPath)
{code}

As the query executes, do a file listing on ""aggPath"" and you'll see 339 byte files at a minimum until we arrive at the first watermark and the initial batch is finalized. Even after that though, as there are empty batches it'll keep generating empty files every trigger.",,apachespark,cloud_fan,sfiorito,uncleGen,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 19 04:36:18 UTC 2017,,,,,,,,,,"0|i3co8n:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"23/Mar/17 08:10;uncleGen;Make sense, I will give a fast update.;;;","23/Mar/17 08:23;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17395;;;","19/Jul/17 02:28;cloud_fan;Does https://github.com/apache/spark/pull/18654 fix this issue?;;;","19/Jul/17 03:02;gurwls223;Yes, I am quite sure that fixes it. Will be back after double checking.;;;","19/Jul/17 04:36;gurwls223;I can confirm that PR (almost) fixes this issue. I tested as below and checked empty files before/after that PR:

{code}
rm -r tmp.csv
rm -r /tmp/spark-checkpoint/*
{code}

{code}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.ProcessingTime


val df = spark.readStream.format(""rate"").option(""rowsPerSecond"", ""10"").load()
val query = df
  .withWatermark(""timestamp"", ""30 seconds"")
  .groupBy(window($""timestamp"", ""10 seconds""))
  .count()
  .select(date_format($""window.start"", ""HH:mm:ss"").as(""time""), $""count"")

query
  .writeStream
  .format(""csv"")
  .option(""checkpointLocation"", ""/tmp/spark-checkpoint"")
  .trigger(ProcessingTime(""10 seconds""))
  .outputMode(""append"")
  .start(""tmp.csv"")
{code}

{code}
ls -al tmp.csv
{code}


*Before*

Roughly 199 empty file per batch

*After*


Roughly 1 empty file per batch
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HbaseCredentialProvider uses wrong classloader,SPARK-20059,13058211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,22/Mar/17 13:00,17/May/20 18:15,14/Jul/23 06:30,29/Mar/17 17:10,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"{{HBaseCredentialProvider}} uses system classloader instead of child classloader, which will make HBase jars specified with {{--jars}} fail to work, so here we should use the right class loader.

Besides in yarn cluster mode jars specified with {{--jars}} is not added into client's class path, which will make it fail to load HBase jars and issue tokens in our scenario. Also some customized credential provider cannot be registered into client.

So here I will fix this two issues.

",,apachespark,jerryshao,praneetsharma,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 02 06:33:33 UTC 2018,,,,,,,,,,"0|i3cmsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/17 13:39;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17388;;;","27/Mar/17 09:49;srowen;Is this actually the same issue as SPARK-20019 or SPARK-11421?;;;","27/Mar/17 10:00;jerryshao;[~sowen], it probably is not the same issue.

In SPARK-20019, jars are added through SQL command in to Hive's SessionState, the code path is different.

And in SPARK-11421, AFAIK,  the target is to add jars to the current classloader in the runtime.

Here the issue is during start, jars specified through {{--jars}} should be added to Spark's child classloader in yarn cluster mode. 

They're probably all classloader related issues, but I think they're targeted to the different area, and touched the different code path.
;;;","02/Nov/18 06:33;praneetsharma;Hi Guys

Regarding the fix done here for yarn-cluster mode, why are we adding primaryResource? Isn't adding args.jars to childClasspath enough?

I ask this because we have a scenario where-in the cleanup of primaryResource fails till the application has completed. Here are the details:
 * We have a primaryResource jar present in an *NFS* mounted location. Our scenario is that we perform the cleanup after the spark job starts on the cluster.
 * Till spark-2.1.0, this clean has worked fine for us because since we are in yarn-cluster mode, this primaryResource is no longer needed on the client once it has been uploaded to the cluster.
 * But with spark-2.2.1 (which contains this fix), when we attempt to clean up the primaryResource, it produces a .nfsxxx file which essentially means that spark-submit process is holding onto an open handle on this primaryResource for the entirety of application lifecycle.

What we found out was that since we are now adding primaryResource to the thread-context-classloader within SparkSubmit.scala even for yarn-cluster mode, the cleanup doesn't seem to happen on NFS. This is a regression for us. And it becomes more problematic for us in case of long running streaming jobs, because now we can't cleanup the primaryResource till the application completes, which is unacceptable to us.

This is how the issue can be reproduced:
 * Have spark-examples jar present in an NFS mounted location.
 * While performing spark-submit in yarn-cluster mode, provide spark-examples jar as the primary resource. 
 * When the job gets submitted onto the cluster, try deleting the jar in the NFS location.
 ** While the jar gets deleted, but it produces an .nfsxx file which locks the deletion of the directory itself until the application fully completes and spark-submit ends

{code:java}
export HADOOP_CONF_DIR=./conf
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster --executor-memory 1G --num-executors 1 /mountloc/spark-test/spark-examples_2.11-2.3.1.jar 1000
{code}
When the above command submits an application onto the cluster, we attempt to delete /mountloc/spark-test/ location (which houses the primaryResource), but it doesn't succeed since NFS locks this directory by producing .nfsxxx file.

Please provide your thoughts on the same. Please let me know if any further information needs to be provided from my side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix StreamSuite.recover from v2.1 checkpoint failing with IOException,SPARK-20051,13058040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kunalkhamar,kunalkhamar,21/Mar/17 23:46,22/Mar/17 01:54,14/Jul/23 06:30,22/Mar/17 01:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"There is a race condition between calling stop on a streaming query and deleting directories in withTempDir that causes test to fail, fixing to do lazy deletion using delete on shutdown JVM hook.",,apachespark,kunalkhamar,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 22 01:54:13 UTC 2017,,,,,,,,,,"0|i3clqv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/17 23:59;apachespark;User 'kunalkhamar' has created a pull request for this issue:
https://github.com/apache/spark/pull/17382;;;","22/Mar/17 01:54;tdas;Issue resolved by pull request 17382
[https://github.com/apache/spark/pull/17382];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cloning SessionState does not clone query execution listeners,SPARK-20048,13058006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kunalkhamar,kunalkhamar,kunalkhamar,21/Mar/17 21:00,29/Mar/17 19:36,14/Jul/23 06:30,29/Mar/17 19:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,kunalkhamar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19540,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 21:20:03 UTC 2017,,,,,,,,,,"0|i3cljb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/17 21:20;apachespark;User 'kunalkhamar' has created a pull request for this issue:
https://github.com/apache/spark/pull/17379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decision Tree loader does not handle uppercase impurity param values,SPARK-20043,13057832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,facai,zsellami,zsellami,21/Mar/17 09:16,28/Mar/17 23:14,14/Jul/23 06:30,28/Mar/17 23:14,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,ML,,,,,0,starter,,,,,,,,"I saved a CrossValidatorModel with a decision tree and a random forest. I use Paramgrid to test ""gini"" and ""entropy"" impurity. CrossValidatorModel are not able to load the saved model, when impurity are written not in lowercase. I obtain an error from Spark ""impurity Gini (Entropy) not recognized.
",,apachespark,josephkb,mlnick,yuhaoyan,zsellami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 23:14:30 UTC 2017,,,,,,,,,,"0|i3ckgv:",9223372036854775807,,,,,josephkb,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"21/Mar/17 17:10;mlnick;I just noticed the error message you put above says ""Entorpy"" - is that a spelling mistake in the JIRA description or in your code?;;;","22/Mar/17 09:34;zsellami; It is a spelling mistake in the JIRA description. I correct my description.;;;","22/Mar/17 22:17;yuhaoyan;Looks like a bug for tree models load. a toLower should be added when loading impurityType from metadata. 
Ideally, we should also check for potential issues like this in other algorithms.;;;","24/Mar/17 01:27;facai;Perhaps it's better to convert impurity Type after setting method is invoked. ;;;","24/Mar/17 02:15;facai;The bug can be reproduced.

I'd like to work on it.;;;","24/Mar/17 04:26;apachespark;User 'facaiy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17407;;;","28/Mar/17 23:14;josephkb;Issue resolved by pull request 17407
[https://github.com/apache/spark/pull/17407];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buttons on executor log page don't work with spark.ui.reverseProxy=true,SPARK-20042,13057804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,okoethibm,okoethibm,okoethibm,21/Mar/17 07:05,05/Apr/17 07:11,14/Jul/23 06:30,05/Apr/17 07:10,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Web UI,,,,,0,reverse-proxy,,,,,,,,"The ""Load More"" and ""Load New"" buttons on the executor log page generate full-path URLs to the ""/log"" REST end point in the worker. If the worker is served via master reverse proxy (spark.ui.reverseProxy=true), the calls go to the master web UI and return the HTML for the master page, instead of the expected REST data.

Successfully tested the following fix:
Use JavaScript in log-view.js that checks for a /proxy/<target>/ portion in the URL and prefixes that to the REST call, similar to the way this is done in executorspage.js

function getRESTEndPoint() {
      var words = document.baseURI.split('/');
      var ind = words.indexOf(""proxy"");
      if (ind > 0) {
          return words.slice(0, ind + 2).join('/') + ""/log"";
      }
      return ""/log""
}

function loadNew() {
  $.ajax({
    type: ""GET"",
    url: getRESTEndPoint() + baseParams + ""&byteLength=0"",
... etc",,ajbozarth,apachespark,IngoSchuster,okoethibm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 07:10:06 UTC 2017,,,,,,,,,,"0|i3ckan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/17 08:01;apachespark;User 'okoethibm' has created a pull request for this issue:
https://github.com/apache/spark/pull/17370;;;","05/Apr/17 07:10;srowen;Issue resolved by pull request 17370
[https://github.com/apache/spark/pull/17370];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compilation fixed in java docs.,SPARK-20027,13057434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,prashant,prashant,prashant,20/Mar/17 05:55,22/Mar/17 13:52,14/Jul/23 06:30,22/Mar/17 13:52,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Documentation,Spark Core,,,,0,,,,,,,,,"During build/sbt publish-local, build breaks due to javadocs errors. This patch fixes those errors.
",,apachespark,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 22 13:52:43 UTC 2017,,,,,,,,,,"0|i3ci0f:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"20/Mar/17 05:58;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/17358;;;","20/Mar/17 08:15;srowen;Doesn't need a JIRA;;;","22/Mar/17 13:52;srowen;Resolved by https://github.com/apache/spark/pull/17358;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document R GLM Tweedie family support in programming guide and code example,SPARK-20026,13057433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,felixcheung,felixcheung,20/Mar/17 05:50,07/Apr/17 17:57,14/Jul/23 06:30,07/Apr/17 17:57,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Documentation,SparkR,,,,0,,,,,,,,,,,actuaryzhang,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18929,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 18:51:03 UTC 2017,,,,,,,,,,"0|i3ci07:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"04/Apr/17 06:29;felixcheung;[~actuaryzhang] would you like to work on this for the 2.2 release?;;;","04/Apr/17 16:28;actuaryzhang;[~felixcheung] Yes, I will work on this. Thanks. ;;;","06/Apr/17 18:51;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Driver fail over will not work, if SPARK_LOCAL* env is set.",SPARK-20025,13057432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,20/Mar/17 05:49,10/Oct/17 12:50,14/Jul/23 06:30,10/Oct/17 12:50,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"In a bare metal system with No DNS setup, spark may be configured with SPARK_LOCAL* for IP and host properties.
During a driver failover, in cluster deployment mode. SPARK_LOCAL* should be ignored while auto deploying and should be picked up from target system's local environment.",,apachespark,dongjoon,prashant,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 06 08:09:12 UTC 2017,,,,,,,,,,"0|i3chzz:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"20/Mar/17 05:53;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/17357;;;","01/Jun/17 23:37;dongjoon;Hi, [~scrapcodes@gmail.com].
Could you adjust the target version here?;;;","03/Jun/17 06:59;prashant;Done!;;;","06/Aug/17 08:09;viirya;I think there is an unsolved issue SPARK-12963 which is the same cause of this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SessionCatalog reset need to set the current database of ExternalCatalog,SPARK-20024,13057420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,20/Mar/17 04:31,21/Mar/17 05:54,14/Jul/23 06:30,21/Mar/17 05:54,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"SessionCatalog API setCurrentDatabase does not set the current database of the underlying ExternalCatalog. Thus, weird errors could come in the test suites after we call reset. We need to fix it.

So far, have not found the direct impact in the other code paths because we expect all the SessionCatalog APIs should always use the current database value we managed, unless some of code paths skip it. Thus, we fix it in the test-only function reset().",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 04:33:05 UTC 2017,,,,,,,,,,"0|i3chxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/17 04:33;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17354;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not see table comment when describe formatted table,SPARK-20023,13057412,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,erlu,erlu,20/Mar/17 02:35,16/Apr/17 12:32,14/Jul/23 06:30,22/Mar/17 11:09,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Spark 2.x implements create table by itself.
https://github.com/apache/spark/commit/7d2ed8cc030f3d84fea47fded072c320c3d87ca7
But in the implement mentioned above, it remove table comment from properties, so user can not see table comment through run ""describe formatted table"". Similarly, when user alters table comment, he still can not see the change of table comment through run ""describe formatted table"".
I wonder why we removed table comments, is this a bug?",,apachespark,cloud_fan,erlu,S71955,smilegator,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 16 12:32:10 UTC 2017,,,,,,,,,,"0|i3chvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/17 02:55;ZenWzh;Can you please take a look at this? [~smilegator] [~cloud_fan];;;","20/Mar/17 04:00;smilegator;Sure, will take a look at it soon. ;;;","21/Mar/17 20:55;smilegator;{{DESC EXTENDED}} works. Obviously, {{DESC FORMATTED}} has a bug;;;","21/Mar/17 23:03;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17381;;;","22/Mar/17 11:09;cloud_fan;Issue resolved by pull request 17381
[https://github.com/apache/spark/pull/17381];;;","23/Mar/17 03:53;erlu;Hi, I review the PR and test this PR, then I found table comment can not be changed once it specified in ""create table comment"", users can not modify the table comment using alter table set tblproperties (""Comment"" = ""I will change the table comment"") like before, I think we you offer the relative sql or interface for users to change table comment in the condition when specify table comment by mistake.Is this a bug or we have better solution？
[~cloud_fan] [~smilegator] [~ZenWzh];;;","16/Apr/17 12:13;apachespark;User 'sujith71955' has created a pull request for this issue:
https://github.com/apache/spark/pull/17649;;;","16/Apr/17 12:32;S71955;@chenerlu, your point is right, after executing the alter command with newly added/modified table comment, the same is not reflecting when we execute desc formatted table query.
table comment which is now directly part of CatalogTable instance is not getting updated and old table comment was shown, to handle this issue while updating the table properties map with newly added/modified properties in CatalogTable instance also update the comment parameter in CatalogTable with the newly added/modified comment.

I raised PR after fixing this issue, https://github.com/apache/spark/pull/17649
 Please let me know for any suggestions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Miss backslash in python code,SPARK-20021,13057406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,uncleGen,uncleGen,uncleGen,20/Mar/17 02:04,22/Mar/17 11:10,14/Jul/23 06:30,22/Mar/17 11:10,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,,,,,0,,,,,,,,,,,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 22 11:10:31 UTC 2017,,,,,,,,,,"0|i3chu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/17 02:05;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17352;;;","22/Mar/17 11:10;srowen;Issue resolved by pull request 17352
[https://github.com/apache/spark/pull/17352];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR should support checkpointing DataFrame,SPARK-20020,13057370,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,19/Mar/17 18:06,20/Mar/17 06:50,14/Jul/23 06:30,20/Mar/17 06:50,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Documentation,SparkR,,,,0,,,,,,,,,"As an user I want to be able to checkpoint DataFrame to run complex queries, iterative algorithms etc.",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 05:44:03 UTC 2017,,,,,,,,,,"0|i3chlz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"19/Mar/17 18:09;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17351;;;","20/Mar/17 05:44;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17356;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pivot with timestamp and count should not print internal representation,SPARK-20018,13057342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,19/Mar/17 13:39,12/Dec/22 17:51,14/Jul/23 06:30,22/Mar/17 16:59,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, when we perform count with timestamp types, it prints the internal representation as the column name as below:

{code}
scala> Seq(new java.sql.Timestamp(1)).toDF(""a"").groupBy(""a"").pivot(""a"").count().show()
+--------------------+----+
|                   a|1000|
+--------------------+----+
|1969-12-31 16:00:...|   1|
+--------------------+----+
{code}

It seems this should be 

{code}
+--------------------+-----------------------+
|                   a|1969-12-31 16:00:00.001|
+--------------------+-----------------------+
|1969-12-31 16:00:...|                      1|
+--------------------+-----------------------+
{code}
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 13:49:02 UTC 2017,,,,,,,,,,"0|i3chfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 13:49;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17348;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Functions ""str_to_map"" and ""explode"" throws NPE exceptioin",SPARK-20017,13057341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,roncen_zhao,roncenzhao,roncenzhao,19/Mar/17 13:38,21/Mar/17 18:34,14/Jul/23 06:30,21/Mar/17 18:34,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"```
val sqlDf = spark.sql(""select k,v from (select str_to_map('') as map_col from range(2)) tbl lateral view explode(map_col) as k,v"")

sqlDf.show
```

The sql throws NPE exception.",,apachespark,kiszk,maropu,roncenzhao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 13:40;roncenzhao;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12859457/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 16:10:27 UTC 2017,,,,,,,,,,"0|i3chfj:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"19/Mar/17 13:45;roncenzhao;The root cause of this exception is:
The `StringToMap`'s `dataType` is `MapType(StringType, StringType, valueContainsNull = false)` and the result of `str_to_map('')` is `"""" -> null`, so in the function `explode`, it will throw NPE exception.

I think we should change the `dataType` of `StringToMap` to `MapType(StringType, StringType)`, the default value of `valueContainsNull` is `true`.;;;","19/Mar/17 15:39;maropu;Ah, as you explained, the nullability is wrong in StringToMap; it could output null
{code}
scala> sql(""SELECT str_to_map('1')"").show
+-------------------+
|str_to_map(1, ,, :)|
+-------------------+
|     Map(1 -> null)|
+-------------------+
{code}
Could you make a pr for this fix?;;;","19/Mar/17 16:07;apachespark;User 'zhaorongsheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/17350;;;","19/Mar/17 16:10;roncenzhao; [~maropu] Please check it, Thanks~;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort information is lost after sort merge join,SPARK-20010,13057228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,18/Mar/17 06:12,21/Mar/17 02:44,14/Jul/23 06:30,21/Mar/17 02:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"After sort merge join for inner join, now we only keep left key ordering. However, after inner join, right key has the same value and order as left key. So if we need another smj on right key, we will unnecessarily add a sort which causes additional cost.

As a more complicated example, A join B on A.key = B.key join C on B.key = C.key join D on A.key = D.key. We will unnecessarily add a sort on B.key when join \{A, B\} and C, and add a sort on A.key when join \{A, B, C\} and D.

To fix this, we need to propagate all sorted information (equivalent expressions) from bottom up through `outputOrdering` and `SortOrder`.",,apachespark,cloud_fan,maropu,tejasp,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 02:44:22 UTC 2017,,,,,,,,,,"0|i3cgqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 15:09;maropu;As you explained, outputOrdering in SortMergeJoinExec certainly does not have rightKeys as sorted (But, I'm not sure why... cc: [~cloud_fan])
https://github.com/apache/spark/blame/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L83

{code}
scala> sql(""SET spark.sql.join.preferSortMergeJoin=true"")
scala> sql(""SET spark.sql.autoBroadcastJoinThreshold=1024"")
scala> val leftDf1 = spark.range(1000000).selectExpr(""id * 2 AS key"", ""0"")
scala> val rightDf1 = spark.range(1000000).selectExpr(""id * 3 AS key"", ""0"")
scala> val leftDf2 = leftDf1.join(rightDf1, leftDf1(""key"") === rightDf1(""key""), ""INNER"")
scala> val rightDf2 = spark.range(1000000).selectExpr(""id * 4 AS key"", ""0"")
scala> leftDf2.join(rightDf2, leftDf1(""key"") === rightDf2(""key""), ""INNER"").explain
== Physical Plan ==
*SortMergeJoin [key#207L], [key#239L], Inner
:- *SortMergeJoin [key#207L], [key#215L], Inner
:  :- *Sort [key#207L ASC NULLS FIRST], false, 0
:  :  +- Exchange hashpartitioning(key#207L, 200)
:  :     +- *Project [(id#204L * 2) AS key#207L, 0 AS 0#208]
:  :        +- *Range (0, 1000000, step=1, splits=Some(4))
:  +- *Sort [key#215L ASC NULLS FIRST], false, 0
:     +- Exchange hashpartitioning(key#215L, 200)
:        +- *Project [(id#212L * 3) AS key#215L, 0 AS 0#216]
:           +- *Range (0, 1000000, step=1, splits=Some(4))
+- *Sort [key#239L ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(key#239L, 200)
      +- *Project [(id#236L * 4) AS key#239L, 0 AS 0#240]
         +- *Range (0, 1000000, step=1, splits=Some(4))

scala> leftDf2.join(rightDf2, rightDf1(""key"") === rightDf2(""key""), ""INNER"").explain
== Physical Plan ==
*SortMergeJoin [key#215L], [key#239L], Inner
:- *Sort [key#215L ASC NULLS FIRST], false, 0        <----- Unnecessary sort?
:  +- *SortMergeJoin [key#207L], [key#215L], Inner
:     :- *Sort [key#207L ASC NULLS FIRST], false, 0
:     :  +- Exchange hashpartitioning(key#207L, 200)
:     :     +- *Project [(id#204L * 2) AS key#207L, 0 AS 0#208]
:     :        +- *Range (0, 1000000, step=1, splits=Some(4))
:     +- *Sort [key#215L ASC NULLS FIRST], false, 0
:        +- Exchange hashpartitioning(key#215L, 200)
:           +- *Project [(id#212L * 3) AS key#215L, 0 AS 0#216]
:              +- *Range (0, 1000000, step=1, splits=Some(4))
+- *Sort [key#239L ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(key#239L, 200)
      +- *Project [(id#236L * 4) AS key#239L, 0 AS 0#240]
         +- *Range (0, 1000000, step=1, splits=Some(4))
{code};;;","19/Mar/17 16:55;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17339;;;","21/Mar/17 02:44;cloud_fan;Issue resolved by pull request 17339
[https://github.com/apache/spark/pull/17339];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FPGrowthModel setMinConfidence should affect rules generation and transform,SPARK-20003,13057104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,17/Mar/17 18:38,05/Apr/17 00:52,14/Jul/23 06:30,05/Apr/17 00:52,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"I was doing some test and find the issue. FPGrowthModel setMinConfidence should affect rules generation and transform. 
Currently associationRules in FPGrowthModel is a lazy val and setMinConfidence in FPGrowthModel has no impact once associationRules got computed .",,apachespark,josephkb,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 05 00:52:03 UTC 2017,,,,,,,,,,"0|i3cfyv:",9223372036854775807,,,,,josephkb,,,,,,,,2.2.0,,,,,,,,,,,"19/Mar/17 16:55;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/17336;;;","05/Apr/17 00:52;josephkb;Issue resolved by pull request 17336
[https://github.com/apache/spark/pull/17336];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using real user to connect HiveMetastore in HiveClientImpl,SPARK-19995,13056927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,17/Mar/17 07:52,20/Sep/18 17:39,14/Jul/23 06:30,28/Mar/17 17:43,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"If user specify ""--proxy-user"" in kerberized environment with Hive catalog implementation, HiveClientImpl will try to connect hive metastore with current user. While we use real user to do kinit, this will make connection failure. We should change like what we did before in yarn code to use real user.

{noformat}
ERROR TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)
	at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271)
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:188)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)
	at org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:173)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)
	at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
	at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)
	at org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)
	at org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:980)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:878)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:745)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)
	at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
	at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
	at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
	at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
	at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192)
	... 79 more
17/03/17 07:09:33 WARN metastore: Failed to connect to the MetaStore Server...
{noformat}",,apachespark,cltlfcjin,jerryshao,meiyoula,rajeshhadoop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19997,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 15 02:11:44 UTC 2018,,,,,,,,,,"0|i3cevj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 16:55;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17335;;;","24/Mar/17 06:21;cltlfcjin;Great patch! I meet this issue recently. And it's looks good to me.;;;","18/Apr/17 09:16;meiyoula;Will the token be expired?;;;","15/May/18 01:51;rajeshhadoop;The patch was not working in the case when spark-sql executed in local mode , works in yarn mode.

bin/spark-sql     --proxy-user x_user --master local

 
{code:java}
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: GSS initiate failed
at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)
at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:316)
at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)
at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:192)
{code}
 

 ;;;","15/May/18 02:11;rajeshhadoop;spark-sql localmode  to support proxy_user

Do we need to update HiveClientImpl.scala  to support in local mode.?

UserGroupInformation.getCurrentUser().getRealUser().doAs(new PrivilegedExceptionAction<TTransport>

[https://github.com/apache/spark/pull/17333/commits/55109d9f7aa0443dff53877e84b9705ef3566067];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong outputOrdering for right/full outer smj,SPARK-19994,13056917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ZenWzh,ZenWzh,ZenWzh,17/Mar/17 07:06,21/Mar/17 10:41,14/Jul/23 06:30,20/Mar/17 06:39,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,correctness,,,,,,,,"For right outer join, values of the left key will be filled with nulls if it can't match the value of the right key, so `nullOrdering` of the left key can't be guaranteed. We should output right key order.

For full outer join, neither left key nor right key guarantees `nullOrdering`. We should not output any ordering.",,apachespark,cloud_fan,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 10:41:02 UTC 2017,,,,,,,,,,"0|i3cetb:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"19/Mar/17 03:33;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17331;;;","20/Mar/17 06:39;cloud_fan;Issue resolved by pull request 17331
[https://github.com/apache/spark/pull/17331];;;","21/Mar/17 10:41;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/17376;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite: create temporary view using,SPARK-19990,13056877,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,kayousterhout,kayousterhout,17/Mar/17 02:10,20/Mar/17 13:37,14/Jul/23 06:30,20/Mar/17 13:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,Tests,,,,0,,,,,,,,,"This test seems to be failing consistently on all of the maven builds: https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite&test_name=create+temporary+view+using and is possibly caused by SPARK-19763.

Here's a stack trace for the failure: 

java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/jenkins/workspace/spark-master-test-maven-hadoop-2.6/sql/core/target/spark-sql_2.11-2.2.0-SNAPSHOT-tests.jar!/test-data/cars.csv
      at org.apache.hadoop.fs.Path.initialize(Path.java:206)
      at org.apache.hadoop.fs.Path.<init>(Path.java:172)
      at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:344)
      at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:343)
      at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
      at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
      at scala.collection.immutable.List.flatMap(List.scala:344)
      at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:343)
      at org.apache.spark.sql.execution.datasources.CreateTempViewUsing.run(ddl.scala:91)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
      at org.apache.spark.sql.Dataset.<init>(Dataset.scala:183)
      at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:68)
      at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
      at org.apache.spark.sql.test.SQLTestUtils$$anonfun$sql$1.apply(SQLTestUtils.scala:62)
      at org.apache.spark.sql.test.SQLTestUtils$$anonfun$sql$1.apply(SQLTestUtils.scala:62)
      at org.apache.spark.sql.execution.command.DDLSuite$$anonfun$38$$anonfun$apply$mcV$sp$8.apply$mcV$sp(DDLSuite.scala:705)
      at org.apache.spark.sql.test.SQLTestUtils$class.withView(SQLTestUtils.scala:186)
      at org.apache.spark.sql.execution.command.DDLSuite.withView(DDLSuite.scala:171)
      at org.apache.spark.sql.execution.command.DDLSuite$$anonfun$38.apply$mcV$sp(DDLSuite.scala:704)
      at org.apache.spark.sql.execution.command.DDLSuite$$anonfun$38.apply(DDLSuite.scala:701)
      at org.apache.spark.sql.execution.command.DDLSuite$$anonfun$38.apply(DDLSuite.scala:701)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
      at org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(HiveDDLSuite.scala:41)
      at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
      at org.apache.spark.sql.hive.execution.HiveCatalogedDDLSuite.runTest(HiveDDLSuite.scala:41)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$class.run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
      at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
      at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$class.run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)",,apachespark,cloud_fan,kayousterhout,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 20 13:36:50 UTC 2017,,,,,,,,,,"0|i3cekf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/17 04:32;windpiger;the root cause is [the csvfile path in this test case|https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/command/DDLSuite.scala#L703] is ""jar:file:/home/jenkins/workspace/spark-master-test-maven-hadoop-2.6/sql/core/target/spark-sql_2.11-2.2.0-SNAPSHOT-tests.jar!/test-data/cars.csv"", which will failed when new Path() [new Path in datasource.scala |https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L344]

and the cars.csv are stored in module core's resources.

after we merge the HiveDDLSuit and DDLSuit  in SPARK-19235   https://github.com/apache/spark/commit/09829be621f0f9bb5076abb3d832925624699fa9，if we test module hive, we will run the DDLSuit in the core module, and this will cause that we get the illegal path like 'jar:file:/xxx' above.

it is not related with SPARK-19763

I will fix this by providing a new test dir which contain the test files in sql/ , and the test case use this file path.

thanks~
;;;","17/Mar/17 05:03;kayousterhout;Thanks [~windpiger]!;;;","19/Mar/17 00:08;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17338;;;","19/Mar/17 04:54;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17344;;;","20/Mar/17 13:36;cloud_fan;Issue resolved by pull request 17338
[https://github.com/apache/spark/pull/17338];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some ML Models error when copy or do not set parent,SPARK-19985,13056792,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,bryanc,bryanc,16/Mar/17 21:14,03/Apr/17 08:56,14/Jul/23 06:30,03/Apr/17 08:56,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"Some ML Models fail when copied due to not having a default constructor and implementing {{copy}} with {{defaultCopy}}.  Other cases do not properly set the parent when the model is copied.  These models were missing the normal check that tests for these in the test suites.

Models with issues are:

* RFormlaModel
* MultilayerPerceptronClassificationModel
* BucketedRandomProjectionLSHModel
* MinHashLSH",,bryanc,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 03 08:56:33 UTC 2017,,,,,,,,,,"0|i3ce1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/17 21:15;bryanc;I'll fix this;;;","17/Mar/17 20:51;bryanc;PR link didn't get added;;;","03/Apr/17 08:56;mlnick;Issue resolved by pull request 17326
[https://github.com/apache/spark/pull/17326];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic Dataset transformation on POJOs does not preserves nulls.,SPARK-19980,13056695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,FlamingMike,FlamingMike,16/Mar/17 15:24,22/Mar/17 00:38,14/Jul/23 06:30,21/Mar/17 03:33,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Applying an identity map transformation on a statically typed Dataset with a POJO produces an unexpected result.

Given POJOs:
{code}
public class Stuff implements Serializable {
    private String name;
    public void setName(String name) { this.name = name; }
    public String getName() { return name; }
}

public class Outer implements Serializable {
    private String name;
    private Stuff stuff;
    public void setName(String name) { this.name = name; }
    public String getName() { return name; }
    public void setStuff(Stuff stuff) { this.stuff = stuff; }
    public Stuff getStuff() { return stuff; }
}
{code}

Produces the result:

{code}
scala> val encoder = Encoders.bean(classOf[Outer])
encoder: org.apache.spark.sql.Encoder[pojos.Outer] = class[name[0]: string, stuff[0]: struct<name:string>]

scala> val schema = encoder.schema
schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(stuff,StructType(StructField(name,StringType,true)),true))

scala> schema.printTreeString
root
 |-- name: string (nullable = true)
 |-- stuff: struct (nullable = true)
 |    |-- name: string (nullable = true)


scala> val df = spark.read.schema(schema).json(""stuff.json"").as[Outer](encoder)
df: org.apache.spark.sql.Dataset[pojos.Outer] = [name: string, stuff: struct<name: string>]

scala> df.show()
+----+-----+
|name|stuff|
+----+-----+
|  v1| null|
+----+-----+

scala> df.map(x => x)(encoder).show()
+----+------+
|name| stuff|
+----+------+
|  v1|[null]|
+----+------+
{code}

After identity transformation, `stuff` becomes an object with null values inside it instead of staying null itself.

Doing the same with case classes preserves the nulls:
{code}
scala> case class ScalaStuff(name: String)
defined class ScalaStuff

scala> case class ScalaOuter(name: String, stuff: ScalaStuff)
defined class ScalaOuter

scala> val encoder2 = Encoders.product[ScalaOuter]
encoder2: org.apache.spark.sql.Encoder[ScalaOuter] = class[name[0]: string, stuff[0]: struct<name:string>]

scala> val schema2 = encoder2.schema
schema2: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true), StructField(stuff,StructType(StructField(name,StringType,true)),true))

scala> schema2.printTreeString
root
 |-- name: string (nullable = true)
 |-- stuff: struct (nullable = true)
 |    |-- name: string (nullable = true)


scala>

scala> val df2 = spark.read.schema(schema2).json(""stuff.json"").as[ScalaOuter]
df2: org.apache.spark.sql.Dataset[ScalaOuter] = [name: string, stuff: struct<name: string>]

scala> df2.show()
+----+-----+
|name|stuff|
+----+-----+
|  v1| null|
+----+-----+


scala> df2.map(x => x).show()
+----+-----+
|name|stuff|
+----+-----+
|  v1| null|
+----+-----+
{code}

stuff.json:
{code}
{""name"":""v1"", ""stuff"":null }
{code}
",,apachespark,FlamingMike,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 08:16:05 UTC 2017,,,,,,,,,,"0|i3cdfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 11:37;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17347;;;","21/Mar/17 08:16;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17372;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table owner should be USER instead of PRINCIPAL in kerberized clusters,SPARK-19970,13056586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,16/Mar/17 09:11,24/Mar/17 01:46,14/Jul/23 06:30,20/Mar/17 17:08,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"In the kerberized hadoop cluster, when Spark creates tables, the owner of tables are filled with PRINCIPAL strings instead of USER names. This is inconsistent with Hive and causes problems when using ROLE in Hive. We had better to fix this.

*BEFORE*
{code}
scala> sql(""create table t(a int)"").show
scala> sql(""desc formatted t"").show(false)
...
|Owner:                      |spark@EXAMPLE.COM                                         |       |
{code}

*AFTER*
{code}
scala> sql(""create table t(a int)"").show
scala> sql(""desc formatted t"").show(false)
...
|Owner:                      |spark                                         |       |
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 01:46:03 UTC 2017,,,,,,,,,,"0|i3ccrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/17 17:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17363;;;","20/Mar/17 21:33;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17366;;;","24/Mar/17 01:46;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17405;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use a cached instance of KafkaProducer for writing to kafka via KafkaSink.,SPARK-19968,13056550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,16/Mar/17 05:22,30/May/17 01:13,14/Jul/23 06:30,30/May/17 01:13,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,kafka,,,,,,,,"KafkaProducer is thread safe and an instance can be reused for writing every batch out. According to Kafka docs, this sort of usage is encouraged. It has impact on performance too.

On an average an addBatch operation takes 25ms with this patch. It takes 250+ ms without this patch.

Results of benchmark results, posted on github PR.
",,apachespark,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 16:54:54 UTC 2017,,,,,,,,,,"0|i3ccjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/17 16:54;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/17308;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame batch reader may fail to infer partitions when reading FileStreamSink's output,SPARK-19965,13056513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lwlin,zsxwing,zsxwing,16/Mar/17 02:10,03/May/17 18:11,14/Jul/23 06:30,03/May/17 18:11,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"Reproducer
{code}
  test(""partitioned writing and batch reading with 'basePath'"") {
    val inputData = MemoryStream[Int]
    val ds = inputData.toDS()

    val outputDir = Utils.createTempDir(namePrefix = ""stream.output"").getCanonicalPath
    val checkpointDir = Utils.createTempDir(namePrefix = ""stream.checkpoint"").getCanonicalPath

    var query: StreamingQuery = null

    try {
      query =
        ds.map(i => (i, i * 1000))
          .toDF(""id"", ""value"")
          .writeStream
          .partitionBy(""id"")
          .option(""checkpointLocation"", checkpointDir)
          .format(""parquet"")
          .start(outputDir)

      inputData.addData(1, 2, 3)
      failAfter(streamingTimeout) {
        query.processAllAvailable()
      }

      spark.read.option(""basePath"", outputDir).parquet(outputDir + ""/*"").show()
    } finally {
      if (query != null) {
        query.stop()
      }
    }
  }
{code}

Stack trace
{code}
[info] - partitioned writing and batch reading with 'basePath' *** FAILED *** (3 seconds, 928 milliseconds)
[info]   java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:
[info] 	***/stream.output-65e3fa45-595a-4d29-b3df-4c001e321637
[info] 	***/stream.output-65e3fa45-595a-4d29-b3df-4c001e321637/_spark_metadata
[info] 
[info] If provided paths are partition directories, please set ""basePath"" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.
[info]   at scala.Predef$.assert(Predef.scala:170)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:133)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:98)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:156)
[info]   at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:54)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:55)
[info]   at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:133)
[info]   at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)
[info]   at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:160)
[info]   at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:536)
[info]   at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:520)
[info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite$$anonfun$8.apply$mcV$sp(FileStreamSinkSuite.scala:292)
[info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite$$anonfun$8.apply(FileStreamSinkSuite.scala:268)
[info]   at org.apache.spark.sql.streaming.FileStreamSinkSuite$$anonfun$8.apply(FileStreamSinkSuite.scala:268)
{code}",,apachespark,codingcat,lwlin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 13:40:02 UTC 2017,,,,,,,,,,"0|i3ccbj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"16/Mar/17 02:10;zsxwing;This is because inferring partitions doesn't ignore the ""_spark_metadata"" folder.;;;","16/Mar/17 03:49;lwlin;Hi [~zsxwing], are you working on a patch? Mind if I work on this in case you haven't started your work?;;;","16/Mar/17 06:35;zsxwing;[~lwlin] Go ahead. I guess the root cause probably is using a globed path to scan a file sink output. If it's hard to support, then we can throw a better exception.;;;","16/Mar/17 19:03;zsxwing;[~lwlin] I think we can just ignore “_spark_metadata” in  InMemoryFileIndex. Could you try it?;;;","19/Mar/17 13:40;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17346;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Flaky test: SparkSubmitSuite ""includes jars passed in through --packages""",SPARK-19964,13056411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,erenavsarogullari,erenavsarogullari,15/Mar/17 19:47,12/Dec/22 18:10,14/Jul/23 06:30,03/Apr/18 01:33,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deploy,Tests,,,,0,flaky-test,,,,,,,,"The following test case has been failed due to TestFailedDueToTimeoutException
*Test Suite:* SparkSubmitSuite
*Test Case:* includes jars passed in through --packages
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/74413/testReport/
*Stacktrace is also attached.*",,apachespark,erenavsarogullari,kayousterhout,Sonia,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23804,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/17 19:49;erenavsarogullari;SparkSubmitSuite_Stacktrace;https://issues.apache.org/jira/secure/attachment/12858941/SparkSubmitSuite_Stacktrace",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 03 01:33:02 UTC 2018,,,,,,,,,,"0|i3cbov:",9223372036854775807,,,,,,,,,,,,,2.3.1,2.4.0,,,,,,,,,,"16/Mar/17 05:05;srowen;It doesn't fail in master. This sounds like the kind of thing that fails when you have old builds lying around - could be local?;;;","17/Mar/17 00:54;kayousterhout;[~srowen] it looks like this is failing periodically in master: https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.deploy.SparkSubmitSuite&test_name=includes+jars+passed+in+through+--jars (I added flaky to the name which is I suspect the source of confusion)
;;;","27/Mar/18 18:13;vanzin;Still flaky:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/88624/testReport/org.apache.spark.deploy/SparkSubmitSuite/includes_jars_passed_in_through___packages/

{noformat}
sbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to failAfter did not complete within 60 seconds.
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.scalatest.concurrent.TimeLimits$class.failAfterImpl(TimeLimits.scala:234)
	at org.apache.spark.deploy.SparkSubmitSuite$.failAfterImpl(SparkSubmitSuite.scala:1066)
	at org.scalatest.concurrent.TimeLimits$class.failAfter(TimeLimits.scala:230)
	at org.apache.spark.deploy.SparkSubmitSuite$.failAfter(SparkSubmitSuite.scala:1066)
	at org.apache.spark.deploy.SparkSubmitSuite$.runSparkSubmit(SparkSubmitSuite.scala:1085)
	at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$9$$anonfun$apply$mcV$sp$1.apply(SparkSubmitSuite.scala:525)
	at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$9$$anonfun$apply$mcV$sp$1.apply(SparkSubmitSuite.scala:514)
	at org.apache.spark.deploy.IvyTestUtils$.withRepository(IvyTestUtils.scala:377)
	at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$9.apply$mcV$sp(SparkSubmitSuite.scala:514)
	at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$9.apply(SparkSubmitSuite.scala:510)
	at org.apache.spark.deploy.SparkSubmitSuite$$anonfun$9.apply(SparkSubmitSuite.scala:510)
{noformat};;;","27/Mar/18 18:52;vanzin;I ran into this failure in our internal jenkins also... the logs look similar to the above failure. It seems the code is taking a long time inside ivy libraries:

{noformat}
18/03/27 02:20:43.516 Utils: SLF4J: Class path contains multiple SLF4J bindings.
...
18/03/27 02:21:22.384 Utils: 	found my.great.lib#mylib;0.1 in repo-1
{noformat}

Those are the first and last log lines for the test. In our internal jenkins spark-submit makes further progress, but still the timeout is caused by the call into ivy taking a long time:

{noformat}
18/03/27 11:21:20.307 Ivy Default Cache set to: /var/lib/jenkins/.ivy2/cache
...
18/03/27 11:21:20.582 :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
18/03/27 11:21:20.582 	confs: [default]
18/03/27 11:21:41.618 	found my.great.lib#mylib;0.1 in repo-1
18/03/27 11:22:11.271 	found my.great.dep#mylib;0.1 in repo-1
...
18/03/27 11:22:18.878 INFO BlockManagerMasterEndpoint: Registering block manager 172.28.195.10:58362 with 366.3 MB RAM, BlockManagerId(0, 172.28.195.10, 58362, None)
{noformat}

Wonder if it has anything to to with ivy trying to access the network during these tests, or some local lock maybe (which would be affected by multiple jenkins jobs on the same machine).;;;","27/Mar/18 19:41;vanzin;http://dl.bintray.com doesn't seem to be loading right now, and that repo is hardcoded in {{SparkSubmit.scala}}. Might be good to make tests skip these remote repos if they don't need them.;;;","27/Mar/18 21:01;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20916;;;","03/Apr/18 01:33;gurwls223;Fixed in https://github.com/apache/spark/pull/20916;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create view from select Fails when nullif() is used,SPARK-19963,13056407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jaydanielsen,jaydanielsen,15/Mar/17 19:25,08/Jun/17 08:42,14/Jul/23 06:30,08/Jun/17 08:42,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Test Case : Any valid query using nullif.

SELECT nullif(mycol,0) from mytable;

Create view FAILS when nullif used in select.

CREATE VIEW my_view as
SELECT nullif(mycol,0) from mytable;

Error: java.lang.RuntimeException: Failed to analyze the canonicalized SQL: ...

I can refactor with CASE statement and create view successfully.

CREATE VIEW my_view as
SELECT CASE WHEN mycol = 0 THEN NULL ELSE mycol END mycol from mytable;",,jaydanielsen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 08 08:42:30 UTC 2017,,,,,,,,,,"0|i3cbnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/17 03:31;sparkdharani;@Jay Danielsen: I'm looking into this.;;;","05/Apr/17 07:04;sparkdharani;it looks like the issue is fixed in 2.2;;;","08/Jun/17 08:42;yumwang;Marked as fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
df[java.lang.Long].collect throws NullPointerException if df includes null,SPARK-19959,13056237,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,kiszk,kiszk,15/Mar/17 10:04,24/Mar/17 05:01,14/Jul/23 06:30,24/Mar/17 05:00,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"The following program throws {{NullPointerException}} during the execution of Java code generated by the wholestage codegen.

{code:java}
sparkContext.parallelize(Seq[java.lang.Long](0L, null, 2L), 1).toDF.collect
{code}

{code}
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(generated.java:37)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:394)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 05:00:47 UTC 2017,,,,,,,,,,"0|i3cam7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/17 10:34;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17302;;;","24/Mar/17 05:00;cloud_fan;Issue resolved by pull request 17302
[https://github.com/apache/spark/pull/17302];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomForest Models should use the UID of Estimator when fit,SPARK-19953,13056107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,bryanc,bryanc,14/Mar/17 21:46,06/Apr/17 07:41,14/Jul/23 06:30,06/Apr/17 07:41,2.0.1,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"Currently, RandomForestClassificationModel and RandomForestRegressionModel use the alternate constructor which creates a new random UID instead of using the parent estimators UID.",,apachespark,bryanc,mlnick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 06 07:41:08 UTC 2017,,,,,,,,,,"0|i3c9tb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/17 21:47;bryanc;I'll push the patch for this;;;","14/Mar/17 21:57;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/17296;;;","06/Apr/17 07:41;mlnick;Issue resolved by pull request 17296
[https://github.com/apache/spark/pull/17296];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test suite for SessionCatalog with HiveExternalCatalog,SPARK-19945,13055880,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,14/Mar/17 09:10,16/Mar/17 18:36,14/Jul/23 06:30,16/Mar/17 18:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently SessionCatalogSuite is only for InMemoryCatalog, there is no suite for HiveExternalCatalog.
And there are some ddl function is not proper to test in ExternalCatalogSuite, because some logic are not full implement in ExternalCatalog, these ddl functions are full implement in SessionCatalog, it is better to test it in SessionCatalogSuite

So we should add a test suite for SessionCatalog with HiveExternalCatalog",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 14 09:17:04 UTC 2017,,,,,,,,,,"0|i3c8ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/17 09:17;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17287;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FPGrowthModel.transform should skip duplicated items,SPARK-19940,13055762,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,13/Mar/17 22:55,16/May/17 09:53,14/Jul/23 06:30,14/Mar/17 14:35,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"Due to misplaced {{distinct}} {{FPGrowthModel.transform}} generates duplicated items in the ""prediction"":

{code}
scala> val data = spark.read.text(""data/mllib/sample_fpgrowth.txt"").select(split($""value"", ""\\s+"").alias(""features"")) 
data: org.apache.spark.sql.DataFrame = [features: array<string>]

scala> val data = spark.read.text(""data/mllib/sample_fpgrowth.txt"").select(split($""value"", ""\\s+"").alias(""features"")) 
data: org.apache.spark.sql.DataFrame = [features: array<string>]

scala> fpm.transform(Seq(Array(""t"", ""s"")).toDF(""features"")).show(1, false)
+--------+---------------------+
|features|prediction           |
+--------+---------------------+
|[t, s]  |[y, x, z, x, y, x, z]|
+--------+---------------------+

{code}",,apachespark,josephkb,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14503,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 14 14:35:06 UTC 2017,,,,,,,,,,"0|i3c7on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/17 23:02;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/17283;;;","13/Mar/17 23:07;zero323;cc [~yuhaoyan] ;;;","14/Mar/17 14:35;josephkb;Issue resolved by pull request 17283
[https://github.com/apache/spark/pull/17283];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TPCDS Q70 went wrong while explaining,SPARK-19933,13050389,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,ZenWzh,ZenWzh,13/Mar/17 05:17,14/Mar/17 17:54,14/Jul/23 06:30,14/Mar/17 17:54,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"
The query run ok in Jan. So I think some recent change broke it.
All tables are in parquet format.
The latest commit of my test version (master branch on Mar 13) is: https://github.com/apache/spark/commit/9456688547522a62f1e7520e9b3564550c57aa5d

Error messages are as follows:
TreeNodeException: Binding attribute, tree: s_state#4
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:88)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:87)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.bind(GeneratePredicate.scala:45)
        at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.bind(GeneratePredicate.scala:40)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:884)
        at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:358)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition$lzycompute(BroadcastNestedLoopJoinExec.scala:87)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$boundCondition(BroadcastNestedLoopJoinExec.scala:85)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(BroadcastNestedLoopJoinExec.scala:191)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2$$anonfun$apply$3.apply(BroadcastNestedLoopJoinExec.scala:191)
        at scala.collection.IndexedSeqOptimized$class.prefixLengthImpl(IndexedSeqOptimized.scala:38)
        at scala.collection.IndexedSeqOptimized$class.exists(IndexedSeqOptimized.scala:46)
        at scala.collection.mutable.ArrayOps$ofRef.exists(ArrayOps.scala:186)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2.apply(BroadcastNestedLoopJoinExec.scala:191)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2.apply(BroadcastNestedLoopJoinExec.scala:190)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:378)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
        at org.apache.spark.scheduler.Task.run(Task.scala:108)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Couldn't find s_state#4 in [ss_store_sk#14,ss_net_profit#29,s_store_sk#58,s_county#81,s_state#82,s_state#136]
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:94)
        at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1$$anonfun$applyOrElse$1.apply(BoundAttribute.scala:88)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        ... 42 more
",,apachespark,dongjoon,hvanhovell,ZenWzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 16:29:02 UTC 2017,,,,,,,,,,"0|i3baxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/17 05:20;ZenWzh;[~cloud_fan] Do you know why?;;;","13/Mar/17 05:24;ZenWzh;The query run ok in Jan. So I think some recent change broke it.;;;","13/Mar/17 14:12;hvanhovell;Yeah, this is on me. I added a rule that removes redundant aliases to fix SPARK-18609 and SPARK-18841, but this unfortunately also changes the output of the subquery which breaks the query plan. I'll fix this.;;;","13/Mar/17 14:28;hvanhovell;Here is a reproducible example:
{noformat}
spark.range(100).createOrReplaceTempView(""tbl_a"")
spark.range(100).createOrReplaceTempView(""tbl_b"")
sql(""select * from tbl_a where id in (select id as id from tbl_b)"")
{noformat};;;","13/Mar/17 16:29;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InMemoryTableScanExec should rewrite output partitioning and ordering when aliasing output attributes,SPARK-19931,13050377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,13/Mar/17 03:40,16/Mar/17 00:19,14/Jul/23 06:30,16/Mar/17 00:19,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Now InMemoryTableScanExec simply takes the outputPartitioning and outputOrdering from the associated InMemoryRelation's child.outputPartitioning and outputOrdering.

However, InMemoryTableScanExec can alias the output attributes. In this case, its outputPartitioning and outputOrdering are not correct and its parent operators can't correctly determine its data distribution.",,apachespark,cloud_fan,emlyn,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19468,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 00:19:07 UTC 2017,,,,,,,,,,"0|i3bav3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/17 03:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17175;;;","16/Mar/17 00:19;cloud_fan;Issue resolved by pull request 17175
[https://github.com/apache/spark/pull/17175];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR spark.getSparkFiles fails on executor,SPARK-19925,13050311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yanboliang,yanboliang,yanboliang,12/Mar/17 08:37,22/Mar/17 05:14,14/Jul/23 06:30,22/Mar/17 05:14,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"SparkR function {{spark.getSparkFiles}} fails when it was called on executors. For examples, the following R code will fail. (See error logs in attachment.) 
{code}
spark.addFile(""./README.md"")
seq <- seq(from = 1, to = 10, length.out = 5)
train <- function(seq) {
path <- spark.getSparkFiles(""README.md"")
print(path)
}
spark.lapply(seq, train)
{code}
However, we can run successfully with Scala API:
{code}
import org.apache.spark.SparkFiles
sc.addFile(""./README.md”)
sc.parallelize(Seq(0)).map{ _ => SparkFiles.get(""README.md"")}.first()
{code}
and also successfully with Python API:
{code}
from pyspark import SparkFiles
sc.addFile(""./README.md"")
sc.parallelize(range(1)).map(lambda x: SparkFiles.get(""README.md"")).first()
{code}",,apachespark,felixcheung,iamshrek,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/17 09:25;yanboliang;error-log;https://issues.apache.org/jira/secure/attachment/12857515/error-log",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 12:33:02 UTC 2017,,,,,,,,,,"0|i3bagf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"12/Mar/17 09:31;yanboliang;The error was caused by {{spark.getSparkFiles}} calling backend Java code, however, it can not connect to backend when it's on executors.
{code}
spark.getSparkFiles <- function(fileName) {
  callJStatic(""org.apache.spark.SparkFiles"", ""get"", as.character(fileName))
}
{code}
I think we should have special handling when it was called on executors, following the PySpark implementation:
{code}
    def getRootDirectory(cls):
        """"""
        Get the root directory that contains files added through
        C{SparkContext.addFile()}.
        """"""
        if cls._is_running_on_worker:
            return cls._root_directory
        else:
            # This will have to change if we support multiple SparkContexts:
            return cls._sc._jvm.org.apache.spark.SparkFiles.getRootDirectory()
{code};;;","12/Mar/17 16:05;yanboliang;cc [~felixcheung] [~shivaram] [~sunrui];;;","13/Mar/17 12:33;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17274;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle InvocationTargetException for all Hive Shim,SPARK-19924,13050310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,12/Mar/17 08:35,13/Apr/17 11:19,14/Jul/23 06:30,14/Mar/17 04:06,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Since we are using shim for most Hive metastore APIs, the exceptions thrown by the underlying method of Method.invoke() are wrapped by `InvocationTargetException`. Instead of doing it one by one, we should handle all of them in the `withClient`. If any of them is missing, the error message could looks unfriendly.",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 13 05:06:02 UTC 2017,,,,,,,,,,"0|i3bag7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/17 08:36;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17265;;;","14/Mar/17 04:06;cloud_fan;Issue resolved by pull request 17265
[https://github.com/apache/spark/pull/17265];;;","13/Apr/17 05:06;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/17627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qualified partition location stored in catalog,SPARK-19917,13050220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,11/Mar/17 04:38,24/Sep/19 06:50,14/Jul/23 06:30,24/Sep/19 06:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,SQL,,,,,0,bulk-closed,,,,,,,,"partition path should be qualified to store in catalog. 
There are some scenes:
1. ALTER TABLE t PARTITION(b=1) SET LOCATION '/path/x' 
   qualified: file:/path/x
2. ALTER TABLE t PARTITION(b=1) SET LOCATION 'x' 
     qualified: file:/tablelocation/x
3. ALTER TABLE t ADD PARTITION(b=1) LOCATION '/path/x'
   qualified: file:/path/x
4. ALTER TABLE t ADD PARTITION(b=1) LOCATION 'x'
     qualified: file:/tablelocation/x

Currently only  ALTER TABLE t ADD PARTITION(b=1) LOCATION for hive serde table has the expected qualified path. we should make other scenes to be consist with it.

",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 11 04:44:03 UTC 2017,,,,,,,,,,"0|i3b9w7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/17 04:44;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17254;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
String literals are not escaped while performing Hive metastore level partition pruning,SPARK-19912,13050171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,lian cheng,lian cheng,10/Mar/17 23:17,21/Mar/17 04:18,14/Jul/23 06:30,21/Mar/17 04:18,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,correctness,,,,,,,,"{{Shim_v0_13.convertFilters()}} doesn't escape string literals while generating Hive style partition predicates.

The following SQL-injection-like test case illustrates this issue:
{code}
  test(""SPARK-19912"") {
    withTable(""spark_19912"") {
      Seq(
        (1, ""p1"", ""q1""),
        (2, ""p1\"" and q=\""q1"", ""q2"")
      ).toDF(""a"", ""p"", ""q"").write.partitionBy(""p"", ""q"").saveAsTable(""spark_19912"")

      checkAnswer(
        spark.table(""foo"").filter($""p"" === ""p1\"" and q = \""q1"").select($""a""),
        Row(2)
      )
    }
  }
{code}
The above test case fails like this:
{noformat}
[info] - spark_19912 *** FAILED *** (13 seconds, 74 milliseconds)
[info]   Results do not match for query:
[info]   Timezone: sun.util.calendar.ZoneInfo[id=""America/Los_Angeles"",offset=-28800000,dstSavings=3600000,useDaylight=true,transitions=185,lastRule=java.util.SimpleTimeZone[id=America/Los_Angeles,offset=-28800000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]
[info]   Timezone Env:
[info]
[info]   == Parsed Logical Plan ==
[info]   'Project [unresolvedalias('a, None)]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Analyzed Logical Plan ==
[info]   a: int
[info]   Project [a#26]
[info]   +- Filter (p#27 = p1"" and q = ""q1)
[info]      +- SubqueryAlias spark_19912
[info]         +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Optimized Logical Plan ==
[info]   Project [a#26]
[info]   +- Filter (isnotnull(p#27) && (p#27 = p1"" and q = ""q1))
[info]      +- Relation[a#26,p#27,q#28] parquet
[info]
[info]   == Physical Plan ==
[info]   *Project [a#26]
[info]   +- *FileScan parquet default.spark_19912[a#26,p#27,q#28] Batched: true, Format: Parquet, Location: PrunedInMemoryFileIndex[], PartitionCount: 0, PartitionFilters: [isnotnull(p#27), (p#27 = p1"" and q = ""q1)], PushedFilters: [], ReadSchema: struct<a:int>
[info]   == Results ==
[info]
[info]   == Results ==
[info]   !== Correct Answer - 1 ==   == Spark Answer - 0 ==
[info]    struct<>                   struct<>
[info]   ![2]
{noformat}",,apachespark,dongjoon,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HIVE-2943,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 18:43:07 UTC 2017,,,,,,,,,,"0|i3b9lb:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"12/Mar/17 08:11;dongjoon;Hi, [~lian cheng].
If you didn't start yet, I'll start to work on this.;;;","12/Mar/17 10:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17266;;;","13/Mar/17 05:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17271;;;","13/Mar/17 18:43;dongjoon;As [~smilegator] mentioned, this is related to the underlying Hive error.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`stack` should not reject NULL values due to type mismatch,SPARK-19910,13050140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Mar/17 20:57,13/Jun/17 04:19,14/Jul/23 06:30,13/Jun/17 04:19,2.0.0,2.0.1,2.1.0,2.1.1,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"Since `stack` function generates a table with nullable columns, it should allow mixed null values.

{code}
scala> sql(""select stack(3, 1, 2, 3)"").printSchema
root
 |-- col0: integer (nullable = true)

scala> sql(""select stack(3, 1, 2, null)"").printSchema
org.apache.spark.sql.AnalysisException: cannot resolve 'stack(3, 1, 2, NULL)' due to data type mismatch: Argument 1 (IntegerType) != Argument 3 (NullType); line 1 pos 7;
'Project [unresolvedalias(stack(3, 1, 2, null), None)]
+- OneRowRelation$
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 05 18:47:07 UTC 2017,,,,,,,,,,"0|i3b9ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 22:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17251;;;","05/May/17 18:47;dongjoon;Hi, [~cloud_fan] and [~smilegator].
Could you review this issue and PR?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dataset.inputFiles is broken for Hive SerDe tables,SPARK-19905,13050116,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,10/Mar/17 19:09,10/Mar/17 23:19,14/Jul/23 06:30,10/Mar/17 23:19,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The following snippet reproduces this issue:
{code}
spark.range(10).createOrReplaceTempView(""t"")
spark.sql(""CREATE TABLE u STORED AS RCFILE AS SELECT * FROM t"")
spark.table(""u"").inputFiles.foreach(println)
{code}
In Spark 2.2, it prints nothing, while in Spark 2.1, it prints something like
{noformat}
file:/Users/lian/local/var/lib/hive/warehouse_1.2.1/u
{noformat}
on my laptop.",,apachespark,cloud_fan,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 23:19:45 UTC 2017,,,,,,,,,,"0|i3b993:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"10/Mar/17 19:47;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/17247;;;","10/Mar/17 23:19;cloud_fan;Issue resolved by pull request 17247
[https://github.com/apache/spark/pull/17247];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Standalone] Master registers application again when driver relaunched,SPARK-19900,13050028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lyc,Ssergey,Ssergey,10/Mar/17 12:38,23/Jan/18 14:39,14/Jul/23 06:30,15/Jun/17 00:08,1.6.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Deploy,Spark Core,,,,0,network,Spark,standalone,supervise,,,,,"I've found some problems when node, where driver is running, has unstable network. A situation is possible when two identical applications are running on a cluster.

*Steps to Reproduce:*
# prepare 3 node. One for the spark master and two for the spark workers.
# submit an application with parameter spark.driver.supervise = true
# go to the node where driver is running (for example spark-worker-1) and close 7077 port
{code}
# iptables -A OUTPUT -p tcp --dport 7077 -j DROP
{code}
# wait more 60 seconds
# look at the spark master UI
There are two spark applications and one driver. The new application has WAITING state and the second application has RUNNING state. Driver has RUNNING or RELAUNCHING state (It depends on the resources available, as I understand it) and it launched on other node (for example spark-worker-2)
# open the port
{code}
# iptables -D OUTPUT -p tcp --dport 7077 -j DROP
{code}
# look an the spark UI again
There are no changes


In addition, if you look at the processes on the node spark-worker-1
{code}
# ps ax | grep spark
{code}
 you will see that the old driver is still working!

*Spark master logs:*
{code}
17/03/10 05:26:27 WARN Master: Removing worker-20170310052240-spark-worker-1-35039 because we got no heartbeat in 60 seconds
17/03/10 05:26:27 INFO Master: Removing worker worker-20170310052240-spark-worker-1-35039 on spark-worker-1:35039
17/03/10 05:26:27 INFO Master: Telling app of lost executor: 1
17/03/10 05:26:27 INFO Master: Telling app of lost executor: 0
17/03/10 05:26:27 INFO Master: Re-launching driver-20170310052347-0000
17/03/10 05:26:27 INFO Master: Launching driver driver-20170310052347-0000 on worker worker-20170310052411-spark-worker-2-40473
17/03/10 05:26:35 INFO Master: Registering app TestApplication
17/03/10 05:26:35 INFO Master: Registered app TestApplication with ID app-20170310052635-0001
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got status update for unknown executor app-20170310052354-0000/1
17/03/10 05:31:07 WARN Master: Got status update for unknown executor app-20170310052354-0000/0
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 WARN Master: Got heartbeat from unregistered worker worker-20170310052240-spark-worker-1-35039. Asking it to re-register.
17/03/10 05:31:07 INFO Master: Registering worker spark-worker-1:35039 with 8 cores, 10.8 GB RAM
17/03/10 05:31:07 INFO Master: Launching executor app-20170310052354-0000/4 on worker worker-20170310052240-spark-worker-1-35039
17/03/10 05:31:07 INFO Master: Launching executor app-20170310052354-0000/5 on worker worker-20170310052240-spark-worker-1-35039
{code}

I expect the following behaviour:
# when the driver is relaunching it should not create a new application or the old application should be removed
# the process with old driver should be killed

Correct me please if I do not understand something or I missed some settings.","Centos 6.5, spark standalone",apachespark,cloud_fan,lyc,neeraj20gupta,Ssergey,zgl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 23 14:39:08 UTC 2018,,,,,,,,,,"0|i3b8pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/17 15:02;Ssergey;This issue does not reproduced in spark 2.1.0 version.;;;","07/May/17 09:07;lyc;I can successfully reproduce in spark master ( commit  63d90e7d, and spark version is 2.3.0), though the symptom is a little different.

To make the application run slowly enough, I modify `examples/LogQuery.scala` to this:

{code}
dataSet.map(line => (extractKey(line), extractStats(line)))
      .reduceByKey((a, b) => {
        Thread.sleep(20*60*1000)
        a.merge(b)
      })
{code}

run two worker each with 1G memory and 2 cores. submit the application with:

{code}
bin/spark-submit --class org.apache.spark.examples.LogQuery  --master spark://master:6066 --deploy-mode cluster --supervise --total-executor-cores 1 --num-executors 1 --executor-memory 512M --driver-memory 512M /vagrant_data/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.3.0-SNAPSHOT.jar
{code}

and follow the steps as stated above. 

The outcome is that at step 5, there are not two drivers, but there are two applications, the old one is still running, the new one is waiting for resources. After step 6, the old application is finished, the new one running, and the only one driver is killed. This is problematic because the driver corresponding to the running application is killed instead of running.

The problem is due to the fact that the relaunched driver uses the same driver id with the original one and when old application is killed, the driver is killed as a side effect. I have create a pull request to fix this. ;;;","07/May/17 09:36;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17888;;;","24/May/17 09:20;apachespark;User 'liyichao' has created a pull request for this issue:
https://github.com/apache/spark/pull/18084;;;","15/Jun/17 00:08;cloud_fan;Issue resolved by pull request 18084
[https://github.com/apache/spark/pull/18084];;;","15/Jun/17 00:10;cloud_fan;liyichao can you provide your JIRA id? thanks!;;;","15/Jun/17 03:00;lyc;My user name (and JIRA login name) is `lyc`, Full name is `Li Yichao`. It seems JIRA has a bug when my full name is the same as my user name, so I change my full name, now I can be mentioned by `@Li Yichao`;;;","23/Jan/18 14:39;neeraj20gupta;We are facing this same issue. Can I get a nightly build to test the same in 2.3.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toDS throws StackOverflowError if case classes have circular references,SPARK-19896,13050020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,10/Mar/17 11:56,18/Mar/17 06:41,14/Jul/23 06:30,18/Mar/17 06:41,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"If case classes have circular references below, it throws StackOverflowError;

{code}
scala> :pasge
case class classA(i: Int, cls: classB)
case class classB(cls: classA)

scala> Seq(classA(0, null)).toDS()
java.lang.StackOverflowError
  at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1494)
  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(JavaMirrors.scala:66)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anonfun$info$1.apply(SynchronizedSymbols.scala:127)
  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)
  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.gilSynchronizedIfNotThreadsafe(SynchronizedSymbols.scala:123)
  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.gilSynchronizedIfNotThreadsafe(JavaMirrors.scala:66)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$class.info(SynchronizedSymbols.scala:127)
  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anon$1.info(JavaMirrors.scala:66)
  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)
  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
  at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:45)
{code}",,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19751,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 18 06:41:11 UTC 2017,,,,,,,,,,"0|i3b8nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 11:59;maropu;We probably need to fix this issue as the same way with https://github.com/apache/spark/pull/17188 like
https://github.com/apache/spark/compare/master...maropu:SPARK-19896;;;","10/Mar/17 12:06;srowen;OK you don't think it's quite the same as SPARK-19751?
Should they just be fixed together?;;;","10/Mar/17 12:09;maropu;Ah, it'd be better to merge them in the single ticket? Both is okay to me.;;;","18/Mar/17 06:41;cloud_fan;Issue resolved by pull request 17318
[https://github.com/apache/spark/pull/17318];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should not run DataFrame set oprations with map type,SPARK-19893,13049932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Mar/17 07:00,29/Oct/19 15:24,14/Jul/23 06:30,11/Mar/17 00:34,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18134,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 07:01:02 UTC 2017,,,,,,,,,,"0|i3b847:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 07:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17236;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Await Batch Lock not signaled on stream execution exit,SPARK-19891,13049817,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tcondie,tcondie,tcondie,10/Mar/17 00:08,10/Mar/17 07:02,14/Jul/23 06:30,10/Mar/17 07:02,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,,,apachespark,tcondie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 00:12:02 UTC 2017,,,,,,,,,,"0|i3b7pr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 00:12;apachespark;User 'tcondie' has created a pull request for this issue:
https://github.com/apache/spark/pull/17231;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Seeing offsets not resetting even when reset policy is configured explicitly,SPARK-19888,13049780,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jrmiller,jrmiller,09/Mar/17 22:33,14/Feb/19 14:00,14/Jul/23 06:30,14/Feb/19 14:00,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DStreams,,,,,4,,,,,,,,,"I was told to post this in a Spark ticket from KAFKA-4396:

I've been seeing a curious error with kafka 0.10 (spark 2.11), these may be two separate errors, I'm not sure. What's puzzling is that I'm setting auto.offset.reset to latest and it's still throwing an OffsetOutOfRangeException, behavior that's contrary to the code. Please help! :)

{code}
val kafkaParams = Map[String, Object](
      ""group.id"" -> consumerGroup,
      ""bootstrap.servers"" -> bootstrapServers,
      ""key.deserializer"" -> classOf[ByteArrayDeserializer],
      ""value.deserializer"" -> classOf[MessageRowDeserializer],
      ""auto.offset.reset"" -> ""latest"",
      ""enable.auto.commit"" -> (false: java.lang.Boolean),
      ""max.poll.records"" -> persisterConfig.maxPollRecords,
      ""request.timeout.ms"" -> persisterConfig.requestTimeoutMs,
      ""session.timeout.ms"" -> persisterConfig.sessionTimeoutMs,
      ""heartbeat.interval.ms"" -> persisterConfig.heartbeatIntervalMs,
      ""connections.max.idle.ms""-> persisterConfig.connectionsMaxIdleMs
    )
{code}

{code}
16/11/09 23:10:17 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on xyz (size: 146.3 KB, free: 8.4 GB)
16/11/09 23:10:23 WARN TaskSetManager: Lost task 15.0 in stage 151.0 (TID 38837, xyz): org.apache.kafka.clients.consumer.OffsetOutOfRangeException: Offsets out of range with no configured reset policy for partitions: {topic=231884473}
        at org.apache.kafka.clients.consumer.internals.Fetcher.parseFetchedData(Fetcher.java:588)
        at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:354)
        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1000)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:938)
        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)
        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:70)
        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)
        at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:397)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:85)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

16/11/09 23:10:29 INFO TaskSetManager: Finished task 10.0 in stage 154.0 (TID 39388) in 12043 ms on xyz (1/16)
16/11/09 23:10:31 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 39375) in 13444 ms on xyz (2/16)
16/11/09 23:10:44 WARN TaskSetManager: Lost task 1.0 in stage 151.0 (TID 38843, xyz): java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
        at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1431)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:929)
        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.poll(CachedKafkaConsumer.scala:99)
        at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:73)
        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
        at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)

{code}",,DLanza,gsomogyi,jrmiller,koeninger,OopsOutOfMemory,twasti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19185,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 14 14:00:49 UTC 2019,,,,,,,,,,"0|i3b7hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 21:32;koeninger;That stacktrace also shows a concurrent modification exception, yes?.  See SPARK-19185 for that

See e.g. SPARK-19680 for background on why offset out of range may occur on executor when it doesn't on driver.  Although if you're using reset latest, unless you have really short retention this is kind of surprising.;;;","12/Dec/18 09:05;gsomogyi;[~jrmiller] SPARK-19185 resolved on 2.4.0. Can you re-test it please?;;;","14/Feb/19 13:58;gsomogyi;Please reopen it if appears again.;;;","14/Feb/19 14:00;gsomogyi;Issue solved in SPARK-19185.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
__HIVE_DEFAULT_PARTITION__ is not interpreted as NULL partition value in partitioned persisted tables,SPARK-19887,13049732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,lian cheng,lian cheng,09/Mar/17 19:57,15/Mar/17 02:31,14/Jul/23 06:30,15/Mar/17 02:31,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,correctness,,,,,,,,"The following Spark shell snippet under Spark 2.1 reproduces this issue:

{code}
val data = Seq(
  (""p1"", 1, 1),
  (""p2"", 2, 2),
  (null, 3, 3)
)

// Correct case: Saving partitioned data to file system.

val path = ""/tmp/partitioned""

data.
  toDF(""a"", ""b"", ""c"").
  write.
  mode(""overwrite"").
  partitionBy(""a"", ""b"").
  parquet(path)

spark.read.parquet(path).filter($""a"".isNotNull).show(truncate = false)
// +---+---+---+
// |c  |a  |b  |
// +---+---+---+
// |2  |p2 |2  |
// |1  |p1 |1  |
// +---+---+---+

// Incorrect case: Saving partitioned data as persisted table.

data.
  toDF(""a"", ""b"", ""c"").
  write.
  mode(""overwrite"").
  partitionBy(""a"", ""b"").
  saveAsTable(""test_null"")

spark.table(""test_null"").filter($""a"".isNotNull).show(truncate = false)
// +---+--------------------------+---+
// |c  |a                         |b  |
// +---+--------------------------+---+
// |3  |__HIVE_DEFAULT_PARTITION__|3  |     <-- This line should not be here
// |1  |p1                        |1  |
// |2  |p2                        |2  |
// +---+--------------------------+---+
{code}

Hive-style partitioned tables use the magic string {{\_\_HIVE_DEFAULT_PARTITION\_\_}} to indicate {{NULL}} partition values in partition directory names. However, in the case persisted partitioned table, this magic string is not interpreted as {{NULL}} but a regular string.",,apachespark,cloud_fan,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 13 16:16:03 UTC 2017,,,,,,,,,,"0|i3b76v:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"09/Mar/17 19:58;lian cheng;cc [~cloud_fan];;;","13/Mar/17 15:19;cloud_fan;This is actually the same behavior of hive, see https://issues.apache.org/jira/browse/HIVE-1309

But I think it doesn't make sense to treat null as an invalid partition value, let's fix it in Spark SQL.;;;","13/Mar/17 16:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17277;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
reportDataLoss cause != null check is wrong for Structured Streaming KafkaSource,SPARK-19886,13049728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,09/Mar/17 19:47,10/Mar/17 01:43,14/Jul/23 06:30,10/Mar/17 01:43,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,,,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 09 19:49:03 UTC 2017,,,,,,,,,,"0|i3b75z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/17 19:49;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/17228;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The config ignoreCorruptFiles doesn't work for CSV,SPARK-19885,13049703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zsxwing,zsxwing,09/Mar/17 18:49,12/Dec/22 18:11,14/Jul/23 06:30,10/Mar/17 19:09,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"CSVFileFormat.inferSchema doesn't use FileScanRDD so the SQL ""ignoreCorruptFiles"" doesn't work.

{code}
java.io.EOFException: Unexpected end of input stream
	at org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:145)
	at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)
	at java.io.InputStream.read(InputStream.java:101)
	at org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:180)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:266)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:211)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1112)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1112)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1114)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1107)
	at org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:67)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:174)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$7.apply(DataSource.scala:174)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:173)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)
{code}

Right now a workaround is also setting ""spark.files.ignoreCorruptFiles"" to true.",,cloud_fan,maropu,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19082,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 19:09:28 UTC 2017,,,,,,,,,,"0|i3b70f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/17 07:21;cloud_fan;This is because we support different charset for CSV files, and our text file format only supports UTF8, so we have to use `HadoopRDD` when infer schema for CSV data source, which doesn't recognize the ignoreCorruptedFiles options

I've checked the history, this feature was there the first day we introduce CSV data source. However, all other text-based data source support only UTF8, also CSV with wholeFile enabled only supports UTF8.

shall we just remove the support for different charsets? or support this feature for all text-based data source?

cc [~hyukjin.kwon];;;","10/Mar/17 08:53;gurwls223;Thank you for cc'ing me. Up to my knowledge, {{LineReader}} itself assumes the input is UTF-8 according to according to [MAPREDUCE-232|https://issues.apache.org/jira/browse/MAPREDUCE-232], it looks [{{TextInputFormat}}|https://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/TextInputFormat.java] does not guarantee all encoding types but officially only UTF-8 (as commented in [{{LineRecordReader#L147}}|https://github.com/apache/hadoop/blob/master/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java#L147]).

So, it seems it only works if the encodings are compatible with this. IMHO, this is an incorrect implementation which we should remove. We should support this when actually we starts to read (not relying on {{TextInputFormat}}) but I guess this is a big job.

BTW, I think we use {{FileScanRDD}} in schema inference only in CSV datasource when encoding is default in favour of SPARK-18362 and I guess this probably would not work for JSON too (let me maybe open a PR for JSON one soon).
;;;","10/Mar/17 09:52;gurwls223;Also, we recently introduced reading a CSV from text dataset. I think there is a workaround even if we remove this. For example, this PR - https://github.com/apache/spark/pull/16854 illustrates an example that creates a dataset from text dataset that contains CSV rows by a custom input format. This encoding problem might be workaround via this.

FYI, I opened and closed a PR before to support non-compatible encodings in CSV - https://github.com/apache/spark/pull/11016 about a year ago. Maybe, we could try this by manually setting the record delimiter if we decide to support this. 

IMHO, I don't think this is also correct way because I am not sure if {{TextInputFormat}} supports other encoding types correctly by above references.;;;","10/Mar/17 19:09;cloud_fan;Oh, so this issue is already fixed by SPARK-18362 in Spark 2.2

Since it's not a critical issue and SPARK-18362 is an optimization, we should not backport it. Let's just resolve this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pivot with null as the pivot value throws NPE,SPARK-19882,13049583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,a1ray,gurwls223,,09/Mar/17 11:51,12/Dec/22 18:10,14/Jul/23 06:30,17/Mar/17 08:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"This seems a regression.

- Spark 1.6

{code}
Seq(Tuple1(None), Tuple1(Some(1))).toDF(""a"").groupBy($""a"").pivot(""a"").count().show()
{code}

prints

{code}
+----+----+---+
|   a|null|  1|
+----+----+---+
|null|   0|  0|
|   1|   0|  1|
+----+----+---+
{code}


- Current master

{code}
Seq(Tuple1(None), Tuple1(Some(1))).toDF(""a"").groupBy($""a"").pivot(""a"").count().show()
{code}

prints

{code}
java.lang.NullPointerException was thrown.
java.lang.NullPointerException
  at org.apache.spark.sql.catalyst.expressions.aggregate.PivotFirst$$anonfun$4.apply(PivotFirst.scala:145)
  at org.apache.spark.sql.catalyst.expressions.aggregate.PivotFirst$$anonfun$4.apply(PivotFirst.scala:143)
  at scala.collection.immutable.List.map(List.scala:273)
  at org.apache.spark.sql.catalyst.expressions.aggregate.PivotFirst.<init>(PivotFirst.scala:143)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolvePivot$$anonfun$apply$7$$anonfun$24.apply(Analyzer.scala:509)
{code}
",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 17 08:44:49 UTC 2017,,,,,,,,,,"0|i3b69r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/17 12:15;gurwls223;Actually, there are two problems here.

It should not throw an exception and the results should be

{code}
+----+----+---+
|   a|null|  1|
+----+----+---+
|null|   1|  0|
|   1|   0|  1|
+----+----+---+
{code}

given

{code}
scala> Seq(Tuple1(None), Tuple1(Some(1))).toDF(""a"").groupBy($""a"").count().show()
+----+-----+
|   a|count|
+----+-----+
|null|    1|
|   1|    1|
+----+-----+
{code};;;","09/Mar/17 12:29;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17224;;;","09/Mar/17 19:01;apachespark;User 'aray' has created a pull request for this issue:
https://github.com/apache/spark/pull/17226;;;","17/Mar/17 08:44;cloud_fan;Issue resolved by pull request 17226
[https://github.com/apache/spark/pull/17226];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hide API docs for ""org.apache.spark.sql.internal""",SPARK-19874,13049437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,08/Mar/17 23:25,09/Mar/17 07:16,14/Jul/23 06:30,09/Mar/17 07:16,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Build,,,,,0,,,,,,,,,"The API docs should not include the ""org.apache.spark.sql.internal"" package because they are internal private APIs.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 23:27:02 UTC 2017,,,,,,,,,,"0|i3b5db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 23:27;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17217;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the user changes the number of shuffle partitions between batches, Streaming aggregation will fail.",SPARK-19873,13049398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kunalkhamar,kunalkhamar,08/Mar/17 21:12,17/Mar/17 23:14,14/Jul/23 06:30,17/Mar/17 23:14,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"If the user changes the shuffle partition number between batches, Streaming aggregation will fail.

Here are some possible cases:

- Change ""spark.sql.shuffle.partitions""
- Use ""repartition"" and change the partition number in codes
- RangePartitioner doesn't generate deterministic partitions. Right now it's safe as we disallow sort before aggregation. Not sure if we will add some operators using RangePartitioner in future.

Fix:
Record # shuffle partitions in offset log and enforce in next batch",,apachespark,kunalkhamar,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19540,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 17 23:14:48 UTC 2017,,,,,,,,,,"0|i3b54n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 22:51;apachespark;User 'kunalkhamar' has created a pull request for this issue:
https://github.com/apache/spark/pull/17216;;;","17/Mar/17 23:14;tdas;Issue resolved by pull request 17216
[https://github.com/apache/spark/pull/17216];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnicodeDecodeError in Pyspark on sc.textFile read with repartition,SPARK-19872,13049391,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,bbruggeman,bbruggeman,08/Mar/17 20:50,12/Dec/22 18:10,14/Jul/23 06:30,15/Mar/17 17:14,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,,,,,1,,,,,,,,,"I'm receiving the following traceback:

{code}
>>> sc.textFile('test.txt').repartition(10).collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py"", line 810, in collect
    return list(_load_from_socket(port, self._jrdd_deserializer))
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py"", line 140, in _load_from_socket
    for item in serializer.load_stream(rf):
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py"", line 539, in load_stream
    yield self.loads(stream)
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py"", line 534, in loads
    return s.decode(""utf-8"") if self.use_unicode else s
  File ""/Users/brianbruggeman/.envs/dg/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte
{code}

I created a textfile (text.txt) with standard linux newlines:
{code}
a
b

d
e
f
g
h
i
j
k
l

{code}

I think ran pyspark:
{code}
$ pyspark
Python 2.7.13 (default, Dec 18 2016, 07:03:39)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/03/08 13:59:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/03/08 13:59:32 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (default, Dec 18 2016 07:03:39)
SparkSession available as 'spark'.
>>> sc.textFile('test.txt').collect()
[u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k', u'l']
>>> sc.textFile('test.txt', use_unicode=False).collect()
['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l']
>>> sc.textFile('test.txt', use_unicode=False).repartition(10).collect()
['\x80\x02]q\x01(U\x01aU\x01bU\x01cU\x01dU\x01eU\x01fU\x01ge.', '\x80\x02]q\x01(U\x01hU\x01iU\x01jU\x01kU\x01le.']
>>> sc.textFile('test.txt').repartition(10).collect()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py"", line 810, in collect
    return list(_load_from_socket(port, self._jrdd_deserializer))
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py"", line 140, in _load_from_socket
    for item in serializer.load_stream(rf):
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py"", line 539, in load_stream
    yield self.loads(stream)
  File ""/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py"", line 534, in loads
    return s.decode(""utf-8"") if self.use_unicode else s
  File ""/Users/brianbruggeman/.envs/dg/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 0: invalid start byte
{code}

This really looks like a bug in the `serializers.py` code.",Mac and EC2,apachespark,bbruggeman,kfb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 15 22:45:23 UTC 2017,,,,,,,,,,"0|i3b533:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 21:09;bbruggeman;This is a regression from spark 2.0.x.;;;","08/Mar/17 21:23;bbruggeman;I reverted `rdd.py` and `serializers.py` to the 2.0.2 branch in github and the code above works without an error.

rdd.py link: https://github.com/apache/spark/blob/branch-2.0/python/pyspark/rdd.py
serializers.py link: https://github.com/apache/spark/blob/branch-2.0/python/pyspark/serializers.py

rdd diff:
{code}
--- tmp2.py 2017-03-08 15:17:45.000000000 -0600
+++ saved2.py   2017-03-08 15:17:59.000000000 -0600
@@ -52,8 +52,6 @@
     get_used_memory, ExternalSorter, ExternalGroupBy
 from pyspark.traceback_utils import SCCallSiteSync

-from py4j.java_collections import ListConverter, MapConverter
-

 __all__ = [""RDD""]

@@ -137,11 +135,12 @@
         break
     if not sock:
         raise Exception(""could not open socket"")
-    # The RDD materialization time is unpredicable, if we set a timeout for socket reading
-    # operation, it will very possibly fail. See SPARK-18281.
-    sock.settimeout(None)
-    # The socket will be automatically closed when garbage-collected.
-    return serializer.load_stream(sock.makefile(""rb"", 65536))
+    try:
+        rf = sock.makefile(""rb"", 65536)
+        for item in serializer.load_stream(rf):
+            yield item
+    finally:
+        sock.close()


 def ignore_unicode_prefix(f):
@@ -264,13 +263,44 @@

     def isCheckpointed(self):
         """"""
-        Return whether this RDD has been checkpointed or not
+        Return whether this RDD is checkpointed and materialized, either reliably or locally.
         """"""
         return self._jrdd.rdd().isCheckpointed()

+    def localCheckpoint(self):
+        """"""
+        Mark this RDD for local checkpointing using Spark's existing caching layer.
+
+        This method is for users who wish to truncate RDD lineages while skipping the expensive
+        step of replicating the materialized data in a reliable distributed file system. This is
+        useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).
+
+        Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed
+        data is written to ephemeral local storage in the executors instead of to a reliable,
+        fault-tolerant storage. The effect is that if an executor fails during the computation,
+        the checkpointed data may no longer be accessible, causing an irrecoverable job failure.
+
+        This is NOT safe to use with dynamic allocation, which removes executors along
+        with their cached blocks. If you must use both features, you are advised to set
+        L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.
+
+        The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.
+        """"""
+        self._jrdd.rdd().localCheckpoint()
+
+    def isLocallyCheckpointed(self):
+        """"""
+        Return whether this RDD is marked for local checkpointing.
+
+        Exposed for testing.
+        """"""
+        return self._jrdd.rdd().isLocallyCheckpointed()
+
     def getCheckpointFile(self):
         """"""
         Gets the name of the file to which this RDD was checkpointed
+
+        Not defined if RDD is checkpointed locally.
         """"""
         checkpointFile = self._jrdd.rdd().getCheckpointFile()
         if checkpointFile.isDefined():
@@ -387,6 +417,9 @@
             with replacement: expected number of times each element is chosen; fraction must be >= 0
         :param seed: seed for the random number generator

+        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
+            count of the given :class:`DataFrame`.
+
         >>> rdd = sc.parallelize(range(100), 4)
         >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
         True
@@ -425,8 +458,8 @@
         """"""
         Return a fixed-size sampled subset of this RDD.

-        Note that this method should only be used if the resulting array is expected
-        to be small, as all the data is loaded into the driver's memory.
+        .. note:: This method should only be used if the resulting array is expected
+            to be small, as all the data is loaded into the driver's memory.

         >>> rdd = sc.parallelize(range(0, 10))
         >>> len(rdd.takeSample(True, 20, 1))
@@ -537,7 +570,7 @@
         Return the intersection of this RDD and another one. The output will
         not contain any duplicate elements, even if the input RDDs did.

-        Note that this method performs a shuffle internally.
+        .. note:: This method performs a shuffle internally.

         >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
         >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
@@ -768,8 +801,9 @@
     def collect(self):
         """"""
         Return a list that contains all of the elements in this RDD.
-        Note that this method should only be used if the resulting array is expected
-        to be small, as all the data is loaded into the driver's memory.
+
+        .. note:: This method should only be used if the resulting array is expected
+            to be small, as all the data is loaded into the driver's memory.
         """"""
         with SCCallSiteSync(self.context) as css:
             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
@@ -1214,12 +1248,12 @@

     def top(self, num, key=None):
         """"""
-        Get the top N elements from a RDD.
+        Get the top N elements from an RDD.

-        Note that this method should only be used if the resulting array is expected
-        to be small, as all the data is loaded into the driver's memory.
+        .. note:: This method should only be used if the resulting array is expected
+            to be small, as all the data is loaded into the driver's memory.

-        Note: It returns the list sorted in descending order.
+        .. note:: It returns the list sorted in descending order.

         >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
         [12]
@@ -1238,11 +1272,11 @@

     def takeOrdered(self, num, key=None):
         """"""
-        Get the N elements from a RDD ordered in ascending order or as
+        Get the N elements from an RDD ordered in ascending order or as
         specified by the optional key function.

-        Note that this method should only be used if the resulting array is expected
-        to be small, as all the data is loaded into the driver's memory.
+        .. note:: this method should only be used if the resulting array is expected
+            to be small, as all the data is loaded into the driver's memory.

         >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
         [1, 2, 3, 4, 5, 6]
@@ -1263,11 +1297,11 @@
         that partition to estimate the number of additional partitions needed
         to satisfy the limit.

-        Note that this method should only be used if the resulting array is expected
-        to be small, as all the data is loaded into the driver's memory.
-
         Translated from the Scala implementation in RDD#take().

+        .. note:: this method should only be used if the resulting array is expected
+            to be small, as all the data is loaded into the driver's memory.
+
         >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
         [2, 3]
         >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
@@ -1331,8 +1365,9 @@

     def isEmpty(self):
         """"""
-        Returns true if and only if the RDD contains no elements at all. Note that an RDD
-        may be empty even when it has at least 1 partition.
+        Returns true if and only if the RDD contains no elements at all.
+
+        .. note:: an RDD may be empty even when it has at least 1 partition.

         >>> sc.parallelize([]).isEmpty()
         True
@@ -1523,8 +1558,8 @@
         """"""
         Return the key-value pairs in this RDD to the master as a dictionary.

-        Note that this method should only be used if the resulting data is expected
-        to be small, as all the data is loaded into the driver's memory.
+        .. note:: this method should only be used if the resulting data is expected
+            to be small, as all the data is loaded into the driver's memory.

         >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()
         >>> m[1]
@@ -1761,8 +1796,7 @@
         set of aggregation functions.

         Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a ""combined
-        type"" C.  Note that V and C can be different -- for example, one might
-        group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).
+        type"" C.

         Users provide three functions:

@@ -1774,6 +1808,9 @@

         In addition, users can control the partitioning of the output RDD.

+        .. note:: V and C can be different -- for example, one might group an RDD of type
+            (Int, Int) into an RDD of type (Int, List[Int]).
+
         >>> x = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
         >>> def add(a, b): return a + str(b)
         >>> sorted(x.combineByKey(str, add, add).collect())
@@ -1845,9 +1882,9 @@
         Group the values for each key in the RDD into a single sequence.
         Hash-partitions the resulting RDD with numPartitions partitions.

-        Note: If you are grouping in order to perform an aggregation (such as a
-        sum or average) over each key, using reduceByKey or aggregateByKey will
-        provide much better performance.
+        .. note:: If you are grouping in order to perform an aggregation (such as a
+            sum or average) over each key, using reduceByKey or aggregateByKey will
+            provide much better performance.

         >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
         >>> sorted(rdd.groupByKey().mapValues(len).collect())
@@ -2018,8 +2055,7 @@
          >>> len(rdd.repartition(10).glom().collect())
          10
         """"""
-        jrdd = self._jrdd.repartition(numPartitions)
-        return RDD(jrdd, self.ctx, self._jrdd_deserializer)
+        return self.coalesce(numPartitions, shuffle=True)

     def coalesce(self, numPartitions, shuffle=False):
         """"""
@@ -2030,7 +2066,15 @@
         >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
         [[1, 2, 3, 4, 5]]
         """"""
-        jrdd = self._jrdd.coalesce(numPartitions, shuffle)
+        if shuffle:
+            # Decrease the batch size in order to distribute evenly the elements across output
+            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.
+            batchSize = min(10, self.ctx._batchSize or 1024)
+            ser = BatchedSerializer(PickleSerializer(), batchSize)
+            selfCopy = self._reserialize(ser)
+            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
+        else:
+            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
         return RDD(jrdd, self.ctx, self._jrdd_deserializer)

     def zip(self, other):
@@ -2316,16 +2360,9 @@
         # The broadcast will have same life cycle as created PythonRDD
         broadcast = sc.broadcast(pickled_command)
         pickled_command = ser.dumps(broadcast)
-    # There is a bug in py4j.java_gateway.JavaClass with auto_convert
-    # https://github.com/bartdag/py4j/issues/161
-    # TODO: use auto_convert once py4j fix the bug
-    broadcast_vars = ListConverter().convert(
-        [x._jbroadcast for x in sc._pickled_broadcast_vars],
-        sc._gateway._gateway_client)
+    broadcast_vars = [x._jbroadcast for x in sc._pickled_broadcast_vars]
     sc._pickled_broadcast_vars.clear()
-    env = MapConverter().convert(sc.environment, sc._gateway._gateway_client)
-    includes = ListConverter().convert(sc._python_includes, sc._gateway._gateway_client)
-    return pickled_command, broadcast_vars, env, includes
+    return pickled_command, broadcast_vars, sc.environment, sc._python_includes


 def _wrap_function(sc, func, deserializer, serializer, profiler=None):
@@ -2433,4 +2470,4 @@


 if __name__ == ""__main__"":
-    _test()
\ No newline at end of file
+    _test()
{code}

serializers diff:
{code}
--- tmp.py  2017-03-08 15:13:45.000000000 -0600
+++ <redacted>/lib/python2.7/site-packages/pyspark/serializers.py   2017-03-08 15:13:03.000000000 -0600
@@ -61,7 +61,7 @@
 if sys.version < '3':
     import cPickle as pickle
     protocol = 2
-    from itertools import izip as zip
+    from itertools import izip as zip, imap as map
 else:
     import pickle
     protocol = 3
@@ -96,7 +96,12 @@
         raise NotImplementedError

     def _load_stream_without_unbatching(self, stream):
-        return self.load_stream(stream)
+        """"""
+        Return an iterator of deserialized batches (lists) of objects from the input stream.
+        if the serializer does not operate on batches the default implementation returns an
+        iterator of single element lists.
+        """"""
+        return map(lambda x: [x], self.load_stream(stream))

     # Note: our notion of ""equality"" is that output generated by
     # equal serializers can be deserialized using the same serializer.
@@ -278,50 +283,57 @@
         return ""AutoBatchedSerializer(%s)"" % self.serializer


-class CartesianDeserializer(FramedSerializer):
+class CartesianDeserializer(Serializer):

     """"""
     Deserializes the JavaRDD cartesian() of two PythonRDDs.
+    Due to pyspark batching we cannot simply use the result of the Java RDD cartesian,
+    we additionally need to do the cartesian within each pair of batches.
     """"""

     def __init__(self, key_ser, val_ser):
-        FramedSerializer.__init__(self)
         self.key_ser = key_ser
         self.val_ser = val_ser

-    def prepare_keys_values(self, stream):
-        key_stream = self.key_ser._load_stream_without_unbatching(stream)
-        val_stream = self.val_ser._load_stream_without_unbatching(stream)
-        key_is_batched = isinstance(self.key_ser, BatchedSerializer)
-        val_is_batched = isinstance(self.val_ser, BatchedSerializer)
-        for (keys, vals) in zip(key_stream, val_stream):
-            keys = keys if key_is_batched else [keys]
-            vals = vals if val_is_batched else [vals]
-            yield (keys, vals)
+    def _load_stream_without_unbatching(self, stream):
+        key_batch_stream = self.key_ser._load_stream_without_unbatching(stream)
+        val_batch_stream = self.val_ser._load_stream_without_unbatching(stream)
+        for (key_batch, val_batch) in zip(key_batch_stream, val_batch_stream):
+            # for correctness with repeated cartesian/zip this must be returned as one batch
+            yield product(key_batch, val_batch)

     def load_stream(self, stream):
-        for (keys, vals) in self.prepare_keys_values(stream):
-            for pair in product(keys, vals):
-                yield pair
+        return chain.from_iterable(self._load_stream_without_unbatching(stream))

     def __repr__(self):
         return ""CartesianDeserializer(%s, %s)"" % \
                (str(self.key_ser), str(self.val_ser))


-class PairDeserializer(CartesianDeserializer):
+class PairDeserializer(Serializer):

     """"""
     Deserializes the JavaRDD zip() of two PythonRDDs.
+    Due to pyspark batching we cannot simply use the result of the Java RDD zip,
+    we additionally need to do the zip within each pair of batches.
     """"""

+    def __init__(self, key_ser, val_ser):
+        self.key_ser = key_ser
+        self.val_ser = val_ser
+
+    def _load_stream_without_unbatching(self, stream):
+        key_batch_stream = self.key_ser._load_stream_without_unbatching(stream)
+        val_batch_stream = self.val_ser._load_stream_without_unbatching(stream)
+        for (key_batch, val_batch) in zip(key_batch_stream, val_batch_stream):
+            if len(key_batch) != len(val_batch):
+                raise ValueError(""Can not deserialize PairRDD with different number of items""
+                                 "" in batches: (%d, %d)"" % (len(key_batch), len(val_batch)))
+            # for correctness with repeated cartesian/zip this must be returned as one batch
+            yield zip(key_batch, val_batch)
+
     def load_stream(self, stream):
-        for (keys, vals) in self.prepare_keys_values(stream):
-            if len(keys) != len(vals):
-                raise ValueError(""Can not deserialize RDD with different number of items""
-                                 "" in pair: (%d, %d)"" % (len(keys), len(vals)))
-            for pair in zip(keys, vals):
-                yield pair
+        return chain.from_iterable(self._load_stream_without_unbatching(stream))

     def __repr__(self):
         return ""PairDeserializer(%s, %s)"" % (str(self.key_ser), str(self.val_ser))
@@ -378,6 +390,16 @@
     _old_namedtuple = _copy_func(collections.namedtuple)

     def namedtuple(*args, **kwargs):
+        import sys
+        if sys.version.startswith('3'):
+            defaults = {
+                'verbose': False,
+                'rename': False,
+                'module': None,
+            }
+            for key, value in defaults.items():
+                if key not in kwargs:
+                    kwargs[key] = value
         cls = _old_namedtuple(*args, **kwargs)
         return _hack_namedtuple(cls)

@@ -559,4 +581,4 @@
     import doctest
     (failure_count, test_count) = doctest.testmod()
     if failure_count:
-        exit(-1)
\ No newline at end of file
+        exit(-1)
{code};;;","08/Mar/17 21:45;bbruggeman;Using the Spark 2.1.0 serializers.py and the Spark 2.0.2 rdd.py, the code runs.;;;","09/Mar/17 08:42;srowen;(Blocker is for committers to determine);;;","13/Mar/17 19:14;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17282;;;","15/Mar/17 21:00;bbruggeman;Wondering if a test will be added to prevent future regressions.;;;","15/Mar/17 22:45;gurwls223;Yup, test was added.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
conflict TasksetManager lead to spark stopped,SPARK-19868,13049242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liujianhuiouc,liujianhui,liujianhui,08/Mar/17 11:52,15/Feb/18 01:15,14/Jul/23 06:30,28/Mar/17 19:16,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"##scenario
 conflict taskSetManager throw an exception which lead to sparkcontext stopped. log as 
{code}
java.lang.IllegalStateException: more than one active taskSet for stage 4571114: 4571114.2,4571114.1
        at org.apache.spark.scheduler.TaskSchedulerImpl.submitTasks(TaskSchedulerImpl.scala:173)
        at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1052)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:921)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1214)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1637)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

the reason for that is the resubmitting of stage conflict with the running stage，the missing task of stage should be resubmit since the zoombie of the tasksetManager assigned by true

{code}
[INFO][dag-scheduler-event-loop][2017-03-03+22:16:29.547][org.apache.spark.scheduler.DAGScheduler]Resubmitting ShuffleMapStage 4571114 (map at MainApp.scala:73) because some of its tasks had failed: 0
[INFO][dag-scheduler-event-loop][2017-03-03+22:16:29.547][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4571114 (MapPartitionsRDD[3719544] at map at MainApp.scala:73), which has no missing parents

{code}

the executor which the shuffle task ran on was lost
{code}
[INFO][dag-scheduler-event-loop][2017-03-03+22:16:27.427][org.apache.spark.scheduler.DAGScheduler]Ignoring possibly bogus ShuffleMapTask(4571114, 0) completion from executor 4
{code}

the time of the task set finished and the resubmit of stage
{code}
handleSuccessfuleTask
[INFO][task-result-getter-2][2017-03-03+22:16:29.999][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4571114.1, whose tasks have all completed, from pool 

resubmit stage
[INFO][dag-scheduler-event-loop][2017-03-03+22:16:29.549][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4571114.2 with 1 tasks
{code}",,apachespark,DjvuLee,irashid,liujianhui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23433,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 12:02:05 UTC 2017,,,,,,,,,,"0|i3b45z:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"08/Mar/17 12:02;apachespark;User 'liujianhuiouc' has created a pull request for this issue:
https://github.com/apache/spark/pull/17208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add makeQualifiedPath in SQLTestUtils to optimize some code,SPARK-19864,13049148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,windpiger,windpiger,windpiger,08/Mar/17 04:20,08/Mar/17 18:49,14/Jul/23 06:30,08/Mar/17 18:49,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently there are lots of places to make the path qualified, it is better to provide a function to do this, then the code will be more simple.",,apachespark,cloud_fan,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 18:49:08 UTC 2017,,,,,,,,,,"0|i3b3l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 04:23;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17204;;;","08/Mar/17 18:49;cloud_fan;Issue resolved by pull request 17204
[https://github.com/apache/spark/pull/17204];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark should not be a negative time.,SPARK-19861,13049138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,08/Mar/17 03:28,09/Mar/17 19:08,14/Jul/23 06:30,09/Mar/17 19:08,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,,,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 03:31:03 UTC 2017,,,,,,,,,,"0|i3b3iv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/17 03:31;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17202;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The new watermark should override the old one,SPARK-19859,13049085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,07/Mar/17 23:49,09/Mar/17 02:33,14/Jul/23 06:30,08/Mar/17 04:35,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"The new watermark should override the old one. Otherwise, we just pick up the first column which has a watermark, it may be unexpected.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 09 02:33:04 UTC 2017,,,,,,,,,,"0|i3b373:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/17 23:55;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17199;;;","09/Mar/17 02:33;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17221;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CredentialUpdater calculates the wrong time for next update,SPARK-19857,13049075,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,07/Mar/17 23:09,17/May/20 18:13,14/Jul/23 06:30,08/Mar/17 00:22,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"This is the code:

{code}
            val remainingTime = getTimeOfNextUpdateFromFileName(credentialsStatus.getPath)
              - System.currentTimeMillis()
{code}

If you spot the problem, you get a virtual cookie.
",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 23:12:04 UTC 2017,,,,,,,,,,"0|i3b34v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/17 23:12;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17198;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uppercase Kafka topics fail when startingOffsets are SpecificOffsets,SPARK-19853,13049051,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,uncleGen,chris.bowden,chris.bowden,07/Mar/17 22:16,13/Mar/17 00:56,14/Jul/23 06:30,13/Mar/17 00:56,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"When using the KafkaSource with Structured Streaming, consumer assignments are not what the user expects if startingOffsets is set to an explicit set of topics/partitions in JSON where the topic(s) happen to have uppercase characters. When StartingOffsets is constructed, the original string value from options is transformed toLowerCase to make matching on ""earliest"" and ""latest"" case insensitive. However, the toLowerCase json is passed to SpecificOffsets for the terminal condition, so topic names may not be what the user intended by the time assignments are made with the underlying KafkaConsumer.

From KafkaSourceProvider:
{code}
val startingOffsets = caseInsensitiveParams.get(STARTING_OFFSETS_OPTION_KEY).map(_.trim.toLowerCase) match {
    case Some(""latest"") => LatestOffsets
    case Some(""earliest"") => EarliestOffsets
    case Some(json) => SpecificOffsets(JsonUtils.partitionOffsets(json))
    case None => LatestOffsets
  }
{code}",,apachespark,chris.bowden,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 12:06:24 UTC 2017,,,,,,,,,,"0|i3b2zj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"08/Mar/17 12:06;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17209;;;","08/Mar/17 12:06;uncleGen;Good catch! I will open a pr to fix this. Could you please help to review?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingDeduplicateExec.watermarkPredicate should filter rows based on keys,SPARK-19841,13048706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,07/Mar/17 00:18,08/Mar/17 04:33,14/Jul/23 06:30,08/Mar/17 04:33,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,Right now it just uses the rows to filter but a column position in keyExpressions may be different than the position in row.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 00:23:02 UTC 2017,,,,,,,,,,"0|i3b107:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/17 00:23;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17183;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch failure throws a SparkException in SparkHiveWriter,SPARK-19837,13048601,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,sitalkedia@gmail.com,sitalkedia@gmail.com,06/Mar/17 18:40,22/Mar/17 16:07,14/Jul/23 06:30,06/Mar/17 20:07,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,,,,,,,,,"Currently Fetchfailure in SparkHiveWriter fails the job with following exception

{code}
0_0): org.apache.spark.SparkException: Task failed while writing rows.
        at org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer.writeToFile(hiveWriterContainers.scala:385)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:84)
        at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:84)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.shuffle.FetchFailedException: Connection reset by peer
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
        at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
        at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedStreamed(SortMergeJoinExec.scala:731)
        at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextOuterJoinRows(SortMergeJoinExec.scala:692)
        at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceStream(SortMergeJoinExec.scala:854)
        at org.apache.spark.sql.execution.joins.OneSideOuterIterator.advanceNext(SortMergeJoinExec.scala:887)
        at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
        at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:211)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
        at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:211)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.hive.SparkHiveDynamicPartitionWriterContainer.writeToFile(hiveWriterContainers.scala:343)
{code}",,irashid,sitalkedia@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 22 16:07:06 UTC 2017,,,,,,,,,,"0|i3b0cv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/17 20:06;sitalkedia@gmail.com;`SparkHiveDynamicPartitionWriterContainer` has been refactored in latest master. Not sure if the issue still exists. Will close this JIRA and reopen if we still see issue with latest. ;;;","22/Mar/17 16:07;irashid;I think this is addressed by SPARK-19276, which handles the main problem here.  We should clean up the exception handling to avoid encapsulating fetch failures, though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicPartitionWriteTask should escape the partition name ,SPARK-19832,13048446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,06/Mar/17 06:52,07/Mar/17 06:37,14/Jul/23 06:30,07/Mar/17 06:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently in DynamicPartitionWriteTask, when we get the paritionPath of a parition, we just escape the partition value, not escape the partition name.

this will cause some problems for some  special partition name situation, for example :
1) if the partition name contains '%' etc,  there will be two partition path created in the filesytem, one is for escaped path like '/path/a%25b=1', another is for unescaped path like '/path/a%b=1'.
and the data inserted stored in unescaped path, while the show partitions table will return 'a%25b=1' which the partition name is escaped. So here it is not consist. And I think the data should be stored in the escaped path in filesystem, which Hive2.0.0 also have the same action.

2) if the partition name contains ':', there will throw exception that new Path(""/path"",""a:b""), this is illegal which has a colon in the relative path.

{code}
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: a:b
  at org.apache.hadoop.fs.Path.initialize(Path.java:205)
  at org.apache.hadoop.fs.Path.<init>(Path.java:171)
  at org.apache.hadoop.fs.Path.<init>(Path.java:88)
  ... 48 elided
Caused by: java.net.URISyntaxException: Relative path in absolute URI: a:b
  at java.net.URI.checkPath(URI.java:1823)
  at java.net.URI.<init>(URI.java:745)
  at org.apache.hadoop.fs.Path.initialize(Path.java:202)
  ... 50 more
{code}


",,apachespark,cloud_fan,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 06:36:56 UTC 2017,,,,,,,,,,"0|i3azen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/17 07:00;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17173;;;","07/Mar/17 06:36;cloud_fan;Issue resolved by pull request 17173
[https://github.com/apache/spark/pull/17173];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R to support JSON array in column from_json,SPARK-19828,13048396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,felixcheung,felixcheung,06/Mar/17 00:17,12/Dec/22 17:35,14/Jul/23 06:30,15/Mar/17 02:52,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,SQL,,,,0,,,,,,,,,,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 16 21:50:02 UTC 2017,,,,,,,,,,"0|i3az3j:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"06/Mar/17 00:17;felixcheung;see SPARK-19595;;;","06/Mar/17 11:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17178;;;","16/Apr/17 21:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17653;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone master JSON not showing cores for running applications,SPARK-19824,13048347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jiangxb1987,MiniMizer,MiniMizer,05/Mar/17 08:46,19/Jun/17 05:05,14/Jul/23 06:30,19/Jun/17 05:05,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Deploy,,,,,0,,,,,,,,,"The JSON API of the standalone master (""/json"") does not show the number of cores for a running application, which is available on the UI.

  ""activeapps"" : [ {
    ""starttime"" : 1488702337788,
    ""id"" : ""app-20170305102537-19717"",
    ""name"" : ""POPAI_Aggregated"",
    ""user"" : ""ibiuser"",
    ""memoryperslave"" : 16384,
    ""submitdate"" : ""Sun Mar 05 10:25:37 IST 2017"",
    ""state"" : ""RUNNING"",
    ""duration"" : 1141934
  } ],",,apachespark,MiniMizer,yongtang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 15:04:03 UTC 2017,,,,,,,,,,"0|i3aysn:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"05/Mar/17 11:36;srowen;I guess it doesn't show ""memory per executor"" either? That came up yesterday.
I don't know the standalone master well but it does look like this is in the UI. I'm not sure it has to be in the JSON but seems reasonable to be consistent.;;;","06/Mar/17 21:13;apachespark;User 'yongtang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17181;;;","14/Jun/17 15:04;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18303;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckpointSuite.testCheckpointedOperation: should not check checkpointFilesOfLatestTime by the PATH string.,SPARK-19822,13048341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,05/Mar/17 06:07,06/Mar/17 02:19,14/Jul/23 06:30,06/Mar/17 02:19,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Tests,,,,,0,,,,,,,,,,,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 05 06:08:02 UTC 2017,,,,,,,,,,"0|i3ayrb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/17 06:08;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17167;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rbind should check for name consistency of input data frames,SPARK-19818,13048248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,04/Mar/17 02:59,07/Mar/17 05:55,14/Jul/23 06:30,07/Mar/17 05:55,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,releasenotes,,,,,,,,"The current implementation accepts data frames with different schemas. See issues below:
{code}
df <- createDataFrame(data.frame(name = c(""Michael"", ""Andy"", ""Justin""), age = c(1, 30, 19)))
union(df, df[, c(2, 1)])
     name     age
1 Michael     1.0
2    Andy    30.0
3  Justin    19.0
4     1.0 Michael
{code}",,actuaryzhang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 04 03:01:03 UTC 2017,,,,,,,,,,"0|i3ay8f:",9223372036854775807,,,,,felixcheung,,,,,,,,2.2.0,,,,,,,,,,,"04/Mar/17 03:01;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17159;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maxFilesPerTrigger combo latestFirst may miss old files in combination with maxFileAge in FileStreamSource,SPARK-19813,13048139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,03/Mar/17 18:55,08/Mar/17 22:36,14/Jul/23 06:30,08/Mar/17 22:36,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"There is a file stream source option called maxFileAge which limits how old the files can be, relative the latest file that has been seen. This is used to limit the files that need to be remembered as ""processed"". Files older than the latest processed files are ignored. This values is by default 7 days.
This causes a problem when both 
 - latestFirst = true
 - maxFilesPerTrigger > total files to be processed.

Here is what happens in all combinations
 1) latestFirst = false - Since files are processed in order, there wont be any unprocessed file older than the latest processed file. All files will be processed.
 2) latestFirst = true AND maxFilesPerTrigger is not set - The maxFileAge thresholding mechanism takes one batch initialize. If maxFilesPerTrigger is not, then all old files get processed in the first batch, and so no file is left behind.
 3) latestFirst = true AND maxFilesPerTrigger is set to X - The first batch process the latest X files. That sets the threshold latest file - maxFileAge, so files older than this threshold will never be considered for processing. 

The bug is with case 3.",,apachespark,brkyvz,ganeshchand@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 03 22:09:02 UTC 2017,,,,,,,,,,"0|i3axk7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"03/Mar/17 22:09;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/17153;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN shuffle service fails to relocate recovery DB across NFS directories,SPARK-19812,13048083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,03/Mar/17 15:42,17/May/20 18:13,14/Jul/23 06:30,26/Apr/17 13:25,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"The yarn shuffle service tries to switch from the yarn local directories to the real recovery directory but can fail to move the existing recovery db's.  It fails due to Files.move not doing directories that have contents.

2017-03-03 14:57:19,558 [main] ERROR yarn.YarnShuffleService: Failed to move recovery file sparkShuffleRecovery.ldb to the path /mapred/yarn-nodemanager/nm-aux-services/spark_shuffle
java.nio.file.DirectoryNotEmptyException:/yarn-local/sparkShuffleRecovery.ldb
        at sun.nio.fs.UnixCopyFile.move(UnixCopyFile.java:498)
        at sun.nio.fs.UnixFileSystemProvider.move(UnixFileSystemProvider.java:262)
        at java.nio.file.Files.move(Files.java:1395)
        at org.apache.spark.network.yarn.YarnShuffleService.initRecoveryDb(YarnShuffleService.java:369)
        at org.apache.spark.network.yarn.YarnShuffleService.createSecretManager(YarnShuffleService.java:200)
        at org.apache.spark.network.yarn.YarnShuffleService.serviceInit(YarnShuffleService.java:174)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:143)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:262)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:357)
        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:636)
        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:684)

This used to use f.renameTo and we switched it in the pr due to review comments and it looks like didn't do a final real test. The tests are using files rather then directories so it didn't catch. We need to fix the test also.

history: https://github.com/apache/spark/pull/14999/commits/65de8531ccb91287f5a8a749c7819e99533b9440",,apachespark,devaraj,jerryshao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 24 18:02:04 UTC 2017,,,,,,,,,,"0|i3ax7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/17 15:43;tgraves;note that it will go ahead and start using the recovery db, it just doesn't copy over the old one so anything running gets lost.;;;","08/Mar/17 07:10;jerryshao;[~tgraves], I'm not quite sure what you mean here?

bq. The tests are using files rather then directories so it didn't catch. We need to fix the test also.

From my understanding this issues happens when dest dir is not empty and try to move with REPLACE_EXISTING. Also be happened when calling rename failed and the source dir is not empty directory.

But I cannot imagine how this happened, from the log it is more like a `rename` failure issue, since the path in Exception points to source dir.
;;;","21/Apr/17 20:44;tgraves;Sorry wasn't clear in the original description, it errors when moving across nfs mounts.  I was wrong on the unit test comment.  Unit test is fine except it doesn't test across mounts which would be really difficult to do.;;;","24/Apr/17 18:02;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/17748;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on zero-size ORC file,SPARK-19809,13047966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,mdawid92,mdawid92,03/Mar/17 09:58,12/Dec/22 18:11,14/Jul/23 06:30,13/Dec/17 06:42,1.6.3,2.0.2,2.1.1,2.2.1,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"When reading from hive ORC table if there are some 0 byte files we get NullPointerException:
{code}java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:190)
	at org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)
	at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)
	at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)
	at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)
	at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)
	at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.zeppelin.spark.ZeppelinContext.showDF(ZeppelinContext.java:209)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:129)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:176)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745){code}",,apachespark,asukhenko,dongjoon,glenn.strycker@gmail.com,mdawid92,prashanthsandela,renu_yadav,shirisht,tafranky@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,SPARK-19752,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/18 04:29;tafranky@gmail.com;image-2018-02-26-20-29-49-410.png;https://issues.apache.org/jira/secure/attachment/12912198/image-2018-02-26-20-29-49-410.png","27/Feb/18 04:33;tafranky@gmail.com;spark.sql.hive.convertMetastoreOrc.txt;https://issues.apache.org/jira/secure/attachment/12912199/spark.sql.hive.convertMetastoreOrc.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 04 19:42:20 UTC 2019,,,,,,,,,,"0|i3awhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/17 09:57;gurwls223;I don't think there is 0 byte ORC file. It should have the footer. Moreover, currently, Spark's ORC datasource does not write out empty files (see https://issues.apache.org/jira/browse/SPARK-15474).

Please reopen this if I misunderstood. It would be great if there is some steps to reproduce maybe to verify this issue.

I am resolving this.;;;","03/Apr/17 11:57;mdawid92;Those empty files have been created while processing with Pig scripts.
{code}-rw-rw-rw-   3 etl hdfs      14103 2017-04-03 01:26 part-v001-o000-r-00000_a_2
-rw-rw-rw-   3 etl hdfs          0 2017-04-03 01:26 part-v001-o000-r-00000_a_3
-rw-rw-rw-   3 etl hdfs      10125 2017-04-03 01:27 part-v001-o000-r-00000_a_4 {code};;;","03/Apr/17 12:26;gurwls223;Shoudn't it contain footer and schema information or a magic number at least? I am not sure if we can say 0 byte file is an ORC file. ;;;","26/May/17 17:09;dongjoon;IMO, we had better be more robust on this. The 3rd party tools (reported pig or sqoop) sometimes introduce this issues. 
{code}
scala> sql(""create table empty_orc(a int) stored as orc location '/tmp/empty_orc'"").show
++
||
++
++

$ touch /tmp/empty_orc/zero.orc

scala> sql(""select * from empty_orc"").show
java.lang.RuntimeException: serious problem
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
{code};;;","27/May/17 08:52;gurwls223;I think this is then rather about handling malformed files (e.g., {{spark.sql.files.ignoreCorruptFiles}}).;;;","27/May/17 16:16;dongjoon;[~hyukjin.kwon]. I don't think so. Parquet file does not need `spark.sql.files.ignoreCorruptFiles` option.

{code}
scala> sql(""create table empty_parquet(a int) stored as parquet location '/tmp/empty_parquet'"").show
++
||
++
++

$ touch /tmp/empty_parquet/zero.parquet

scala> sql(""select * from empty_parquet"").show
+---+
|  a|
+---+
+---+
{code}

You can test this in Spark with SPARK-20728.

{code}
scala> sql(""create table empty_orc2(a int) using orc location '/tmp/empty_orc'"").show
++
||
++
++

scala> sql(""select * from empty_orc2"").show
+---+
|  a|
+---+
+---+
{code}

I think this is a part of SPARK-20901. And ORC community will handle this. What we need is just to use latest ORC. One thing I'm wondering is this is tracked in https://issues.apache.org/jira/browse/ORC-162 (Open).;;;","29/May/17 01:26;gurwls223;Yea, I agree that it should be dependent on the format specification/implementation, whether it is malformed or not. I think Parquet itself treats 0 bytes files as malformed file because it should read footer but it throws an exception up to my knowledge. 

The former case looks filtering out the whole partitions in {{FileSourceScanExec}}. Parquet requires to read the footers and it throws an exception, for example, I manually updated the code path to not skip the partitions so that the parquet reader is actually being called as below:

{code}
java.lang.RuntimeException: file:/.../tmp.abc is not a Parquet file (too small)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)
	at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:568)
	at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:492)
	at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:166)
	at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:147)
{code}

If we don't specify the schema, it also throws an exception as below:

{code}
spark.read.parquet("".../tmp.abc"").show()
{code}

{code}
java.io.IOException: Could not read footer for file: FileStatus{path=file:/.../tmp.abc; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:498)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:485)
	at scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)
	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)
	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)
{code}

Assuming it is treated as a malformed file (per the ORC JIRA you pointed out above) for the current status, it looks a malformed file and it sounds we should be able to skip this in client side whether it should be dealt with {{spark.sql.files.ignoreCorruptFiles}} or not.

For example, I found a related JIRA - https://issues.apache.org/jira/browse/AVRO-1530 and https://issues.apache.org/jira/browse/HIVE-11977. _If I read this correctly_, Avro looks decided not to change the behaviour but Hive deals with it.

Only for this issue, I also agree that this could be a subset of the issues you pointed out.;;;","29/May/17 03:01;dongjoon;Great investigation! Thank you.;;;","19/Jun/17 13:52;renu_yadav;What is the resolution of this issue. spark.sql.files.ignoreCorruptFiles does not work for orc file.
Please help.;;;","19/Jun/17 14:18;gurwls223;What you see is what you get. This is ""Reopened"" per the discussion above and ""Unresolved"" yet.;;;","19/Jun/17 14:33;dongjoon;Yep. I'm trying to fix this with new ORC data source. It will be 2.3.0.;;;","12/Dec/17 01:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19948;;;","12/Dec/17 22:42;gurwls223;Issue resolved by pull request 19948
[https://github.com/apache/spark/pull/19948];;;","13/Dec/17 02:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19960;;;","13/Dec/17 02:32;dongjoon;Since Hive 1.2.1 library code path still has this problem, users may hit this when spark.sql.hive.convertMetastoreOrc=false. However, after SPARK-22279, Apache Spark with the default configuration doesn't hit this bug. The PR adds a test coverage for `convertMetastoreOrc=true (default)` on both `native` and `hive` ORC implementation in order to prevent regression.
;;;","27/Feb/18 04:33;tafranky@gmail.com;Need a pointer on the following.  

Env : Spark 2.2.1

1- I set the property  spark.sql.hive.convertMetastoreOrc to true

2- My hive table has the following  schema

CREATE TABLE `ft_orc`(
 `int` int,
 `double` double,
 `big+int` bigint,
 `$tring` string,
 `(decimal)` decimal(15,8),
 `flo@t` float,
 `datetime` date,
 `timestamp` timestamp,
 `01` int)
 CLUSTERED BY (
 `int`)
 INTO 20 BUCKETS
 ROW FORMAT SERDE
 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
 WITH SERDEPROPERTIES (
 'field.delim'=',',
 'serialization.format'=',')
 STORED AS INPUTFORMAT
 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
 OUTPUTFORMAT
 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' ;

 

I loaded the table  with 1 row of  data 

!image-2018-02-26-20-29-49-410.png!

I tried to  run the following simple statement  

scala> var res =spark.sql("" SELECT alias.`int` as a0, alias.`double` as a1, alias.`big+int` as a2, alias.`$tring` as a3, CAST(alias.`(decimal)` AS DOUBLE) as a4, CAST(alias.`flo@t` AS DOUBLE) as a5, CAST(alias.`datetime` AS TIMESTAMP) as a6, alias.`timestamp` as a7, alias.`01` as a8 FROM default.ft_orc alias"" )
18/02/27 04:30:57 WARN HiveConf: HiveConf of name hive.conf.hidden.list does not exist
18/02/27 04:30:57 WARN HiveConf: HiveConf of name hive.conf.hidden.list does not exist
java.lang.IndexOutOfBoundsException
 at java.nio.Buffer.checkIndex(Buffer.java:540)
 at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:374)
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:316)
 at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:187)
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:68)
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:67)
 at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
 at scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)
 at scala.collection.AbstractIterator.collectFirst(Iterator.scala:1336)
 at org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:69)
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
 at scala.collection.immutable.List.foreach(List.scala:381)
 at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
 at scala.collection.immutable.List.flatMap(List.scala:344)

 

Any pointer ? 

Should I file a separate Jira ? ;;;","27/Feb/18 04:36;dongjoon;[~tafranky@gmail.com]. Please see the fixed version of this JIRA issue. It's 2.3.0, not 2.2.1.;;;","27/Feb/18 04:43;tafranky@gmail.com;1- I am kind of constrained to  spark 2.2.1  at the moment . 

2- My understanding is that the only thing different with spark 2.3.0 is that  spark.sql.hive.convertMetastoreOrc  is defaulted to true. 

I looked at  [https://github.com/apache/spark/pull/19948]  and  [https://github.com/apache/spark/pull/19960] . Am I missing  anything ? ;;;","27/Feb/18 04:52;dongjoon;2.3.0 RC5 voting will end tonight. You had better try 2.3.0 RC5 and report a JIRA issue against 2.3.0.

1. For 2.1.1, that's too bad. There is no plan to resolve this in 2.1.1. It's already too old. This will works only Spark 2.3.0. If there are some issues, they will be fixed on Spark 2.3.1.

2. As you see, those PRs are adding test cases. ORC also added zero-size file supports, but Spark handles them in an upper layer, too. That's the difference you are wondering.
;;;","27/Feb/18 05:01;tafranky@gmail.com;Just to confirm , your  earlier comment referred to  spark 2.1.1. You meant  spark 2.2.1  right ?  ;;;","27/Feb/18 17:14;dongjoon;I meant 2.1.1 literally. :);;;","31/Aug/18 04:13;shirisht;[~dongjoon] I am encountering the same problem even with Spark version 2.3.1.
{code:java}
[local:~] spark-shell
2018-08-30 21:07:25 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1535688452266).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sql(""create table empty_orc(a int) stored as orc location '/tmp/empty_orc'"").show
2018-08-30 21:07:44 WARN  ObjectStore:6666 - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2018-08-30 21:07:44 WARN  ObjectStore:568 - Failed to get database default, returning NoSuchObjectException
2018-08-30 21:07:45 WARN  ObjectStore:568 - Failed to get database global_temp, returning NoSuchObjectException
++
||
++
++

// in a different terminal, I did ""touch /tmp/empty_orc/zero.orc""

scala> sql(""select * from empty_orc"").show
java.lang.RuntimeException: serious problem
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)
  ... 49 elided
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010)
  ... 99 more

scala> 

{code};;;","31/Aug/18 09:08;dongjoon;Hi, [~shirisht]. You need to turn on `convertMetastoreOrc`

{code}
scala> sql(""set spark.sql.hive.convertMetastoreOrc=true"")
res4: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> sql(""select * from empty_orc"").show
+---+
|  a|
+---+
+---+
{code};;;","05/Sep/18 20:51;shirisht;Thank you [~dongjoon];;;","04/Jan/19 19:27;prashanthsandela;[~dongjoon] I'm encountering same similar issue with spark version 2.3.1

I'm trying to read from a table which was ingested by sqoop. There are few 0 byte files for this table. The file sizes looks like below: 
{noformat}
-rw-rw-r-- 3 cloud-user root 17.3 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00000
-rw-rw-r-- 3 cloud-user root 10.3 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00001
-rw-rw-r-- 3 cloud-user root 19.9 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00002
-rw-rw-r-- 3 cloud-user root 13.0 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00003
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00004
-rw-rw-r-- 3 cloud-user root 3.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00005
-rw-rw-r-- 3 cloud-user root 13.8 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00006
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00007
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00008
-rw-rw-r-- 3 cloud-user root 6.9 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00009
-rw-rw-r-- 3 cloud-user root 9.0 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00010
-rw-rw-r-- 3 cloud-user root 11.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00011
-rw-rw-r-- 3 cloud-user root 14.7 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00012
-rw-rw-r-- 3 cloud-user root 17.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00013
-rw-rw-r-- 3 cloud-user root 17.1 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00014{noformat}
 

Spark throws exception while reading this table.
{noformat}
scala> spark.read.table(""table_with_few_zero_byte_files"").show() 
java.lang.RuntimeException: serious problem at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048) at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253) at org.apache.spark.sql.Dataset.head(Dataset.scala:2484) at org.apache.spark.sql.Dataset.take(Dataset.scala:2698) at org.apache.spark.sql.Dataset.showString(Dataset.scala:254) at org.apache.spark.sql.Dataset.show(Dataset.scala:723) at org.apache.spark.sql.Dataset.show(Dataset.scala:682) at org.apache.spark.sql.Dataset.show(Dataset.scala:691) ... 49 elided Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010) ... 99 more 
 
scala> sql(""set spark.sql.hive.convertMetastoreOrc=true"") 
res22: org.apache.spark.sql.DataFrame = [key: string, value: string] 
 
scala> spark.read.table(""table_with_few_zero_byte_files"").show() 
java.lang.IndexOutOfBoundsException at java.nio.Buffer.checkIndex(Buffer.java:540) at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:377) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:319) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:187) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:75) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:73) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145) at scala.collection.AbstractIterator.collectFirst(Iterator.scala:1336) at org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:86) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:95) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:95) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.immutable.List.foreach(List.scala:381) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.immutable.List.flatMap(List.scala:344) at org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:95) at org.apache.spark.sql.hive.orc.OrcFileFormat.inferSchema(OrcFileFormat.scala:63) at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:193) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:192) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:192) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:185) at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54) at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:185) at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:205) at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:226) at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:215) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286) at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:215) at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:180) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76) at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124) at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118) at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103) at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57) at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:627) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:623) at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:654) ... 49 elided

{noformat}
Unfortunately, we don't control the creation of table. What would be a config that could help me read this table?

 

 ;;;","04/Jan/19 19:37;dongjoon;Hi, did you use `spark.sql.orc.impl=native`, too? New ORC is not default in Spark 2.3.x.;;;","04/Jan/19 19:42;prashanthsandela;Awesome! This config of `spark.sql.orc.impl=native` works. Thanks for quick response. ;;;",,,,,,,,,,,,
HiveClientImpl does not work with Hive 2.2.0 metastore,SPARK-19804,13047871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,03/Mar/17 00:40,15/Dec/17 21:50,14/Jul/23 06:30,04/Mar/17 02:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"I know that Spark currently does not officially support Hive 2.2 (perhaps because it hasn't been released yet); but we have some 2.2 patches in CDH and the current code in the isolated client fails. The most probably culprit are changes added in HIVE-13149.

The fix is simple, and here's the patch we applied in CDH:
https://github.com/cloudera/spark/commit/954f060afe6ed469e85d656abd02790a79ec07a0

Fixing that doesn't affect any existing Hive version support, but will make it easier to support 2.2 when it's out.",,csun,curt,smilegator,toop,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22742,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 15 21:50:26 UTC 2017,,,,,,,,,,"0|i3avwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/17 02:06;vanzin;For posterity, the error you get looks like this:

{noformat}
java.lang.ExceptionInInitializerError: null
	at java.lang.Class.getConstructor0(Class.java:2892)
	at java.lang.Class.getDeclaredConstructor(Class.java:2058)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1541)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:67)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:82)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3220)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3239)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3464)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:226)
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:210)
	at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:333)
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:294)
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:269)
	at org.apache.spark.sql.hive.client.ClientWrapper.client(ClientWrapper.scala:272)
{noformat}

Which is rather cryptic; it's caused by one of the classes in the constructor being loaded by two different class loaders, so {{getDeclaredConstructor}} fails to find the right constructor and returns null.;;;","04/Mar/17 02:46;smilegator;Resolved by https://github.com/apache/spark/pull/17154;;;","15/Dec/17 21:47;curt;[~smilegator] , I've made some comments on above PR, so for this issue, does it mean, before this fix (spark 2.2.0 release), all the version of spark can not talk to hive 2.2.0 metastore?

;;;","15/Dec/17 21:50;vanzin;Spark still doesn't have explicit support for Hive 2.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky BlockManagerProactiveReplicationSuite tests,SPARK-19803,13047770,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shubhamc,sitalkedia@gmail.com,sitalkedia@gmail.com,02/Mar/17 19:20,28/Mar/17 01:48,14/Jul/23 06:30,28/Mar/17 01:48,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,Tests,,,,0,flaky-test,,,,,,,,The tests added for BlockManagerProactiveReplicationSuite has made the jenkins build flaky. Please refer to the build for more details - https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/73640/testReport/,,apachespark,cloud_fan,kayousterhout,shubhamc,sitalkedia@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 01:48:16 UTC 2017,,,,,,,,,,"0|i3ava7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/17 03:05;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17144;;;","07/Mar/17 20:30;kayousterhout;Thanks for fixing this [~uncleGen] and for reporting it [~sitalkedia@gmail.com];;;","15/Mar/17 00:57;kayousterhout;This does not appear to be fixed -- it looks like there's some error condition in the underlying code that can cause this to break?  From https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/74412/testReport/org.apache.spark.storage/BlockManagerProactiveReplicationSuite/proactive_block_replication___5_replicas___4_block_manager_deletions/: 

org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 493 times over 5.007521253999999 seconds. Last failure message: 4 did not equal 5.
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:420)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
	at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
	at org.apache.spark.storage.BlockManagerProactiveReplicationSuite.testProactiveReplication(BlockManagerReplicationSuite.scala:492)
	at org.apache.spark.storage.BlockManagerProactiveReplicationSuite$$anonfun$12$$anonfun$apply$mcVI$sp$1.apply$mcV$sp(BlockManagerReplicationSuite.scala:464)
	at org.apache.spark.storage.BlockManagerProactiveReplicationSuite$$anonfun$12$$anonfun$apply$mcVI$sp$1.apply(BlockManagerReplicationSuite.scala:464)
	at org.apache.spark.storage.BlockManagerProactiveReplicationSuite$$anonfun$12$$anonfun$apply$mcVI$sp$1.apply(BlockManagerReplicationSuite.scala:464)

[~shubhamc] and [~cloud_fan], since you worked on the original code for this, can you take a look at this?  I looked at this for a bit and based on some experimentation it looked like there were some race conditions in the underlying code.;;;","16/Mar/17 00:57;kayousterhout;This failed again today: 

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/74621/testReport/org.apache.spark.storage/BlockManagerProactiveReplicationSuite/proactive_block_replication___3_replicas___2_block_manager_deletions/;;;","16/Mar/17 03:05;shubhamc;I am looking into this and will try to submit a fix in a day or so. Mostly trying to isolate the race condition and simplify the test cases. ;;;","16/Mar/17 05:53;kayousterhout;Awesome thanks!;;;","17/Mar/17 14:46;shubhamc;Adding a PR link. Not sure why JIRA did not pick up the PR yesterday.;;;","23/Mar/17 16:17;shubhamc;Any feedback on the PR - https://github.com/apache/spark/pull/17325 ? ;;;","27/Mar/17 15:26;shubhamc;The PR enforces a refresh of the peer list cached at the executor that is trying to proactively replicate the block. This fix ensures that the peer will never try to replicate to a previously failed executor due to a stale reference. In addition, in the unit test, the block managers are explicitly stopped when they are being removed from the master.;;;","28/Mar/17 01:48;cloud_fan;Issue resolved by pull request 17325
[https://github.com/apache/spark/pull/17325];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove JDK7 from Travis CI,SPARK-19801,13047766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,02/Mar/17 19:09,03/Mar/17 11:01,14/Jul/23 06:30,03/Mar/17 11:01,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Build,,,,,0,,,,,,,,,"Since Spark 2.1.0, Travis CI was supported by SPARK-15207 for automated PR verification (JDK7/JDK8 maven compilation and Java Linter) and contributors can see the additional result via their Travis CI dashboard (or PC).

This issue aims to make `.travis.yml` up-to-date by removing JDK7 which was removed via SPARK-19550.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 03 11:01:05 UTC 2017,,,,,,,,,,"0|i3av9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 19:14;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17143;;;","03/Mar/17 11:01;srowen;Issue resolved by pull request 17143
[https://github.com/apache/spark/pull/17143];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML pipelines document error,SPARK-19797,13047665,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,ymwdalex,ymwdalex,ymwdalex,02/Mar/17 12:16,03/Mar/17 10:57,14/Jul/23 06:30,03/Mar/17 10:56,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,ML,,,,,0,documentation,,,,,,,,"Description about pipeline in this paragraph is incorrect https://spark.apache.org/docs/latest/ml-pipeline.html#how-it-works, which misleads the user
bq. If the Pipeline had more *stages*, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage.

The description is not accurate, because *Transformer* could also be a stage. But only another Estimator will invoke an extra transform call.

So, the description should be corrected as: *If the Pipeline had more _Estimators_*. 

The code to prove it is here https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/Pipeline.scala#L160",,apachespark,ymwdalex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 03 10:56:25 UTC 2017,,,,,,,,,,"0|i3aumv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 12:28;srowen;I don't think that's true. The resulting pipeline would contain a LogisticRegressionModel, and when invoked, its transform() method would be called, and the result passed to subsequent transformers if any. You are pointing out that the trailing transformations aren't necessary to compute when _fitting_ the pipeline. That's what the if statement here is optimizing away.;;;","02/Mar/17 12:36;srowen;Hm, on second look, the placement of the sentence suggest it applies to fitting. It is a bit of an implementation detail that this is optimized away, and the user won't actually care whether the pointless transforms happen during fitting or not. It is probably OK as is, but, might be clearer to say something like, ""has more stages that require the output of the LogisticRegressionModel to fit"" or something?;;;","02/Mar/17 12:39;apachespark;User 'ymwdalex' has created a pull request for this issue:
https://github.com/apache/spark/pull/17137;;;","02/Mar/17 12:39;ymwdalex;A pull request was created https://github.com/apache/spark/pull/17137;;;","02/Mar/17 12:51;ymwdalex;Hi Sean, thanks for your quick reply. 

bq. If the Pipeline had more stages, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage.

Let's use IDF as an example. If the pipeline is like:
bq. Tokenizer -> HashingTF -> IDF -> LogisticRegression
When we fit this pipeline, *IDF* will first call _fit_, then call _transform_ and pass the idf result to LogisticRegression. Because LogisticRegression is an Estimator and _fit_ of LogisticRegression needs the data from _transformer_ of *IDF*.

However, if the last stage of pipeline is Normalizer (https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer)
bq. Tokenizer -> HashingTF -> IDF -> Normalizer 
When fitting this pipeline, *IDF* will only call _fit_, and do not need to call _transform_

That's why I think it is better to modify the description as below to make it accurate.
bq. If the Pipeline had more Estimators, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage.
;;;","02/Mar/17 13:05;srowen;Yes, it's not true of scoring though, and the difference in fitting won't matter to the caller though.;;;","03/Mar/17 10:56;srowen;Issue resolved by pull request 17137
[https://github.com/apache/spark/pull/17137];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
taskScheduler fails serializing long statements received by thrift server,SPARK-19796,13047640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,irashid,gbloisi,gbloisi,02/Mar/17 10:25,06/Mar/17 20:18,14/Jul/23 06:30,06/Mar/17 20:06,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"This problem was observed after the changes made for SPARK-17931.

In my use-case I'm sending very long insert statements to Spark thrift server and they are failing at TaskDescription.scala:89 because writeUTF fails if requested to write strings longer than 64Kb (see https://www.drillio.com/en/2009/java-encoded-string-too-long-64kb-limit/ for a description of the issue).

As suggested by Imran Rashid I tracked down the offending key: it is ""spark.job.description"" and it contains the complete SQL statement.

The problem can be reproduced by creating a table like:
create table test (a int) using parquet

and by sending an insert statement like:
scala> val r = 1 to 128000
scala> println(""insert into table test values ("" + r.mkString(""),("") + "")"")

",,apachespark,gbloisi,irashid,kayousterhout,mridulm80,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17931,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 06 20:18:26 UTC 2017,,,,,,,,,,"0|i3auhb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 15:16;irashid;Since its a regression, I'm making this a blocker for 2.2.0  (or else we revert SPARK-17931, but the fix should be simple).;;;","02/Mar/17 16:12;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/17140;;;","02/Mar/17 16:32;irashid;[~kayousterhout] [~shivaram] here's another example of serializing lots of pointless data in each task -- in this case, {{TaskDescription.properties}} contains lots of data which the executors don't care about.  and this gets serialized once per task.

For this jira, I'll just do a small fix, but I thought you might be interested in this.;;;","03/Mar/17 00:28;kayousterhout;Do you think we should (separately) fix the underlying problem?  Specifically, we could:

(a) not send the SPARK_JOB_DESCRIPTION property to the workers, since it's only used on the master for the UI (and while users *could* access it, the variable name SPARK_JOB_DESCRIPTION is spark-private, which suggests that it shouldn't be used by users).  Perhaps this is too risky because users could be using it?

(b) Truncate SPARK_JOB_DESCRIPTION to something reasonable (100 characters?) before sending it to the workers.  This is more backwards compatible if users are actually reading the property, but maybe a useless intermediate approach?

(c) (Possibly in addition to one of the above) Log a warning if any of the properties is longer than 100 characters (or some threshold).

Thoughts?  I can file a JIRA if you think any of these is worthwhile.;;;","03/Mar/17 02:12;mridulm80;
I would not prefer (b) - if we are worried that users are depending on a private property, sending a truncated version of it is to aggravate it ! I would rather fail-fast with missing value.

Having said that, while we should limit our internal usage of properties, since this is also used to propagate user specified key value pairs; adding limits or log messages might not be optimal. Worst case, if we start detecting that the properties Map is growing really large, we could broadcast it (ugh ?).;;;","03/Mar/17 04:42;shivaram;I think (a) is worth exploring in a new JIRA -- We should try to avoid sending data that we dont need on the executors during task execution.;;;","06/Mar/17 20:06;irashid;Issue resolved by pull request 17140
[https://github.com/apache/spark/pull/17140];;;","06/Mar/17 20:18;irashid;I'm opposed to (b) as well.

It feels wrong to only do a one-off just for JOB_DESCRIPTION, but maybe its a large enough savings that its worth doing.  I was thinking of something larger, along the lines of SPARK-19108.  Another option would be to add new apis, eg., jobs would take `driverProperties` and `executorProperties`, but maybe that is overkill.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"R should support column functions to_json, from_json",SPARK-19795,13047628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,02/Mar/17 09:50,05/Mar/17 20:37,14/Jul/23 06:30,05/Mar/17 20:37,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,Particularly since R does not comes with support for process JSON,,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 09:51:13 UTC 2017,,,,,,,,,,"0|i3auen:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"02/Mar/17 09:51;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/17134;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use clock.getTimeMillis when mark task as finished in TaskSetManager.,SPARK-19793,13047622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,02/Mar/17 09:24,17/May/20 17:47,14/Jul/23 06:30,09/Mar/17 18:56,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"TaskSetManager is now using *System.getCurrentTimeMillis* when mark task as finished in *handleSuccessfulTask* and *handleFailedTask*. Thus developer cannot set the tasks finishing time in unit test. When *handleSuccessfulTask*, task's duration = System.getCurrentTimeMillis - launchTime(which can be set by *clock*), the result is not correct.",,apachespark,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 09:37:03 UTC 2017,,,,,,,,,,"0|i3audb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 09:37;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/17133;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the Master Page,the column named “Memory per Node” ,I think  it is not all right",SPARK-19792,13047582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,10110346,10110346,10110346,02/Mar/17 06:42,05/Mar/17 10:24,14/Jul/23 06:30,05/Mar/17 10:24,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Web UI,,,,,0,,,,,,,,,"Open the spark web page,in the Master Page ,have two tables:Running Applications table and  Completed Applications table, to the column named “Memory per Node” ,I think it is not all right ,because a node may be not have only one executor.So I think that should be named as “Memory per Executor”.Otherwise easy to let the user misunderstanding",,10110346,ajbozarth,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 05 10:24:07 UTC 2017,,,,,,,,,,"0|i3au4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/17 07:15;apachespark;User '10110346' has created a pull request for this issue:
https://github.com/apache/spark/pull/17132;;;","03/Mar/17 10:59;srowen;Hm, I'm honestly not sure. Does this refer to the memory allocated to each executor by the worker, or, does it refer to the amount of memory the worker can assign to executors?;;;","04/Mar/17 08:06;10110346;I think it refers to the memory allocated to each executor by the worker.
From the code, the value of this column is ""{Utils.megabytesToString(app.desc.memoryPerExecutorMB)}"";;;","05/Mar/17 10:24;srowen;Issue resolved by pull request 17132
[https://github.com/apache/spark/pull/17132];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refresh datasource table after alter the location,SPARK-19784,13047272,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,windpiger,windpiger,01/Mar/17 10:28,03/Feb/20 10:28,14/Jul/23 06:30,07/Jan/20 03:42,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,SQL,,,,,0,,,,,,,,,"currently if we alter the location of a datasource table, then we select from it, it still return the data of  the old location.",,apachespark,cloud_fan,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28413,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 07 03:42:24 UTC 2020,,,,,,,,,,"0|i3as7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/17 10:30;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17119;;;","12/Nov/18 05:52;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/22721;;;","07/Jan/20 03:42;cloud_fan;Issue resolved by pull request 22721
[https://github.com/apache/spark/pull/22721];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
structured streaming exist needless tmp file ,SPARK-19779,13047189,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,guifengleaf@gmail.com,guifengleaf@gmail.com,guifengleaf@gmail.com,01/Mar/17 04:50,03/Mar/17 05:21,14/Jul/23 06:30,03/Mar/17 05:20,2.0.3,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Structured Streaming,,,,,0,,,,,,,,,"The PR (https://github.com/apache/spark/pull/17012) can to fix restart a Structured Streaming application using hdfs as fileSystem, but also exist a problem that a tmp file of delta file is still reserved in hdfs. And Structured Streaming don't delete the tmp file generated when restart streaming job in future, so we need to delete the tmp file after restart streaming job.",,apachespark,guifengleaf@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19677,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 16:49:44 UTC 2017,,,,,,,,,,"0|i3arpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/17 13:48;srowen;If it's a temp file and is only not deleted in a failure case is that really a problem? tmp gets cleaned up eventually anyway. ;;;","01/Mar/17 16:46;guifengleaf@gmail.com;[~srowen] The `Background maintenance` don't clean files started with `temp`, so I think the temp file is not deleted. However, the temp file don't impact to get incorrect results for Structured Streaming Job.;;;","01/Mar/17 16:49;apachespark;User 'gf53520' has created a pull request for this issue:
https://github.com/apache/spark/pull/17124;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove an obsolete `partitionBy().insertInto()` test case,SPARK-19775,13047122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,28/Feb/17 22:46,01/Mar/17 23:46,14/Jul/23 06:30,01/Mar/17 23:46,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,Tests,,,,0,,,,,,,,,"This issue removes [a test case|https://github.com/apache/spark/blame/master/sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertIntoHiveTableSuite.scala#L287-L298] which was introduced by [SPARK-14459|https://github.com/apache/spark/commit/652bbb1bf62722b08a062c7a2bf72019f85e179e] and was superseded by [SPARK-16033|https://github.com/apache/spark/blame/master/sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertIntoHiveTableSuite.scala#L365-L371]. Basically, we cannot use `partitionBy` and `insertInto` together.

{code}
  test(""Reject partitioning that does not match table"") {
    withSQLConf((""hive.exec.dynamic.partition.mode"", ""nonstrict"")) {
      sql(""CREATE TABLE partitioned (id bigint, data string) PARTITIONED BY (part string)"")
      val data = (1 to 10).map(i => (i, s""data-$i"", if ((i % 2) == 0) ""even"" else ""odd""))
          .toDF(""id"", ""data"", ""part"")

      intercept[AnalysisException] {
        // cannot partition by 2 fields when there is only one in the table definition
        data.write.partitionBy(""part"", ""data"").insertInto(""partitioned"")
      }
    }
  }
{code}
",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 23:46:18 UTC 2017,,,,,,,,,,"0|i3araf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/17 22:53;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/17106;;;","01/Mar/17 23:46;srowen;Issue resolved by pull request 17106
[https://github.com/apache/spark/pull/17106];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamExecution should call stop() on sources when a stream fails,SPARK-19774,13047109,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,28/Feb/17 22:02,03/Mar/17 18:35,14/Jul/23 06:30,03/Mar/17 18:35,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"We call stop() on a Structured Streaming Source only when the stream is shutdown when a user calls streamingQuery.stop(). We should actually stop all sources when the stream fails as well, otherwise we may leak resources, e.g. connections to Kafka.",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 22:54:02 UTC 2017,,,,,,,,,,"0|i3ar7j:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"28/Feb/17 22:54;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/17107;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INNER JOIN on constant alias columns return incorrect results,SPARK-19766,13046963,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,stanzhai,stanzhai,stanzhai,28/Feb/17 12:58,02/Mar/17 16:17,14/Jul/23 06:30,01/Mar/17 15:58,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,Correctness,,,,,,,,"We can demonstrate the problem with the following data set and query:

{code}
val spark = SparkSession.builder().appName(""test"").master(""local"").getOrCreate()

val sql1 =
  """"""
    |create temporary view t1 as select * from values
    |(1)
    |as grouping(a)
  """""".stripMargin

val sql2 =
  """"""
    |create temporary view t2 as select * from values
    |(1)
    |as grouping(a)
  """""".stripMargin

val sql3 =
  """"""
    |create temporary view t3 as select * from values
    |(1),
    |(1)
    |as grouping(a)
  """""".stripMargin

val sql4 =
  """"""
    |create temporary view t4 as select * from values
    |(1),
    |(1)
    |as grouping(a)
  """""".stripMargin

val sqlA =
  """"""
    |create temporary view ta as
    |select a, 'a' as tag from t1 union all
    |select a, 'b' as tag from t2
  """""".stripMargin

val sqlB =
  """"""
    |create temporary view tb as
    |select a, 'a' as tag from t3 union all
    |select a, 'b' as tag from t4
  """""".stripMargin

val sql =
  """"""
    |select tb.* from ta inner join tb on
    |ta.a = tb.a and
    |ta.tag = tb.tag
  """""".stripMargin

spark.sql(sql1)
spark.sql(sql2)
spark.sql(sql3)
spark.sql(sql4)
spark.sql(sqlA)
spark.sql(sqlB)
spark.sql(sql).show()
{code}

The results which is incorrect:

{code}
+---+---+
|  a|tag|
+---+---+
|  1|  b|
|  1|  b|
|  1|  a|
|  1|  a|
|  1|  b|
|  1|  b|
|  1|  a|
|  1|  a|
+---+---+
{code}


The correct results should be:

{code}
+---+---+
|  a|tag|
+---+---+
|  1|  a|
|  1|  a|
|  1|  b|
|  1|  b|
+---+---+
{code}",,apachespark,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 06:23:03 UTC 2017,,,,,,,,,,"0|i3aqbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/17 13:16;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17099;;;","02/Mar/17 06:23;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/17131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNCACHE TABLE should also un-cache all cached plans that refer to this table,SPARK-19765,13046902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,28/Feb/17 09:03,25/Jun/18 17:12,14/Jul/23 06:30,07/Mar/17 17:22,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,release_notes,,,,,,,,"DropTableCommand, TruncateTableCommand, AlterTableRenameCommand, UncacheTableCommand, RefreshTable and InsertIntoHiveTable will un-cache all the cached plans that refer to this table",,ant_nebula,apachespark,cloud_fan,glenn.strycker@gmail.com,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21607,,,,,,,,,,,,,,,,SPARK-21579,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 25 17:12:41 UTC 2018,,,,,,,,,,"0|i3apxr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/17 09:11;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/17097;;;","31/Jul/17 14:40;ant_nebula;What a bad change it is!
Now it can not support for this scene any more.
https://issues.apache.org/jira/browse/SPARK-21579;;;","25/Jun/18 17:12;smilegator;The reported issue has been resolved by https://issues.apache.org/jira/browse/SPARK-24596;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
qualified external datasource table location stored in catalog,SPARK-19763,13046851,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,28/Feb/17 03:44,09/Mar/17 09:19,14/Jul/23 06:30,09/Mar/17 09:18,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"If we create a external datasource table with a non-qualified location , we should qualified it to store in catalog.

{code}
CREATE TABLE t(a string)
USING parquet
LOCATION '/path/xx'


CREATE TABLE t1(a string, b string)
USING parquet
PARTITIONED BY(b)
LOCATION '/path/xx'
{code}

when we get the table from catalog, the location should be qualified, e.g.'file:/path/xxx' ",,apachespark,cloud_fan,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 09 09:18:31 UTC 2017,,,,,,,,,,"0|i3apmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/17 03:52;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17095;;;","09/Mar/17 09:18;cloud_fan;Issue resolved by pull request 17095
[https://github.com/apache/spark/pull/17095];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
create InMemoryFileIndex with empty rootPaths when set PARALLEL_PARTITION_DISCOVERY_THRESHOLD to zero,SPARK-19761,13046839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,windpiger,windpiger,28/Feb/17 02:20,12/Nov/18 06:33,14/Jul/23 06:30,12/Nov/18 06:33,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"if we create a InMemoryFileIndex with an empty rootPaths when set PARALLEL_PARTITION_DISCOVERY_THRESHOLD to zero, it will throw an  exception:

{code}
Positive number of slices required
java.lang.IllegalArgumentException: Positive number of slices required
        at org.apache.spark.rdd.ParallelCollectionRDD$.slice(ParallelCollectionRDD.scala:119)
        at org.apache.spark.rdd.ParallelCollectionRDD.getPartitions(ParallelCollectionRDD.scala:97)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2084)
        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:935)
        at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex$.org$apache$spark$sql$execution$datasources$PartitioningAwareFileIndex$$bulkListLeafFiles(PartitioningAwareFileIndex.scala:357)
        at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.listLeafFiles(PartitioningAwareFileIndex.scala:256)
        at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:74)
        at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:50)
        at org.apache.spark.sql.execution.datasources.FileIndexSuite$$anonfun$9$$anonfun$apply$mcV$sp$2.apply$mcV$sp(FileIndexSuite.scala:186)
        at org.apache.spark.sql.test.SQLTestUtils$class.withSQLConf(SQLTestUtils.scala:105)
        at org.apache.spark.sql.execution.datasources.FileIndexSuite.withSQLConf(FileIndexSuite.scala:33)
        at org.apache.spark.sql.execution.datasources.FileIndexSuite$$anonfun$9.apply$mcV$sp(FileIndexSuite.scala:185)
        at org.apache.spark.sql.execution.datasources.FileIndexSuite$$anonfun$9.apply(FileIndexSuite.scala:185)
        at org.apache.spark.sql.execution.datasources.FileIndexSuite$$anonfun$9.apply(FileIndexSuite.scala:185)
        at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
        at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
{code}",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 02:24:03 UTC 2017,,,,,,,,,,"0|i3apjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/17 02:24;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17093;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting string to timestamp in inline table definition fails with AnalysisException,SPARK-19758,13046825,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,joshrosen,joshrosen,28/Feb/17 01:04,03/Mar/17 15:15,14/Jul/23 06:30,03/Mar/17 15:15,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The following query runs succesfully on Spark 2.1.x but fails in the current master:

{code}
sql(""""""CREATE TEMPORARY VIEW table_4(timestamp_col_3) AS VALUES TIMESTAMP('1991-12-06 00:00:00.0')"""""")
{code}

Here's the error:

{code}
scala> sql(""""""CREATE TEMPORARY VIEW table_4(timestamp_col_3) AS VALUES TIMESTAMP('1991-12-06 00:00:00.0')"""""")
org.apache.spark.sql.AnalysisException: failed to evaluate expression CAST('1991-12-06 00:00:00.0' AS TIMESTAMP): None.get; line 1 pos 50
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$4$$anonfun$apply$4.apply(ResolveInlineTables.scala:105)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$4$$anonfun$apply$4.apply(ResolveInlineTables.scala:95)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.immutable.List.map(List.scala:285)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$4.apply(ResolveInlineTables.scala:95)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$4.apply(ResolveInlineTables.scala:94)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$.convert(ResolveInlineTables.scala:94)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$apply$1.applyOrElse(ResolveInlineTables.scala:36)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$$anonfun$apply$1.applyOrElse(ResolveInlineTables.scala:32)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$.apply(ResolveInlineTables.scala:32)
  at org.apache.spark.sql.catalyst.analysis.ResolveInlineTables$.apply(ResolveInlineTables.scala:31)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:63)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:51)
  at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:128)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:588)
  ... 48 elided
{code}

It appears that this bug was introduced by SPARK-18936. /cc [~ueshin]",,apachespark,cloud_fan,joshrosen,maropu,ueshin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18936,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 07:07:02 UTC 2017,,,,,,,,,,"0|i3apgn:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"01/Mar/17 07:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17114;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove all shuffle files on a host in case of slave lost of fetch failure,SPARK-19753,13046754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sitalkedia@gmail.com,sitalkedia@gmail.com,sitalkedia@gmail.com,27/Feb/17 20:31,17/May/20 17:46,14/Jul/23 06:30,14/Jun/17 03:34,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"Currently, when we detect fetch failure, we only remove the shuffle files produced by the executor, while the host itself might be down and all the shuffle files are not accessible. In case we are running multiple executors on a host, any host going down currently results in multiple fetch failures and multiple retries of the stage, which is very inefficient. If we remove all the shuffle files on that host, on first fetch failure, we can rerun all the tasks on that host in a single stage retry. 

",,apachespark,cloud_fan,csun,irashid,jay.pranavamurthi,jonathak,longcao,rajesh.balamohan,roczei,sitalkedia@gmail.com,tejasp,tgraves,xwc3504,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20178,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 14 03:34:47 UTC 2017,,,,,,,,,,"0|i3ap0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 20:47;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/17088;;;","30/May/17 22:27;apachespark;User 'sitalkedia' has created a pull request for this issue:
https://github.com/apache/spark/pull/18150;;;","14/Jun/17 03:34;cloud_fan;Issue resolved by pull request 18150
[https://github.com/apache/spark/pull/18150];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create Data frame API fails with a self referencing bean,SPARK-19751,13046668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maropu,avinashmeda,avinashmeda,27/Feb/17 15:42,16/Mar/17 00:51,14/Jul/23 06:30,16/Mar/17 00:50,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"createDataset API throws a stack overflow exception when we try creating a 
Dataset using a bean encoder. The bean is self referencing

BEAN:
public class HierObj implements Serializable {
    String name;
    List<HierObj> children;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public List<HierObj> getChildren() {
        return children;
    }

    public void setChildren(List<HierObj> children) {
        this.children = children;
    }
}


// create an object
        HierObj hierObj = new HierObj();
        hierObj.setName(""parent"");
        List children = new ArrayList();

        HierObj child1 = new HierObj();
        child1.setName(""child1"");
        HierObj child2 = new HierObj();
        child2.setName(""child2"");
        children.add(child1);
        children.add(child2);

        hierObj.setChildren(children);

// create a dataset
        Dataset ds = sparkSession().createDataset(Arrays.asList(hierObj), Encoders.bean(HierObj.class));

",,apachespark,avinashmeda,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19896,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 00:50:40 UTC 2017,,,,,,,,,,"0|i3aohr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 16:26;srowen;Can you show the stack trace, at least the part before it just starts repeating?
To be clear, your bean doesn't reference itself, it just references instances of its same class right?
I'm not sure that's supposed to work, because it's not clear how that translates to a fixed nested schema, but it seems like the error could be better;;;","27/Feb/17 16:46;avinashmeda;Yes, Instances of the same class.

Stacktrace below

java.lang.StackOverflowError
	at org.spark_project.guava.collect.RegularImmutableMap.get(RegularImmutableMap.java:140)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:207)
	at org.spark_project.guava.reflect.TypeResolver$1.resolveTypeVariable(TypeResolver.java:194)
	at org.spark_project.guava.reflect.TypeResolver$1.resolveTypeVariable(TypeResolver.java:194)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:197)
	at org.spark_project.guava.reflect.TypeResolver.resolveType(TypeResolver.java:157)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:218)
	at org.spark_project.guava.reflect.TypeResolver$1.resolveTypeVariable(TypeResolver.java:194)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:197)
	at org.spark_project.guava.reflect.TypeResolver.resolveType(TypeResolver.java:157)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:218)
	at org.spark_project.guava.reflect.TypeResolver.resolveTypeVariable(TypeResolver.java:197)
	at org.spark_project.guava.reflect.TypeResolver.resolveType(TypeResolver.java:157)
	at org.spark_project.guava.reflect.TypeResolver.resolveParameterizedType(TypeResolver.java:229)
	at org.spark_project.guava.reflect.TypeResolver.resolveType(TypeResolver.java:159)
	at org.spark_project.guava.reflect.TypeToken.resolveType(TypeToken.java:268)
	at org.spark_project.guava.reflect.TypeToken.resolveSupertype(TypeToken.java:279)
	at org.spark_project.guava.reflect.TypeToken.getSupertype(TypeToken.java:401)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.elementType(JavaTypeInference.scala:142)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:111)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:111)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)

...

	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:111)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:111)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234);;;","27/Feb/17 16:49;avinashmeda;Does it mean that we cant have beans those reference their own instance ? ;;;","07/Mar/17 08:45;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17188;;;","16/Mar/17 00:50;cloud_fan;Issue resolved by pull request 17188
[https://github.com/apache/spark/pull/17188];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI http -> https redirect error,SPARK-19750,13046602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,27/Feb/17 10:35,03/Mar/17 01:19,14/Jul/23 06:30,03/Mar/17 01:19,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,,,,Web UI,,,,,0,,,,,,,,,"Spark's http redirect uses port 0 as a secure port to do redirect if http port is not set, this will introduce {{ java.net.NoRouteToHostException: Can't assign requested address }}, so here fixed to use bound port for redirect.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 03:48:34 UTC 2017,,,,,,,,,,"0|i3ao33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 10:49;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17083;;;","28/Feb/17 03:48;jerryshao;This issue was found by [~yeshavora], credits to her.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refresh for InMemoryFileIndex with FileStatusCache does not work correctly,SPARK-19748,13046568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,27/Feb/17 07:43,28/Feb/17 08:18,14/Jul/23 06:30,28/Feb/17 08:17,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"If we refresh a InMemoryFileIndex with a FileStatusCache, it will first use the FileStatusCache to generate the cachedLeafFiles etc, then call FileStatusCache.invalidateAll. the order to do these two actions is wrong, this lead to the refresh action does not take effect.

{code}
  override def refresh(): Unit = {
    refresh0()
    fileStatusCache.invalidateAll()
  }

  private def refresh0(): Unit = {
    val files = listLeafFiles(rootPaths)
    cachedLeafFiles =
      new mutable.LinkedHashMap[Path, FileStatus]() ++= files.map(f => f.getPath -> f)
    cachedLeafDirToChildrenFiles = files.toArray.groupBy(_.getPath.getParent)
    cachedPartitionSpec = null
  }
{code}",,apachespark,cloud_fan,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 08:17:55 UTC 2017,,,,,,,,,,"0|i3anvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 07:54;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/17079;;;","28/Feb/17 08:17;cloud_fan;Issue resolved by pull request 17079
[https://github.com/apache/spark/pull/17079];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New analysis rule for reporting unregistered functions without relying on relation resolution,SPARK-19737,13046254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,25/Feb/17 06:06,11/Mar/18 15:54,14/Jul/23 06:30,06/Mar/17 18:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Let's consider the following simple SQL query that reference an undefined function {{foo}} that is never registered in the function registry:
{code:sql}
SELECT foo(a) FROM t
{code}
Assuming table {{t}} is a partitioned  temporary view consisting of a large number of files stored on S3, it may take the analyzer a long time before realizing that {{foo}} is not registered yet.

The reason is that the existing analysis rule {{ResolveFunctions}} requires all child expressions to be resolved first. Therefore, {{ResolveRelations}} has to be executed first to resolve all columns referenced by the unresolved function invocation. This further leads to partition discovery for {{t}}, which may take a long time.

To address this case, we propose a new lightweight analysis rule {{LookupFunctions}} that
# Matches all unresolved function invocations
# Look up the function names from the function registry
# Report analysis error for any unregistered functions

Since this rule doesn't actually try to resolve the unresolved functions, it doesn't rely on {{ResolveRelations}} and therefore doesn't trigger partition discovery.

We may put this analysis rule in a separate {{Once}} rule batch that sits between the ""Substitution"" batch and the ""Resolution"" batch to avoid running it repeatedly and make sure it gets executed before {{ResolveRelations}}.",,apachespark,kevinyu98,LANDAIS Christophe,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23486,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 11 15:54:16 UTC 2018,,,,,,,,,,"0|i3ambz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/17 09:03;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/17168;;;","06/Mar/17 18:37;lian cheng;Issue resolved by pull request 17168
[https://github.com/apache/spark/pull/17168];;;","21/Feb/18 13:02;LANDAIS Christophe;Hello,

Migrating our application from spark 2.1.1 to spark 2.2.1, we see a major degradation in spark-SQL timing. One insert takes 5 seconds in 2.1.1 and 75 seconds in spark 2.2.1. Looking in executor traces (I force configuration to one executor) , we see it takes time between spark.sql(“insert into”) is done and task is submitted to executor

My application traces :

2018-02-21 06:30:53 - Executor[1] Going to execute request …

2018-02-21 06:32:08 - Executor[1] request executed (tag: NO_TAG) (table: ca4mn.sys_4g_pcmd_mme_15min) (date: 20180221061500) - duration (s)  74.846

 

Executor trace :

18/02/21 06:30:52 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 4675 bytes result sent to driver  (landais note: this is the previous task that is terminated)

18/02/21 06:32:06 INFO CoarseGrainedExecutorBackend: Got assigned task 2

 

What is doing spark between 06:30:53 and 06:32:06 ? I have taken several thread dump in the container while execution was in progress, with a delay of 2 seconds between thread dump. They are identical. Thread dump is put at the end of this comment.

Thread dump shows time is taken while verifying function exists: it is SPARK-19737 modification.

My SQL request contains 1000 functions because we are doing aggregation on many columns. Functions are like MAX, MIN, etc …

 

Please, can you perform a modification that improves this check ? For example: doing only one check for each different function ? Or why not introducing a spark parameter to bypass this check ?

----------------

Thread dump

178 ""Executor[1]"" #95 prio=5 os_prio=0 tid=0x00007f587f355800 nid=0x7c runnable [0x00007f57549f7000]

179    java.lang.Thread.State: RUNNABLE

180         at java.net.SocketInputStream.socketRead0(Native Method)

181         at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)

182         at java.net.SocketInputStream.read(SocketInputStream.java:171)

183         at java.net.SocketInputStream.read(SocketInputStream.java:141)

184         at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)

185         at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)

186         at java.io.BufferedInputStream.read(BufferedInputStream.java:345)

187         - locked <0x000000008913b110> (a java.io.BufferedInputStream)

188         at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)

189         at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)

190         at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)

191         at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)

192         at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)

193         at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:77)

194         at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_database(ThriftHiveMetastore.java:654)

195         at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_database(ThriftHiveMetastore.java:641)

196         at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:1158)

197         at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)                            **

198         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     

199         at java.lang.reflect.Method.invoke(Method.java:498)

200         at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)

201         at com.sun.proxy.$Proxy31.getDatabase(Unknown Source)

202         at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1301)

203         at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1290)      **

204         at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply$mcZ$sp(HiveClientImpl.scala:358)

205         at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:358)

206         at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$databaseExists$1.apply(HiveClientImpl.scala:358)

207         at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:290)

208         at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:231)

209         at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:230)

210         - locked <0x000000008900dd88> (a org.apache.spark.sql.hive.client.IsolatedClientLoader)

211         at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)

212         at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:357)

213         at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)

214         at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)

215         at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)

216         at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)

217         - locked <0x0000000089037ae0> (a org.apache.spark.sql.hive.HiveExternalCatalog)

218         at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)

219         at org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:246)

220         at org.apache.spark.sql.catalyst.catalog.SessionCatalog.org$apache$spark$sql$catalyst$catalog$SessionCatalog$$requireDbExists(SessionCatalog.scala:172)

221         at org.apache.spark.sql.catalyst.catalog.SessionCatalog.functionExists(SessionCatalog.scala:1044)

222         at org.apache.spark.sql.hive.HiveSessionCatalog.functionExists(HiveSessionCatalog.scala:173)

223         at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1196)

224         at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1195)

225         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

226         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

227         at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)

228         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)

229         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

230         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

231         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

232         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

233         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

234         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

235         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:258)

236         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:258)

237         at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)

238         at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)

239         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)

240         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

241         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

242         at scala.collection.immutable.List.foreach(List.scala:381)

243         at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)

244         at scala.collection.immutable.List.map(List.scala:285)

245         at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)

246         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)

247         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

248         at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)

249         at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:258)

250         at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:249)

251         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:309)

252         at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:308)

253         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

254         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)

255         at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)

256         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)

257         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

258         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

259         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$11.apply(TreeNode.scala:335)

260         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

261         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)

262         at scala.collection.immutable.List.foreach(List.scala:381)

263         at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)

264         at scala.collection.immutable.List.map(List.scala:285)

265         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:333)

266         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

267         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

268         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

269         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

270         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

271         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

272         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

273         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

274         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

275         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

276         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

277         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

278         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

279         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

280         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

281         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

282         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)

283         at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)

284         at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)

285         at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)

286         at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)

287         at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)

288         at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:308)

289         at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1195)

290         at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:1194)

291         at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)

292         at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)

293         at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)

294         at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)

295         at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)

296         at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)

297         at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)

298         at scala.collection.immutable.List.foreach(List.scala:381)

299         at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)

300         at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)

301         - locked <0x00000000f9c68cd8> (a org.apache.spark.sql.execution.QueryExecution)

302         at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)

303         at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)

304         at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)

305         at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)

306         at com.nokia.rtna.sumrz.engine.JobExecutorTask.run(JobExecutorTask.java:181)

307         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

308         at java.util.concurrent.FutureTask.run(FutureTask.java:266)

309         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

310         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

311         at java.lang.Thread.run(Thread.java:748)

 

Thanks and BR,

Christophe;;;","22/Feb/18 01:21;lian cheng;[~LANDAIS Christophe], I filed SPARK-23486 for this. Should be relatively straightforward to fix and I'd like to have a new contributor to try it as a starter task. Thanks for reporting!;;;","11/Mar/18 15:54;kevinyu98;[~LANDAIS Christophe], I submit a PR under  SPARK-23486, can you try and to see if it helps ?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refreshByPath should clear all cached plans with the specified path,SPARK-19736,13046253,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,25/Feb/17 05:53,17/Mar/17 02:59,14/Jul/23 06:30,01/Mar/17 08:20,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Catalog.refreshByPath can refresh the cache entry and the associated metadata for all dataframes (if any), that contain the given data source path. 

However, CacheManager.invalidateCachedPath doesn't clear all cached plans with the specified path. It causes some strange behaviors reported in SPARK-15678.",,apachespark,cloud_fan,gen,kiszk,Robin Shao,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 08:20:14 UTC 2017,,,,,,,,,,"0|i3ambr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/17 06:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/17064;;;","01/Mar/17 08:20;cloud_fan;Issue resolved by pull request 17064
[https://github.com/apache/spark/pull/17064];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 PythonUDF with multiple parents shouldn't be pushed down when used as a predicate,SPARK-19728,13046036,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zero323,zero323,24/Feb/17 14:32,15/May/19 14:59,14/Jul/23 06:30,23/Mar/17 14:04,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,SQL,,,,2,,,,,,,,,"Prior to Spark 2.0 it was possible to use Python UDF output as a predicate:

{code}
from pyspark.sql.functions import udf
from pyspark.sql.types import BooleanType

df1 = sc.parallelize([(1, ), (2, )]).toDF([""col_a""])
df2 = sc.parallelize([(2, ), (3, )]).toDF([""col_b""])
pred = udf(lambda x, y: x == y, BooleanType())

df1.join(df2).where(pred(""col_a"", ""col_b"")).show()
{code}

In Spark 2.0 this is no longer possible:

{code}
spark.conf.set(""spark.sql.crossJoin.enabled"", True)
df1.join(df2).where(pred(""col_a"", ""col_b"")).show()

## ...
## Py4JJavaError: An error occurred while calling o731.showString.
: java.lang.RuntimeException: Invalid PythonUDF <lambda>(col_a#132L, col_b#135L), requires attributes from more than one child.
## ...
{code}",,bahchis,krishnasistla,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18589,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 15 14:59:33 UTC 2019,,,,,,,,,,"0|i3akzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/18 10:56;bahchis;This is still happening in 2.3.1 when using the UDF inside the join ""on"" condition.
e.g.
{quote}df1.join(df2, pred(df1.col_a, df2.col_b)).show()
{quote}

I have opened for this: SPARK-25314;;;","15/May/19 14:59;krishnasistla;Is it resolved ? I am still seeing this error in 2.3.1 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL round function modifies original column,SPARK-19727,13045987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wojtek-szymanski,sbogutyn,sbogutyn,24/Feb/17 10:58,25/Oct/17 20:36,14/Jul/23 06:30,08/Mar/17 20:36,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{code:java}
import org.apache.spark.sql.functions

case class MyRow(value : BigDecimal)
val values = List(MyRow(BigDecimal.valueOf(1.23456789)))
val dataFrame = spark.createDataFrame(values)
dataFrame.show()
dataFrame.withColumn(""value_rounded"", functions.round(dataFrame.col(""value""))).show()
{code}

This produces output:

{noformat}
+--------------------+
|               value|
+--------------------+
|1.234567890000000000|
+--------------------+

+--------------------+-------------+
|               value|value_rounded|
+--------------------+-------------+
|1.000000000000000000|            1|
+--------------------+-------------+
{noformat}

Same problem occurs when I use round function to filter dataFrame.",,apachespark,cloud_fan,kiszk,liupengcheng,sbogutyn,wojtek-szymanski,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 25 20:36:04 UTC 2017,,,,,,,,,,"0|i3akov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/17 01:40;wojtek-szymanski;I can look at this;;;","27/Feb/17 02:05;apachespark;User 'wojtek-szymanski' has created a pull request for this issue:
https://github.com/apache/spark/pull/17075;;;","08/Mar/17 20:36;cloud_fan;Issue resolved by pull request 17075
[https://github.com/apache/spark/pull/17075];;;","25/Oct/17 20:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19576;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Faild to insert null timestamp value to mysql using spark jdbc,SPARK-19726,13045980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wangshuang,yuananf,yuananf,24/Feb/17 10:41,04/Jul/17 16:47,14/Jul/23 06:30,04/Jul/17 16:47,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"1. create a table in mysql
{code:borderStyle=solid}
CREATE TABLE `timestamp_test` (
  `id` bigint(23) DEFAULT NULL,
  `time_stamp` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=InnoDB DEFAULT CHARSET=utf8
{code}

2. insert one row using spark
{code:borderStyle=solid}
CREATE OR REPLACE TEMPORARY VIEW jdbcTable
USING org.apache.spark.sql.jdbc
OPTIONS (
  url 'jdbc:mysql://xxx.xxx.xxx.xxx:3306/default?characterEncoding=utf8&useServerPrepStmts=false&rewriteBatchedStatements=true',
  dbtable 'timestamp_test',
  driver 'com.mysql.jdbc.Driver',
  user 'root',
  password 'root'
);

insert into jdbcTable values (1, null);
{code}

the insert statement failed with exceptions:
{code:borderStyle=solid}
Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 599 in stage 1.0 failed 4 times, most recent failure: Lost task 599.3 in stage 1.0 (TID 1202, A03-R07-I12-135.JD.LOCAL): java.sql.BatchUpdateException: Data truncation: Incorrect datetime value: '1970-01-01 08:00:00' for column 'time_stamp' at row 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)
	at com.mysql.jdbc.Util.getInstance(Util.java:387)
	at com.mysql.jdbc.SQLError.createBatchUpdateException(SQLError.java:1154)
	at com.mysql.jdbc.PreparedStatement.executeBatchedInserts(PreparedStatement.java:1582)
	at com.mysql.jdbc.PreparedStatement.executeBatchInternal(PreparedStatement.java:1248)
	at com.mysql.jdbc.StatementImpl.executeBatch(StatementImpl.java:959)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:227)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:300)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:299)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:902)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.mysql.jdbc.MysqlDataTruncation: Data truncation: Incorrect datetime value: '1970-01-01 08:00:00' for column 'time_stamp' at row 1
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3876)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3814)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2478)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2625)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2551)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)
	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2073)
	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2009)
	at com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5094)
	at com.mysql.jdbc.PreparedStatement.executeBatchedInserts(PreparedStatement.java:1543)
	... 15 more
{code}",,apachespark,baibaichen,maropu,shuang wang,wangshuang,yuananf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 28 03:15:03 UTC 2017,,,,,,,,,,"0|i3aknb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/17 08:15;shuang wang;How to reappear this issue via testcase but not mysql jdbc? Anyone can help me？;;;","23/Jun/17 08:17;srowen;I'm not sure this is a Spark problem. You're trying to insert a null into a non-null column. Something should go wrong.;;;","23/Jun/17 11:11;baibaichen;@srowen, As we investigated, Spark translate null to 0 in case of timestamp which is wrong, since 0 is interpreted as ""1970-01-01 00:00:00""

{code:title=UnsafeRow.scala|borderStyle=solid}
  public void setNullAt(int i) {
    assertIndexIsValid(i);
    BitSetMethods.set(baseObject, baseOffset, i);
    // To preserve row equality, zero out the value when setting the column to null.
    // Since this row does does not currently support updates to variable-length values, we don't
    // have to worry about zeroing out that data.
    Platform.putLong(baseObject, getFieldOffset(i), 0);
  }
{code}

Yes, user insert null into a non-null column, but  
# Spark should pass *null* to underlying DB engine instead of 0.  then let DB report error, _or_
# Spark report error by itself;;;","28/Jun/17 03:15;apachespark;User 'shuangshuangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/18445;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Good error message for version mismatch in log files,SPARK-19721,13045857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lwlin,marmbrus,marmbrus,24/Feb/17 01:46,17/Mar/17 17:41,14/Jul/23 06:30,17/Mar/17 17:41,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"There are several places where we write out version identifiers in various logs for structured streaming (usually {{v1}}).  However, in the places where we check for this, we throw a confusing error message.  Instead, we should do the following:
 - Find all of the places where we do this kind of check.
 - for {{vN}} where {{n>1}} say ""UnsupportedLogFormat: The file {{path}} was produced by a newer version of Spark and cannot be read by this version.  Please upgrade""
 - for anything else throw an error saying the file is malformed.",,apachespark,codingcat,lwlin,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 26 11:58:03 UTC 2017,,,,,,,,,,"0|i3ajvz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"24/Feb/17 06:02;lwlin;I'd like to work on this too. Thanks.;;;","26/Feb/17 11:58;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redact sensitive information from SparkSubmit console output,SPARK-19720,13045856,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgrover,mgrover,mgrover,24/Feb/17 01:42,12/Dec/22 18:10,14/Jul/23 06:30,02/Mar/17 18:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Submit,,,,,0,,,,,,,,,"SPARK-18535 took care of redacting sensitive information from Spark event logs and UI. However, it intentionally didn't bother redacting the same sensitive information from SparkSubmit's console output because it was on the client's machine, which already had the sensitive information on disk (in spark-defaults.conf) or on terminal (spark-submit command line).

However, it seems now that it's better to redact information from SparkSubmit's console output as well because orchestration software like Oozie usually expose SparkSubmit's console output via a UI. To make matters worse, Oozie, in particular, always sets the {{--verbose}} flag on SparkSubmit invocation, making the sensitive information readily available in its UI (see [code|https://github.com/apache/oozie/blob/master/sharelib/spark/src/main/java/org/apache/oozie/action/hadoop/SparkMain.java#L248] here).

This is a JIRA for tracking redaction of sensitive information from SparkSubmit's console output.",,apachespark,diogo.mvieira,mgrover,rkanter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18535,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 19:21:57 UTC 2017,,,,,,,,,,"0|i3ajvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/17 01:55;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/17047;;;","26/Jul/17 02:51;diogo.mvieira;Do you have plans to apply this fix in a Hadoop 2.5 compatible version of Spark?

Version 2.2 removed Hadoop 2.5 support;;;","28/Jul/17 18:16;mgrover;I wasn't planning on. One could argue the case that this could be backported to branch-2.1 given that it's a rather simple change. However, 2.2 brought in some changes that were long overdue - dropping support for Java 7, Hadoop 2.5 and even if we got this change backported, you won't be able to make use of goodness down the road you didn't upgrade to Hadoop 2.6, Java 8, etc. So, my recommendation here would be to brave the new world of hadoop 2.6.;;;","28/Jul/17 19:08;diogo.mvieira;Yes, but it's a major security bug as described here. It should not be ported to 2.1.2?;;;","29/Jul/17 01:09;diogo.mvieira;I did a merge request for this compatibility feature on version 2.1.2: https://github.com/apache/spark/pull/18765;;;","02/Aug/17 19:20;gurwls223;User 'dmvieira' has created a pull request for this issue:
https://github.com/apache/spark/pull/18765;;;","02/Aug/17 19:21;gurwls223;User 'dmvieira' has created a pull request for this issue:
https://github.com/apache/spark/pull/18802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky test: org.apache.spark.sql.kafka010.KafkaSourceStressForDontFailOnDataLossSuite: stress test for failOnDataLoss=false,SPARK-19718,13045800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,23/Feb/17 21:38,04/Mar/17 01:10,14/Jul/23 06:30,04/Mar/17 01:10,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Structured Streaming,,,,,0,,,,,,,,,"SPARK-19617 changed HDFSMetadataLog to enable interrupts when using the local file system. However, now we hit HADOOP-12074: `Shell.runCommand` converts `InterruptedException` to `new IOException(ie.toString())` before Hadoop 2.8.

Test failure: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.6/2504/consoleFull

{code}
[info] - stress test for failOnDataLoss=false *** FAILED *** (1 minute, 1 second)
[info]   org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 27d45f4f-14dc-4c74-8b52-4bbd4f2b9bec, runId = 23b8c1ea-4da9-4096-967a-692933e4b319] terminated with exception: java.lang.InterruptedException
[info]   at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:304)
[info]   at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:190)
[info]   Cause: java.io.IOException: java.lang.InterruptedException
[info]   at org.apache.hadoop.util.Shell.runCommand(Shell.java:578)
[info]   at org.apache.hadoop.util.Shell.run(Shell.java:478)
[info]   at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:766)
[info]   at org.apache.hadoop.util.Shell.execCommand(Shell.java:859)
[info]   at org.apache.hadoop.util.Shell.execCommand(Shell.java:842)
[info]   at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)
[info]   at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:300)
[info]   at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1014)
[info]   at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:85)
[info]   at org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:354)
[info]   at org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:394)
[info]   at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)
[info]   at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:680)
[info]   at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:676)
[info]   at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
[info]   at org.apache.hadoop.fs.FileContext.create(FileContext.java:676)
{code}",,apachespark,lwlin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-12074,SPARK-19617,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 22:28:05 UTC 2017,,,,,,,,,,"0|i3ajjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/17 22:28;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/17044;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Failures in SQLQueryTests on big endian platforms,SPARK-19710,13045606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,robbinspg,robbinspg,robbinspg,23/Feb/17 12:21,03/Mar/17 15:53,14/Jul/23 06:30,03/Mar/17 15:53,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Some of the new test queries introduced by https://issues.apache.org/jira/browse/SPARK-18871 fail when run on zLinux (big endian)

The order of the return rows is different to the results file, hence the failures, but the results are valid for the queries as insufficient ordering is specified to give absolute results.

The failing tests are in o.a.s.SQLQuerTestSuite
in-joins.sql
not-in-joins.sql
in-set-operations.sql

These can be fixed by adding to the ORDER BY clauses to determine the resulting row order.

PR on it's way",,apachespark,robbinspg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 12:31:06 UTC 2017,,,,,,,,,,"0|i3aicf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/17 12:31;apachespark;User 'robbinspg' has created a pull request for this issue:
https://github.com/apache/spark/pull/17039;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CSV datasource fails to read empty file,SPARK-19709,13045577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wojtek-szymanski,gurwls223,,23/Feb/17 10:42,12/Dec/22 18:10,14/Jul/23 06:30,06/Mar/17 21:19,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"I just {{touch a}} and then ran the codes below:

{code}
scala> spark.read.csv(""a"")
java.util.NoSuchElementException: next on empty iterator
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.
{code}

It seems we should produce an empty dataframe consistently with `spark.read.json(""a"")`.",,apachespark,cloud_fan,maropu,wojtek-szymanski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 06 21:19:49 UTC 2017,,,,,,,,,,"0|i3ai5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/17 10:42;gurwls223;Let me fix this soon.;;;","24/Feb/17 00:31;wojtek-szymanski;[~hyukjin.kwon] I can also look at this if you don't mind. It seems it's very easy to reproduce.;;;","24/Feb/17 00:44;gurwls223;Please go ahead. (but I _personally_ recommend you open a PR in few days just to avoid potential conflicts because, for example, if https://github.com/apache/spark/pull/16976 gets merged, the code path will be changed rapidly).;;;","24/Feb/17 01:02;wojtek-szymanski;Thanks, I will try to fix it soon.;;;","26/Feb/17 00:05;apachespark;User 'wojtek-szymanski' has created a pull request for this issue:
https://github.com/apache/spark/pull/17068;;;","06/Mar/17 21:19;cloud_fan;Issue resolved by pull request 17068
[https://github.com/apache/spark/pull/17068];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the invalid path check for sc.addJar,SPARK-19707,13045553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,23/Feb/17 09:14,15/May/17 15:58,14/Jul/23 06:30,24/Feb/17 17:29,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"Currently in Spark there're two issues when we add jars with invalid path:

* If the jar path is a empty string {--jar "",dummy.jar""}, then Spark will resolve it to the current directory path and add to classpath / file server, which is unwanted.
* If the jar path is a invalid path (file doesn't exist), file server doesn't check this and will still added file server, the exception will be thrown until job is running. This local path could be checked immediately, no need to wait until task running. We have similar check in {{addFile}}, but lacks similar one in {{addJar}}.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 15 15:58:03 UTC 2017,,,,,,,,,,"0|i3ai0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/17 09:32;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/17038;;;","15/May/17 15:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17987;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the `in` operator in pyspark is broken,SPARK-19701,13045436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,cloud_fan,cloud_fan,22/Feb/17 23:26,12/Dec/22 18:10,14/Jul/23 06:30,06/Mar/17 02:05,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,,,,,0,,,,,,,,,"{code}
>>> textFile = spark.read.text(""/Users/cloud/dev/spark/README.md"")
>>> linesWithSpark = textFile.filter(""Spark"" in textFile.value)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/cloud/product/spark/python/pyspark/sql/column.py"", line 426, in __nonzero__
    raise ValueError(""Cannot convert column into bool: please use '&' for 'and', '|' for 'or', ""
ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.
{code}",,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 06 02:05:08 UTC 2017,,,,,,,,,,"0|i3ahan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/17 11:07;gurwls223;[~cloud_fan], I took a look this for my curiosity. It seems this is what happens now :

{code}
class Column(object):
    def __contains__(self, item):
        print ""I am contains""
        return Column()
    def __nonzero__(self):
        raise Exception(""I am nonzero."")

>>> 1 in Column()
I am contains
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 6, in __nonzero__
Exception: I am nonzero.
{code}

It seems it calls {{__contains__}} first and then {{__nonzero__}} or {{__bool__}} is being called against {{Column()}}
to make this a bool.

It seems {{__nonzero__}} (for Python 2), {{__bool__}} (for Python 3) and {{__contains__}} forcing the the return
into a bool unlike other operators.

I also referred the references as below to check my assumption and little knowledge:

http://stackoverflow.com/questions/12244074/python-source-code-for-built-in-in-operator/12244378#12244378
http://stackoverflow.com/questions/38542543/functionality-of-python-in-vs-contains/38542777

I tested the codes above in 1.6.3, 2.1.0 and in the master branch. It seems it has not been working so far.

Should we maybe remove this?;;;","03/Mar/17 11:11;gurwls223;I was thinking a way to work around (e.g., hijacking..) but it seems we can't.
BTW, the below codes seems throwing a {{TypeError}} if {{__nonzero__}} or {{__bool__}} returns other types.

{code}
class Column(object):
    def __contains__(self, item):
        print ""I am contains""
        return Column()
    def __nonzero__(self):
        return ""a""

>>> 1 in Column()
I am contains
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: __nonzero__ should return bool or int, returned str
{code}
;;;","03/Mar/17 18:17;cloud_fan;let's remove it then;;;","04/Mar/17 03:58;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17160;;;","06/Mar/17 02:05;cloud_fan;Issue resolved by pull request 17160
[https://github.com/apache/spark/pull/17160];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong Documentation for Java Word Count Example,SPARK-19696,13045072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,gaurav.gopi123,gaurav.gopi123,22/Feb/17 04:54,22/Feb/17 13:53,14/Jul/23 06:30,22/Feb/17 13:53,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.0,,,,,Documentation,,,,,0,,,,,,,,,"Java Word Count example of http://spark.apache.org/examples.html page is defined incorrectly 

'''
JavaRDD<String> textFile = sc.textFile(""hdfs://..."");
JavaRDD<String> words = textFile.flatMap(s -> Arrays.asList(s.split("" "")).iterator())
                            .mapToPair(word -> new Tuple2<>(word, 1))
                            .reduceByKey((a, b) -> a + b);
counts.saveAsTextFile(""hdfs://..."");
''

It should be 

'''
JavaRDD<String> textFile = sc.textFile(""hdfs://..."");
JavaPairRDD<String, Integer> counts = textFile.flatMap(s -> Arrays.asList(s.split("" "")).iterator())
                            .mapToPair(word -> new Tuple2<>(word, 1))
                            .reduceByKey((a, b) -> a + b);
counts.saveAsTextFile(""hdfs://..."");
''''",,gaurav.gopi123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 13:15:47 UTC 2017,,,,,,,,,,"0|i3af2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/17 13:03;srowen;My fault, I actually introduced this when fixing/updating this and other examples a couple days ago. 

https://github.com/apache/spark-website/commit/879303593efa229d416eb4178913c1c1a6f7033c;;;","22/Feb/17 13:15;srowen;https://github.com/apache/spark-website/pull/37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calculating percentile of decimal column fails with ClassCastException,SPARK-19691,13045040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,joshrosen,joshrosen,22/Feb/17 01:40,24/Feb/17 09:55,14/Jul/23 06:30,23/Feb/17 17:12,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Running

{code}
spark.range(10).selectExpr(""cast (id as decimal) as x"").selectExpr(""percentile(x, 0.5)"").collect()
{code}

results in a ClassCastException:

{code}
 java.lang.ClassCastException: org.apache.spark.sql.types.Decimal cannot be cast to java.lang.Number
	at org.apache.spark.sql.catalyst.expressions.aggregate.Percentile.update(Percentile.scala:141)
	at org.apache.spark.sql.catalyst.expressions.aggregate.Percentile.update(Percentile.scala:58)
	at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:514)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$anonfun$applyOrElse$1.apply(AggregationIterator.scala:171)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1$$anonfun$applyOrElse$1.apply(AggregationIterator.scala:171)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateProcessRow$1.apply(AggregationIterator.scala:187)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateProcessRow$1.apply(AggregationIterator.scala:181)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:151)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:78)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:109)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:113)
{code}",,apachespark,joshrosen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 24 01:15:04 UTC 2017,,,,,,,,,,"0|i3aevj:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"22/Feb/17 14:10;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17028;;;","24/Feb/17 01:15;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/17046;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Yarn Credentials File set to different application directory,SPARK-19688,13045003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,saturday_s,j.devaraj,j.devaraj,21/Feb/17 23:09,17/May/20 18:15,14/Jul/23 06:30,19/Jun/17 17:27,1.6.3,,,,,,,,,,,,,,,,,,,,,,,,,,1.6.4,2.0.3,2.1.2,2.2.0,2.3.0,DStreams,Spark Core,YARN,,,0,,,,,,,,,spark.yarn.credentials.file property is set to different application Id instead of actual Application Id ,,apachespark,j.devaraj,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21008,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 08 10:56:03 UTC 2017,,,,,,,,,,"0|i3aenb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/17 06:44;jerryshao;Can you please elaborate the problem you met, otherwise it is hard for others to identify.

Also ""spark.yarn.credentials.file"" is a internal configuration, usually user should not configure it.;;;","22/Feb/17 20:36;j.devaraj;When Spark Application is restarted  spark.yarn.credentials.file is set to 
hdfs://node/user/*****/.sparkStaging/application_someotherApplicationId/credentials-d8c33609-72f9-4770-9e50-aab848424e62

Streaming Application with check-pointing enabled.;;;","23/Feb/17 00:30;jerryshao;I see, so we should exclude this configuration in checkpoint and make it re-configured after restarted.;;;","23/Feb/17 05:13;jerryshao;[~j.devaraj], when you say Spark application is restarted, are you pointing to yarn's reattempt mechanism or you manually restart the application?;;;","23/Feb/17 05:24;j.devaraj;[~jerryshao] I did not check for Yarn's reattempt. I am seeing this behavior for manual restarts.;;;","23/Feb/17 05:31;jerryshao;I see. So what issue did you encounter when you restart the application manually, or you just saw the abnormal credential configuration?

From my understanding, this credential configuration will be overwritten when you restart the application, so it should be fine.;;;","24/Feb/17 05:54;jerryshao;According to my test, ""spark.yarn.credentials.file"" will be overwritten in yarn-client to point to a correct path when launching application (https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L737). So even Spark Streaming checkpoint still keeps the old configuration, it will be overwritten when the new application is started. So I don't see an issue here except this weird setting.;;;","27/Apr/17 11:12;victor-wong;[~jerryshao]
In yarn-cluster mode, ""spark.yarn.credentials.file"" will be recovered from checkpoint file. It will not be overwritten when the new application is started. And there will be some unwanted behaviors:
https://github.com/apache/spark/pull/17782
;;;","27/Apr/17 12:49;jerryshao;Does this issue exist in the latest master code, from the PR seems you submit a patch based on 1.6. I guess this issue could be self recovered in Spark 2.1+.;;;","28/Apr/17 08:40;victor-wong;[~jerryshao]
I did not test this in Spark 2.1+, but I wrote a test suite and the result showed that it seemed the issue still exist.
```
  test(""temp test"") {
    ssc = new StreamingContext(master, framework, batchDuration)
    ssc.conf.set(""spark.yarn.credentials.file"", ""tmp1"")

    val cp = new Checkpoint(ssc, Time(1000))
    val newCp = Utils.deserialize[Checkpoint](Utils.serialize(cp))

    val newCpConf = newCp.createSparkConf()
    assert(newCpConf.get(""spark.yarn.credentials.file"") === ""tmp1"")

    System.setProperty(""spark.yarn.credentials.file"", ""tmp2"")
    val newCpConf2 = newCp.createSparkConf()
    assert(newCpConf2.get(""spark.yarn.credentials.file"") === ""tmp2"")
  }
```;;;","08/Jun/17 07:21;jerryshao;The issue still exists in master branch, so reopen it.;;;","08/Jun/17 10:56;apachespark;User 'saturday-shi' has created a pull request for this issue:
https://github.com/apache/spark/pull/18230;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
save and load pipeline and then use it yield java.lang.RuntimeException,SPARK-19681,13044915,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,clemencb,clemencb,21/Feb/17 18:00,21/Feb/17 23:47,14/Jul/23 06:30,21/Feb/17 23:47,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Spark Core,,,,,0,spark-ml,,,,,,,,"Here is the unit test that fails:


import org.apache.spark.SparkConf
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{SQLTransformer, VectorAssembler}
import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.scalatest.{BeforeAndAfter, FlatSpec, Matchers}

import scala.util.Random


/**
  * Created by borisclemencon on 21/02/2017.
  */
class PipelineTest extends FlatSpec with Matchers with BeforeAndAfter {

  val featuresCol = ""features""
  val responseCol = ""response""
  val weightCol = ""weight""
  val features = Array(""X1"", ""X2"")
  val lambdas = Array(0.01)

  val alpha = 0.2
  val maxIter = 50
  val nfolds = 5

  var spark: SparkSession = _

  before {
    val sparkConf: SparkConf = new SparkConf().
      set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"").
      set(""spark.ui.enabled"", ""false""). // faster and remove 'spark test java.net.BindException: Address already in use' warnings!
      set(""spark.driver.host"", ""127.0.0.1"")

    spark = SparkSession.
      builder().
      config(sparkConf).
      appName(""BlendWeightTransformerTest"").
      master(""local[*]"").
      getOrCreate()
  }


  def makeDataset(n: Int = 100): DataFrame = {
    val sc = spark
    import sc.implicits._
    val n = 1000
    val data =
      for (i <- 1 to n) yield {
        val pn = if (Random.nextDouble() < 0.1) ""a"" else ""b""
        val x1: Double = Random.nextGaussian() * 5
        val x2: Double = Random.nextGaussian() * 2
        val response: Int = if (Random.nextBoolean()) 1 else 0
        (pn, x1, x2, response)
      }
    data.toDF(packageNameCol, ""X1"", ""X2"", responseCol)
  }

  ""load()"" should ""produce the same pipeline and result before and after save()"" in {

    val lr = new LogisticRegression().
      setFitIntercept(true).
      setMaxIter(maxIter).
      setElasticNetParam(alpha).
      setStandardization(true).
      setFamily(""binomial"").
      setFeaturesCol(featuresCol).
      setLabelCol(responseCol)

    val assembler = new VectorAssembler().setInputCols(features).setOutputCol(featuresCol)
    val pipeline = new Pipeline().setStages(Array(assembler, lr))
    val evaluator = new BinaryClassificationEvaluator().
      setLabelCol(responseCol).
      setMetricName(""areaUnderROC"")
    val paramGrid = new ParamGridBuilder().
      addGrid(lr.regParam, lambdas).
      build()

    // Train with simple grid cross validation
    val cv = new CrossValidator().
      setEstimator(pipeline).
      setEvaluator(evaluator).
      setEstimatorParamMaps(paramGrid).
      setNumFolds(nfolds) // Use 3+ in practice

    val df = makeDataset(100).cache
    val cvModel = cv.fit(df)

    val answer = cvModel.transform(df)
    answer.show(truncate = false)

    val path = ""./PipelineTestcvModel""
    cvModel.write.overwrite().save(path)

    val cvModelLoaded = CrossValidatorModel.load(path)
    val output = cvModelLoaded.transform(df)
    output.show(truncate = false)
    Compare.assertDataFrameEquals(answer, output)
  }
}

yield exception

should produce the same blent pipeline and result before and after save() *** FAILED ***
[info]   java.lang.RuntimeException: no default for type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7
[info]   at org.apache.spark.sql.catalyst.expressions.Literal$.default(literals.scala:179)
[info]   at org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:121)
[info]   at org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:114)
[info]   at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
[info]   at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
[info]   at scala.collection.immutable.List.foreach(List.scala:381)
[info]   at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
[info]   at scala.collection.immutable.List.flatMap(List.scala:344)
[info]   at org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$.unapply(patterns.scala:114)
[info]   at org.apache.spark.sql.execution.SparkStrategies$JoinSelection$.apply(SparkStrategies.scala:158)
",,clemencb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 21 23:47:15 UTC 2017,,,,,,,,,,"0|i3ae3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/17 23:47;clemencb;EDIT:
the problem is in Compare.assertDataFrameEquals, not in Spark. I close the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFSBackedStateStoreProvider fails to overwrite existing file,SPARK-19677,13044746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vitillo,vitillo,vitillo,21/Feb/17 08:24,02/Mar/17 07:11,14/Jul/23 06:30,28/Feb/17 18:51,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Structured Streaming,,,,,2,,,,,,,,,"I got the exception below after restarting a crashed Structured Streaming application. This seems to be due to the fact that {{/tmp/checkpoint/state/0/0/214451.delta}} already exists in HDFS.

{code}
17/02/20 14:14:26 ERROR StreamExecution: Query [id = 5023231c-2433-4013-a8b9-d54bb5751445, runId = 4168cf31-7d0b-4435-9b58-28919abd937b] terminated with error
org.apache.spark.SparkException: Job aborted.
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)
        at org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:78)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply$mcV$sp(StreamExecution.scala:503)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply(StreamExecution.scala:503)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply(StreamExecution.scala:503)
        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)
        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)
        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(StreamExecution.scala:502)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcV$sp(StreamExecution.scala:255)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:244)
        at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:262)
        at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:46)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:244)
        at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)
        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:239)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:177)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 100, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: Error committing version 214451 into HDFSStateStore[id = (op=0, part=0), dir = /tmp/checkpoint/state/0/0]
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:162)
        at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(StatefulAggregate.scala:173)
        at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(StatefulAggregate.scala:123)
        at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:64)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:100)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:99)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask.execute(FileFormatWriter.scala:365)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193)
        ... 8 more
Caused by: java.io.IOException: Failed to rename /tmp/checkpoint/state/0/0/temp--6958924364117377568 to /tmp/checkpoint/state/0/0/214451.delta
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:259)
        at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:156)
        ... 25 more

{code}",,apachespark,codingcat,hster,vitillo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19645,,,,,,,,,SPARK-19779,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 21 21:21:01 UTC 2017,,,,,,,,,,"0|i3ad27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/17 08:37;apachespark;User 'vitillo' has created a pull request for this issue:
https://github.com/apache/spark/pull/17012;;;","21/Feb/17 21:21;hster;Thank you for reporting this issue!  I just wanted to add that we get the same HDFS error when we restart our structured streaming drivers but also when we try to run  more complex driver using withWatermark/agg/groupBy/orderBy, we get in the first run without restart:

java.lang.IllegalStateException: Error committing version 1 into HDFSStateStore[id = (op=0, part=15), dir = /user/spark/checkpoints/StructuredStreamingSignalAggregation/ss_StructuredStreamingSignalAggregation/state/0/15]
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:162)
	at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(StatefulAggregate.scala:138)
	at org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3.apply(StatefulAggregate.scala:123)
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:64)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ignore driver accumulator updates don't belong to the execution when merging all accumulator updates,SPARK-19674,13044714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,carsonwang,carsonwang,carsonwang,21/Feb/17 05:10,25/Mar/17 12:37,14/Jul/23 06:30,23/Feb/17 22:31,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"In SQLListener.getExecutionMetrics, driver accumulator updates don't belong to the execution should be ignored when merging all accumulator updates to prevent NoSuchElementException.",,apachespark,carsonwang,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 21:50:02 UTC 2017,,,,,,,,,,"0|i3acv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/17 05:13;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/17009;;;","23/Feb/17 22:31;cloud_fan;Issue resolved by pull request 17009
[https://github.com/apache/spark/pull/17009];;;","24/Mar/17 21:50;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/17418;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftServer default app name is changed wrong,SPARK-19673,13044709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,lvdongr,lvdongr,lvdongr,21/Feb/17 04:34,25/Feb/17 21:48,14/Jul/23 06:30,25/Feb/17 21:48,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"In spark 1.x ,the name of ThriftServer is SparkSQL:localHostName. While the ThriftServer default name is changed to the className of HiveThfiftServer2 (org.apache.spark.sql.hive.thriftserver.HiveThriftServer2) , which is not appropriate.",,apachespark,liushaohui,lvdongr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 25 21:48:18 UTC 2017,,,,,,,,,,"0|i3actz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/17 04:41;srowen;What is the problem?;;;","21/Feb/17 05:28;apachespark;User 'lvdongr' has created a pull request for this issue:
https://github.com/apache/spark/pull/17010;;;","21/Feb/17 05:53;lvdongr;Before spark1.4.x, the ThriftServer name is ""SparkSQL:localhostname"",while https://issues.apache.org/jira/browse/SPARK-8650 change the rule as a side effect. Then the ThriftServer show the class name of HiveThriftServer2, which is not appropriate.;;;","25/Feb/17 21:48;srowen;Resolved by https://github.com/apache/spark/pull/17010;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception when calling createDataFrame with typed RDD,SPARK-19666,13044537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,colinbreame,colinbreame,20/Feb/17 12:13,12/Dec/22 18:11,14/Jul/23 06:30,22/Feb/17 20:42,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"The following code:

{code}
    var tmp = sc.parallelize(Seq(new __Message()))
    val spark = SparkSession.builder().getOrCreate()
    var df = spark.createDataFrame(tmp, classOf[__Message])
{code}

Produces this error message.

{code}
Exception in thread ""main"" java.lang.NullPointerException
	at org.spark_project.guava.reflect.TypeToken.method(TypeToken.java:465)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:126)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:127)
	at org.apache.spark.sql.catalyst.JavaTypeInference$$anonfun$2.apply(JavaTypeInference.scala:125)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.org$apache$spark$sql$catalyst$JavaTypeInference$$inferDataType(JavaTypeInference.scala:125)
	at org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(JavaTypeInference.scala:55)
	at org.apache.spark.sql.SparkSession.getSchema(SparkSession.scala:708)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:347)
	at uk.co.gresearch.aggregator.Main$.main(Main.scala:46)
	at uk.co.gresearch.aggregator.Main.main(Main.scala)
{code}
",,apachespark,cloud_fan,colinbreame,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 20:42:45 UTC 2017,,,,,,,,,,"0|i3abrr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/17 16:49;maropu;It'd be better to put at least a simple query to reproduce the issue you'ld like to report.
What's ""__Message""?
A query below works well, so what's a different between this query and yours?
{code}
scala> :paste
class BeanClass extends Serializable {
  private[this] var data: Int = 0
  def getData(): Int = data
  def setData(i: Int): Unit = { data = i }
}
scala> val rdd = sc.parallelize((0 until 10)).map { i => val data = new BeanClass(); data.setData(i); data }
scala> val df = spark.createDataFrame(rdd, classOf[BeanClass])
scala> df.show
+----+
|data|
+----+
|   0|
|   1|
|   2|
|   3|
|   4|
|   5|
|   6|
|   7|
|   8|
|   9|
+----+
{code};;;","21/Feb/17 08:43;gurwls223;Hm, probably the {{__Message}} is an invalid java bean. Maybe, at least we could improve the error message.;;;","21/Feb/17 09:09;maropu;In my case, when I passed a invalid bean, I got a empty DF;
{code}
scala> :paste
class BeanClass extends Serializable {
  private[this] var data: Int = 0
  def get(): Int = data
  def set(i: Int): Unit = { data = i }
}
scala> val rdd = sc.parallelize((0 until 10)).map { i => val data = new BeanClass(); data.set(i); data }
rdd: org.apache.spark.rdd.RDD[BeanClass] = MapPartitionsRDD[1] at map at <console>:25
scala> val df = spark.createDataFrame(rdd, classOf[BeanClass])
df: org.apache.spark.sql.DataFrame = []
scala> df.show
++
||
++
||
||
||
||
||
||
||
||
||
||
++
{code}
Even in this case, I want better error messages.
Probably, it seems okay to add error handling around here: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala#L124
i.e.) If we get a empty `properties`, it just throws an AnalysisException, or something.;;;","21/Feb/17 09:30;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17013;;;","21/Feb/17 09:30;gurwls223;Ah, yes. I just tested some cases too. I could reproduce the (I assume) same case and proposed a PR.;;;","21/Feb/17 09:31;maropu;Thanks!;;;","22/Feb/17 20:42;cloud_fan;Issue resolved by pull request 17013
[https://github.com/apache/spark/pull/17013];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Structured Streaming API for R,SPARK-19654,13044188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,18/Feb/17 05:25,18/Mar/17 23:28,14/Jul/23 06:30,18/Mar/17 23:27,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,As a user I want to be able to process data from a streaming source in R.,,apachespark,codingcat,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20015,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 18 05:29:04 UTC 2017,,,,,,,,,,"0|i3a9m7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"18/Feb/17 05:29;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16982;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
REST API does not perform user auth for individual apps,SPARK-19652,13044155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,18/Feb/17 00:08,22/Feb/17 23:49,14/Jul/23 06:30,22/Feb/17 23:49,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Web UI,,,,,0,,,,,,,,,"(This goes back further than 2.0.0, btw.)

The REST API currently only performs authorization at the root of the UI; this works for live UIs, but not for the history server, where the root allows everybody to read data. That means that currently any user can see any application in the SHS through the REST API, when auth is enabled.

Instead, the REST API should behave like the regular UI and perform authentication at the app level too.",,ajbozarth,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19642,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 21:36:05 UTC 2017,,,,,,,,,,"0|i3a9ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/17 01:17;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16978;;;","22/Feb/17 00:19;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17019;;;","22/Feb/17 21:36;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17029;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metastore-only operations shouldn't trigger a spark job,SPARK-19650,13044148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,sameerag,sameerag,17/Feb/17 23:42,25/Feb/17 07:06,14/Jul/23 06:30,25/Feb/17 07:06,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"We currently trigger a spark job even for simple metastore operations ({{SHOW TABLES}}, {{SHOW DATABASES}}, {{CREATE TABLE}} etc.). Even though these otherwise get executed on a driver, it prevents a user from doing these operations on a driver-only cluster.",,apachespark,cloud_fan,huasanyelao,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 25 07:06:18 UTC 2017,,,,,,,,,,"0|i3a9db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/17 14:07;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17027;;;","25/Feb/17 07:06;cloud_fan;Issue resolved by pull request 17027
[https://github.com/apache/spark/pull/17027];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
binaryRecords replicates records in scala API,SPARK-19646,13043880,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,bahaelaila7,bahaelaila7,17/Feb/17 06:45,20/Feb/17 17:47,14/Jul/23 06:30,20/Feb/17 17:20,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Spark Core,,,,,0,,,,,,,,,"The scala sc.binaryRecords replicates one record for the entire set.
for example, I am trying to load the cifar binary data where in a big binary file, each 3073 represents a 32x32x3 bytes image with 1 byte for the label label. The file resides on my local filesystem.
.take(5) returns 5 records all the same, .collect() returns 10,000 records all the same.
What is puzzling is that the pyspark one works perfectly even though underneath it is calling the scala implementation.
I have tested this on 2.1.0 and 2.0.0",,apachespark,bahaelaila7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 20 17:47:05 UTC 2017,,,,,,,,,,"0|i3a7xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/17 08:36;srowen;I have a good guess that this is not a bug, and it's because you're reusing the objects you get from the API, and not cloning them. The objects are reused by the InputFormat, so you have to.;;;","17/Feb/17 09:14;bahaelaila7;Thank you very much for the quick reply.
All I did was the following in spark-shell:
val x = sc.binaryRecords('binary_file.bin',3073)
val t= x.take(3)
t(0)
t(1)
t(2)
// all returning the same array, even though they shouldn't be the same

in pyspark, I do the same:
x = sc.binaryRecords('binary_file.bin',3073)
t = x.take(3)
t[0]
t[1]
t[2]
// different legit results, verified manually as well.

;;;","17/Feb/17 09:38;srowen;Ah, I take it back. With that info I think this is in fact a problem. Although the problem is indeed because of Hadoop reusing Writables, this is not a case where the user is touching Writables. binaryRecords is getting the byte[] from a BytesWritable but actually this reference is the same every time, including the internal byte array. It needs to be copied. Simple fix.;;;","17/Feb/17 09:44;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16974;;;","17/Feb/17 10:11;bahaelaila7;Thank you very much for the speedy fix!;;;","17/Feb/17 10:14;bahaelaila7;What's puzzling though, is I looked at pyspark's implementation of binaryRecords, and it's just calling _jsc.binaryRecords and wrapping it with a pyspark RDD
so, if it is indeed calling the scala implementation, shouldn't pyspark have the same problem?;;;","17/Feb/17 11:06;srowen;I think it's because the array is copied elsewhere as it moves between the JVM and Python anyway;;;","20/Feb/17 17:20;srowen;Issue resolved by pull request 16974
[https://github.com/apache/spark/pull/16974];;;","20/Feb/17 17:47;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/17003;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in Spark Streaming (Encoder/Scala Reflection),SPARK-19644,13043870,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,deenbandhu,deenbandhu,17/Feb/17 06:00,10/Nov/17 22:15,14/Jul/23 06:30,10/Nov/17 22:15,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,DStreams,SQL,Structured Streaming,,,4,memory_leak,performance,,,,,,,"I am using streaming on the production for some aggregation and fetching data from cassandra and saving data back to cassandra. 

I see a gradual increase in old generation heap capacity from 1161216 Bytes to 1397760 Bytes over a period of six hours.

After 50 hours of processing instances of class scala.collection.immutable.$colon$colon incresed to 12,811,793 which is a huge number. 

I think this is a clear case of memory leak

Updated: The root cause is when creating an encoder object, it leaks several Scala internal objects due to a Scala memory leak issue: https://github.com/scala/bug/issues/8302","3 AWS EC2 c3.xLarge
Number of cores - 3
Number of executors 3 
Memory to each executor 2GB",agateaaa,apachespark,deenbandhu,djh4230,dongjoon,lishuming,lwlin,maropu,palaiya,raviteja-ms,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/17 14:31;deenbandhu;Dominator_tree.png;https://issues.apache.org/jira/secure/attachment/12853565/Dominator_tree.png","20/Feb/17 14:34;deenbandhu;Path2GCRoot.png;https://issues.apache.org/jira/secure/attachment/12853566/Path2GCRoot.png","17/Feb/17 06:06;deenbandhu;heapdump.png;https://issues.apache.org/jira/secure/attachment/12853218/heapdump.png",,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 10 19:49:03 UTC 2017,,,,,,,,,,"0|i3a7vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/17 06:06;deenbandhu;Snap shot of heap dump after 50 hours;;;","17/Feb/17 08:38;srowen;What you have described so far is not a memory leak in Spark. It's normal for the heap to grow unless it has reason to even garbage collect. You're not evidently running out of memory. You're talking about a heap change of 1.1 to 1.3MB, which is trivial (is this a typo?). I'd close this unless you have a clearer case.;;;","17/Feb/17 08:43;deenbandhu;you can ignore that change in memory but if you look in the snapshot the number of instances of class scala.collection.immutable.$colon$colon it had increased too high and it keep on increasing over the period of time;;;","17/Feb/17 08:50;srowen;There are just linked list objects. Why are they too high? if you have plenty of heap, Java won't bother GCing  until it needs to.;;;","17/Feb/17 09:03;deenbandhu;No i had just given 2 GB driver memory and if there is not any reference to them the Full GC should clean them but it is not get cleaned thats why i think there is memory leak;;;","17/Feb/17 09:14;deenbandhu;And after 40-50 hours full gc is too frequent that all cores of machines are over utilized and batches start to queue up in streaming and I need to restart the streaming;;;","17/Feb/17 09:19;srowen;You only show 400MB of lists in your screen dump.
Running out of memory doesn't mean a leak. The question is what is holding on to the memory? This isn't a big heap to begin with.
;;;","17/Feb/17 09:28;deenbandhu;Yes that's right running out of memory doesn't mean a leak but gradual increase in heap size and inability of GC to clear the memory is a memory leak. Ideally the number of linked list objects should not be increasing over the period of time and that increase is suggesting that there is a memory leak. ;;;","17/Feb/17 09:29;srowen;This is still not necessarily true in general. Your app could be retaining state. This is why this isn't actionable as-is.;;;","17/Feb/17 09:34;deenbandhu;We are not using any state or window operation and not using any check pointing so i don't think  that app is retaining state.;;;","17/Feb/17 22:39;zsxwing;[~deenbandhu] Could you check the GC root, please? These objects are from Scala reflection. Did you run the job in Spark shell?;;;","20/Feb/17 14:39;deenbandhu;Sorry for the delayed response.

No, I didn't run in spark shell. I ran using spark submit in client deploy mode on a standalone spark cluster.
I ran eclipse MAT on heap dump and attached the screenshot of dominator tree. I hope this will help you out to find the cause of memory leak. 

Also attached Path to GC root of the object of `scala.reflect.runtime.JavaUniverse` (for smaller heap dump taken at the application start).

When I checked the path to GC root for an object of `scala.collection.immutable.$colon$colon` the path contains the same object(`scala.reflect.runtime.JavaUniverse`);;;","21/Feb/17 09:53;deenbandhu;I have analysed the issue more. I performed some of the experiments as follows and analysed the heapdump using jvisualvm at some intervals.

1. Dstream.foreachRdd(rdd => rdd.map(r => someCaseClass(r)).take(10).foreach(println))

2. Dstream.foreachRdd(rdd => rdd.map(r => someCaseClass(r)).toDF.show(10,false))

3. Dstream.foreachRdd(rdd => rdd.map(r => someCaseClass(r)).toDS.show(10,false))

I Observed that the number of instances of scala.collection.immutable.$colon$colon remain constant in 1 scenario but it keeps on increasing in 2 and 3 scenario. So I think there is something leaky in toDS or toDF function this may help you out to find out the issue.;;;","22/Feb/17 22:44;zsxwing;[~deenbandhu] Do you use Scala 2.10 or Scala 2.11?;;;","23/Feb/17 04:12;deenbandhu;I am using scala 2.11;;;","17/Mar/17 13:35;deenbandhu;Any updates ??;;;","18/Mar/17 09:50;srowen;I don't think there is evidence of a memory leak here. It's not even clear it has GCed;;;","19/Mar/17 15:58;deenbandhu;It's not even clear it has GCed ?

The increase in total GC time is a clear indication of GC ;;;","19/Mar/17 16:03;srowen;I don't see any GC time here. 
Is this not simple stuff like you have lots of old jobs and stage metrics info in the driver? There is not much memory used here compared to normal operation. Unless you've tried stuff like restricting the number of retained jobs in the UI I don't think there is evidence of a problem. The dumps don't show anything that odd. ;;;","19/Mar/17 16:07;deenbandhu;Yes i have tried restricting the number of jobs retained in UI to 200 and moreover the default value is 1000 for number of retained batches and my batch interval is 10s so for 1000 batches it will take somewhere around 10000 sec which is equal to 3-4 hrs but it keeps on accumulating after that. I think there is something else which is creating problem ;;;","19/Mar/17 16:09;srowen;Use jcmd to trigger a full GC on the process to see what happens. ;;;","19/Mar/17 16:11;deenbandhu;full gc is trigger so many times and its frequency increases with time because of accumulated memory of that big object ;;;","19/Mar/17 16:17;srowen;The weird thing is memory retained by Scala runtime universe. I am still not clear if you are saying you run out memory or not. I also don't recall any other reports like this. If you have leads, post them here. ;;;","19/Jun/17 05:49;dongjoon;Hi, [~deenbandhu].
Could you try this with the latest versions like 2.1.1 or 2.2.0-RC4?;;;","19/Jun/17 05:51;deenbandhu;yes i can try but is there any report of such events in that particular version 
;;;","19/Jun/17 05:55;deenbandhu;And the spark cassandra connector is also not out for those spark version. which is a dependency for us;;;","19/Jun/17 06:02;dongjoon;I see. Thank you anyway, [~deenbandhu].;;;","01/Nov/17 07:57;djh4230;did the issue has been fixed? I am using spark 2.1.0 and i also encount the same scene. The driver memory keep increasing. I analysis the dump heap and find the class scala.collection.immutable.$colon$colon keep increasing.;;;","01/Nov/17 19:54;zsxwing;I happened to investigate a similar issue and found out the leak is caused by Scala reflection. Please see my comment in https://github.com/scala/bug/issues/8302

My workaround is calling ""scala.reflect.runtime.universe.asInstanceOf[scala.reflect.runtime.JavaUniverse].undoLog.clear()"" manually to clean up these garbage objects. You need to put this line in the same thread that you create Dataset/DataFrame as the leak happens in a thread local object. I think the best place in Spark streaming is foreachRDD.;;;","01/Nov/17 19:58;zsxwing;By the way, you can confirm this issue by checking if the number of ""scala.reflect.internal.tpe.TypeConstraints$TypeConstraint"" is large.;;;","01/Nov/17 20:05;zsxwing;I added more components since it also affects them. The major issue is when creating an encoder object, it leaks several Scala internal objects due to a Scala memory leak issue.;;;","08/Nov/17 00:38;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19687;;;","10/Nov/17 19:49;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/19718;;;",,,,,,
JSON schema inference in DROPMALFORMED mode produces incorrect schema,SPARK-19641,13043826,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,NathanHowell,NathanHowell,17/Feb/17 01:39,12/Dec/22 18:11,14/Jul/23 06:30,03/Apr/17 09:45,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"In {{DROPMALFORMED}} mode the inferred schema may incorrectly contain no columns. This occurs when one document contains a valid JSON value (such as a string or number) and the other documents contain objects or arrays.

When the default case in {{JsonInferSchema.compatibleRootType}} is reached when merging a {{StringType}} and a {{StructType}} the resulting type will be a {{StringType}}, which is then discarded because a {{StructType}} is expected.",,apachespark,cloud_fan,maropu,NathanHowell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 03 09:45:36 UTC 2017,,,,,,,,,,"0|i3a7lj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"17/Feb/17 04:26;maropu;Could you show us a simple query to reproduce this? cc: [~hyukjin.kwon];;;","17/Feb/17 05:22;gurwls223;Ah, thanks for cc'ing me. I happened to see the related comment. I will leave them here just in case anyone is curious.

https://github.com/apache/spark/pull/16386#issuecomment-280525182
https://github.com/NathanHowell/spark/commit/e233fd03346a73b3b447fa4c24f3b12c8b2e53ae;;;","17/Feb/17 05:26;maropu;okay, thanks!;;;","24/Mar/17 01:23;gurwls223;[~NathanHowell], I just happened to revisit this. Are you going to open a PR with the change?;;;","24/Mar/17 01:40;NathanHowell;[~hyukjin.kwon], I'm super busy through next Tuesday. I can get it open it before then but probably won't have time to do any work on it until later in the week. Are you trying to get this in before the 2.2 branch?;;;","24/Mar/17 02:13;gurwls223;Not sure, just IMHO, it sounds not super urgent one because schema inference is not recommended in the production up to my knowledge. I think it is fine to take your time :).

Otherwise, I am willing to pick up your commit and open a PR which should credit to you if you want.;;;","24/Mar/17 02:15;NathanHowell;Please pick it up if you have cycles and want to take it over, otherwise I'll get to it later next week. Thanks!;;;","31/Mar/17 05:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/17492;;;","03/Apr/17 09:45;cloud_fan;Issue resolved by pull request 17492
[https://github.com/apache/spark/pull/17492];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutputCommitCoordinator should not allow commits for already failed tasks,SPARK-19631,13043687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwoody,pwoody,pwoody,16/Feb/17 16:44,17/May/20 17:48,14/Jul/23 06:30,02/Mar/17 23:57,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"This is similar to SPARK-6614, but there a race condition where a task may fail (e.g. Executor heartbeat timeout) and still manage to go through the commit protocol successfully. After this any retries of the task will fail indefinitely because of TaskCommitDenied.",,aash,apachespark,lwlin,pwoody,robert3005,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19790,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 16:53:28 UTC 2017,,,,,,,,,,"0|i3a6qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 16:53;apachespark;User 'pwoody' has created a pull request for this issue:
https://github.com/apache/spark/pull/16959;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration `spark.yarn.credentials.updateTime` takes no effect,SPARK-19626,13043573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,16/Feb/17 09:41,21/Feb/17 17:59,14/Jul/23 06:30,21/Feb/17 17:59,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,YARN,,,,0,,,,,,,,,"In [SPARK-14743|https://github.com/apache/spark/pull/14065], we introduced a configurable credential manager for Spark running on YARN. Also two configs *spark.yarn.credentials.renewalTime* and *spark.yarn.credentials.updateTime* were added, one is for the credential renewer and the other updater. But now we just query *spark.yarn.credentials.renewalTime* by mistake during CREDENTIALS UPDATING, where should be actually *spark.yarn.credentials.updateTime*. ",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 09:54:06 UTC 2017,,,,,,,,,,"0|i3a61b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 09:54;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/16955;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a http error in a paged table when using a `Go` button to search.,SPARK-19622,13043522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stanzhai,stanzhai,stanzhai,16/Feb/17 07:21,14/Apr/17 02:21,14/Jul/23 06:30,17/Feb/17 15:11,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Web UI,,,,,0,,,,,,,,,The search function of paged table is not available because of we don't skip the hash data of the reqeust path. ,,ajbozarth,apachespark,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20293,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 07:22;stanzhai;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12852996/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 15:11:17 UTC 2017,,,,,,,,,,"0|i3a5pz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 07:25;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/16953;;;","17/Feb/17 15:11;srowen;Issue resolved by pull request 16953
[https://github.com/apache/spark/pull/16953];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect exchange coordinator Id in physical plan,SPARK-19620,13043507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,carsonwang,carsonwang,carsonwang,16/Feb/17 06:33,10/Mar/17 19:15,14/Jul/23 06:30,10/Mar/17 19:14,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"When adaptive execution is enabled, an exchange coordinator is used to in the Exchange operators. For Join, the same exchange coordinator is used for its two Exchanges. But the physical plan shows two different coordinator Ids which is confusing.

Here is an example:
{code}
== Physical Plan ==
*Project [key1#3L, value2#12L]
+- *SortMergeJoin [key1#3L], [key2#11L], Inner
   :- *Sort [key1#3L ASC NULLS FIRST], false, 0
   :  +- Exchange(coordinator id: 1804587700) hashpartitioning(key1#3L, 10), coordinator[target post-shuffle partition size: 67108864]
   :     +- *Project [(id#0L % 500) AS key1#3L]
   :        +- *Filter isnotnull((id#0L % 500))
   :           +- *Range (0, 1000, step=1, splits=Some(10))
   +- *Sort [key2#11L ASC NULLS FIRST], false, 0
      +- Exchange(coordinator id: 793927319) hashpartitioning(key2#11L, 10), coordinator[target post-shuffle partition size: 67108864]
         +- *Project [(id#8L % 500) AS key2#11L, id#8L AS value2#12L]
            +- *Filter isnotnull((id#8L % 500))
               +- *Range (0, 1000, step=1, splits=Some(10))

{code}",,apachespark,carsonwang,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 19:14:38 UTC 2017,,,,,,,,,,"0|i3a5mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 06:39;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16952;;;","10/Mar/17 19:14;yhuai;Issue resolved by pull request 16952
[https://github.com/apache/spark/pull/16952];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistency wrt max. buckets allowed from Dataframe API vs SQL,SPARK-19618,13043443,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejasp,tejasp,tejasp,16/Feb/17 01:44,13/May/18 17:47,14/Jul/23 06:30,16/Feb/17 06:46,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"High number of buckets is allowed while creating a table via SQL query:

{code}
sparkSession.sql(""""""
CREATE TABLE bucketed_table(col1 INT) USING parquet 
CLUSTERED BY (col1) SORTED BY (col1) INTO 147483647 BUCKETS
"""""")

sparkSession.sql(""DESC FORMATTED bucketed_table"").collect.foreach(println)
....
[Num Buckets:,147483647,]
[Bucket Columns:,[col1],]
[Sort Columns:,[col1],]
....
{code}

Trying the same via dataframe API does not work:

{code}
> df.write.format(""orc"").bucketBy(147483647, ""j"",""k"").sortBy(""j"",""k"").saveAsTable(""bucketed_table"")

java.lang.IllegalArgumentException: requirement failed: Bucket number must be greater than 0 and less than 100000.
  at scala.Predef$.require(Predef.scala:224)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$getBucketSpec$2.apply(DataFrameWriter.scala:293)
  at org.apache.spark.sql.DataFrameWriter$$anonfun$getBucketSpec$2.apply(DataFrameWriter.scala:291)
  at scala.Option.map(Option.scala:146)
  at org.apache.spark.sql.DataFrameWriter.getBucketSpec(DataFrameWriter.scala:291)
  at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:429)
  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:410)
  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:365)
  ... 50 elided
{code}
",,apachespark,cloud_fan,ferdonline,matz-e,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 13 17:47:14 UTC 2018,,,,,,,,,,"0|i3a58f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 01:50;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/16948;;;","16/Feb/17 06:46;cloud_fan;Issue resolved by pull request 16948
[https://github.com/apache/spark/pull/16948];;;","15/Apr/18 19:57;ferdonline;Is there any technological problem in using more than 100k buckets? Otherwise what about making it configurable?

We have an 80TB workload and to keep partitions ""manageable"" we do need to use a large number of buckets. While it might seem a lot today it is expected that workloads will continue to increase in size...;;;","16/Apr/18 01:34;cloud_fan;making it configurable sounds like a good idea, can you open a JIRA for it? thanks!;;;","16/Apr/18 22:01;ferdonline;Opened [SPARK-23997|https://issues.apache.org/jira/browse/SPARK-23997]

Thanks;;;","13/May/18 17:47;ferdonline;[~cloud_fan] I have created the Jira and an implementation to lift the limit via a configuration option. Internally we are forced to use our mod, and it would be nice to get in sync with upstream again at some point. It is a very small patch in the end. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the race condition when starting and stopping a query quickly,SPARK-19617,13043435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,16/Feb/17 00:58,28/Feb/17 18:56,14/Jul/23 06:30,22/Feb/17 04:16,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"The streaming thread in StreamExecution uses the following ways to check if it should exit:
- Catch an InterruptException.
- `StreamExecution.state` is TERMINATED.

when starting and stopping a query quickly, the above two checks may both fail.
- Hit [HADOOP-14084|https://issues.apache.org/jira/browse/HADOOP-14084] and swallow InterruptException
- StreamExecution.stop is called before `state` becomes `ACTIVE`. Then [runBatches|https://github.com/apache/spark/blob/dcc2d540a53f0bd04baead43fdee1c170ef2b9f3/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L252] changes the state from `TERMINATED` to `ACTIVE`.

If the above cases both happen, the query will hang forever.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-14084,,,SPARK-19718,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 18 03:13:03 UTC 2017,,,,,,,,,,"0|i3a56n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/17 01:05;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16947;;;","18/Feb/17 03:13;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16979;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: StateStoreRDDSuite,SPARK-19613,13043399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,kayousterhout,kayousterhout,15/Feb/17 22:03,12/Dec/22 18:10,14/Jul/23 06:30,29/May/18 02:37,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.1,2.4.0,,,,Structured Streaming,Tests,,,,0,,,,,,,,,"This test: org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite.versioning and immutability failed on a recent PR: https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/72948/testReport/junit/org.apache.spark.sql.execution.streaming.state/StateStoreRDDSuite/versioning_and_immutability/
",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24139,,,,,,,,,,,,,SPARK-26496,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 02:37:07 UTC 2018,,,,,,,,,,"0|i3a4yn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/17 21:55;kayousterhout;I'm closing this because, while it had a burst of failures about a month ago (see here: https://spark-tests.appspot.com/test-details?suite_name=org.apache.spark.sql.execution.streaming.state.StateStoreRDDSuite&name=versioning+and+immutability) it hasn't failed since.;;;","28/May/18 16:03;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/21446;;;","29/May/18 02:37;gurwls223;Fixed in https://github.com/apache/spark/pull/21446;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests failing with timeout,SPARK-19612,13043363,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shaneknapp,kayousterhout,kayousterhout,15/Feb/17 19:56,20/May/19 23:27,14/Jul/23 06:30,20/May/19 23:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tests,,,,,0,,,,,,,,,"I've seen at least one recent test failure due to hitting the 250m timeout: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/72882/

Filing this JIRA to track this; if it happens repeatedly we should up the timeout.

cc [~shaneknapp]",,kayousterhout,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 20 23:27:31 UTC 2019,,,,,,,,,,"0|i3a4qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/17 20:59;srowen;I think this happens when Jenkins is quite busy; it probably isn't even a flaky test situation. That has been my experience.
Not that it isn't a problem but may not be due to a test per se.;;;","15/Feb/17 21:09;kayousterhout;Does that mean we could potentially fix this by limiting the concurrency on Jenkins? ;;;","22/Mar/17 21:49;kayousterhout;Closing this for now because I haven't seen this issue in a while (we can re-open if this starts occurring again);;;","24/Mar/17 05:44;kayousterhout;This seems to be back: saw two recently:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/75124
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/75127;;;","27/Mar/17 22:50;kayousterhout;And another: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/75272/console;;;","20/May/19 23:27;shaneknapp;timeout was increased a while back!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 2.1.0 breaks some Hive tables backed by case-sensitive data files,SPARK-19611,13043351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,budde,budde,budde,15/Feb/17 19:24,30/Oct/17 23:13,14/Jul/23 06:30,09/Mar/17 20:55,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"This issue replaces [SPARK-19455|https://issues.apache.org/jira/browse/SPARK-19455] and [PR #16797|https://github.com/apache/spark/pull/16797]

[SPARK-16980|https://issues.apache.org/jira/browse/SPARK-16980] removed the schema inferrence from the HiveMetastoreCatalog class when converting a MetastoreRelation to a LoigcalRelation (HadoopFsRelation, in this case) in favor of simply using the schema returend by the metastore. This results in an optimization as the underlying file status no longer need to be resolved until after the partition pruning step, reducing the number of files to be touched significantly in some cases. The downside is that the data schema used may no longer match the underlying file schema for case-sensitive formats such as Parquet.

[SPARK-17183|https://issues.apache.org/jira/browse/SPARK-17183] added support for saving a case-sensitive copy of the schema in the metastore table properties, which HiveExternalCatalog will read in as the table's schema if it is present. If it is not present, it will fall back to the case-insensitive metastore schema.

Unfortunately, this silently breaks queries over tables where the underlying data fields are case-sensitive but a case-sensitive schema wasn't written to the table properties by Spark. This situation will occur for any Hive table that wasn't created by Spark or that was created prior to Spark 2.1.0. If a user attempts to run a query over such a table containing a case-sensitive field name in the query projection or in the query filter, the query will return 0 results in every case.

The change we are proposing is to bring back the schema inference that was used prior to Spark 2.1.0 if a case-sensitive schema can't be read from the table properties.
- INFER_AND_SAVE: Infer a schema from the data files if no case-sensitive schema can be read from the table properties. Attempt to save the inferred schema in the table properties to avoid future inference.
- INFER_ONLY: Infer the schema if no case-sensitive schema can be read but don't attempt to save it.
- NEVER_INFER: Fall back to using the case-insensitive schema returned by the Hive Metatore. Useful if the user knows that none of the underlying data is case-sensitive.

See the discussion on [PR #16797|https://github.com/apache/spark/pull/16797] for more discussion around this issue and the proposed solution.",,apachespark,budde,cloud_fan,ctang,liushaohui,roczei,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-22306,SPARK-22329,,SPARK-20450,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 30 23:13:04 UTC 2017,,,,,,,,,,"0|i3a4nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/17 19:41;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/16942;;;","15/Feb/17 22:13;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/16944;;;","15/Feb/17 22:18;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/16942;;;","16/Feb/17 15:53;rxin;Rather than this fix, can we just save the case sensitive schema in the catalog?
;;;","16/Feb/17 16:45;budde;[SPARK-17183|https://issues.apache.org/jira/browse/SPARK-17183] added support for saving the case-sensitive schema in the table properties in order to avoid the conflicts introduced by Hive metastore downcasing without the need for schema inference. The problem stated here occurs when Spark doesn't find a case-sensitive schema in the table properties and falls back to the case insensitive metastore schema. This will happen for any Hive table that wasn't created by Spark or that was created with Spark 2.1.0.

In the [PR 16797 discussion|https://github.com/apache/spark/pull/16797] I provided my reasoning for why I think simply offering a way to perform migrations to write case-sensitive schemas to the table properties won't be sufficient to solve this alone.;;;","09/Mar/17 20:55;cloud_fan;Issue resolved by pull request 16944
[https://github.com/apache/spark/pull/16944];;;","09/Mar/17 23:07;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/17229;;;","10/Mar/17 21:13;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/17249;;;","30/Oct/17 23:13;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/19615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix StreamingQuery explain command,SPARK-19603,13043134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,15/Feb/17 01:02,16/Feb/17 04:52,14/Jul/23 06:30,16/Feb/17 04:52,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,Right now StreamingQuery.explain doesn't show the correct streaming physical plan.,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 15 01:09:03 UTC 2017,,,,,,,,,,"0|i3a3br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/17 01:09;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CollapseRepartition rule to preserve shuffle-enabled Repartition,SPARK-19601,13043078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,14/Feb/17 21:20,08/Mar/17 17:37,14/Jul/23 06:30,08/Mar/17 17:37,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"When users use the shuffle-enabled `repartition` API, they expect the partition they got should be the exact number they provided, even if they call shuffle-disabled `coalesce` later. Currently, `CollapseRepartition` rule does not consider whether shuffle is enabled or not. Thus, we got the following unexpected result.

{noformat}
    val df = spark.range(0, 10000, 1, 5)
    val df2 = df.repartition(10)
    assert(df2.coalesce(13).rdd.getNumPartitions == 5)
    assert(df2.coalesce(7).rdd.getNumPartitions == 5)
    assert(df2.coalesce(3).rdd.getNumPartitions == 3)
{noformat}
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 21:21:05 UTC 2017,,,,,,,,,,"0|i3a2zb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 21:21;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16933;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
from_json produces only a single row when input is a json array,SPARK-19595,13042978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,14/Feb/17 15:35,12/Dec/22 17:51,14/Jul/23 06:30,05/Mar/17 22:37,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently, {{from_json}} reads a single row when it is a json array. For example,

{code}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
val schema = StructType(StructField(""a"", IntegerType) :: Nil)
Seq((""""""[{""a"": 1}, {""a"": 2}]"""""")).toDF(""struct"").select(from_json(col(""struct""), schema)).show()
+--------------------+
|jsontostruct(struct)|
+--------------------+
|                 [1]|
+--------------------+
{code}

Maybe we should not support this in that function or it should work like a generator expression.
",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19849,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 05 22:37:56 UTC 2017,,,,,,,,,,"0|i3a2d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 16:42;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16929;;;","05/Mar/17 22:37;brkyvz;Resolved by https://github.com/apache/spark/pull/16929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingQueryListener fails to handle QueryTerminatedEvent if more then one listeners exists,SPARK-19594,13042928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,eyalzit,eyalzit,eyalzit,14/Feb/17 12:34,26/Feb/17 23:59,14/Jul/23 06:30,26/Feb/17 23:58,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"reproduce:
*create a spark session
*add multiple streaming query listeners
*create a simple query
*stop the query
result -> only the first listener handle the QueryTerminatedEvent

this might happen because the query run id is being removed from activeQueryRunIds once the onQueryTerminated is called (StreamingQueryListenerBus:115)",,apachespark,chris.bowden,eyalzit,lwlin,uncleGen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 19 11:36:03 UTC 2017,,,,,,,,,,"0|i3a21z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 21:52;zsxwing;Good catch. Would you like to submit a PR to fix it?;;;","15/Feb/17 07:56;eyalzit;sure, i can do that, will it make sense to fix it by marking the query id as terminated instead of removing it from the list?;;;","15/Feb/17 19:51;zsxwing;I suggest that overriding ""def postToAll(event: E)"" and remove the query id after all listeners process the event.;;;","16/Feb/17 07:01;eyalzit;that will work but i will have to remove the ""final""  from the ""postToAll"" method which is part of spark core

another option can be to change the method post(event: StreamingQueryListener.Event):

def post(event: StreamingQueryListener.Event) {
    event match {
      case s: QueryStartedEvent =>
        activeQueryRunIds.synchronized { activeQueryRunIds += s.runId }
        sparkListenerBus.post(s)
        // post to local listeners to trigger callbacks
        postToAll(s)
     case t: QueryTerminatedEvent =>
        // run all the listeners synchronized before removing the id from the list
        postToAll(t)
        activeQueryRunIds.synchronized { activeQueryRunIds -= t.runId }
      case _ =>
        sparkListenerBus.post(event)
    }
  };;;","19/Feb/17 11:36;apachespark;User 'eyalzit' has created a pull request for this issue:
https://github.com/apache/spark/pull/16991;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow when sort columns are part of partitioning columns,SPARK-19587,13042776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,tejasp,tejasp,14/Feb/17 02:39,15/Feb/17 16:15,14/Jul/23 06:30,15/Feb/17 16:15,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"This came up in discussion at https://github.com/apache/spark/pull/16898#discussion_r100697138

Allowing partition columns to be a part of sort columns should not be supported (logically it does not make sense). 
{code}
        df.write
          .format(source)
          .partitionBy(""i"")
          .bucketBy(8, ""x"")
          .sortBy(""i"")
          .saveAsTable(""bucketed_table"")
{code}

Hive fails for such case.

{code}
CREATE TABLE user_info_bucketed(user_id BIGINT) 
PARTITIONED BY(ds STRING)
CLUSTERED BY(user_id)
SORTED BY (ds ASC)
INTO 8 BUCKETS;
    
FAILED: SemanticException [Error 10002]: Invalid column reference
Caused by: SemanticException: Invalid column reference
{code}",,apachespark,cloud_fan,maropu,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 15 16:15:19 UTC 2017,,,,,,,,,,"0|i3a14f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 08:43;maropu;Do you work on this? If no, I'll do that, Thanks!;;;","14/Feb/17 19:23;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16931;;;","15/Feb/17 16:15;cloud_fan;Issue resolved by pull request 16931
[https://github.com/apache/spark/pull/16931];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect push down filter for double negative in SQL,SPARK-19586,13042775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,everett,everett,14/Feb/17 02:38,16/Feb/17 21:32,14/Jul/23 06:30,14/Feb/17 08:41,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,,,,,SQL,,,,,0,,,,,,,,,"Opening this as it's a somewhat serious issue in the 2.0.x tree in case there's a 2.0.3 planned, but it is fixed in 2.1.0.

While it works in 1.6.2 and 2.1.0, it appears 2.0.2 has a significant filter optimization error.

Example:

{noformat}
// Create some fake data

import org.apache.spark.sql.Row
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.types._

val rowsRDD = sc.parallelize(Seq(
    Row(1, ""fred""),
    Row(2, ""amy""),
    Row(3, null)))

val schema = StructType(Seq(
    StructField(""id"", IntegerType, nullable = true),
    StructField(""username"", StringType, nullable = true)))
    
val data = sqlContext.createDataFrame(rowsRDD, schema)
val path = ""/tmp/test_data""

data.write.mode(""overwrite"").parquet(path)

val testData = sqlContext.read.parquet(path)

testData.registerTempTable(""filter_test_table"")
{noformat}

{noformat}
%sql
explain select count(*) from filter_test_table where not( username is not null)
{noformat}

or

{noformat}
spark.sql(""select count(*) from filter_test_table where not( username is not null)"").explain
{noformat}

In 2.0.2, I'm seeing

{noformat}
== Physical Plan ==
*HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition
 +- *HashAggregate(keys=[], functions=[partial_count(1)])
 +- *Project
 +- *Filter (isnotnull(username#35) && NOT isnotnull(username#35))
 +- *BatchedScan parquet default.<hive table name>[username#35] Format: ParquetFormat, InputPaths: <path to parquet>, PartitionFilters: [], PushedFilters: [IsNotNull(username), Not(IsNotNull(username))], ReadSchema: struct<username:string>
{noformat}

which seems like both an impossible Filter and an impossible pushed filter.

In Spark 1.6.2 it was

{noformat}
== Physical Plan ==
TungstenAggregate(key=[], functions=[(count(1),mode=Final,isDistinct=false)], output=[_c0#1822L])
+- TungstenExchange SinglePartition, None
 +- TungstenAggregate(key=[], functions=[(count(1),mode=Partial,isDistinct=false)], output=[count#1825L])
 +- Project
 +- Filter NOT isnotnull(username#1590)
 +- Scan ParquetRelation[username#1590] InputPaths: <path to parquet>, PushedFilters: [Not(IsNotNull(username))]
{noformat}

and 2.1.0 it's working again as

{noformat}
== Physical Plan ==
*HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition
   +- *HashAggregate(keys=[], functions=[partial_count(1)])
      +- *Project
         +- *Filter NOT isnotnull(username#14)
            +- *FileScan parquet [username#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/test_table], PartitionFilters: [], PushedFilters: [Not(IsNotNull(username))], ReadSchema: struct<username:string>
{noformat}

while it's easy for humans in interactive cases to work around this by removing the double negative, it's a bit harder if it's a programmatic construct.
",,everett,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 17:25:18 UTC 2017,,,,,,,,,,"0|i3a147:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 08:40;maropu;This issue has been fixed in https://github.com/apache/spark/pull/16894 and the fix's already been backported into the 2.0 branch. So, I'll set ""Resolved"".;;;","14/Feb/17 17:25;everett;[~maropu] Awesome! Thanks so much!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the cacheTable and uncacheTable API call in the SQL Programming Guide,SPARK-19585,13042745,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ksunitha,ksunitha,ksunitha,14/Feb/17 00:33,14/Feb/17 06:50,14/Jul/23 06:30,14/Feb/17 06:50,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Documentation,,,,,0,,,,,,,,,"https://spark.apache.org/docs/latest/sql-programming-guide.html#caching-data-in-memory

In the doc, the call spark.cacheTable(“tableName”) and spark.uncacheTable(“tableName”) actually needs to be spark.catalog.cacheTable and spark.catalog.uncacheTable",,apachespark,ksunitha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 00:39:04 UTC 2017,,,,,,,,,,"0|i3a0xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 00:39;apachespark;User 'skambha' has created a pull request for this issue:
https://github.com/apache/spark/pull/16919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS for data source tables with an created location does not work,SPARK-19583,13042671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,smilegator,smilegator,13/Feb/17 20:27,02/Mar/17 06:51,14/Jul/23 06:30,02/Mar/17 06:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"{noformat}
        spark.sql(
          s""""""
             |CREATE TABLE t
             |USING parquet
             |PARTITIONED BY(a, b)
             |LOCATION '$dir'
             |AS SELECT 3 as a, 4 as b, 1 as c, 2 as d
           """""".stripMargin)
{noformat}

Failed with the error message:
{noformat}
path file:/private/var/folders/6r/15tqm8hn3ldb3rmbfqm1gf4c0000gn/T/spark-195cd513-428a-4df9-b196-87db0c73e772 already exists.;
org.apache.spark.sql.AnalysisException: path file:/private/var/folders/6r/15tqm8hn3ldb3rmbfqm1gf4c0000gn/T/spark-195cd513-428a-4df9-b196-87db0c73e772 already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:102)
{noformat}",,apachespark,cloud_fan,smilegator,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 02 06:50:40 UTC 2017,,,,,,,,,,"0|i3a0h3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/17 20:28;smilegator;This works well for hive tables. We should make it work. Could you take this [~windpiger]?;;;","14/Feb/17 05:42;windpiger;ok, I'd like to take this one, thanks a lot!;;;","15/Feb/17 13:13;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16938;;;","02/Mar/17 06:50;cloud_fan;Issue resolved by pull request 16938
[https://github.com/apache/spark/pull/16938];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for avro.schema.url while writing to hive table,SPARK-19580,13042581,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vinodkc,mateo7,mateo7,13/Feb/17 13:56,22/Nov/17 17:23,14/Jul/23 06:30,22/Nov/17 17:23,1.6.3,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.1,2.3.0,,,,SQL,,,,,0,,,,,,,,,"Support for writing to Hive table which uses Avro schema pointed to by avro.schema.url is missing. 

I have Hive table with Avro data format. Table is created with query like this:

{code:sql}
CREATE TABLE some_table
  PARTITIONED BY (YEAR int, MONTH int, DAY int)
  ROW FORMAT SERDE
        'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
      STORED AS INPUTFORMAT
        'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
      OUTPUTFORMAT
        'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
      LOCATION 'hdfs:///user/some_user/some_table'
      TBLPROPERTIES (
        'avro.schema.url'='hdfs:///user/some_user/some_table.avsc'
      )
{code}

Please notice that there is `avro.schema.url` and not `avro.schema.literal` property, as we have to keep schemas in separate files for some reasons.
Trying to write to such table results in NPE.

Tried to find workaround for this, but nothing helps. Tried:
    - setting df.write.option(""avroSchema"", avroSchema) with explicit schema in string
    - changing TBLPROPERTIES to SERDEPROPERTIES
    - replacing explicit detailed SERDE specification with STORED AS AVRO

I found that this can be solved by adding a couple of lines in `org.apache.spark.sql.hive.HiveShim` next to `AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL` is referenced.",,apachespark,mateo7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19878,,,SPARK-17920,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 22 13:01:08 UTC 2017,,,,,,,,,,"0|i39zx3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/17 16:41;mateo7;Similar to SPARK-17920;;;","18/Nov/17 09:02;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19779;;;","22/Nov/17 13:01;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/19795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Liquid Exception: Start indices amount is not equal to end indices amount,SPARK-19574,13042491,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,smilegator,smilegator,smilegator,13/Feb/17 07:01,13/Feb/17 11:19,14/Jul/23 06:30,13/Feb/17 11:19,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Documentation,,,,,0,,,,,,,,,"{noformat}
  Liquid Exception: Start indices amount is not equal to end indices amount, see /Users/xiao/IdeaProjects/sparkDelivery/docs/../examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java. in ml-features.md
{noformat}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 13 11:19:26 UTC 2017,,,,,,,,,,"0|i39zd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/17 07:07;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16908;;;","13/Feb/17 11:19;srowen;Issue resolved by pull request 16908
[https://github.com/apache/spark/pull/16908];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make NaN/null handling consistent in approxQuantile,SPARK-19573,13042486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,13/Feb/17 06:22,27/Jul/17 18:23,14/Jul/23 06:30,21/Mar/17 01:27,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"As discussed in https://github.com/apache/spark/pull/16776, this jira is used to track the following issue:
Multi-column version of approxQuantile drop the rows containing *any* NaN/null, the results are not consistent with outputs of the single-version.",,apachespark,barrybecker4,mlnick,podongfeng,timhunter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19339,,,SPARK-21550,,,,,,,,,,,,,SPARK-19339,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 22:37:51 UTC 2017,,,,,,,,,,"0|i39zbz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"17/Feb/17 03:24;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16971;;;","20/Feb/17 10:30;mlnick;cc [~timhunter] - can you take a look at the discussion (here: https://github.com/apache/spark/pull/16776#discussion_r101155454)? What is your view? thanks;;;","22/Feb/17 22:37;timhunter;I do not have too strong an opinion, as long as:
 1. we are consistent within Spark, or
 2. we follow the standard for numerical stuff (IEEE-754)

I am not sure what the standard is for SQL, though.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tests are failing to run on Windows with another instance Derby error with Hadoop 2.6.5,SPARK-19571,13042460,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,felixcheung,felixcheung,13/Feb/17 02:55,12/Dec/22 18:10,14/Jul/23 06:30,14/Feb/17 19:02,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,SQL,,,,0,,,,,,,,,"Between https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/build/751-master
https://github.com/apache/spark/commit/7a7ce272fe9a703f58b0180a9d2001ecb5c4b8db

And

https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/build/758-master
https://github.com/apache/spark/commit/c618ccdbe9ac103dfa3182346e2a14a1e7fca91a

Something is changed (not likely caused by R) such that tests running on Windows are consistently failing with

{code}
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database C:\Users\appveyor\AppData\Local\Temp\1\spark-75266bb9-bd54-4ee2-ae54-2122d2c011e8\metastore.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)
	at org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)
	at org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)

{code}

Since we run appveyor only when there is R changes, it is a bit harder to track down which change specifically caused this.
We also can't run appveyor on branch-2.1, so it could also be broken there.

This could be a blocker, since it could fail tests for the R release.",,apachespark,dongjoon,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 25 01:01:48 UTC 2017,,,,,,,,,,"0|i39z67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/17 02:57;felixcheung;https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark/build/758-master;;;","13/Feb/17 03:09;felixcheung;possibly the originating point of the exception with TestHiveSparkSession:
{code}
[00:36:18] 	at org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:65)
[00:36:18] 	at sun.reflect.GeneratedConstructorAccessor225.newInstance(Unknown Source)
[00:36:18] 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[00:36:18] 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
[00:36:18] 	at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)
[00:36:18] 	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$sharedState$1.apply(TestHive.scala:144)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession$$anonfun$sharedState$1.apply(TestHive.scala:144)
[00:36:18] 	at scala.Option.getOrElse(Option.scala:121)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sharedState$lzycompute(TestHive.scala:144)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sharedState(TestHive.scala:143)
[00:36:18] 	at org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:159)
[00:36:18] 	at org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSessionState.<init>(TestHive.scala:497)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sessionState$lzycompute(TestHive.scala:151)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sessionState(TestHive.scala:150)
[00:36:18] 	at org.apache.spark.sql.hive.test.TestHiveSparkSession.sessionState(TestHive.scala:115)
[00:36:18] 	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:65)
[00:36:18] 	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:552)
[00:36:18] 	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:307)
[00:36:18] 	at org.apache.spark.sql.api.r.SQLUtils$.createDF(SQLUtils.scala:139)
[00:36:18] 	at org.apache.spark.sql.api.r.SQLUtils.createDF(SQLUtils.scala)
[00:36:18] 	at sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source)
{code};;;","13/Feb/17 17:31;shivaram;cc [~hyukjin.kwon];;;","13/Feb/17 18:00;gurwls223;Oh, I overlooked and I thought it is just because of https://github.com/apache/spark/pull/16890 
Let me try to investigate when I get to my laptop tomorrow.;;;","14/Feb/17 14:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16927;;;","23/Aug/17 18:17;dongjoon;Hi, [~hyukjin.kwon].
Could you set `Fix Version`? Thanks!;;;","25/Aug/17 01:01;gurwls223;Sure, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaOffsetReader's consumers should not be in the same group,SPARK-19564,13042390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lwlin,lwlin,lwlin,12/Feb/17 11:30,13/Feb/17 07:02,14/Jul/23 06:30,13/Feb/17 07:02,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"In `KafkaOffsetReader`, when error occurs, we abort the existing consumer and create a new consumer. In our current implementation, the first consumer and the second consumer would be in the same group, which violates our intention of the two consumers not being in the same group.

The cause is that, in our current implementation, the first consumer is created before `groupId` and `nextId` are initialized in the constructor. Then even if `groupId` and `nextId` are increased during the creation of that first consumer, `groupId` and `nextId` would still be initialized to default values in the constructor.

We should make sure that `groupId` and `nextId` are initialized before any consumer is created.",,apachespark,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 12 15:59:03 UTC 2017,,,,,,,,,,"0|i39yqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/17 11:47;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16900;;;","12/Feb/17 14:49;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16902;;;","12/Feb/17 15:59;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16900;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyspark Dataframes don't allow timestamps near epoch,SPARK-19561,13042323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jason.white,jason.white,jason.white,11/Feb/17 21:37,08/Mar/17 01:23,14/Jul/23 06:30,07/Mar/17 21:12,2.0.1,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,SQL,,,,0,,,,,,,,,"Pyspark does not allow timestamps at or near the epoch to be created in a DataFrame. Related issue: https://issues.apache.org/jira/browse/SPARK-19299

TimestampType.toInternal converts a datetime object to a number representing microseconds since the epoch. For all times more than 2148 seconds before or after 1970-01-01T00:00:00+0000, this number is greater than 2^31 and Py4J automatically serializes it as a long.

However, for times within this range (~35 minutes before or after the epoch), Py4J serializes it as an int. When creating the object on the Scala side, ints are not recognized and the value goes to null. This leads to null values in non-nullable fields, and corrupted Parquet files.

The solution is trivial - force TimestampType.toInternal to always return a long.",,apachespark,jason.white,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 01:23:03 UTC 2017,,,,,,,,,,"0|i39ybz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/17 21:48;apachespark;User 'JasonMWhite' has created a pull request for this issue:
https://github.com/apache/spark/pull/16896;;;","08/Mar/17 01:23;apachespark;User 'JasonMWhite' has created a pull request for this issue:
https://github.com/apache/spark/pull/17200;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky KafkaSourceSuite.subscribing topic by pattern with topic deletions,SPARK-19559,13042247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lwlin,kayousterhout,kayousterhout,11/Feb/17 02:02,13/Feb/17 07:01,14/Jul/23 06:30,13/Feb/17 07:01,2.1.1,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,Tests,,,,0,,,,,,,,,"This test has started failing frequently recently; e.g., https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/72720/testReport/junit/org.apache.spark.sql.kafka010/KafkaSourceSuite/subscribing_topic_by_pattern_with_topic_deletions/ and https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/72725/testReport/org.apache.spark.sql.kafka010/KafkaSourceSuite/subscribing_topic_by_pattern_with_topic_deletions/

cc [~zsxwing] and [~tcondie] who seemed to have modified the related code most recently",,apachespark,kayousterhout,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 12 15:17:06 UTC 2017,,,,,,,,,,"0|i39xv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/17 10:07;lwlin;I think I found the root cause; will submit a patch soon.;;;","12/Feb/17 15:17;apachespark;User 'lw-lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16902;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broadcast data is not encrypted when I/O encryption is on,SPARK-19556,13042208,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,10/Feb/17 22:56,29/Mar/17 12:28,14/Jul/23 06:30,29/Mar/17 12:28,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"{{TorrentBroadcast}} uses a couple of ""back doors"" into the block manager to write and read data:

{code}
      if (!blockManager.putBytes(pieceId, bytes, MEMORY_AND_DISK_SER, tellMaster = true)) {
        throw new SparkException(s""Failed to store $pieceId of $broadcastId in local BlockManager"")
      }
{code}

{code}
      bm.getLocalBytes(pieceId) match {
        case Some(block) =>
          blocks(pid) = block
          releaseLock(pieceId)
        case None =>
          bm.getRemoteBytes(pieceId) match {
            case Some(b) =>
              if (checksumEnabled) {
                val sum = calcChecksum(b.chunks(0))
                if (sum != checksums(pid)) {
                  throw new SparkException(s""corrupt remote block $pieceId of $broadcastId:"" +
                    s"" $sum != ${checksums(pid)}"")
                }
              }
              // We found the block from remote executors/driver's BlockManager, so put the block
              // in this executor's BlockManager.
              if (!bm.putBytes(pieceId, b, StorageLevel.MEMORY_AND_DISK_SER, tellMaster = true)) {
                throw new SparkException(
                  s""Failed to store $pieceId of $broadcastId in local BlockManager"")
              }
              blocks(pid) = b
            case None =>
              throw new SparkException(s""Failed to get $pieceId of $broadcastId"")
          }
      }
{code}

The thing these block manager methods have in common is that they bypass the encryption code; so broadcast data is stored unencrypted in the block manager, causing unencrypted data to be written to disk if those blocks need to be evicted from memory.

The correct fix here is actually not to change {{TorrentBroadcast}}, but to fix the block manager so that:

- data stored in memory is not encrypted
- data written to disk is encrypted

This would simplify the code paths that use BlockManager / SerializerManager APIs (e.g. see SPARK-19520), but requires some tricky changes inside the BlockManager to still be able to use file channels to avoid reading whole blocks back into memory so they can be decrypted.",,apachespark,cloud_fan,lwlin,uncleGen,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 29 12:28:40 UTC 2017,,,,,,,,,,"0|i39xmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/17 03:32;uncleGen;[~vanzin] I am working on this, could you please assign it to me?
;;;","15/Feb/17 03:41;vanzin;We don't generally assign bugs. Leaving a message should be enough in case anyone else was also thinking about working on it.;;;","17/Feb/17 04:02;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16972;;;","14/Mar/17 21:40;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/17295;;;","29/Mar/17 12:28;cloud_fan;Issue resolved by pull request 17295
[https://github.com/apache/spark/pull/17295];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive UDF should support List and Map types,SPARK-19548,13042024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,10/Feb/17 10:22,10/Feb/17 22:47,14/Jul/23 06:30,10/Feb/17 22:47,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,1,,,,,,,,,We currently do not support List and Map types for Hive UDFs. We should improve this.,,apachespark,azeroth2b,cloud_fan,hvanhovell,maropu,mylesbaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 10 22:47:55 UTC 2017,,,,,,,,,,"0|i39whj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"10/Feb/17 11:27;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16886;;;","10/Feb/17 22:47;cloud_fan;Issue resolved by pull request 16886
[https://github.com/apache/spark/pull/16886];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compilation error with method not found when build against Hadoop 2.6.0.,SPARK-19545,13041996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,10/Feb/17 08:19,17/May/20 18:14,14/Jul/23 06:30,10/Feb/17 13:44,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"{code}
./build/sbt -Phadoop-2.6 -Pyarn -Dhadoop.version=2.6.0
{code}

{code}
[error] /Users/sshao/projects/apache-spark/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:249: value setRolledLogsIncludePattern is not a member of org.apache.hadoop.yarn.api.records.LogAggregationContext
[error]       logAggregationContext.setRolledLogsIncludePattern(includePattern)
[error]                             ^
[error] /Users/sshao/projects/apache-spark/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:251: value setRolledLogsExcludePattern is not a member of org.apache.hadoop.yarn.api.records.LogAggregationContext
[error]         logAggregationContext.setRolledLogsExcludePattern(excludePattern)
[error]                               ^
[error] two errors found
{code}",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19464,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 10 13:44:42 UTC 2017,,,,,,,,,,"0|i39wbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/17 09:41;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16884;;;","10/Feb/17 13:44;srowen;Issue resolved by pull request 16884
[https://github.com/apache/spark/pull/16884];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
from_json fails when the input row is empty ,SPARK-19543,13041938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,10/Feb/17 04:07,03/Jan/19 05:54,14/Jul/23 06:30,10/Feb/17 11:56,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Using from_json on a column with an empty string results in: java.util.NoSuchElementException: head of empty list
",,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 10 04:11:04 UTC 2017,,,,,,,,,,"0|i39vyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/17 04:11;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16881;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE TEMPORARY TABLE needs to avoid existing temp view,SPARK-19539,13041905,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xwu0226,xwu0226,xwu0226,10/Feb/17 01:21,14/Feb/17 03:47,14/Jul/23 06:30,14/Feb/17 03:47,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Current ""CREATE TEMPORARY TABLE ... "" is deprecated and recommend users to use ""CREATE TEMPORARY VIEW ..."" And it does not support ""IF NOT EXISTS"" clause.  However, if there is an existing temporary view defined, it is possible to unintentionally replace this existing view by issuing ""CREATE TEMPORARY TABLE ... "" with the same table/view name. ",,apachespark,xwu0226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 10 01:30:05 UTC 2017,,,,,,,,,,"0|i39vr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/17 01:30;apachespark;User 'xwu0226' has created a pull request for this issue:
https://github.com/apache/spark/pull/16878;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server doesn't refresh jobs for long-life apps like thriftserver,SPARK-19531,13041687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,olegd,olegd,olegd,09/Feb/17 12:38,26/Oct/18 18:06,14/Jul/23 06:30,20/Jul/17 16:39,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Spark Core,,,,,0,,,,,,,,,"If spark.history.fs.logDirectory points to hdfs, then spark history server doesn't refresh jobs page. This is caused by Hadoop - during writing to the .inprogress file Hadoop doesn't update file length until close and therefor Spark's history server is not able to detect any changes.

I'm gonna submit a PR to fix this.",,apachespark,chakravarthi,devaraj,DjvuLee,olegd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-24787,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 14 11:05:04 UTC 2017,,,,,,,,,,"0|i39uen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 11:05;apachespark;User 'dosoft' has created a pull request for this issue:
https://github.com/apache/spark/pull/16924;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TransportClientFactory.createClient() shouldn't call awaitUninterruptibly(),SPARK-19529,13041613,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,09/Feb/17 07:25,14/Feb/17 19:29,14/Jul/23 06:30,14/Feb/17 19:29,1.6.0,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,1.6.4,2.0.3,2.1.1,2.2.0,,Shuffle,Spark Core,,,,0,,,,,,,,,"In Spark's Netty RPC layer, TransportClientFactory.createClient() calls awaitUninterruptibly() on a Netty future while waiting for a connection to be established. This creates problem when a Spark task is interrupted while blocking in this call (which can happen in the event of a slow connection which will eventually time out). This has bad impacts on task cancellation when interruptOnCancel = true.

As an example of the impact of this problem, I experienced significant numbers of uncancellable ""zombie tasks"" on a production cluster where several tasks were blocked trying to connect to a dead shuffle server and then continued running as zombies after I cancelled the associated Spark stage. The zombie tasks ran for several minutes with the following stack:

{code}
java.lang.Object.wait(Native Method)
java.lang.Object.wait(Object.java:460)
io.netty.util.concurrent.DefaultPromise.await0(DefaultPromise.java:607) 
io.netty.util.concurrent.DefaultPromise.awaitUninterruptibly(DefaultPromise.java:301) 
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:224) 
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179) => holding Monitor(java.lang.Object@1849476028}) 
org.apache.spark.network.shuffle.ExternalShuffleClient$1.createAndStart(ExternalShuffleClient.java:105) 
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140) 
org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120) 
org.apache.spark.network.shuffle.ExternalShuffleClient.fetchBlocks(ExternalShuffleClient.java:114) 
org.apache.spark.storage.ShuffleBlockFetcherIterator.sendRequest(ShuffleBlockFetcherIterator.scala:169) 
org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchUpToMaxBytes(ShuffleBlockFetcherIterator.scala:
350) 
org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:286) 
org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:120) 
org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) 
org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:169) 
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) 
org.apache.spark.rdd.RDD.iterator(RDD.scala:287) 
[...]
{code}

I believe that we can easily fix this by using the InterruptedException-throwing await() instead.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 13 22:21:03 UTC 2017,,,,,,,,,,"0|i39ty7:",9223372036854775807,,,,,,,,,,,,,1.6.4,2.0.3,2.1.1,2.2.0,,,,,,,,"09/Feb/17 07:39;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16866;;;","13/Feb/17 22:21;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16917;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WAL should not be encrypted,SPARK-19520,13041512,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,08/Feb/17 22:12,13/Feb/17 22:34,14/Jul/23 06:30,13/Feb/17 22:34,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,DStreams,,,,,0,,,,,,,,,"If I/O encryption is enabled in Spark (SPARK-5682), the write ahead logs written by streaming applications will also be encrypted.

That creates issues because only the driver that wrote those files knows the key to decrypt them. So basically the WAL is unusuable in that scenario.

Instead, the WAL should not be encrypted by Spark; if the user has a need to encrypt that data, other means (such as HDFS encryption zones) should be used instead.",,apachespark,codingcat,lwlin,vanzin,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 00:55:06 UTC 2017,,,,,,,,,,"0|i39tbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/17 00:55;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16862;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSource fails to initialize partition offsets,SPARK-19517,13041415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vitillo,vitillo,vitillo,08/Feb/17 15:44,17/Feb/17 20:12,14/Jul/23 06:30,17/Feb/17 20:02,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,1,,,,,,,,,"A Kafka source with many partitions can cause the check-pointing logic to fail on restart. I got the following exception when trying to restart a Structured Streaming app that reads from a Kafka topic with hundred partitions.

{code}
17/02/08 15:10:09 ERROR StreamExecution: Query [id = 24e2a21a-4545-4a3e-80ea-bbe777d883ab, runId = 025609c9-d59c-4de3-88b3-5d5f7eda4a66] terminated with error
java.lang.IllegalArgumentException: Expected e.g. {""topicA"":{""0"":23,""1"":-1},""topicB"":{""0"":-2}}, got {""telemetry"":{""92"":302854
	at org.apache.spark.sql.kafka010.JsonUtils$.partitionOffsets(JsonUtils.scala:74)
	at org.apache.spark.sql.kafka010.KafkaSourceOffset$.apply(KafkaSourceOffset.scala:59)
	at org.apache.spark.sql.kafka010.KafkaSource$$anon$1.deserialize(KafkaSource.scala:134)
	at org.apache.spark.sql.kafka010.KafkaSource$$anon$1.deserialize(KafkaSource.scala:123)
	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.get(HDFSMetadataLog.scala:237)
	at org.apache.spark.sql.kafka010.KafkaSource.initialPartitionOffsets$lzycompute(KafkaSource.scala:138)
	at org.apache.spark.sql.kafka010.KafkaSource.initialPartitionOffsets(KafkaSource.scala:121)
           …
{code}",,apachespark,codingcat,lwlin,rwibowo,vitillo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/17 00:48;zsxwing;SPARK-19517ProposalforfixingKafkaOffsetMetadata.pdf;https://issues.apache.org/jira/secure/attachment/12852447/SPARK-19517ProposalforfixingKafkaOffsetMetadata.pdf",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 15:51:06 UTC 2017,,,,,,,,,,"0|i39sq7:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"08/Feb/17 15:51;apachespark;User 'vitillo' has created a pull request for this issue:
https://github.com/apache/spark/pull/16857;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range is not interruptible,SPARK-19514,13041381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,ala.luszczak,ala.luszczak,08/Feb/17 13:30,13/Feb/17 16:34,14/Jul/23 06:30,09/Feb/17 18:07,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Currently Range cannot be interrupted.

For example, if you start executing

spark.range(0, A_LOT, 1).crossJoin(spark.range(0, A_LOT, 1)).count()

and then call

DAGScheduler.cancellStage(...)

the execution won't stop.",,ala.luszczak,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 13 16:34:05 UTC 2017,,,,,,,,,,"0|i39sin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/17 13:14;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/16872;;;","13/Feb/17 16:34;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/16914;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
codegen for compare structs fails,SPARK-19512,13041343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bograd,bograd,bograd,08/Feb/17 10:18,09/May/18 06:06,14/Jul/23 06:30,09/Feb/17 18:16,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"This (1 struct field)

{code:java|title=1 struct field}
spark.range(10)
      .selectExpr(""named_struct('a', id) as col1"", ""named_struct('a', id+2) as col2"")
      .filter(""col1 = col2"").count
{code}

fails with

{code}
[info]   Cause: java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 144, Column 32: Expression ""range_value"" is not an rvalue
{code}

This (2 struct fields)
{code:java|title=2 struct fields}
spark.range(10)
    .selectExpr(""named_struct('a', id, 'b', id) as col1"", ""named_struct('a',id+2, 'b',id+2) as col2"")
    .filter($""col1"" === $""col2"").count
{code}

fails with 
{code}

Caused by: java.lang.IndexOutOfBoundsException: 1
  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)
  at scala.collection.immutable.List.apply(List.scala:84)
  at org.apache.spark.sql.catalyst.expressions.BoundReference.doGenCode(BoundAttribute.scala:64)
{code}
",,apachespark,bograd,dmcwhorter,howieyu,kiszk,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20111,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 09 06:06:41 UTC 2018,,,,,,,,,,"0|i39sa7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 11:58;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/16852;;;","09/Feb/17 19:37;apachespark;User 'bogdanrdc' has created a pull request for this issue:
https://github.com/apache/spark/pull/16875;;;","07/May/18 03:10;howieyu;Hi 
I still have this issue in 2.3.0

[https://stackoverflow.com/questions/50185228/spark-left-outer-join-cause-codegenerator-error]

 ;;;","08/May/18 08:04;maropu;I checked in the released v2.3.0 and the master though, I didn't hit this issue.;;;","08/May/18 13:18;howieyu;Hi

I think this  is similar issue, may not the same.

I have create a  project can re-produce this error

[https://github.com/howie/spark-codegen-error]

 S

 ;;;","09/May/18 06:06;maropu;can you put a simple query to reproduce here?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GROUPING SETS throws NullPointerException when use an empty column,SPARK-19509,13041321,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stanzhai,stanzhai,stanzhai,08/Feb/17 09:01,09/Feb/17 20:05,14/Jul/23 06:30,09/Feb/17 20:05,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,,,,SQL,,,,,0,,,,,,,,,"{code:sql|title=A simple case}
select count(1) from test group by e grouping sets(e)
{code}

{code:title=Schema of the test table}
scala> spark.sql(""desc test"").show()
+--------+---------+-------+
|col_name|data_type|comment|
+--------+---------+-------+
|       e|   string|   null|
+--------+---------+-------+
{code}

{code:sql|title=The column `e` is empty}
scala> spark.sql(""select e from test"").show()
+----+
|   e|
+----+
|null|
|null|
+----+
{code}

{code:title=Exception}
Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2112)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2327)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:248)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:636)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:595)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:604)
  ... 48 elided
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
  at org.apache.spark.scheduler.Task.run(Task.scala:99)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
{code}
",,apachespark,hvanhovell,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 09 13:54:03 UTC 2017,,,,,,,,,,"0|i39s5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 10:04;hvanhovell;Hi I have tried this on master and 2.1, and this works. I think this has been fixed in SPARK-18528.;;;","08/Feb/17 12:35;stanzhai;It doesn't look like the same problem.
I have tried to merge https://github.com/apache/spark/pull/15980 into branch-2.1.0. The problem still exist.;;;","08/Feb/17 12:37;hvanhovell;It is in 2.1

I cannot reproduce this issue on the latest apache spark 2.1 branch. What branch are you on?;;;","08/Feb/17 13:53;stanzhai;But, these‘s another problem, I've modified the description.;;;","09/Feb/17 13:44;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16873;;;","09/Feb/17 13:54;apachespark;User 'stanzhai' has created a pull request for this issue:
https://github.com/apache/spark/pull/16874;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing warnings import in pyspark.ml.util,SPARK-19506,13041232,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,08/Feb/17 01:10,13/Feb/17 18:01,14/Jul/23 06:30,13/Feb/17 18:01,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,ML,PySpark,,,,0,,,,,,,,,"Missing {{warnings}} import in {{pyspark.ml.util}} will cause {{NameError}} when calling {{JavaML(Reader|Writer).context}} methods.",,apachespark,holden,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 13 18:01:50 UTC 2017,,,,,,,,,,"0|i39rlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 01:14;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/16846;;;","13/Feb/17 18:01;holden;Thanks for reporting and fixing this issue! :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AttributeError on Exception.message in Python3; hides true exceptions in cloudpickle.py and broadcast.py",SPARK-19505,13041228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dgingrich,dgingrich,dgingrich,08/Feb/17 00:50,11/Apr/17 19:25,14/Jul/23 06:30,11/Apr/17 19:24,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,,,,,0,,,,,,,,,cloudpickle.py and broadcast.py both catch Exceptions then look at the 'message' field.  The message field doesn't exist in Python 3 so it rethrows and the original exception is lost.,"macOS Sierra 10.12.3
Spark 2.1.0, installed via Homebrew",apachespark,dgingrich,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 11 19:24:10 UTC 2017,,,,,,,,,,"0|i39rkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/17 01:09;apachespark;User 'dgingrich' has created a pull request for this issue:
https://github.com/apache/spark/pull/16845;;;","11/Apr/17 19:24;holden;Issue resolved by pull request 16845
[https://github.com/apache/spark/pull/16845];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to spill the aggregated hash map when radix sort is used,SPARK-19500,13041106,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,07/Feb/17 19:45,17/Feb/17 17:36,14/Jul/23 06:30,17/Feb/17 17:36,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"Radix sort requires that only half of the array could be occupied. But the aggregated hash map have a off-by-1 bug that could have 1 more item than half of the array, when this happen, the spilling will fail as:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 171 in stage 10.0 failed 4 times, most recent failure: Lost task 171.3 in stage 10.0 (TID 23899, 10.145.253.180, executor 24): java.lang.IllegalStateException: There is no space for new record 
at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:227) 
at org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:130) 
at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:250) 
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) 
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) 
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396) 
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 
at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:166) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 
at org.apache.spark.scheduler.Task.run(Task.scala:99) 
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace: 
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435) 
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423) 
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422) 
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) 
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) 
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422) 
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) 
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802) 
at scala.Option.foreach(Option.scala:257) 
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802) 
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650) 
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605) 
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594) 
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) 
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628) 
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918) 
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931) 
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951) 
at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:137) 
... 32 more 
Caused by: java.lang.IllegalStateException: There is no space for new record 
at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.insertRecord(UnsafeInMemorySorter.java:227) 
at org.apache.spark.sql.execution.UnsafeKVExternalSorter.<init>(UnsafeKVExternalSorter.java:130) 
at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter(UnsafeFixedWidthAggregationMap.java:250) 
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source) 
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) 
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$2.hasNext(WholeStageCodegenExec.scala:396) 
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 
at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:166) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) 
at org.apache.spark.scheduler.Task.run(Task.scala:99) 
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
... 1 more
{code}",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 17 17:36:24 UTC 2017,,,,,,,,,,"0|i39qtj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/17 22:08;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/16844;;;","17/Feb/17 17:36;davies;Issue resolved by pull request 16844
[https://github.com/apache/spark/pull/16844];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
to_date with format has weird behavior,SPARK-19496,13041006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,cloud_fan,cloud_fan,07/Feb/17 15:22,12/Dec/22 18:11,14/Jul/23 06:30,13/Feb/17 11:26,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,release-notes,,,,,,,,"Today, if we run
{code}
SELECT to_date('2015-07-22', 'yyyy-dd-MM')
{code}
will result to `2016-10-07`, while running
{code}
SELECT to_date('2014-31-12')   # default format
{code}
will return null.

this behavior is weird and we should check other systems like hive to see if this is expected.",,apachespark,cloud_fan,joshrosen,smilegator,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 24 00:55:23 UTC 2017,,,,,,,,,,"0|i39q7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/17 15:33;windpiger;I am working on this~;;;","08/Feb/17 05:55;gurwls223;- Hive

{code}
hive> SELECT to_date('2014-31-12');
2016-07-12
...

hive> SELECT to_date('2014-12-32');
2015-01-01
...

hive> SELECT to_date('2014-12-31');
2014-12-31
{code}

{code}
hive> SELECT to_date('2015-07-22', 'yyyy-dd-MM')
org.apache.hadoop.hive.ql.parse.SemanticException: Line 1:7 Arguments length mismatch ''yyyy-dd-MM'': to_date() requires 1 argument, got 2
...
{code}

- Postgres

{code}
postgres=# SELECT to_date('2014-12-31');
ERROR:  function to_date(unknown) does not exist
LINE 1: SELECT to_date('2014-12-31');
               ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
{code}

{code}
postgres=# SELECT to_date('2014-12-31', 'yyyy-MM-dd');
  to_date
------------
 2014-12-31
(1 row)

postgres=# SELECT to_date('2014-13-31', 'yyyy-MM-dd');
  to_date
------------
 2015-01-31
(1 row)
{code}


- Spark

{code}
spark-sql> SELECT to_date('2014-31-12');
NULL
...

spark-sql> SELECT to_date('2014-12-32');
NULL
...

spark-sql> SELECT to_date('2014-12-31');
2014-12-31
...
{code}

{code}
spark-sql> SELECT to_date('2015-07-22', 'yyyy-dd-MM')
2016-10-07
...
{code}

- MySQL

{code}
mysql> SELECT str_to_date('2014-12-31');
ERROR 1582 (42000): Incorrect parameter count in the call to native function 'str_to_date'
{code}

{code}
mysql> SELECT str_to_date('2014-12-31', '%Y-%m-%d');
+---------------------------------------+
| str_to_date('2014-12-31', '%Y-%m-%d') |
+---------------------------------------+
| 2014-12-31                            |
+---------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT str_to_date('2014-13-31', '%Y-%m-%d');
+---------------------------------------+
| str_to_date('2014-13-31', '%Y-%m-%d') |
+---------------------------------------+
| NULL                                  |
+---------------------------------------+
1 row in set, 1 warning (0.00 sec)
{code}


MySQL/SparkSQL - it seems returning {{NULL}}. MySQL seems not supporting omitting the format.

Hive/Postgres - it seems returning calculated dates. Hive seems not supporting the format. Postgres seems not supporting omitting the format.;;;","08/Feb/17 06:03;cloud_fan;The weird part is, Spark may have different behaviors depend on if the format is given. I think we should make it consistent, always return null or return calculated dates.;;;","08/Feb/17 06:12;gurwls223;Oh, yes. I just found and updated my comment.;;;","08/Feb/17 07:07;windpiger;mysql: select str_to_date('2014-12-31','%Y-%d-%m')   also return null

that is mysql both return null when the date is invalidate or the formate is invalidate.

and hive will transform the invalidate  date to  valid, e.g 2014-31-12 -> 31/12 = 2 -> 2014+2=2016
, 31 - 12*2=7 -> 2016-07-12

currently spark can handle wrong format / wrong date  when to_date has the format parameter (like hive's transform), what about we also make to_date without format parameter follow its action, that is replace null with a transformed date to return;;;","08/Feb/17 07:15;gurwls223;Yea, thank you for explanation. I was just curious so tested some cases and left it above to share it just in case for people like me :).;;;","08/Feb/17 07:17;cloud_fan;returning null looks better, [~smilegator] what do you think?;;;","08/Feb/17 07:20;windpiger;[~hyukjin.kwon]  😁;;;","09/Feb/17 06:20;smilegator;[~cloud_fan] Yeah, null looks better. Illegal inputs should not be silently accepted and converted.;;;","09/Feb/17 10:06;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16870;;;","24/Mar/17 00:55;joshrosen;Let's make sure to document this clearly in the release notes. I just spent a bunch of time debugging what turned out to be an invalid pattern which happened to work by accident before but now is returning {{null}}. Adding this to the release notes may save some pain when folks upgrade. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CSV infer schema does not take into account Inf,-Inf,NaN",SPARK-19488,13040883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,sdalmia_asf,sdalmia_asf,07/Feb/17 08:01,08/Feb/17 06:32,14/Jul/23 06:30,08/Feb/17 06:32,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,easyfix,features,,,,,,,"I observed that while loading a CSV as a dataframe, user-specified values for nanValue, positiveInf and negativeInf are disregarded when inferSchema = true. (They work if a user-specified schema is provided). However, even the spark defaults for the infinities (Inf and -Inf) do not work with inferSchema. 

Taking a look at the source code for the inferSchema for CSV (CSVInferSchema.scala), I found the following code snippet.
{code}
1.		private def tryParseDouble(field: String, options: CSVOptions): DataType = {
2.		    if ((allCatch opt field.toDouble).isDefined) {
3.		      DoubleType
4.		    } else {
5.		      tryParseTimestamp(field, options)
6.		    }
7.		  }
8.		
9.		  private def tryParseTimestamp(field: String, options: CSVOptions): DataType = {
10.		    // This case infers a custom `dataFormat` is set.
11.		    if ((allCatch opt options.timestampFormat.parse(field)).isDefined) {
12.		      TimestampType
13.		    } else if ((allCatch opt DateTimeUtils.stringToTime(field)).isDefined) {
14.		      // We keep this for backwords competibility.
15.		      TimestampType
16.		    } else {
17.		      tryParseBoolean(field, options)
18.		    }
19.		  }
{code}
Interestingly, the user-specified csv options are not at all used while determining if the field is of type double (as we can see in line 2). We can see that the options is used for timestamp type (line 11), which is why the 'dateFormat' option does work. 
However, when the field is NaN, it works because scala's toDouble function does convert the string NaN to the double equivalent of NaN. (I tried it using the shell):

{code}
scala> allCatch.opt(field.toDouble)
res12: Option[Double] = Some(8.0942)

scala> var field = ""NaN"";
field: String = NaN

scala> allCatch.opt(field.toDouble)
res13: Option[Double] = Some(NaN)

scala> var field = ""Inf"";
field: String = Inf

scala> allCatch.opt(field.toDouble)
res14: Option[Double] = None
{code}
Interestingly, scala does have Double equivalents of Infinity and -Infinity (but spark defaults are Inf and -Inf, which is why they don't work):

{code}
scala> field = ""Infinity"";
field: String = Infinity

scala> allCatch.opt(field.toDouble)
res15: Option[Double] = Some(Infinity)

scala> field = ""-Infinity"";
field: String = -Infinity

scala> allCatch.opt(field.toDouble)
res16: Option[Double] = Some(-Infinity)
{code}

The following csv, when ingested with inferSchema = true, therefore interprets the value column as a Double! (Regardless of the user-specified options)

{code}
ID,name,value,irrational,prime,real
1,e,2.7,true,false,true
2,pi,3.14,true,false,true
3,inf,Infinity,false,false,true
4,-inf,-Infinity,false,false,true
5,i,NaN,false,false,false

{code}

","Windows 10, SparkShell",apachespark,cloud_fan,sdalmia_asf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Feb 08 06:32:01 UTC 2017,,,,,,,,,,"0|i39pfz:",9223372036854775807,,,,,$iddhe$h,,,,,,,,,,,,,,,,,,,"07/Feb/17 15:17;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16834;;;","08/Feb/17 06:32;cloud_fan;Issue resolved by pull request 16834
[https://github.com/apache/spark/pull/16834];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL]SQLParser fails to resolve nested CASE WHEN statement with parentheses,SPARK-19472,13040534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,stanzhai,stanzhai,06/Feb/17 04:39,06/Feb/17 20:30,14/Jul/23 06:30,06/Feb/17 20:30,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"SQLParser fails to resolve nested CASE WHEN statement like this:

select case when
  (1) +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
from tb

==================== Exception ====================
Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'then' expecting {'.', '[', 'OR', 'AND', 'IN', NOT, 'BETWEEN', 'LIKE', RLIKE, 'IS', 'WHEN', EQ, '<=>', '<>', '!=', '<', LTE, '>', GTE, '+', '-', '*', '/', '%', 'DIV', '&', '|', '^'}(line 5, pos 0)

== SQL ==

select case when
  (1) +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
^^^
from tb

But，remove parentheses will be fine：

select case when
  1 +
  case when 1>0 then 1 else 0 end = 2
then 1 else 0 end
from tb
",,apachespark,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 15:12:04 UTC 2017,,,,,,,,,,"0|i39naf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/17 15:12;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16821;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A confusing NullPointerException when creating table,SPARK-19471,13040527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,donnyzone,stanzhai,stanzhai,06/Feb/17 04:26,31/Jan/18 17:51,14/Jul/23 06:30,14/Aug/17 16:46,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"After upgrading our Spark from 1.6.2 to 2.1.0, I encounter a confusing NullPointerException when creating table under Spark 2.1.0, but the problem does not exists in Spark 1.6.1. 

Environment: Hive 1.2.1, Hadoop 2.6.4 
{noformat}
==================== Code ==================== 
// spark is an instance of HiveContext 
// merge is a Hive UDF 
val df = spark.sql(""SELECT merge(field_a, null) AS new_a, field_b AS new_b FROM tb_1 group by field_a, field_b"") 
df.createTempView(""tb_temp"") 
spark.sql(""create table tb_result stored as parquet as "" + 
  ""SELECT new_a"" + 
  ""FROM tb_temp"" + 
  ""LEFT JOIN `tb_2` ON "" + 
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL), concat('GrLSRwZE_', cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`"") 

==================== Physical Plan ==================== 
*Project [new_a] 
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_, cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b], [fka6862f17], LeftOuter, BuildRight 
   :- HashAggregate(keys=[field_a, field_b], functions=[], output=[new_a, new_b, _nondeterministic]) 
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180), coordinator[target post-shuffle partition size: 1024880] 
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[], output=[field_a, field_b]) 
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) 
      +- *Project [fka6862f17] 
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 

What does '*' mean before HashAggregate? 

==================== Exception ==================== 
org.apache.spark.SparkException: Task failed while writing rows 
... 
java.lang.NullPointerException 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_2$(Unknown Source) 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) 
        at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:260) 
        at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:259) 
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:392) 
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:79) 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) 
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:252) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:199) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:197) 
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:202) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:138) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:137) 
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 
        at org.apache.spark.scheduler.Task.run(Task.scala:99) 
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
        at java.lang.Thread.run(Thread.java:745) 
{noformat}
I also found that when I changed my code as follow: 
{noformat}
spark.sql(""create table tb_result stored as parquet as "" + 
  ""SELECT new_b"" + 
  ""FROM tb_temp"" + 
  ""LEFT JOIN `tb_2` ON "" + 
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL), concat('GrLSRwZE_', cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`"") 

or 

spark.sql(""create table tb_result stored as parquet as "" + 
  ""SELECT new_a"" + 
  ""FROM tb_temp"" + 
  ""LEFT JOIN `tb_2` ON "" + 
  ""if(((`tb_temp`.`new_b`) = '' OR (`tb_temp`.`new_b`) IS NULL), concat('GrLSRwZE_', cast((200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`"") 

will not have this problem. 

== Physical Plan of select new_b ... == 
*Project [new_b] 
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_, cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b], [fka6862f17], LeftOuter, BuildRight 
   :- *HashAggregate(keys=[field_a, field_b], functions=[], output=[new_b, _nondeterministic]) 
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180), coordinator[target post-shuffle partition size: 1024880] 
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[], output=[field_a, field_b]) 
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) 
      +- *Project [fka6862f17] 
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
{noformat}
Difference is `HashAggregate(keys=[field_a, field_b], functions=[], output=[new_b, _nondeterministic])` has a '*' char before it. 

It looks like something wrong with WholeStageCodegen when combine HiveUDF + rand() + group by + join. ",,apachespark,cenyuhai,kiszk,stanzhai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jul 22 10:03:33 UTC 2017,,,,,,,,,,"0|i39n8v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/17 12:22;apachespark;User 'yangw1234' has created a pull request for this issue:
https://github.com/apache/spark/pull/16820;;;","22/Jul/17 10:03;cenyuhai;I try this pr, it works well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refresh the table cache after InsertIntoHadoopFsRelation,SPARK-19463,13040455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,05/Feb/17 11:04,28/Feb/17 20:00,14/Jul/23 06:30,28/Feb/17 20:00,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"If we first cache a DataSource table, then we insert some data into the table, we should refresh the data in the cache after the insert command. ",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 05 11:08:07 UTC 2017,,,,,,,,,,"0|i39mt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/17 11:08;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16809;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update dataset used in R documentation, examples to reduce warning noise and confusions",SPARK-19460,13040416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,felixcheung,felixcheung,05/Feb/17 00:14,01/Mar/17 06:32,14/Jul/23 06:30,01/Mar/17 06:32,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"Running build we have a bunch of warnings from using the `iris` dataset, for example.

Warning in FUN(X[[1L]], ...) :
Use Sepal_Length instead of Sepal.Length as column name
Warning in FUN(X[[2L]], ...) :
Use Sepal_Width instead of Sepal.Width as column name
Warning in FUN(X[[3L]], ...) :
Use Petal_Length instead of Petal.Length as column name
Warning in FUN(X[[4L]], ...) :
Use Petal_Width instead of Petal.Width as column name
Warning in FUN(X[[1L]], ...) :
Use Sepal_Length instead of Sepal.Length as column name
Warning in FUN(X[[2L]], ...) :
Use Sepal_Width instead of Sepal.Width as column name
Warning in FUN(X[[3L]], ...) :
Use Petal_Length instead of Petal.Length as column name
Warning in FUN(X[[4L]], ...) :
Use Petal_Width instead of Petal.Width as column name
Warning in FUN(X[[1L]], ...) :
Use Sepal_Length instead of Sepal.Length as column name
Warning in FUN(X[[2L]], ...) :
Use Sepal_Width instead of Sepal.Width as column name
Warning in FUN(X[[3L]], ...) :
Use Petal_Length instead of Petal.Length as column name

These are the results of having `.` in the column name. For reference, see SPARK-12191, SPARK-11976. Since it involves changing SQL, if we couldn't support that there then we should strongly consider using other dataset without `.`, eg. `cars`

And we should update this in API doc (roxygen2 doc string), vignettes, programming guide, R code example.
",,apachespark,felixcheung,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 00:51:04 UTC 2017,,,,,,,,,,"0|i39mmv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"16/Feb/17 01:04;wm624;Seems a lots of work. :) I can give a try.;;;","16/Feb/17 01:05;wm624;By the way, I remembered that you had discussion about fixing the underlying issue on some PR review.;;;","16/Feb/17 03:22;felixcheung;Yes- it's better to address the root issue with column name but it wouldn't hurt to avoid confusing everyone by not using iris every where.

;;;","23/Feb/17 00:51;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/17032;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC tables cannot be read when they contain char/varchar columns,SPARK-19459,13040342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,04/Feb/17 10:45,11/Jan/19 00:37,14/Jul/23 06:30,10/Feb/17 19:07,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"Reading from an ORC table which contains char/varchar columns can fail if the table has been created using Spark. This is caused by the fact that spark internally replaces char and varchar columns with a string column, this causes the ORC reader to use the wrong reader, and that eventually causes a ClassCastException.",,akhilnaidu,apachespark,hvanhovell,hvivani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20515,SPARK-19430,,,,,SPARK-21997,SPARK-23774,,SPARK-20901,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 23 04:07:06 UTC 2018,,,,,,,,,,"0|i39m6f:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"04/Feb/17 11:07;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/16804;;;","22/Feb/17 23:14;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17030;;;","23/Feb/17 20:21;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/17041;;;","15/Sep/17 02:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19235;;;","23/Mar/18 04:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/19235;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bug in the name assignment method in SparkR,SPARK-19452,13040215,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,03/Feb/17 19:37,05/Feb/17 19:38,14/Jul/23 06:30,05/Feb/17 19:38,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"The names method fails to check for validity of the assignment values. This can be fixed by calling colnames within names. See example below.

{code}
df <- suppressWarnings(createDataFrame(iris))
# this is error
colnames(df) <- NULL
# this should report error
names(df) <- NULL
{code}",,actuaryzhang,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 03 20:28:03 UTC 2017,,,,,,,,,,"0|i39le7:",9223372036854775807,,,,,felixcheung,,,,,,,,2.2.0,,,,,,,,,,,"03/Feb/17 20:28;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16794;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rangeBetween method should accept Long value as boundary,SPARK-19451,13040178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jchamp,jchamp,03/Feb/17 17:29,30/Jul/17 03:37,14/Jul/23 06:30,29/Jul/17 17:12,1.6.1,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,2,,,,,,,,,"Hi there,

there seems to be a major limitation in spark window functions and rangeBetween method.

If I have the following code :
{code:title=Exemple |borderStyle=solid}
    val tw =  Window.orderBy(""date"")
      .partitionBy(""id"")
      .rangeBetween( from , 0)
{code}

Everything seems ok, while *from* value is not too large... Even if the rangeBetween() method supports Long parameters.
But.... If i set *-2160000000L* value to *from* it does not work !

It is probably related to this part of code in the between() method, of the WindowSpec class, called by rangeBetween()

{code:title=between() method|borderStyle=solid}
    val boundaryStart = start match {
      case 0 => CurrentRow
      case Long.MinValue => UnboundedPreceding
      case x if x < 0 => ValuePreceding(-start.toInt)
      case x if x > 0 => ValueFollowing(start.toInt)
    }
{code}
( look at this *.toInt* )

Does anybody know it there's a way to solve / patch this behavior ?

Any help will be appreciated

Thx",,apachespark,hvanhovell,jchamp,kiszk,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 09:12:17 UTC 2017,,,,,,,,,,"0|i39l5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/17 01:53;uncleGen;Good catch! I will dig deeply into code and fix it if it is really a bug.;;;","06/Feb/17 08:31;jchamp;Thanks [~uncleGen] for your answer.

I was tempted to try to fix it myself.. But not sure of possible side effects.
If I can help you in any way ( code / debug / tests ), feel free to ask.

P.S. : I think this affects all version of Spark with Window functions, but I have only tested it on spark 1.6.1 and 2.0.2;;;","06/Feb/17 09:44;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16818;;;","06/Feb/17 09:58;uncleGen;[~jchamp] I have taken a fast look through the code, and did not find any strong point to set the type of index as Int. In the window function, there is really a underlying integer overflow issue. I  made a pull request (https://github.com/apache/spark/pull/16818), and any suggestion is appreciated.;;;","06/Feb/17 10:05;hvanhovell;[~jchamp] how may rows are in your partitions? 2 billion? So this is an oversight, but I am not sure we should even try to support more than {{1 << 32 - 1}} values in a partition.;;;","06/Feb/17 10:19;jchamp;Let's imagine that this window is used on timestamp values in ms : I can ask for a window with a range between [-2160000000L, 0] and only have a few values inside, not necessarily 2160000000L.

I can understand the limitation for the rowBetween() method but the rangeBetween() method is nice for this kind of usage.;;;","06/Feb/17 10:23;hvanhovell;Yeah, you are right about that. We should definitely support this.;;;","06/Feb/17 12:52;jchamp;Glad to see that I'm not the only one convinced by this usage !

This probably needs to use different data structures for rowBetween() and rangeBetween();;;","06/Feb/17 13:11;hvanhovell;At the end of the day I would like to support arbitrary literals for range frames. See: https://issues.apache.org/jira/browse/SPARK-9221;;;","04/Jul/17 09:22;jchamp;Any news on this bug / feature request ?

Or any workaround ? May be using stream I can efficiently do what I want ?;;;","05/Jul/17 09:15;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/18540;;;","11/Jul/17 09:12;jchamp;Nice so see that there is such a pull request for this issue !

If I can help you in some way, feel free to ask.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix input metrics for range operator,SPARK-19447,13040119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ala.luszczak,rxin,rxin,03/Feb/17 12:45,10/May/17 12:51,14/Jul/23 06:30,07/Feb/17 13:22,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Range operator currently does not output any input metrics, and as a result in the SQL UI the number of rows shown is always 0.

",,apachespark,rxin,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 12:51:03 UTC 2017,,,,,,,,,,"0|i39ksv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"06/Feb/17 03:19;windpiger;spark.range(1,100).show

there is some information in the SQL UI like:
`
Range
number of output rows: 99
`

I didn't see some information like `0 rows`

maybe I didn't get the right place. could you help to describe it more clearly?

thanks!;;;","07/Feb/17 10:26;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/16829;;;","16/Feb/17 17:12;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/16960;;;","10/May/17 12:51;apachespark;User 'ala' has created a pull request for this issue:
https://github.com/apache/spark/pull/17939;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorId in HearbeatReceiverSuite is incorrect.,SPARK-19437,13039844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,02/Feb/17 14:12,03/Feb/17 07:18,14/Jul/23 06:30,03/Feb/17 07:18,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"The current code in *HeartbeatReceiverSuite*, executorId is set as below:
{code}
  private val executorId1 = ""executor-1""
  private val executorId2 = ""executor-2""
{code}

The executorId is sent to driver when register as below:

{code}
test(""expire dead hosts should kill executors with replacement (SPARK-8119)"")  {
  ...
  fakeSchedulerBackend.driverEndpoint.askSync[Boolean](
      RegisterExecutor(executorId1, dummyExecutorEndpointRef1, ""1.2.3.4"", 0, Map.empty))
  ...
}
{code}

Receiving *RegisterExecutor*, the executorId will be compared with *currentExecutorIdCounter* as below:
{code}
case RegisterExecutor(executorId, executorRef, hostname, cores, logUrls)  =>
  if (executorDataMap.contains(executorId)) {
    executorRef.send(RegisterExecutorFailed(""Duplicate executor ID: "" + executorId))
    context.reply(true)
  } else {
  ...
  executorDataMap.put(executorId, data)
  if (currentExecutorIdCounter < executorId.toInt) {
    currentExecutorIdCounter = executorId.toInt
  }
  ...
{code}

*executorId.toInt* will cause NumberformatException.

This unit test can pass currently because of *askWithRetry*, when catching exception, RPC will call again, thus it will go *if* branch and return true.",,apachespark,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 02 14:21:04 UTC 2017,,,,,,,,,,"0|i39j3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/17 14:21;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/16779;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix an unexpected failure when connecting timeout,SPARK-19432,13039647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,01/Feb/17 23:55,02/Feb/17 05:40,14/Jul/23 06:30,02/Feb/17 05:40,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"When connecting timeout, `ask` may fail with a confusing message:

{code}
17/02/01 23:15:19 INFO Worker: Connecting to master ...
java.lang.IllegalArgumentException: requirement failed: TransportClient has not yet been set.
        at scala.Predef$.require(Predef.scala:224)
        at org.apache.spark.rpc.netty.RpcOutboxMessage.onTimeout(Outbox.scala:70)
        at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$ask$1.applyOrElse(NettyRpcEnv.scala:232)
        at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$ask$1.applyOrElse(NettyRpcEnv.scala:231)
        at scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:138)
        at scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:136)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
{code}",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 23:57:05 UTC 2017,,,,,,,,,,"0|i39hvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/17 23:57;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16773;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column.__getitem__ should support slice arguments,SPARK-19429,13039573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,01/Feb/17 19:54,13/Feb/17 23:24,14/Jul/23 06:30,13/Feb/17 23:24,1.6.3,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"PySpark {{Column}} uses {{\_\_getslice\_\_}} as a synonym for {{substr}}. {{\_\_getslice\_\_}} is deprecated in Python 2.0 (since 2.0) and has been removed in Python 3.

If we want to support slicing notation it should be handled by {{\_\_getitem\_\_}}.",,apachespark,holden,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 13 23:24:41 UTC 2017,,,,,,,,,,"0|i39hfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/17 19:59;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/16771;;;","13/Feb/17 23:24;holden;Issue resolved by pull request 16771
[https://github.com/apache/spark/pull/16771];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ExtractEquiJoinKeys support UDT columns,SPARK-19425,13039494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,01/Feb/17 15:14,04/Feb/17 23:59,14/Jul/23 06:30,04/Feb/17 23:59,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"DataFrame.except doesn't work for UDT columns. It is because ExtractEquiJoinKeys will run Literal.default against UDT. However, we don't handle UDT in Literal.default and an exception will throw like:

java.lang.RuntimeException: no default for type 
org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7
  at org.apache.spark.sql.catalyst.expressions.Literal$.default(literals.scala:179)
  at org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:117)
  at org.apache.spark.sql.catalyst.planning.ExtractEquiJoinKeys$$anonfun$4.apply(patterns.scala:110)

More simple fix is just let Literal.default handle UDT by its sql type. So we can use more efficient join type on UDT.

Besides except, this also fixes other similar scenarios, so in summary this fixes:

* except on two Datasets with UDT
* intersect on two Datasets with UDT
* Join with the join conditions using <=> on UDT columns
",,apachespark,viirya,vijoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 15:20:04 UTC 2017,,,,,,,,,,"0|i39gxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/17 15:16;viirya;I remember affects version can be None before. But when create this issue, it becomes required field.;;;","01/Feb/17 15:20;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16765;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Parquet to 1.8.2,SPARK-19409,13039132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,31/Jan/17 08:41,02/Jun/17 00:50,14/Jul/23 06:30,31/Jan/17 10:43,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Build,,,,,0,,,,,,,,,"Apache Parquet 1.8.2 is released officially last week on 26 Jan.
This issue aims to bump Parquet version to 1.8.2 since it includes many fixes.

https://lists.apache.org/thread.html/af0c813f1419899289a336d96ec02b3bbeecaea23aa6ef69f435c142@%3Cdev.parquet.apache.org%3E",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18539,SPARK-20958,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 18:05:04 UTC 2017,,,,,,,,,,"0|i39eqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/17 08:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16751;;;","03/Feb/17 19:09;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/16791;;;","03/Feb/17 20:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16795;;;","07/Feb/17 18:05;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
defaultFS is used FileSystem.get instead of getting it from uri scheme,SPARK-19407,13039018,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,uncleGen,amit.assudani@gmail.com,amit.assudani@gmail.com,30/Jan/17 22:24,06/Nov/19 09:41,14/Jul/23 06:30,07/Feb/17 05:03,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,checkpoint,filesystem,starter,streaming,,,,,"Caused by: java.lang.IllegalArgumentException: Wrong FS: s3a://**************/checkpoint/7b2231a3-d845-4740-bfa3-681850e5987f/metadata, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:649)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:82)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)
	at org.apache.spark.sql.execution.streaming.StreamMetadata$.read(StreamMetadata.scala:51)
	at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:100)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:232)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:269)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:262)

Can easily replicate on spark standalone cluster by providing checkpoint location uri scheme anything other than ""file://"" and not overriding in config.

WorkAround  --conf spark.hadoop.fs.defaultFS=s3a://somebucket or set it in sparkConf or spark-default.conf",,amit.assudani@gmail.com,apachespark,hryhoriev.nick,kerbylane,lwlin,stevel@apache.org,uncleGen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Nov 06 09:41:32 UTC 2019,,,,,,,,,,"0|i39e1b:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"31/Jan/17 00:07;zsxwing;Good catch. Do you want to submit a PR to fix it? It's just replacing ""FileSystem.get(hadoopConf)"" with ""metadataFile.getFileSystem(hadoopConf)"".;;;","31/Jan/17 15:01;amit.assudani@gmail.com;I'll do it over the weekend, you may assign it to me. I may need some logistics support as it will be my first PR.;;;","03/Feb/17 10:16;stevel@apache.org;Yes, looks like  {{StreamMetadata.read()}} is getting it wrong. Which is funny, as {{StreamingQueryManager.createQuery()}} gets it right

{code}

  /** Read the metadata from file if it exists */
  def read(metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] = {
    val fs = FileSystem.get(hadoopConf)  /* HERE */
    if (fs.exists(metadataFile)) {
      var input: FSDataInputStream = null
      try {
{code}

when it should be
{code}
val fs = FileSystem.get(metadataFile, hadoopConf)
{code}

The hard part will be testing this;;;","05/Feb/17 11:55;srowen;[~aassudani] are you making a pull request?;;;","06/Feb/17 01:27;uncleGen;[~aassudani] Are you still working on this? As this issue is clear and easy to fix, I will making a pr later if it is busy for you to work on this.;;;","06/Feb/17 01:40;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16815;;;","06/Nov/19 09:35;hryhoriev.nick;I have the same issue with spark 2.4.4.
 When I Use spark on YARN in Client mode.
{code:java}
val sparkConf = SparkOnYarnAppController.sparkHadoopKeys(configuration)
 .foldLeft(new SparkConf()
 .setAppName(appName)
 .setIfMissing(""spark.ui.enabled"", ""false"")
 .setMaster(""yarn"")
 .setIfMissing(""spark.hadoop.yarn.resourcemanager.hostname"",s""hadoop-$cluster.com"")
 .setIfMissing(""spark.yarn.archive"", s""hdfs:///sparkDistributions/$distribution.tgz"")
 .setIfMissing(""spark.dynamicAllocation.enabled"", ""false"")
 .setIfMissing(""spark.driver.memory"", ""1g"")
 .setIfMissing(""spark.driver.cores"", ""1"")
 .setIfMissing(""spark.executor.memory"", ""1g"")
 .setIfMissing(""spark.executor.instances"", ""1"")
 .setIfMissing(""spark.executor.cores"", ""1"")
 .setIfMissing(""spark.yarn.maxAppAttempts"", ""1"")
 )((sparkConf, hadoopProps) => sparkConf.set(hadoopProps._1, hadoopProps._2))
SparkSession.builder().config(sparkConf).getOrCreate()
case class TestRecord(partition: Long, value: String)
object CommonTools {

implicit class MockDataframe(session: SparkSession) {
implicit val sqlContext: SQLContext = session.sqlContext
def mockDataFrame[T: Encoder](mockData: Seq[T]): DataFrame ={ 
 val mockStream = MemoryStream[T]
 mockStream.addData(mockData)
 mockStream.toDF() }
}
implicit class StreamSinkToHadoopFileSystem(dataFrame: DataFrame) {
 def sinkToS3(s3path: Str ing, format: String, checkpointDir: String, trigger:  Trigger): StreamingQuery ={
 dataFrame.writeStream
 .format(""parquet"")
 .queryName(""Test-TooManyVersionPerRootPrefixInS3"")
 .trigger(trigger)
 .option(""checkpointLocation"", checkpointDir)
 .format(format)
 .partitionBy(""partition"")
 .option(""path"", s3path) .start() }
 }
}
 
val stream = sparkSession
 .mockDataFrame[TestRecord]((0 to 100).map { i => TestRecord(i, s""i-${UUID.randomUUID().toString}"") })
 .sinkToS3(
 s3path = s""$outputDir/TooManyVersionPerRootPrefixInS3/"",
 format = ""parquet"",
 checkpointDir = s""$checkpointDir/TooManyVersionPerRootPrefixInS3-checkpoint"",
 trigger = Trigger.ProcessingTime(5.seconds)
 ){code}

exception
{code:java}
2019-11-06 11:31:47 ERROR org.apache.spark.sql.execution.streaming.StreamMetadata:91 - Error writing stream metadata StreamMetadata(bc32a9a9-8328-406d-8b37-30770e10962b) to s3a://bukcet/mhr/TooManyVersionPerRootPrefixInS3-checkpoint/metadata2019-11-06 11:31:47 ERROR org.apache.spark.sql.execution.streaming.StreamMetadata:91 - Error writing stream metadata StreamMetadata(bc32a9a9-8328-406d-8b37-30770e10962b) to s3a://af-eu-west-1-stg-data-lake-orc-with-crr/mhr/TooManyVersionPerRootPrefixInS3-checkpoint/metadataorg.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories. at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461) at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200) at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:475) at org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:66) at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:663) at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1177) at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100) at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699) at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90) at org.apache.hadoop.fs.FileContext.create(FileContext.java:699) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:311)Exception in thread ""main"" at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:318) at org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:125) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:123) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:123) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:48) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories. at com.appsflyer.spark.s3.it.CommonTools$StreamSinkToHadoopFileSystem.sinkToS3(TooManyVersionPerRootPrefixInS3.scala:45) at com.appsflyer.spark.s3.it.TooManyVersionPerRootPrefixInS3$.main(TooManyVersionPerRootPrefixInS3.scala:80) at com.appsflyer.spark.s3.it.TooManyVersionPerRootPrefixInS3.main(TooManyVersionPerRootPrefixInS3.scala) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461) at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200) at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:475) at org.apache.hadoop.fs.s3a.S3AOutputStream.<init>(S3AOutputStream.java:66) at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:663) at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1177) at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100) at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699) at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90) at org.apache.hadoop.fs.FileContext.create(FileContext.java:699) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:311) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:318) at org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:125) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:123) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:123) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:48) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)
{code}
 ;;;","06/Nov/19 09:41;hryhoriev.nick;[~uncleGen];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Function to_json ignores the user-provided options,SPARK-19406,13039009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,30/Jan/17 21:43,31/Jan/17 02:39,14/Jul/23 06:30,31/Jan/17 02:39,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
val df = Seq(Tuple1(Tuple1(java.sql.Timestamp.valueOf(""2015-08-26 18:00:00.0"")))).toDF(""a"")
val options = Map(""timestampFormat"" -> ""dd/MM/yyyy HH:mm"")
df.select(to_json($""a"", options)).show(false)
{noformat}
The current output is like
{noformat}
+--------------------------------------+
|structtojson(a)                       |
+--------------------------------------+
|{""_1"":""2015-08-26T18:00:00.000-07:00""}|
+--------------------------------------+
{noformat}

The expected output should be like
{noformat}
+-------------------------+
|structtojson(a)          |
+-------------------------+
|{""_1"":""26/08/2015 18:00""}|
+-------------------------+
{noformat}",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 30 21:49:04 UTC 2017,,,,,,,,,,"0|i39dzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/17 21:49;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16745;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.sql.column exports non-existent names,SPARK-19403,13038888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,30/Jan/17 13:14,30/Jan/17 17:01,14/Jul/23 06:30,30/Jan/17 17:01,1.4.0,1.5.0,1.6.0,2.0.0,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,,,,,,1,,,,,,,,,"There is a number of names, which are exported with {{\_\_all\_\_}}, but don't exist in the module ({{DataFrame}}, {{DataFrameNaFunctions}}, {{DataFrameStatFunctions}}). 

https://github.com/apache/spark/blob/a15ca5533db91fefaf3248255a59c4d94eeda1a9/python/pyspark/sql/column.py#L30

This results in unexpected import erros:

{code}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0-SNAPSHOT
      /_/

Using Python version 3.5.2 (default, Jul  2 2016 17:53:06)
SparkSession available as 'spark'.

In [1]: from pyspark.sql.column import *
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-1-94d9560dc0eb> in <module>()
----> 1 from pyspark.sql.column import *

AttributeError: module 'pyspark.sql.column' has no attribute 'DataFrame'
{code}",,apachespark,Elie A.,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 30 13:20:03 UTC 2017,,,,,,,,,,"0|i39d8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/17 13:20;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/16742;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLM fails for intercept only model,SPARK-19400,13038835,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,30/Jan/17 08:07,08/Feb/17 18:42,14/Jul/23 06:30,08/Feb/17 18:42,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,,,,,0,,,,,,,,,"Intercept-only GLM fails for non-Gaussian family because of reducing an empty array in IWLS. 

{code}
val dataset = Seq(
          (1.0, 1.0, 2.0, 0.0, 5.0),
          (0.5, 2.0, 1.0, 1.0, 2.0),
          (1.0, 3.0, 0.5, 2.0, 1.0),
          (2.0, 4.0, 1.5, 3.0, 3.0)
        ).toDF(""y"", ""w"", ""off"", ""x1"", ""x2"")

val formula = new RFormula().setFormula(""y ~ 1"")
val output = formula.fit(dataset).transform(dataset)
val glr = new GeneralizedLinearRegression().setFamily(""poisson"")
val model = glr.fit(output)

java.lang.UnsupportedOperationException: empty.reduceLeft
{code}",,actuaryzhang,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 18:42:37 UTC 2017,,,,,,,,,,"0|i39cwn:",9223372036854775807,,,,,yanboliang,,,,,,,,,,,,,,,,,,,"30/Jan/17 08:13;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16740;;;","08/Feb/17 18:42;josephkb;Issue resolved by pull request 16740
[https://github.com/apache/spark/pull/16740];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R Coalesce on DataFrame and coalesce on column,SPARK-19399,13038831,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,30/Jan/17 07:47,16/Feb/17 03:29,14/Jul/23 06:30,15/Feb/17 18:59,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"coalesce on DataFrame is different from repartition, where shuffling is avoided. We should have that in SparkR.
coalesce on Column is convenient to have in expression.",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 03:29:05 UTC 2017,,,,,,,,,,"0|i39cvr:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"30/Jan/17 07:51;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16739;;;","16/Feb/17 03:29;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16950;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Option names of LIBSVM and TEXT are not case insensitive.,SPARK-19397,13038805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,29/Jan/17 22:11,08/Feb/17 01:35,14/Jul/23 06:30,08/Feb/17 01:35,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,1,,,,,,,,,"Prior to Spark 2.1, the option names are case sensitive for all the formats. Since Spark 2.1, the option key names become case insensitive except the format TEXT and LIBSVM. 
",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 01:35:32 UTC 2017,,,,,,,,,,"0|i39cpz:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"30/Jan/17 02:40;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16737;;;","08/Feb/17 01:35;cloud_fan;Issue resolved by pull request 16737
[https://github.com/apache/spark/pull/16737];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[DOC] Options are case-insensitive since Spark 2.1,SPARK-19396,13038793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,29/Jan/17 20:39,30/Jan/17 22:07,14/Jul/23 06:30,30/Jan/17 22:07,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"After resolving the JIRA https://issues.apache.org/jira/browse/SPARK-18433, the doc needs an update.",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18433,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 29 20:48:05 UTC 2017,,,,,,,,,,"0|i39cnb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/17 20:48;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16734;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convert coefficients in summary to matrix,SPARK-19395,13038783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,actuaryzhang,actuaryzhang,actuaryzhang,29/Jan/17 18:28,31/Jan/17 20:24,14/Jul/23 06:30,31/Jan/17 20:24,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,The coefficients component in model summary should be 'matrix' but the underlying structure is indeed list. This affects several models except for 'AFTSurvivalRegressionModel' which has the correct implementation. The fix is to first unlist the coefficients returned from the callJMethod before converting to matrix.,,actuaryzhang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 29 18:30:03 UTC 2017,,,,,,,,,,"0|i39cl3:",9223372036854775807,,,,,felixcheung,,,,,,,,2.2.0,,,,,,,,,,,"29/Jan/17 18:30;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16730;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading an empty folder as parquet causes an Analysis Exception,SPARK-19388,13038550,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,franklynDsouza,franklynDsouza,27/Jan/17 22:02,28/Aug/17 10:30,14/Jul/23 06:30,27/Jan/17 22:08,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PySpark,,,,,0,,,,,,,,,"Reading an empty folder as parquet used to return an empty dataframe up till 2.0 .

Now this causes an analysis exception like so 

{code}
In [1]: df = sc.sql.read.parquet(""empty_dir/"")
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
----> 1 df = sqlCtx.read.parquet(""empty_dir/"")

spark/99f3dfa6151e312379a7381b7e65637df0429941/python/pyspark/sql/readwriter.pyc in parquet(self, *paths)
    272         [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]
    273         """"""
--> 274         return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
    275
    276     @ignore_unicode_prefix

park/99f3dfa6151e312379a7381b7e65637df0429941/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134
   1135         for temp_arg in temp_args:

spark/99f3dfa6151e312379a7381b7e65637df0429941/python/pyspark/sql/utils.pyc in deco(*a, **kw)
     67                                              e.java_exception.getStackTrace()))
     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):
---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)
     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):
     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)

AnalysisException: u'Unable to infer schema for Parquet. It must be specified manually.;'
{code}
",,franklynDsouza,sdalmia_asf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 10:30:06 UTC 2017,,,,,,,,,,"0|i39bhj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/17 10:30;sdalmia_asf;I am also facing this issue; any reason why it was closed so quickly?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bisecting k-means in SparkR documentation,SPARK-19386,13038503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,KrishnaKalyan3,felixcheung,felixcheung,27/Jan/17 18:59,04/Feb/17 01:26,14/Jul/23 06:30,03/Feb/17 20:20,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,SparkR,,,,0,,,,,,,,,"we need updates to programming guide, example and vignettes
",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18821,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 04 01:26:03 UTC 2017,,,,,,,,,,"0|i39b73:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"01/Feb/17 17:39;apachespark;User 'krishnakalyan3' has created a pull request for this issue:
https://github.com/apache/spark/pull/16767;;;","04/Feb/17 01:26;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16799;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateOperator metrics should still return the total number of rows in state even if there was no data for a trigger,SPARK-19378,13038326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,brkyvz,brkyvz,26/Jan/17 23:51,13/Jul/18 05:33,14/Jul/23 06:30,01/Feb/17 01:14,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"If you have a StreamingDataFrame with an aggregation, we report a metric called stateOperators which consists of a list of data points per aggregation for our query (With Spark 2.1, only one aggregation is supported).

These data points report:
 - numUpdatedStateRows
 - numTotalStateRows

If a trigger had no data - therefore was not fired - we return 0 data points, however we should actually return a data point with
 - numTotalStateRows: numTotalStateRows in lastExecution
 - numUpdatedStateRows: 0

This also affects eventTime statistics. We should still provide the min, max, avg even through the data didn't change.",,apachespark,brkyvz,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 01:14:03 UTC 2017,,,,,,,,,,"0|i39a3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/17 00:44;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/16716;;;","01/Feb/17 01:14;tdas;Issue resolved by pull request 16716
[https://github.com/apache/spark/pull/16716];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos implementation of spark.scheduler.minRegisteredResourcesRatio looks at acquired cores rather than registerd cores,SPARK-19373,13038223,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mgummelt,mgummelt,mgummelt,26/Jan/17 18:38,01/Mar/17 23:54,14/Jul/23 06:30,28/Feb/17 23:13,1.6.3,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Mesos,,,,,0,,,,,,,,,"We're currently using `totalCoresAcquired` to account for registered resources, which is incorrect.  That variable measures the number of cores the scheduler has accepted.  We should be using `totalCoreCount` like the other schedulers do.

Fixing this is important for locality, since users often want to wait for all executors to come up before scheduling tasks to ensure they get a node-local placement. 

original PR to add support: https://github.com/apache/spark/pull/8672/files",,apachespark,mgummelt,skonto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 23:54:59 UTC 2017,,,,,,,,,,"0|i399fr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/17 23:52;apachespark;User 'mgummelt' has created a pull request for this issue:
https://github.com/apache/spark/pull/17045;;;","28/Feb/17 09:43;skonto;How spark leverages the node-local placement with this change? ;;;","28/Feb/17 19:10;mgummelt;This change makes it so that the user can instruct the driver to wait for all executors to register before scheduling tasks.  The TaskSchedulerImpl understand locality, so it can then make the optimal placement. Otherwise, tasks are scheduled as soon as the first executor is registered, which of course might not be node-local for the first task.

However, this is still assuming that executors will be scheduled on the correct nodes, which isn't guaranteed unless you're launching executors on every node in your cluster.  For the best locality functionality, we need to integrate task locality information with dynamic allocation, so that the driver can dynamically spin up executors on the needed nodes.  That is outside the scope of this JIRA, though.;;;","28/Feb/17 23:14;srowen;Resolved by https://github.com/apache/spark/pull/17045;;;","01/Mar/17 08:05;skonto;[~mgummelt]  +1 for task locality + dynamic allocation. That would mean also decline offers if locality is not satisfied, until you get the appropriate nodes? In other words, it means trying to optimize locality, while getting random offers I guess...;;;","01/Mar/17 22:54;apachespark;User 'mgummelt' has created a pull request for this issue:
https://github.com/apache/spark/pull/17129;;;","01/Mar/17 23:54;mgummelt;[~skonto] Either decline or hoard.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code generation for Filter predicate including many OR conditions exceeds JVM method size limit ,SPARK-19372,13038203,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kiszk,jay.pranavamurthi,jay.pranavamurthi,26/Jan/17 17:26,12/Dec/22 18:11,14/Jul/23 06:30,16/May/17 21:47,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,1,,,,,,,,,"For the attached csv file, the code below causes the exception ""org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/InternalRow;)Z"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate"" grows beyond 64 KB

Code:
{code:borderStyle=solid}
  val conf = new SparkConf().setMaster(""local[1]"")
  val sqlContext = SparkSession.builder().config(conf).getOrCreate().sqlContext

  val dataframe =
    sqlContext
      .read
      .format(""com.databricks.spark.csv"")
      .load(""wide400cols.csv"")

  val filter = (0 to 399)
    .foldLeft(lit(false))((e, index) => e.or(dataframe.col(dataframe.columns(index)) =!= s""column${index+1}""))

  val filtered = dataframe.filter(filter)
  filtered.show(100)
{code}",,aash,apachespark,dongjoon,jay.pranavamurthi,kiszk,lwlin,pbeauvois,poplav,srinivasanm,vish741,waldoppper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/17 17:27;jay.pranavamurthi;wide400cols.csv;https://issues.apache.org/jira/secure/attachment/12849539/wide400cols.csv",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 16:55:23 UTC 2017,,,,,,,,,,"0|i399bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/17 13:09;kiszk;I was able to reproduce this. I am thinking how to reduce bytecode size per Java method.;;;","27/Feb/17 19:40;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/17087;;;","23/Mar/17 19:23;aash;I've seen this as well on parquet files.;;;","24/Mar/17 00:22;gurwls223;I have seen this too before.;;;","24/Mar/17 01:03;kiszk;I implemented the code to take care of it, and am waiting for the review.;;;","26/May/17 03:27;dongjoon;Hi, [~kiszk]. I met this failure also.
Is it possible to backport this to 2.2.0?;;;","26/May/17 03:34;vish741;+1 for backporting this to 2.2.0!;;;","26/May/17 04:31;kiszk;I see. Let me create a PR for 2.2.0;;;","26/May/17 07:29;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18119;;;","27/May/17 05:59;dongjoon;Thank you so much all!;;;","11/Aug/17 08:27;srinivasanm;Hi [~kiszk], the fix does not work in 2.2.0 for 

select * from temp where not( Field1 = '' and  Field2 = '' and  Field3 = '' and  Field4 = '' and  Field5 = '' and  BLANK_5 = '' and  Field7 = '' and  Field8 = '' and  Field9 = '' and  Field10 = '' and  Field11 = '' and  Field12 = '' and  Field13 = '' and  Field14 = '' and  Field15 = '' and  Field16 = '' and  Field17 = '' and  Field18 = '' and  Field19 = '' and  Field20 = '' and  Field21 = '' and  Field22 = '' and  Field23 = '' and  Field24 = '' and  Field25 = '' and  Field26 = '' and  Field27 = '' and  Field28 = '' and  Field29 = '' and  Field30 = '' and  Field31 = '' and  Field32 = '' and  Field33 = '' and  Field34 = '' and  Field35 = '' and  Field36 = '' and  Field37 = '' and  Field38 = '' and  Field39 = '' and  Field40 = '' and  Field41 = '' and  Field42 = '' and  Field43 = '' and  Field44 = '' and  Field45 = '' and  Field46 = '' and  Field47 = '' and  Field48 = '' and  Field49 = '' and  Field50 = '' and  Field51 = '' and  Field52 = '' and  Field53 = '' and  Field54 = '' and  Field55 = '' and  Field56 = '' and  Field57 = '' and  Field58 = '' and  Field59 = '' and  Field60 = '' and  Field61 = '' and  Field62 = '' and  Field63 = '' and  Field64 = '' and  Field65 = '' and  Field66 = '' and  Field67 = '' and  Field68 = '' and  Field69 = '' and  Field70 = '' and  Field71 = '' and  Field72 = '' and  Field73 = '' and  Field74 = '' and  Field75 = '' and  Field76 = '' and  Field77 = '' and  Field78 = '' and  Field79 = '' and  Field80 = '' and  Field81 = '' and  Field82 = '' and  Field83 = '' and  Field84 = '' and  Field85 = '' and  Field86 = '' and  Field87 = '' and  Field88 = '' and  Field89 = '' and  Field90 = '' and  Field91 = '' and  Field92 = '' and  Field93 = '' and  Field94 = '' and  Field95 = '' and  Field96 = '' and  Field97 = '' and  Field98 = '' and  Field99 = '' and  Field100 = '' and  Field101 = '' and  Field102 = '' and  Field103 = '' and  Field104 = '' and  Field105 = '' and  Field106 = '' and  Field107 = '' and  Field108 = '' and  Field109 = '' and  Field110 = '' and  Field111 = '' and  Field112 = '' and  Field113 = '' and  Field114 = '' and  Field115 = '' and  Field116 = '' and  Field117 = '' and  Field118 = '' and  Field119 = '' and  Field120 = '' and  Field121 = '' and  Field122 = '' and  Field123 = '' and  Field124 = '' and  Field125 = '' and  Field126 = '' and  Field127 = '' and  Field128 = '' and  Field129 = '' and  Field130 = '' and  Field131 = '' and  Field132 = '' and  Field133 = '' and  Field134 = '' and  Field135 = '' and  Field136 = '' and  Field137 = '' and  Field138 = '' and  Field139 = '' and  Field140 = '' and  Field141 = '' and  Field142 = '' and  Field143 = '' and  Field144 = '' and  Field145 = '' and  Field146 = '' and  Field147 = '' and  Field148 = '' and  Field149 = '' and  Field150 = '' and  Field151 = '' and  Field152 = '' and  Field153 = '' and  Field154 = '' and  Field155 = '' and  Field156 = '' and  Field157 = '' and  Field158 = '' and  Field159 = '' and  Field160 = '' and  Field161 = '' and  Field162 = '' and  Field163 = '' and  Field164 = '' and  Field165 = '' and  Field166 = '' and  Field167 = '' and  Field168 = '' and  Field169 = '' and  Field170 = '' and  Field171 = '' and  Field172 = '' and  Field173 = '' and  Field174 = '' and  Field175 = '' and  Field176 = '' and  Field177 = '' and  Field178 = '' and  Field179 = '' and  Field180 = '' and  Field181 = '' and  Field182 = '' and  Field183 = '' and  Field184 = '' and  Field185 = '' and  Field186 = '' and  Field187 = '' and  Field188 = '' and  Field189 = '' and  Field190 = '' and  Field191 = '' and  Field192 = '' and  Field193 = '' and  Field194 = '' and  Field195 = '' and  Field196 = '' and  Field197 = '' and  Field198 = '' and  Field199 = '' and  Field200 = '' and  Field201 = '' and  Field202 = '' and  Field203 = '' and  Field204 = '' and  Field205 = '' and  Field206 = '' and  Field207 = '' and  Field208 = '' and  Field209 = '' and  Field210 = '' and  Field211 = '' and  Field212 = '' and  Field213 = '' and  Field214 = '' and  Field215 = '' and  Field216 = '' and  Field217 = '' and  Field218 = '' and  Field219 = '' and  Field220 = '' and  Field221 = '' and  Field222 = '' and  Field223 = '' and  Field224 = '' and  Field225 = '' and  Field226 = '' and  Field227 = '' and  Field228 = '' and  Field229 = '' and  Field230 = '' and  Field231 = '' and  Field232 = '' and  Field233 = '' and  Field234 = '' and  Field235 = '' and  Field236 = '' and  Field237 = '' and  Field238 = '' and  Field239 = '' and  Field240 = '' and  Field241 = '' and  Field242 = '' and  Field243 = '' and  Field244 = '' and  Field245 = '' and  Field246 = '' and  Field247 = '' and  Field248 = '' and  Field249 = '' and  Field250 = '' and  Field251 = '' and  Field252 = '' and  Field253 = '' and  Field254 = '' and  Field255 = '' and  Field256 = '' and  Field257 = '' and  Field258 = '' and  Field259 = '' and  Field260 = '' and  Field261 = '' and  Field262 = '' and  Field263 = '' and  Field264 = '' and  Field265 = '' and  Field266 = '' and  Field267 = '' and  Field268 = '' and  Field269 = '' and  Field270 = '' and  Field271 = '' and  Field272 = '' and  Field273 = '' and  Field274 = '' and  Field275 = '' and  Field276 = '' and  Field277 = '' and  Field278 = '' and  Field279 = '' and  Field280 = '' and  Field281 = '' and  Field282 = '' and  Field283 = '' and  Field284 = '' and  Field285 = '' and  Field286 = '' and  Field287 = '' and  Field288 = '' and  Field289 = '' and  Field290 = '' and  Field291 = '' and  Field292 = '' and  Field293 = '' and  Field294 = '' and  Field295 = '' and  Field296 = '' and  Field297 = '' and  Field298 = '' and  Field299 = '' and  Field300 = '' and  Field301 = '' and  Field302 = '' and  Field303 = '' and  Field304 = '' and  Field305 = '' and  Field306 = '' and  Field307 = '' and  Field308 = '' and  Field309 = '' and  Field310 = '' and  Field311 = '' and  Field312 = '' and  Field313 = '' and  Field314 = '' and  Field315 = '' and  Field316 = '' and  Field317 = '' and  Field318 = '' and  Field319 = '' and  Field320 = '' and  Field321 = '' and  Field322 = '' and  Field323 = '' and  Field324 = '' and  Field325 = '' and  Field326 = '' and  Field327 = '' and  Field328 = '' and  Field329 = '' and  Field330 = '' and  Field331 = '' and  Field332 = '' and  Field333 = '' and  Field334 = '')


The error thrown is 

2017-08-11 16:25:48 ERROR Logging$class:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.StackOverflowError
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:370)
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)
	at org.codehaus.janino.CodeContext.flowAnalysis(CodeContext.java:541)

Any advice? Thanks;;;","13/Aug/17 09:30;kiszk;Thank you for letting us know the problem. I investigate this.;;;","13/Aug/17 17:05;kiszk;[~srinivasanm] I can reproduce this issue by using the master branch. I think that this is another problem.
Could you please create another JIRA entry to track this issue? I will work for this.
;;;","14/Aug/17 02:14;srinivasanm;[~kiszk] I created a new ticket , https://issues.apache.org/jira/browse/SPARK-21720
Please verify the description. Thanks;;;","14/Aug/17 18:35;poplav;Hi, [~kiszk] I met this failure also.
Is it possible to backport this to 2.1.1?
Appreciate it!;;;","14/Aug/17 23:34;poplav;I opened a PR for backporting this at https://github.com/apache/spark/pull/18942, thanks.;;;","28/Aug/17 16:55;apachespark;User 'poplav' has created a pull request for this issue:
https://github.com/apache/spark/pull/18942;;;",,,,,,,,,,,,,,,,,,,,,,
pyspark.ml.Pipeline gets corrupted under multi threaded use,SPARK-19348,13037317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bryanc,vijoshi,vijoshi,24/Jan/17 09:54,08/Mar/17 04:47,14/Jul/23 06:30,08/Mar/17 04:47,1.6.0,2.0.0,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,ML,PySpark,,,,1,,,,,,,,,"When pyspark.ml.Pipeline objects are constructed concurrently in separate python threads, it is observed that the stages used to construct a pipeline object get corrupted i.e the stages supplied to a Pipeline object in one thread appear inside a different Pipeline object constructed in a different thread. 

Things work fine if construction of pyspark.ml.Pipeline objects is serialized, so this looks like a thread safety problem with pyspark.ml.Pipeline object construction. 

Confirmed that the problem exists with Spark 1.6.x as well as 2.x.

While the corruption of the Pipeline stages is easily caught, we need to know if performing other pipeline operations, such as pyspark.ml.pipeline.fit( ) are also affected by the underlying cause of this problem. That is, whether other pipeline operations like pyspark.ml.pipeline.fit( )  may be performed in separate threads (on distinct pipeline objects) concurrently without any cross contamination between them.",,apachespark,bryanc,josephkb,peterdkirchner,vijoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/17 10:00;vijoshi;pyspark_pipeline_threads.py;https://issues.apache.org/jira/secure/attachment/12849085/pyspark_pipeline_threads.py",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 08 04:47:19 UTC 2017,,,,,,,,,,"0|i394en:",9223372036854775807,,,,,josephkb,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"24/Jan/17 10:01;vijoshi;Refer the attached pyspark_pipeline_threads.py file containing the complete code to reproduce the issue. It can be run on the pyspark shell or in a jupyter notebook. ;;;","02/Feb/17 23:08;bryanc;The problem here is with the @keyword_only decorator that is used in the Pipeline constructor (and every other ML class also).  It relies on saving the params, stages in this case, to a static class variable.  When multiple threads call the wrapped constructor, it becomes a race condition to read from that static variable before it is over-written by another thread.  I can put up a potential fix, but it affects all of PySpark ML so it would need to be checked out carefully.

As a workaround, you could just protect the construction of {{Pipeline}} with a shared lock.  Other calls to {{fit}} etc, should be ok since they don't use that keyword_only decorator.;;;","02/Feb/17 23:27;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/16782;;;","07/Feb/17 20:07;peterdkirchner;To save folks some time, the keyword_only decorator came in to pyspark/ml/util.py with SPARK-4586
design:
https://docs.google.com/document/d/1vL-4f5Xm-7t-kwVSaBylP_ZPrktPZjaOb2dWONtZU2s/edit
commit:
https://github.com/apache/spark/commit/cd4a15366244657c4b7936abe5054754534366f2#diff-dd5670d3fb55faba1859e9778e4026e5;;;","07/Feb/17 22:19;peterdkirchner;Two things happen with this wrapper.

First, it appears to me (after confirming with some simplified examples) that the modifications to incoming arguments made in the body of the wrapped function are lost (to take the pipeline.py example, stages=) when they are not updated in the dictionary that is passed in the calls made from inside the wrapped functions.  The decorator explicitly states that it saves the 'actual input arguments' but does not clarify why.  It seems as if the wrapped code should either update the dictionary or that lines of code that have no lasting effect should be deleted.

Second, bryanc has pointed out that passing the kwargs to the wrapped function via a static class variable is thread-unsafe within each of the many ml classes that use the decorator.  Passing the kwargs as an instance variable as bryanc has proposed seems satisfactory for the second solution, as would using a thread-local class variable.  Both require changes to any files using the decorator. Locking in the decorator, if it could be implemented in spite of the nested calls of decorated functions, could confine the changes to the decorator definition.  Passing the wrapper's kwargs dictionary as an additional entry in the kwargs dictionary passed to the wrapped function would be threadsafe but change the public API of many functions.  It looks possible for a decorator to introspect the wrapped function's parameters in which case the wrapper could pass the dictionary on one of those keywords (the purpose being to leave the public API intact), then inside the wrapped function there would need to be code to detect the wrapper, retrieve the dictionary and restore the coopted variable.  (As-is, the wrapped functions already have wrapper-specific code in order to function.);;;","09/Feb/17 16:22;peterdkirchner;Per the above, perhaps a fix could address both threadsafety and the orphaned modifications to variables in the wrapped function bodies.  For instance, in Pipeline.__init__() and setParams() the fix could remove references to _input_kwargs and instead invoke setParams(stages=stages) within __init__() and _set(stages=stages) within setParams(), respectively,  Parallel changes would be needed in all wrapped functions but the resulting code would be functional, readable, and threadsafe.

A fix for Pipeline alone is insufficient, because multiple pipelines could have stages consisting of instances of the same class, e.g. LogisticRegression.  All the classes using @keyword_only need to be addressed by whatever fix is decided upon.;;;","07/Mar/17 19:20;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/17193;;;","08/Mar/17 04:47;josephkb;Issue resolved by pull request 17195
[https://github.com/apache/spark/pull/17195];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReceiverSupervisorImpl can add block to ReceiverTracker multiple times because of askWithRetry,SPARK-19347,13037313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,24/Jan/17 09:31,01/Feb/17 21:55,14/Jul/23 06:30,01/Feb/17 21:55,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,DStreams,Spark Core,,,,0,,,,,,,,,"*ReceiverSupervisorImpl* on executor side reports block's meta back to *ReceiverTracker* on driver side. In current code, *askWithRetry* is used. However, for *AddBlock*, *ReceiverTracker* is not idempotent, which may result in messages are processed multiple times.

To reproduce:
1. Check if it is the first time receiving *AddBlock* in *ReceiverTracker*, if so sleep long enough(say 200 seconds), thus the first RPC call will be timeout in *askWithRetry*, then *AddBlock* will be resent.
2. Rebuild Spark and run following job:
{code}
  def streamProcessing(): Unit = {
    val conf = new SparkConf()
      .setAppName(""StreamingTest"")
      .setMaster(masterUrl)
    val ssc = new StreamingContext(conf, Seconds(200))
    val stream = ssc.socketTextStream(""localhost"", 1234)
    stream.print()
    ssc.start()
    ssc.awaitTermination()
  }
{code}

To fix:
It makes sense to provide a blocking version *ask* in *RpcEndpointRef*, as [~vanzin] mentioned in SPARK-18113 (https://github.com/apache/spark/pull/16503#event-927953218), *askWithRetry* is a leftover from akka days. It imposes restrictions on the caller(e.g. idempotency) and other things that people generally don't pay that much attention to when using it.",,apachespark,jinxing6042@126.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 09:46:03 UTC 2017,,,,,,,,,,"0|i394dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/17 09:46;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/16690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Datatype tImestamp is converted to numeric in collect method ,SPARK-19342,13037282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Titicaca,Titicaca,Titicaca,24/Jan/17 06:04,13/Feb/17 17:53,14/Jul/23 06:30,13/Feb/17 17:53,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"Get double instead of POSIX in collect method for timestamp column datatype, when NA exists at the top of the column.

The following codes and outputs show that, how the bug can be reproduced:

{code}
> sparkR.session(master = ""local"")
Spark package found in SPARK_HOME: /home/titicaca/spark-2.1
Launching java with spark-submit command /home/titicaca/spark-2.1/bin/spark-submit   sparkr-shell /tmp/RtmpqmpZUg/backend_port363a898be92 
Java ref type org.apache.spark.sql.SparkSession id 1 
> df <- data.frame(col1 = c(0, 1, 2), 
+                  col2 = c(as.POSIXct(""2017-01-01 00:00:01""), NA, as.POSIXct(""2017-01-01 12:00:01"")))
> sdf1 <- createDataFrame(df)
> print(dtypes(sdf1))
[[1]]
[1] ""col1""   ""double""

[[2]]
[1] ""col2""      ""timestamp""

> df1 <- collect(sdf1)
> print(lapply(df1, class))
$col1
[1] ""numeric""

$col2
[1] ""POSIXct"" ""POSIXt"" 

> sdf2 <- filter(sdf1, ""col1 > 0"")
> print(dtypes(sdf2))
[[1]]
[1] ""col1""   ""double""

[[2]]
[1] ""col2""      ""timestamp""

> df2 <- collect(sdf2)
> print(lapply(df2, class))
$col1
[1] ""numeric""

$col2
[1] ""numeric""
{code}

As we can see, the data type of col2 is converted to numberic unexpectedly in the collected local data frame df2",,apachespark,Titicaca,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 06:51:03 UTC 2017,,,,,,,,,,"0|i3946v:",9223372036854775807,,,,,felixcheung,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"24/Jan/17 06:51;apachespark;User 'titicaca' has created a pull request for this issue:
https://github.com/apache/spark/pull/16689;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Always Identical Name for UDF in the EXPLAIN output ,SPARK-19338,13037197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,smilegator,smilegator,23/Jan/17 22:32,26/Jan/17 17:52,14/Jul/23 06:30,26/Jan/17 17:52,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"{noformat}
    sql(""SELECT udf1(udf2(42))"").explain()
{noformat}

{noformat}
== Physical Plan ==
*Project [UDF(UDF(42)) AS UDF(UDF(42))#7]
+- Scan OneRowRelation[]
{noformat}


Although udf1 and udf2 are different UDF, but the name in the plans are the same. It looks confusing. 
",,apachespark,dongjoon,maropu,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 26 06:08:03 UTC 2017,,,,,,,,,,"0|i393nz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/17 07:00;maropu;It makes sense and do you take on this?;;;","24/Jan/17 15:26;maropu;You know, we can easily add the UDF names that are registered via `UDFRegistration`;
https://github.com/apache/spark/compare/master...maropu:SPARK-19338
{code}
scala> spark.udf.register(""udf1"", (i: Long) => i)
scala> spark.udf.register(""udf2"", (i: Long) => i)
scala> spark.range(10).createOrReplaceTempView(""t"")
scala> sql(""SELECT udf1(udf2(id)) FROM t"").explain
== Physical Plan ==
*Project [if (isnull(udf2(id#0L))) null else udf1(udf2(id#0L)) AS udf1(udf2(id))#8L]
+- *Range (0, 10, step=1, splits=Some(4))
{code}
On the other hand, since we do not pass UDF names in `sq.functions.udf`, it some hard to add the names in explain results;
{code}
scala> val udf1 = udf { (i: Long) => i }
scala> val udf2 = udf { (i: Long) => i }
scala> spark.range(10).select(udf1(udf2('id))).explain
== Physical Plan ==
*Project [if (isnull(UDF(id#11L))) null else UDF(UDF(id#11L)) AS UDF(UDF(id))#14L]
+- *Range (0, 10, step=1, splits=Some(4))
{code};;;","25/Jan/17 16:21;smilegator;Yes. I am not working on this. Please submit it. Thanks!;;;","26/Jan/17 00:29;maropu;okay, I'll do!;;;","26/Jan/17 06:08;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/16707;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the code injection vulnerability related to Generator functions.,SPARK-19334,13037093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,23/Jan/17 16:25,24/Jan/17 23:28,14/Jul/23 06:30,24/Jan/17 22:36,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"Similar to SPARK-15165, codegen is in danger of arbitrary code injection. The root cause is how variable names are created by codegen.
In GenerateExec#codeGenAccessor, a variable name is created like as follows.

{code}
val value = ctx.freshName(name)
{code}

The variable `value` is named based on the value of the variable `name` and the value of `name` is from schema given by user so an attacker can attack with queries like as follows.

{code}
SELECT inline(array(cast(struct(1) AS struct<`=new Object() { {f();} public void f() {throw new RuntimeException(""This exception is injected."");} public int x;}.x`:int>)))
{code}

In the example above, a RuntimeException is thrown but attacker can replace it with arbitrary code.",,apachespark,hvanhovell,kiszk,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 23:28:52 UTC 2017,,,,,,,,,,"0|i3930v:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"23/Jan/17 16:29;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/16681;;;","24/Jan/17 22:37;hvanhovell;I have set the target version to 2.2, since we only do whole stage code generation for generate since Spark 2.2 and not 2.1.;;;","24/Jan/17 23:28;sarutak;[~hvanhovell] Thanks. The affects version was also 2.2.0(SNAPSHOT) so I've changed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
after alter a datasource table's location to a not exist location and then insert data throw Exception,SPARK-19329,13036866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,22/Jan/17 09:17,16/Mar/17 17:31,14/Jul/23 06:30,15/Feb/17 21:23,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"spark.sql(""create table t(a string, b int) using parquet"")
spark.sql(s""alter table t set location '$notexistedlocation'"")
spark.sql(""insert into table t select 'c', 1"")

this will throw an exception:

com.google.common.util.concurrent.UncheckedExecutionException: org.apache.spark.sql.AnalysisException: Path does not exist: $notexistedlocation;
	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4814)
	at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4830)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:122)
	at org.apache.spark.sql.hive.HiveSessionCatalog.lookupRelation(HiveSessionCatalog.scala:69)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:456)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:465)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:463)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:463)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:453)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 22 09:31:04 UTC 2017,,,,,,,,,,"0|i391mf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/17 09:31;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16672;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculated task attempts do not get launched in few scenarios,SPARK-19326,13036841,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tejasp,tejasp,22/Jan/17 03:45,17/May/20 17:46,14/Jul/23 06:30,23/Aug/17 03:32,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"Speculated copies of tasks do not get launched in some cases.

Examples:
- All the running executors have no CPU slots left to accommodate a speculated copy of the task(s). If the all running executors reside over a set of slow / bad hosts, they will keep the job running for long time
- `spark.task.cpus` > 1 and the running executor has not filled up all its CPU slots. Since the [speculated copies of tasks should run on different host|https://github.com/apache/spark/blob/2e139eed3194c7b8814ff6cf007d4e8a874c1e4d/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L283] and not the host where the first copy was launched.

In both these cases, `ExecutorAllocationManager` does not know about pending speculation task attempts and thinks that all the resource demands are well taken care of. ([relevant code|https://github.com/apache/spark/blob/6ee28423ad1b2e6089b82af64a31d77d3552bb38/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L265])

This adds variation in the job completion times and more importantly SLA misses :( In prod, with a large number of jobs, I see this happening more often than one would think. Chasing the bad hosts or reason for slowness doesn't scale.

Here is a tiny repro. Note that you need to launch this with (Mesos or YARN or standalone deploy mode) along with `--conf spark.speculation=true --conf spark.executor.cores=4 --conf spark.dynamicAllocation.maxExecutors=100`

{code}
val n = 100
val someRDD = sc.parallelize(1 to n, n)
someRDD.mapPartitionsWithIndex( (index: Int, it: Iterator[Int]) => {
if (index == 1) {
  Thread.sleep(Long.MaxValue)  // fake long running task(s)
}
it.toList.map(x => index + "", "" + x).iterator
}).collect
{code}
",,andrewor14,apachespark,cloud_fan,devaraj,Dhruve Ashar,irashid,kayousterhout,lwlin,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-28403,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 23 03:34:23 UTC 2017,,,,,,,,,,"0|i391gv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/17 03:58;tejasp;cc [~kayousterhout], [~irashid] [~vanzin] 

`ExecutorAllocationManager` currently determines ""is there any need to allocate new executor ?"" based on the `running` and `pending` tasks. This accounting is purely based on a listener which does not know anything speculated tasks. From the code I could see that this was [intentionally done|https://github.com/apache/spark/blob/6ee28423ad1b2e6089b82af64a31d77d3552bb38/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L579] in the original patch which introduced this. Changing this doesn't look like a point fix to me so would like to get your opinion about this.;;;","02/Feb/17 17:28;tejasp;ping [~kayousterhout], [~irashid] [~vanzin]  !!;;;","02/Feb/17 21:32;kayousterhout;What is the bad behavior that occurs with your example code?  Is the problem that only one executor is requested, so no speculation can occur because there's not a different node to run tasks on?;;;","03/Feb/17 03:36;tejasp;[~kayousterhout] : I have updated the configs in the repro example in jira description. 

>> Is the problem that only one executor is requested, so no speculation can occur because there's not a different node to run tasks on

No. There are multiple executors launched in the start but later as tasks finish they are released. In the end there might be one single executor left running a single long running task. There is no speculation happening in such case.

;;;","03/Feb/17 20:34;kayousterhout;I see that makes sense; thanks for the additional explanation.  [~andrewor14] did you think about this issue when implementing dynamic allocation originally? I noticed there'a a [comment saying that speculation is not considered for simplicity](https://github.com/apache/spark/blob/6ee28423ad1b2e6089b82af64a31d77d3552bb38/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L579), but it does seem like this functionality can prevent speculation from occurring.;;;","16/Feb/17 00:33;tejasp;[~andrewor14] : Ping !!;;;","16/Feb/17 02:41;andrewor14;Sorry for slipping on this. When I was implementing the feature the goal was to get it working for normal cases first, so I wouldn't be surprised if it doesn't work with speculation. I don't think there's a fundamental reason why it can't be supported. Someone just needs to implement it.;;;","16/Feb/17 04:00;tejasp;Thanks for the info !!

[~andrewor14] / [~kayousterhout] : I am happy to work on this. Two approaches I can think of are:
- Add an event in listener to inform `ExecutorAllocationManager` about tasks from speculation.
- `ExecutorAllocationManager` should not be depending on listener and have some other event based mechanism to drive have communication between `ExecutorAllocationManager` and `TaskSetManager`. This is cleaner solution but it will be bigger change.

What do you think ?;;;","16/Feb/17 05:27;andrewor14;I would say it's a bad idea to make ExecutorAllocationManager talk to the TaskSetManager. The existing listener interface is relatively isolated. I'm not sure if you need to introduce a new event to capture speculation. You might be able to just write an `if` case that checks whether speculation is enabled and run some logic in the listener to detect speculated tasks.;;;","16/Feb/17 06:19;tejasp;> You might be able to just write an `if` case that checks whether speculation is enabled and run some logic in the listener to detect speculated tasks.

For ExecutorAllocationManager to detect that there needs to be speculation, it would basically would have to duplicate what TaskSetManager does to find candidates for speculation (unless you have some better way). Thats bad because there would be two entities making decisions about speculation.;;;","30/Jun/17 17:30;apachespark;User 'janewangfb' has created a pull request for this issue:
https://github.com/apache/spark/pull/18492;;;","23/Aug/17 03:32;cloud_fan;Issue resolved by pull request 18492
[https://github.com/apache/spark/pull/18492];;;","23/Aug/17 03:34;cloud_fan;janewangfb can you provide your JIRA id so that I can assign this ticket to you? thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
JVM stdout output is dropped in SparkR,SPARK-19324,13036830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,22/Jan/17 01:12,27/Jan/17 20:44,14/Jul/23 06:30,27/Jan/17 20:44,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"Whenever there are stdout outputs from Spark in JVM (typically when calling println()) they are dropped by SparkR.

For example, explain() for Column
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1111
",,apachespark,bdwyer,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 27 20:44:02 UTC 2017,,,,,,,,,,"0|i391ef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/17 02:20;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16670;;;","27/Jan/17 20:44;shivaram;Resolved by https://github.com/apache/spark/pull/16670;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR Kmeans summary returns error when the cluster size doesn't equal to k,SPARK-19319,13036750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,21/Jan/17 01:04,12/Feb/17 18:50,14/Jul/23 06:30,01/Feb/17 05:17,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,ML,SparkR,,,,0,,,,,,,,,"When Kmeans using initMode = ""random"" and some random seed, it is possible the actual cluster size doesn't equal to the configured `k`.

In this case, summary(model) returns error due to the number of cols of coefficient matrix doesn't equal to k.",,apachespark,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3261,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 06:01:04 UTC 2017,,,,,,,,,,"0|i390wn:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"21/Jan/17 01:08;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16666;;;","01/Feb/17 06:01;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16761;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker test case failure: `SPARK-16625: General data types to be mapped to Oracle`,SPARK-19318,13036739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsuresh,smilegator,smilegator,20/Jan/17 23:58,19/Sep/17 23:56,14/Jul/23 06:30,14/Feb/17 23:34,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,SQL,,,,,0,,,,,,,,,"===== FINISHED o.a.s.sql.jdbc.OracleIntegrationSuite: 'SPARK-16625: General data types to be mapped to Oracle' =====

- SPARK-16625: General data types to be mapped to Oracle *** FAILED ***
  types.apply(9).equals(""class java.sql.Date"") was false (OracleIntegrationSuite.scala:136)",,apachespark,cloud_fan,smilegator,tsuresh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 17 08:29:03 UTC 2017,,,,,,,,,,"0|i390u7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"21/Jan/17 00:20;tsuresh;I am looking into this test failure. ;;;","08/Feb/17 01:31;apachespark;User 'sureshthalamati' has created a pull request for this issue:
https://github.com/apache/spark/pull/16847;;;","11/Feb/17 04:38;apachespark;User 'sureshthalamati' has created a pull request for this issue:
https://github.com/apache/spark/pull/16891;;;","14/Feb/17 23:34;cloud_fan;Issue resolved by pull request 16891
[https://github.com/apache/spark/pull/16891];;;","17/Sep/17 08:29;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/19259;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not allow sort before aggregation in Structured Streaming plan,SPARK-19314,13036658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,20/Jan/17 19:26,24/Jan/17 06:27,14/Jul/23 06:30,20/Jan/17 22:04,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Structured Streaming,,,,,0,,,,,,,,,Sort in a streaming plan should be allowed only after a aggregation in complete mode. Currently it is incorrectly allowed when present anywhere in the plan. It gives unpredictable potentially incorrect results.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 22:04:17 UTC 2017,,,,,,,,,,"0|i390c7:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"20/Jan/17 19:31;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/16662;;;","20/Jan/17 22:04;tdas;Issue resolved by pull request 16662
[https://github.com/apache/spark/pull/16662];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GaussianMixture throws cryptic error when number of features is too high,SPARK-19313,13036649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sethah,sethah,sethah,20/Jan/17 19:13,25/Jan/17 15:14,14/Jul/23 06:30,25/Jan/17 15:14,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,ML,MLlib,,,,0,,,,,,,,,"The following fails

{code}
    val df = Seq(
      Vectors.sparse(46400, Array(0, 4), Array(3.0, 8.0)),
      Vectors.sparse(46400, Array(1, 5), Array(4.0, 9.0)))
      .map(Tuple1.apply).toDF(""features"")
    val gm = new GaussianMixture()
    gm.fit(df)
{code}

It fails because GMMs allocate an array of size {{numFeatures * numFeatures}} and in this case we'll get integer overflow. We should limit the number of features appropriately.",,apachespark,sethah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 19:24:05 UTC 2017,,,,,,,,,,"0|i390a7:",9223372036854775807,,,,,yanboliang,,,,,,,,,,,,,,,,,,,"20/Jan/17 19:24;apachespark;User 'sethah' has created a pull request for this issue:
https://github.com/apache/spark/pull/16661;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UDFs disregard UDT type hierarchy,SPARK-19311,13036568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gmoehler,gmoehler,gmoehler,20/Jan/17 14:08,25/Jan/17 16:28,14/Jul/23 06:30,25/Jan/17 16:28,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,1,,,,,,,,,"When you define UDTs based on hierarchical traits UDFs disregard the type hierarchy:

E.g. I have 2 UDTs based on 2 hierarchical traits. I then define 2 UDFs: The first one returns the derived type, the second takes the base type. This results in an error, although i believe it should be feasible:

{quote}
(...)cannot resolve 'UDF(UDF(22))' due to data type mismatch: argument 1 requires exampleBaseType type, however, 'UDF(22)' is of exampleFirstSubType type.
{quote}

The reason is that DataType defines
{quote}
override private[sql] def acceptsType(dataType: DataType) =
    this.getClass == dataType.getClass
{quote}

However I believe it should be:

{quote}
override private[sql] def acceptsType(dataType: DataType) = dataType match \{
    case other: UserDefinedType[_] =>
      this.getClass == other.getClass || this.userClass.isAssignableFrom(other.userClass)
    case _ => false
  \}
{quote}",,apachespark,gmoehler,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 15:24:05 UTC 2017,,,,,,,,,,"0|i38zs7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"20/Jan/17 15:05;viirya;[~gmoehler] I think you already have the fixing. Can you directly submit the PR? Thanks.;;;","20/Jan/17 15:24;apachespark;User 'gmoehler' has created a pull request for this issue:
https://github.com/apache/spark/pull/16660;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disable common subexpression elimination for conditional expressions,SPARK-19309,13036539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Jan/17 12:29,23/Jan/17 05:32,14/Jul/23 06:30,23/Jan/17 05:32,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,kiszk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 05:32:13 UTC 2017,,,,,,,,,,"0|i38zlr:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"20/Jan/17 12:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16659;;;","23/Jan/17 05:32;cloud_fan;Issue resolved by pull request 16659
[https://github.com/apache/spark/pull/16659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK-17387 caused ignorance of conf object passed to SparkContext:,SPARK-19307,13036467,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,yuriy_hupalo,yuriy_hupalo,20/Jan/17 07:57,29/Aug/17 22:48,14/Jul/23 06:30,25/Jan/17 20:08,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,,,,,0,,,,,,,,,"after patch SPARK-17387 was applied -- Sparkconf object is ignored when launching SparkContext programmatically via python from spark-submit:

https://github.com/apache/spark/blob/master/python/pyspark/context.py#L128:

in case when we are running python SparkContext(conf=xxx) from spark-submit:
    conf is set, conf._jconf is None ()

    passed as arg  conf object is ignored (and used only when we are launching java_gateway).

how to fix:

python/pyspark/context.py:132

{code:title=python/pyspark/context.py:132}
        if conf is not None and conf._jconf is not None:
            # conf has been initialized in JVM properly, so use conf directly. This represent the
            # scenario that JVM has been launched before SparkConf is created (e.g. SparkContext is
            # created and then stopped, and we create a new SparkConf and new SparkContext again)
            self._conf = conf
        else:
            self._conf = SparkConf(_jvm=SparkContext._jvm)
+             if conf:
+                 for key, value in conf.getAll():
+                     self._conf.set(key,value)
+                     print(key,value)
{code}


",,apachespark,ctsai,dongjoon,irinatruong,yuriy_hupalo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19369,SPARK-20362,,,,,SPARK-19301,,,SPARK-17387,,,,,,,,,,,,,,"20/Jan/17 11:30;yuriy_hupalo;SPARK-19307.patch;https://issues.apache.org/jira/secure/attachment/12848520/SPARK-19307.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 22:42:32 UTC 2017,,,,,,,,,,"0|i38z5r:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"20/Jan/17 11:09;srowen;CC [~zjffdu];;;","20/Jan/17 11:30;yuriy_hupalo;git diff;;;","20/Jan/17 11:31;srowen;[~yuriy_hupalo] we don't use patches. Please read http://spark.apache.org/contributing.html;;;","20/Jan/17 11:59;yuriy_hupalo;pull:
https://github.com/yuriyhupalo/spark/pull/1;;;","20/Jan/17 12:11;srowen;That's also not how to make a pull request vs apache/spark, but, maybe best to hear from the author of the change to verify it before procdeding anyway.;;;","23/Jan/17 21:13;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16682;;;","07/Jun/17 15:23;irinatruong;Is this available in 2.1.1? I could not find it in release notes.;;;","23/Aug/17 18:27;dongjoon;Hi, [~irinatruong].
Yes. It's available in 2.1.1. Maybe, the release note is missing due to the missing fixed version.;;;","29/Aug/17 22:42;ctsai;Hi,

I am using 2.2.0 but find that command line {{--conf}} arguments are still not available when the {{SparkConf()}} object is instantiated. As a result, I can't check what has already been set using the command line {{--conf}} arguments in my driver and set additional configuration using {{setIfMissing}}. Instead, {{setIfMissing}} effectively overwrites whatever is passed in through the CLI.

For example, if my job is:
{code}
# debug.py

import pyspark

if __name__ == '__main__':
    print(pyspark.SparkConf()._jconf)    # is `None` but should include `--conf` arguments

    default_conf = {
        ""spark.dynamicAllocation.maxExecutors"": ""36"",
        ""spark.yarn.executor.memoryOverhead"": ""1500"",
    }

    # these are supposed to be set only if not provided by the CLI args
    spark_conf = pyspark.SparkConf()
    for (k, v) in default_conf.items():
        spark_conf.setIfMissing(k, v)
{code}

Running
{code}
spark-submit \
    --master yarn \
    --deploy-mode client \
    --conf spark.yarn.executor.memoryOverhead=2500 \
    --conf spark.dynamicAllocation.maxExecutors=128 \
    debug.py
{code}

In 1.6.2 the CLI args take precedent, whereas in 2.2.0, {{SparkConf().getAll()}} appears empty even though {{--conf}} args were passed in already.

Is this a separate problem or was this intended to be addressed by this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix inconsistent state in DiskBlockObjectWriter when exception occurred,SPARK-19306,13036463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,20/Jan/17 07:49,23/Jan/17 21:38,14/Jul/23 06:30,23/Jan/17 21:38,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,,,,,0,,,,,,,,,"In {{DiskBlockObjectWriter}}, when some errors happened during writing, it will call {{revertPartialWritesAndClose}}, if this method again failed due to some hardware issues like out of disk, it will throw exception without resetting the state of this writer, also skipping the revert. So here propose to fix this issue to offer user a chance to recover from such issue.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 08:17:03 UTC 2017,,,,,,,,,,"0|i38z4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/17 08:17;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16657;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partitioned table should always put partition columns at the end of table schema,SPARK-19305,13036440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Jan/17 06:02,21/Jan/17 05:58,14/Jul/23 06:30,21/Jan/17 05:58,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 21 05:58:49 UTC 2017,,,,,,,,,,"0|i38yzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/17 06:11;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16655;;;","21/Jan/17 05:58;cloud_fan;Issue resolved by pull request 16655
[https://github.com/apache/spark/pull/16655];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
filter with partition columns should be case-insensitive on Hive tables,SPARK-19292,13036239,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Jan/17 15:28,20/Jan/17 04:12,14/Jul/23 06:30,20/Jan/17 04:12,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 19 15:45:04 UTC 2017,,,,,,,,,,"0|i38xqv:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"19/Jan/17 15:45;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16647;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JavaPairRDD flatMapValues requires function returning Iterable, not Iterator",SPARK-19287,13036178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,19/Jan/17 11:09,28/Jan/20 07:41,14/Jul/23 06:30,12/Oct/18 23:11,2.1.1,,,,,,,,,,,,,,,,,,,,,,,,,,3.0.0,,,,,Java API,,,,,0,release-notes,,,,,,,,"SPARK-3369 corrected an old oversight in the Java API, wherein {{FlatMapFunction}} required an {{Iterable}} rather than {{Iterator}}. As reported by [~akrim], it seems that this same type of problem was overlooked also in {{JavaPairRDD}} (https://github.com/apache/spark/blob/6c00c069e3c3f5904abd122cea1d56683031cca0/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala#L677 ):

{code}
def flatMapValues[U](f: JFunction[V, java.lang.Iterable[U]]): JavaPairRDD[K, U] =
{code}

As in {{PairRDDFunctions.scala}}, whose {{flatMapValues}} operates on {{TraversableOnce}}, this should really take a function that returns an {{Iterator}} -- really, {{FlatMapFunction}}.

We can easily add an overload and deprecate the existing method.

{code}
def flatMapValues[U](f: FlatMapFunction[V, U]): JavaPairRDD[K, U]
{code}

This is source- and binary-backwards-compatible, in Java 7. It's binary-backwards-compatible in Java 8, but not source-compatible. The following natural usage with Java 8 lambdas becomes ambiguous and won't compile -- Java won't figure out which to implement even based on the return type unfortunately:

{code}
JavaPairRDD<Integer, String> pairRDD = ...
JavaPairRDD<Integer, Integer> mappedRDD = 
  pairRDD.flatMapValues(s -> Arrays.asList(s.length()).iterator());
{code}

It can be resolved by explicitly casting the lambda.

We can at least document this. One day in Spark 3.x this can just be changed outright.

It's conceivable to resolve this by making the new method called ""flatMapValues2"" or something ugly.",,akrim,apachespark,kiszk,lwlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,"JavaPairRDD/JavaPairDStream.flatMapValues() now requires a FlatMapFunction as an argument. This means this function now must return an Iterator, not Iterable. This corrects a long-standing inconsistency between the Scala and Java API, and allows the caller to supply merely an Iterator, not a full Iterable. Existing functions passed to this method can simply invoke "".iterator()"" on their existing return value to comply with the new signature.",false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 23:11:16 UTC 2018,,,,,,,,,,"0|i38xdb:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,"19/Jan/17 11:11;srowen;For the moment, targeting this to a hypothetical future Spark 3.x, because at least, this can be fixed in a major release. We may do something earlier for 2.x.;;;","19/Jan/17 22:24;akrim;Considering that this was an oversight and should have been corrected in spark 2, is it really wrong to just fix it now, even at the cost of it being a breaking change? There are sometimes behavioral changes between spark minor releases, at least this change will fail loudly and the fix will be self evident.;;;","20/Jan/17 11:50;srowen;Whether it was an oversight doesn't really change things. Someone could be using it. Generally minor versions must be backwards-compatible. The catch here is that it would only be source-incompatible, which is a lesser problem, but still not great. In exceptional circumstances you can take a breaking change. Here I would want someone else to agree that's the right thing; I think it's at best add a new method, which is ugly. It's not a critical bug or blocking usage.;;;","10/Oct/18 21:15;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22690;;;","10/Oct/18 21:15;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22690;;;","12/Oct/18 23:11;srowen;Issue resolved by pull request 22690
[https://github.com/apache/spark/pull/22690];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
append to a existed partitioned datasource table should have no CustomPartitionLocations,SPARK-19284,13036124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,windpiger,windpiger,windpiger,19/Jan/17 08:15,23/Jan/17 11:06,14/Jul/23 06:30,23/Jan/17 11:06,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"when we append data to a existed partitioned datasource table, the InsertIntoHadoopFsRelationCommand.getCustomPartitionLocations currently
return the same location with Hive default, it should return None.",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 19 08:20:04 UTC 2017,,,,,,,,,,"0|i38x1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/17 08:20;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow Users to Create a Hive Table With an Empty Schema,SPARK-19279,13035968,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,18/Jan/17 19:15,08/Feb/17 04:14,14/Jul/23 06:30,06/Feb/17 05:30,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"So far, we allow users to create a table with an empty schema: {{CREATE TABLE tab1}}. This could break many code paths if we enable it. Thus, we should follow Hive to block it. ",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 08 04:14:04 UTC 2017,,,,,,,,,,"0|i38w2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/17 19:18;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16636;;;","06/Feb/17 05:30;cloud_fan;Issue resolved by pull request 16636
[https://github.com/apache/spark/pull/16636];;;","08/Feb/17 04:14;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16848;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FetchFailures can be hidden by user (or sql) exception handling,SPARK-19276,13035951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,imranr,irashid,irashid,18/Jan/17 18:22,05/Apr/18 21:23,14/Jul/23 06:30,03/Mar/17 00:48,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Scheduler,Spark Core,SQL,,,0,,,,,,,,,"The scheduler handles node failures by looking for a special {{FetchFailedException}} thrown by the shuffle block fetcher.  This is handled in {{Executor}} and then passed as a special msg back to the driver: https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/core/src/main/scala/org/apache/spark/executor/Executor.scala#L403

However, user code exists in between the shuffle block fetcher and that catch block -- it could intercept the exception, wrap it with something else, and throw a different exception.  If that happens, spark treats it as an ordinary task failure, and retries the task, rather than regenerating the missing shuffle data.  The task eventually is retried 4 times, its doomed to fail each time, and the job is failed.

You might think that no user code should do that -- but even sparksql does it:
https://github.com/apache/spark/blob/278fa1eb305220a85c816c948932d6af8fa619aa/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L214

Here's an example stack trace.  This is from Spark 1.6, so the sql code is not the same, but the problem is still there:

{noformat}
17/01/13 19:18:02 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1983.0 (TID 304851, xxx): org.apache.spark.SparkException: Task failed while writing rows.
        at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:414)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.shuffle.FetchFailedException: Failed to connect to xxx/yyy:zzz
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
...
17/01/13 19:19:29 ERROR scheduler.TaskSetManager: Task 0 in stage 1983.0 failed 4 times; aborting job
{noformat}

I think the right fix here is to also set a fetch failure status in the {{TaskContextImpl}}, so the executor can check that instead of just one exception.",,apachespark,irashid,joshrosen,liushaohui,markhamstra,roczei,tgraves,xchen12138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20480,,,,,,,,,SPARK-20633,SPARK-23816,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 04 02:56:57 UTC 2018,,,,,,,,,,"0|i38vyv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/17 18:28;irashid;I haven't been successful in creating a test case to reproduce this.  In my attempts, I see retries sometimes failing to fetch block *status*, which happens in the initialization of the {{ShuffleBlockFetcherIterator}}.  since that is outside user code, it does get handled correctly, and the shuffle data is regenerated.  But, the problem is clearly there, and I've seen it effect real users.

open to ideas for the test case & reproduction.;;;","19/Jan/17 06:32;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/16639;;;","19/Jan/17 18:40;markhamstra;This all makes sense, and the PR is a good effort to fix this kind of ""accidental"" swallowing of FetchFailedException.  I guess my only real question is if we should allow for the possibility of a FetchFailedException not only being caught, but also the failure being remedied by some means other than the usual handling in the driver. I'm not sure exactly how or why that kind of ""fix the fetch failure before Spark tries to handle it"" would be done; and it would seem that something like that would be prone to subtle errors, so maybe we should just set it in stone that nobody but the driver should try to fix a fetch failure -- which would make the approach of your ""guarantee that the FetchFailedException is seen by the driver"" PR completely correct.;;;","19/Jan/17 19:48;irashid;[~markhamstra]
bq. I guess my only real question is if we should allow for the possibility of a FetchFailedException not only being caught, but also the failure being remedied by some means other than the usual handling in the driver.

I wondered about this too -- in fact, in the end, the pr *does* allow that.  You'll notice that if the fetch failure is set in the task context, but the task succeeds, than I log an error but otherwise let the task continue.  We only send it back to the driver if there is also a task failure.

I decided to do it that way in case there is some crazy existing code out there which relies on this behavior ... but honestly I'm not sure that is the right decision, maybe it should still send the fetch failure back to the driver and fail the task in that case too.;;;","19/Jan/17 20:09;markhamstra;Ok, I haven't read your PR closely yet, so I missed that.

This question looks like something that could use more eyes and insights. [~kayousterhout][~matei][~rxin@databricks.com]
;;;","02/Apr/18 02:25;xchen12138;I believe it cause another bug. If the task is killed with reason ""another attempt success"" and calling 

interrupt of the thread. The thread will throw a ClosedByInterruptException. It's an IO exception but not a Fetch fail exception. So when a speculative task creating input stream and kill by interrupt function of the thread, spark will treat it as fetch failure and try to regenerate shuffle data, which is completely wrong.;;;","02/Apr/18 14:57;irashid;Oh thanks for pointing that out [~xchen12138], I think you are absolutely correct.  Would you mind opening another bug and cc'ing me?  It would be great if you can attach logs;;;","04/Apr/18 02:56;xchen12138;[~imranr], I have created another task about this issue several days ago: https://issues.apache.org/jira/browse/SPARK-23816. Could you please help review it?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File does not exist: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta,SPARK-19268,13035744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,liyan,liyan,18/Jan/17 01:37,28/Nov/17 11:31,14/Jul/23 06:30,24/Jan/17 06:31,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"bq. ./run-example sql.streaming.JavaStructuredKafkaWordCount 192.168.3.110:9092 subscribe topic03

when i run the spark example raises the following error:
{quote}
Exception in thread ""main"" 17/01/17 14:13:41 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(4)
org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 2 in stage 9.0 failed 1 times, most recent failure: Lost task 2.0 in stage 9.0 (TID 46, localhost, executor driver): java.lang.IllegalStateException: Error reading delta file /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta of HDFSStateStoreProvider[id = (op=0, part=2), dir = /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2]: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta does not exist
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:354)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:306)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1$$anonfun$6.apply(HDFSBackedStateStoreProvider.scala:303)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:303)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap$1.apply(HDFSBackedStateStoreProvider.scala:302)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$loadMap(HDFSBackedStateStoreProvider.scala:302)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.getStore(HDFSBackedStateStoreProvider.scala:220)
	at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:151)
	at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:61)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: File does not exist: /tmp/temporary-157b89c1-27bb-49f3-a70c-ca1b75022b4d/state/0/2/1.delta
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:769)
	at org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(HDFSBackedStateStoreProvider.scala:351)
	... 24 more
{quote}


I checked my spark configuration file, found the value of {{spark.sql.adaptive.enabled}} is {{true}}, to modify it into {{false}}, the example program can work.

Is this a bug?

thanks!","- hadoop2.7
- Java 7",apachespark,liyan,mgaido,skrishna,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 28 11:31:22 UTC 2017,,,,,,,,,,"0|i38up3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/17 22:15;zsxwing;Right now Structured Streaming doesn't support ""spark.sql.adaptive.enabled"". We should add an explicit error message.;;;","23/Jan/17 22:21;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16683;;;","08/May/17 07:38;skrishna;Hi [~zsxwing]

I have tested with spark 2.1.1, still the same problem exist. FYI.. I am running in Mesos environment. Do I need to add any config settings?

thanks;;;","08/May/17 21:59;zsxwing;[~skrishna] could you provide your codes, or the output of ""dataset.explain(true)"", please? Perhaps there is another bug in aggregation.;;;","27/Nov/17 09:18;mgaido;[~zsxwing] I am hitting this too and I am running 2.2.0. My code looks very similar to the one of the {{org.apache.spark.examples.sql.streaming.StructuredNetworkWordCountWindowed}} example.;;;","28/Nov/17 11:31;mgaido;In my case, deleting `_spark_metadata` solved the issue. Thus likely this is caused by a bad status of the `_spark_metadata` dir. [~zsxwing] Should we reopen this or create a new ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a race condition when stopping StateStore,SPARK-19267,13035732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,18/Jan/17 00:27,24/Jan/17 06:28,14/Jul/23 06:30,21/Jan/17 01:48,2.0.0,2.0.1,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,0,,,,,,,,,,,apachespark,tdas,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 21 01:48:21 UTC 2017,,,,,,,,,,"0|i38umf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/17 00:36;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16627;;;","21/Jan/17 01:48;tdas;Issue resolved by pull request 16627
[https://github.com/apache/spark/pull/16627];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler should avoid sending conflicting task set.,SPARK-19263,13035594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jinxing6042@126.com,jinxing6042@126.com,jinxing6042@126.com,17/Jan/17 16:44,17/May/20 17:47,14/Jul/23 06:30,18/Feb/17 14:57,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Scheduler,Spark Core,,,,0,,,,,,,,,"In current *DAGScheduler handleTaskCompletion* code, when *event.reason* is *Success*, it will first do *stage.pendingPartitions -= task.partitionId*, which maybe a bug when *FetchFailed* happens. Think about below:

# Stage 0 runs and generates shuffle output data.
# Stage 1 reads the output from stage 0 and generates more shuffle data. It has two tasks: ShuffleMapTask1 and ShuffleMapTask2, and these tasks are launched on executorA.
# ShuffleMapTask1 fails to fetch blocks locally and sends a FetchFailed to the driver. The driver marks executorA as lost and updates failedEpoch;
# The driver resubmits stage 0 so the missing output can be re-generated, and then once it completes, resubmits stage 1 with ShuffleMapTask1x and ShuffleMapTask2x.
# ShuffleMapTask2 (from the original attempt of stage 1) successfully finishes on executorA and sends Success back to driver. This causes DAGScheduler::handleTaskCompletion to remove partition 2 from stage.pendingPartitions (line 1149), but it does not add the partition to the set of output locations (line 1192), because the task’s epoch is less than the failure epoch for the executor (because of the earlier failure on executor A)
# ShuffleMapTask1x successfully finishes on executorB, causing the driver to remove partition 1 from stage.pendingPartitions. Combined with the previous step, this means that there are no more pending partitions for the stage, so the DAGScheduler marks the stage as finished (line 1196). However, the shuffle stage is not available (line 1215) because the completion for ShuffleMapTask2 was ignored because of its epoch, so the DAGScheduler resubmits the stage.
# ShuffleMapTask2x is still running, so when TaskSchedulerImpl::submitTasks is called for the re-submitted stage, it throws an error, because there’s an existing active task set

To reproduce the bug:
1. We need to do some modification in *ShuffleBlockFetcherIterator*: check whether the task's index in *TaskSetManager* and stage attempt equal to 0 at the same time, if so, throw FetchFailedException;
2. Rebuild spark then submit following job:
{code}
    val rdd = sc.parallelize(List((0, 1), (1, 1), (2, 1), (3, 1), (1, 2), (0, 3), (2, 1), (3, 1)), 2)
    rdd.reduceByKey {
      (v1, v2) => {
        Thread.sleep(10000)
        v1 + v2
      }
    }.map {
      keyAndValue => {
        (keyAndValue._1 % 2, keyAndValue._2)
      }
    }.reduceByKey {
      (v1, v2) => {
        Thread.sleep(10000)
        v1 + v2

      }
    }.collect
{code}",,apachespark,irashid,jinxing6042@126.com,kayousterhout,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19502,,,,,,,SPARK-14658,,,,,,,,,SPARK-19262,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 22:21:43 UTC 2017,,,,,,,,,,"0|i38trr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/17 17:03;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/16620;;;","07/Feb/17 11:34;apachespark;User 'jinxing64' has created a pull request for this issue:
https://github.com/apache/spark/pull/16831;;;","23/Feb/17 22:21;kayousterhout;Just noting that this was fixed by https://github.com/apache/spark/pull/16620 (the other PR was accidentally created with the same JIRA ID);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
" Spaces or ""%20"" in path parameter are not correctly handled with HistoryServer",SPARK-19260,13035444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,17/Jan/17 09:33,07/Feb/17 12:21,14/Jul/23 06:30,07/Feb/17 12:21,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"case1: .sbin/start-history-server.sh ""file:/a b""
And you get the error ===>
Caused by: java.lang.IllegalArgumentException: Log directory specified does not exist: file:/root/file:/a%20b.
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$startPolling(FsHistoryProvider.scala:201)

case2: ./start-history-server.sh ""/a%20c""
And you get the error ===>
Caused by: java.lang.IllegalArgumentException: Log directory specified does not exist: file:/a%2520c/.
at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$startPolling(FsHistoryProvider.scala:201)",linux,apachespark,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 12:21:45 UTC 2017,,,,,,,,,,"0|i38suf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/17 10:06;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16614;;;","20/Jan/17 09:11;zuo.tingbing9;run .sbin/start-history-server.sh ""file:/a b""

And you get the error:

Caused by: java.lang.IllegalArgumentException: Log directory specified does not exist: file:/root/file:/a%20b.
	at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$startPolling(FsHistoryProvider.scala:201)
	at org.apache.spark.deploy.history.FsHistoryProvider.initialize(FsHistoryProvider.scala:153)
	at org.apache.spark.deploy.history.FsHistoryProvider.<init>(FsHistoryProvider.scala:149)
	at org.apache.spark.deploy.history.FsHistoryProvider.<init>(FsHistoryProvider.scala:75);;;","20/Jan/17 10:55;srowen;In that case, you're not supplying a valid URI. Are you sure that's not the cause?;;;","22/Jan/17 01:30;zuo.tingbing9;Yes it is a invaild URI but the err msg "" Log directory specified does not exist: file:/root/file:/a%20b"" is unreasonable anyway.
What is the point of using URI? It seems that the param of ""spark.history.fs.logDirectory"" is a path of a directory.  

===============================================
p.s. ""/abc%20c"" is a valid URI but still run failed.
./start-history-server.sh ""/abc%20c""

Caused by: java.lang.IllegalArgumentException: Log directory specified does not exist: file:/abc%2520c/.
	at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$startPolling(FsHistoryProvider.scala:201)
;;;","22/Jan/17 03:01;zuo.tingbing9;i have tested these cases successfully which failed before 

local:
.sbin/start-history-server.sh ""file:/a b""
.sbin/start-history-server.sh ""/abc%20c""  (without hdfs-site.xml,core-site.xml)
.sbin/start-history-server.sh ""/a b""  (without hdfs-site.xml,core-site.xml)
.sbin/start-history-server.sh ""/a b/a bc%20c"" (without hdfs-site.xml,core-site.xml)

hdfs:
.sbin/start-history-server.sh ""hdfs:/namenode:9000/a b""
.sbin/start-history-server.sh ""/a b"" (with hdfs-site.xml,core-site.xml)
.sbin/start-history-server.sh ""/a b/a bc%20c"" (with hdfs-site.xml,core-site.xml)
;;;","07/Feb/17 12:21;srowen;Issue resolved by pull request 16614
[https://github.com/apache/spark/pull/16614];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CataLogTable's partitionSchema should check order&exist,SPARK-19246,13035242,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,windpiger,windpiger,windpiger,16/Jan/17 14:55,24/Jan/17 12:50,14/Jul/23 06:30,24/Jan/17 12:50,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"get CataLogTable's partitionSchema should check if each column name in partitionColumnNames must match one and only one field in schema, if not we should throw an exception

and CataLogTable's partitionSchema should keep order with partitionColumnNames ",,apachespark,windpiger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 16 15:06:03 UTC 2017,,,,,,,,,,"0|i38rlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/17 15:06;apachespark;User 'windpiger' has created a pull request for this issue:
https://github.com/apache/spark/pull/16606;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR package on Windows waiting for a long time when no java is found launching spark-submit,SPARK-19237,13035139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,16/Jan/17 07:56,21/Mar/17 21:26,14/Jul/23 06:30,21/Mar/17 21:25,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Spark Core,SparkR,,,,0,,,,,,,,,"When installing SparkR as a R package (install.packages) on Windows, it will check for Spark distribution and automatically download and cache it. But if there is no java runtime on the machine spark-submit will just hang.",,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 21 21:25:30 UTC 2017,,,,,,,,,,"0|i38qz3:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"16/Jan/17 08:01;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16596;;;","21/Mar/17 21:25;shivaram;Issue resolved by pull request 16596
[https://github.com/apache/spark/pull/16596];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR distribution cache location is wrong on Windows,SPARK-19232,13035066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,felixcheung,felixcheung,felixcheung,16/Jan/17 00:40,16/Jan/17 17:36,14/Jul/23 06:30,16/Jan/17 17:36,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"On Linux:

{code}
~/.cache/spark# ls -lart
total 12
drwxr-xr-x 12  500  500 4096 Dec 16 02:18 spark-2.1.0-bin-hadoop2.7
drwxr-xr-x  3 root root 4096 Dec 18 00:03 ..
drwxr-xr-x  3 root root 4096 Dec 18 00:06 .
{code}

On Windows:
{code}
C:\Users\felix\AppData\Local\spark\spark\Cache
01/13/2017  11:25 AM    <DIR>          spark-2.1.0-bin-hadoop2.7
01/13/2017  11:25 AM        33,471,940 spark-2.1.0-bin-hadoop2.7.tgz
{code}

If we follow https://pypi.python.org/pypi/appdirs, appauthor should be ""Apache""?
",,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 16 17:36:10 UTC 2017,,,,,,,,,,"0|i38qiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/17 00:55;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16590;;;","16/Jan/17 17:36;shivaram;Issue resolved by pull request 16590
[https://github.com/apache/spark/pull/16590];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR hangs when there is download or untar failure,SPARK-19231,13035065,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,16/Jan/17 00:31,18/Jan/17 17:54,14/Jul/23 06:30,18/Jan/17 17:54,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"When there is any partial download, or download error it is not cleaned up, and sparkR.session will continue to stuck with no error message.

{code}
> sparkR.session()
Spark not found in SPARK_HOME:
Spark not found in the cache directory. Installation will start.
MirrorUrl not provided.
Looking for preferred site from apache website...
Preferred mirror site found: http://www-eu.apache.org/dist/spark
Downloading spark-2.1.0 for Hadoop 2.7 from:
- http://www-eu.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
trying URL 'http://www-eu.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz'
Content type 'application/x-gzip' length 195636829 bytes (186.6 MB)
downloaded 31.9 MB
 
Installing to C:\Users\felix\AppData\Local\spark\spark\Cache
Error in untar2(tarfile, files, list, exdir) : incomplete block on file

In addition: Warning message:
In download.file(remotePath, localPath) :
  downloaded length 33471940 != reported length 195636829
> sparkR.session()
Spark not found in SPARK_HOME:
spark-2.1.0 for Hadoop 2.7 found, setting SPARK_HOME to C:\Users\felix\AppData\Local\spark\spark\Cache/spark-2.1.0-bin-hadoop2.7
Launching java with spark-submit command C:\Users\felix\AppData\Local\spark\spark\Cache/spark-2.1.0-bin-hadoop2.7/bin/spark-submit2.cmd   sparkr-shell C:\Users\felix\AppData\Local\Temp\RtmpCqNdne\backend_port16d04191e7
{code}

{code}
Directory of C:\Users\felix\AppData\Local\spark\spark\Cache
01/13/2017  11:25 AM    <DIR>          spark-2.1.0-bin-hadoop2.7
01/13/2017  11:25 AM        33,471,940 spark-2.1.0-bin-hadoop2.7.tgz
{code}
",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 16 00:38:02 UTC 2017,,,,,,,,,,"0|i38qin:",9223372036854775807,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,,"16/Jan/17 00:38;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16589;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow Creating Hive Source Tables when Hive Support is Not Enabled,SPARK-19229,13035043,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,15/Jan/17 19:11,23/Jan/17 04:38,14/Jul/23 06:30,23/Jan/17 04:38,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"It is weird to create Hive source tables when using InMemoryCatalog. We are unable to operate it. We should block users to create Hive source tables.
",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 04:38:33 UTC 2017,,,,,,,,,,"0|i38qdr:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"15/Jan/17 19:13;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16587;;;","23/Jan/17 04:38;smilegator;Issue resolved by pull request 16587
[https://github.com/apache/spark/pull/16587];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputFileBlockHolder doesn't work with Python UDF for datasource other than FileFormat,SPARK-19223,13034906,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,14/Jan/17 06:32,18/Jan/17 15:08,14/Jul/23 06:30,18/Jan/17 15:08,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,PySpark,SQL,,,,0,,,,,,,,,"For the datasource other than FileFormat, such as spark-xml which is based on BaseRelation and uses HadoopRDD, NewHadoopRDD, InputFileBlockHolder doesn't work with Python UDF.

The method to reproduce it is, running the following codes with {{bin/pyspark --packages com.databricks:spark-xml_2.11:0.4.1}}:


{code}
from pyspark.sql.functions import udf,input_file_name
from pyspark.sql.types import StringType
from pyspark.sql import SparkSession

def filename(path):
    return path

session = SparkSession.builder.appName('APP').getOrCreate()

session.udf.register('sameText',filename)
sameText = udf(filename, StringType())

df = session.read.format('xml').load('a.xml', rowTag='root').select('*',input_file_name().alias('file'))
df.select('file').show()  // works
df.select(sameText(df['file'])).show()  // returns empty content
{code}

a.xml:
{code}
<root>
  <x>TEXT</x>
  <y>TEXT2</y>
</root>
{code}

",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 14 06:52:36 UTC 2017,,,,,,,,,,"0|i38pjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/17 06:45;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16585;;;","14/Jan/17 06:52;viirya;Hi [~someonehere15],

For the issue on spark-xml package that when applying UDF on the column of input_file_name the result will be empty, I created this jira and submitted a PR to fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add winutils binaries to Path in AppVeyor for Hadoop libraries to call native libraries properly,SPARK-19221,13034903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,14/Jan/17 05:22,12/Dec/22 17:50,14/Jul/23 06:30,14/Jan/17 16:32,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Project Infra,SparkR,,,,0,,,,,,,,,"It seems Hadoop libraries need {{hadoop.dll}} for native libraries in the path. It is not a problem in tests for now because we are only testing SparkR on Windows via AppVeyor but it can be a problem if we run Scala tests via AppVeyor as below:

{code}
 - SPARK-18220: read Hive orc table with varchar column *** FAILED *** (3 seconds, 937 milliseconds)
   org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask. org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
   at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1.apply(HiveClientImpl.scala:625)
   at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$runHive$1.apply(HiveClientImpl.scala:609)
   at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:283)
   at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:230)
   at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:229)
   at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:272)
   at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:609)
   at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:599)
   at org.apache.spark.sql.hive.orc.OrcSuite$$anonfun$7.apply$mcV$sp(OrcSourceSuite.scala:159)
   at org.apache.spark.sql.hive.orc.OrcSuite$$anonfun$7.apply(OrcSourceSuite.scala:155)
   at org.apache.spark.sql.hive.orc.OrcSuite$$anonfun$7.apply(OrcSourceSuite.scala:155)
   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
   at org.scalatest.Transformer.apply(Transformer.scala:22)
   at org.scalatest.Transformer.apply(Transformer.scala:20)
   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
   at scala.collection.immutable.List.foreach(List.scala:381)
   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
   at org.scalatest.Suite$class.run(Suite.scala:1424)
   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:31)
   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   at java.lang.Thread.run(Thread.java:745)
{code}

This is a potential problem which seems good to fix.
",,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 14 16:32:17 UTC 2017,,,,,,,,,,"0|i38pin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/17 05:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16584;;;","14/Jan/17 16:32;shivaram;Resolved by https://github.com/apache/spark/pull/16584;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL redirect handler only redirects the server's root,SPARK-19220,13034816,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,13/Jan/17 20:46,27/Jan/17 18:16,14/Jul/23 06:30,26/Jan/17 08:29,2.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,Web UI,,,,,0,,,,,,,,,"The redirect handler that is started in the HTTP port when SSL is enabled only redirects the root of the server. Additional handlers do not go through the handler, so if you have a deep link to the non-https server, you won't be redirected to the https port.

I tested this with the history server, but it should be the same for the normal UI; the fix should be the same for both too.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 27 01:10:03 UTC 2017,,,,,,,,,,"0|i38ozj:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"13/Jan/17 22:44;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16582;;;","26/Jan/17 18:02;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16711;;;","27/Jan/17 01:10;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/16717;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SET command to show a result correctly and in a sorted order,SPARK-19218,13034790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,13/Jan/17 19:00,17/Feb/17 09:09,14/Jul/23 06:30,23/Jan/17 09:22,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"This issue aims to fix the following two things.

1. `sql(""SET -v"").collect()` or `sql(""SET -v"").show()` raises the following exceptions for String configuration with default value, `null`. For the test, please see [Jenkins result](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/71539/testReport/) and https://github.com/apache/spark/commit/60953bf1f1ba144e709fdae3903a390ff9479fd0 in #16624 .

{code}
sbt.ForkMain$ForkError: java.lang.RuntimeException: Error while decoding: java.lang.NullPointerException
createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, StructField(key,StringType,false), StructField(value,StringType,false), StructField(meaning,StringType,false))
:- input[0, string, false].toString
:  +- input[0, string, false]
:- input[1, string, false].toString
:  +- input[1, string, false]
+- input[2, string, false].toString
   +- input[2, string, false]
{code}

2. Currently, `SET` and `SET -v` commands show unsorted result.
    We had better show a sorted result for UX. Also, this is compatible with Hive.

*BEFORE*
{code}
scala> sql(""set"").show(false)
...
|spark.driver.host              |10.22.16.140                                                                                                                                 |
|spark.driver.port              |63893                                                                                                                                        |
|spark.repl.class.uri           |spark://10.22.16.140:63893/classes                                                                                                           |
...
|spark.app.name                 |Spark shell                                                                                                                                  |
|spark.driver.memory            |4G                                                                                                                                           |
|spark.executor.id              |driver                                                                                                                                       |
|spark.submit.deployMode        |client                                                                                                                                       |
|spark.master                   |local[*]                                                                                                                                     |
|spark.home                     |/Users/dhyun/spark                                                                                                                           |
|spark.sql.catalogImplementation|hive                                                                                                                                         |
|spark.app.id                   |local-1484333618945                                                                                                                          |
{code}

*AFTER*

{code}
scala> sql(""set"").show(false)
...
|spark.app.id                   |local-1484333925649                                                                                                                          |
|spark.app.name                 |Spark shell                                                                                                                                  |
|spark.driver.host              |10.22.16.140                                                                                                                                 |
|spark.driver.memory            |4G                                                                                                                                           |
|spark.driver.port              |64994                                                                                                                                        |
|spark.executor.id              |driver                                                                                                                                       |
|spark.jars                     |                                                                                                                                             |
|spark.master                   |local[*]                                                                                                                                     |
|spark.repl.class.uri           |spark://10.22.16.140:64994/classes                                                                                                           |
|spark.sql.catalogImplementation|hive                                                                                                                                         |
|spark.submit.deployMode        |client                                                                                                                                       |
{code}
",,apachespark,dongjoon,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-14819,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 09:22:22 UTC 2017,,,,,,,,,,"0|i38otr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/17 19:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16579;;;","23/Jan/17 09:22;smilegator;Issue resolved by pull request 16579
[https://github.com/apache/spark/pull/16579];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update outdated parameter descriptions in external-kafka module,SPARK-19206,13034533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,13/Jan/17 08:37,13/Sep/17 07:54,14/Jul/23 06:30,15/Jan/17 11:16,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,DStreams,,,,,0,,,,,,,,,,,apachespark,uncleGen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 13 07:54:04 UTC 2017,,,,,,,,,,"0|i38n7j:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,,,,,,,,,,"13/Jan/17 08:43;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16569;;;","15/Jan/17 11:16;srowen;Issue resolved by pull request 16569
[https://github.com/apache/spark/pull/16569];;;","13/Sep/17 07:54;apachespark;User 'Chaos-Ju' has created a pull request for this issue:
https://github.com/apache/spark/pull/19206;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationExceptions with CachedKafkaConsumers when Windowing,SPARK-19185,13034056,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,kalvinnchau,kalvinnchau,12/Jan/17 02:16,15/May/19 08:17,14/Jul/23 06:30,22/May/18 20:44,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.4.0,,,,,DStreams,,,,,9,streaming,windowing,,,,,,,"We've been running into ConcurrentModificationExcpetions ""KafkaConsumer is not safe for multi-threaded access"" with the CachedKafkaConsumer. I've been working through debugging this issue and after looking through some of the spark source code I think this is a bug.

Our set up is:
Spark 2.0.2, running in Mesos 0.28.0-2 in client mode, using Spark-Streaming-Kafka-010
spark.executor.cores 1
spark.mesos.extra.cores 1

Batch interval: 10s, window interval: 180s, and slide interval: 30s

We would see the exception when in one executor there are two task worker threads assigned the same Topic+Partition, but a different set of offsets.

They would both get the same CachedKafkaConsumer, and whichever task thread went first would seek and poll for all the records, and at the same time the second thread would try to seek to its offset but fail because it is unable to acquire the lock.

Time0 E0 Task0 - TopicPartition(""abc"", 0) X to Y
Time0 E0 Task1 - TopicPartition(""abc"", 0) Y to Z

Time1 E0 Task0 - Seeks and starts to poll
Time1 E0 Task1 - Attempts to seek, but fails

Here are some relevant logs:

{code}
17/01/06 03:10:01 Executor task launch worker-1 INFO KafkaRDD: Computing topic test-topic, partition 2 offsets 4394204414 -> 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO KafkaRDD: Computing topic test-topic, partition 2 offsets 4394238058 -> 4394257712
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested 4394204414
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO CachedKafkaConsumer: Initial fetch for spark-executor-consumer test-topic 2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer: Seeking to test-topic-2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Putting block rdd_199_2 failed due to an exception
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Block rdd_199_2 could not be removed as it was not found on disk or in memory
17/01/06 03:10:01 Executor task launch worker-0 ERROR Executor: Exception in task 49.0 in stage 45.0 (TID 3201)
java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1431)
	at org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1132)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.seek(CachedKafkaConsumer.scala:95)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:69)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:360)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:951)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Polled [test-topic-2]  8237
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204415 requested 4394204415
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204416 requested 4394204416
... 
{code}

It looks like when WindowedDStream does the getOrCompute call its computing all the sets of of offsets it needs and tries to farm out the work in parallel. So each available worker task gets each set of offsets that need to be read.

After realizing what was going on I tested four states:

* spark.executor.cores 1 and spark.mesos.extra.cores 0
** No Exceptions
* spark.executor.cores 1 and spark.mesos.extra.cores 1
** ConcurrentModificationException
* spark.executor.cores 2 and spark.mesos.extra.cores 0
** ConcurrentModificationException
* spark.executor.cores 2 and spark.mesos.extra.cores 1
** ConcurrentModificationException


Minimal set of code I was able to reproduce with:
Streaming batch interval was set to 2 seconds. This increased the rate of exceptions I saw.

{code}
val kafkaParams = Map[String, Object](
  ""bootstrap.servers"" -> brokers,
  ""key.deserializer"" -> classOf[KafkaAvroDeserializer],
  ""value.deserializer"" -> classOf[KafkaAvroDeserializer],
  ""enable.auto.commit"" -> (false: java.lang.Boolean),
  ""group.id"" -> groupId,
  ""schema.registry.url"" -> schemaRegistryUrl,
  ""auto.offset.reset"" -> offset
)
val inputStream = KafkaUtils.createDirectStream[Object, Object](
  ssc,
  PreferConsistent,
  Subscribe[Object, Object]
    (kafkaTopic, kafkaParams)
)
val windowStream = inputStream.map(_.toString).window(Seconds(180), Seconds(30))

windowStream.foreachRDD{
  rdd => {
    val filtered = rdd.filter(_.contains(""idb""))

    filtered.foreach(
      message => {
        var i = 0
        if (i == 0) {
          logger.info(message)
          i = i + 1
        }
      }
    )
  }
}
{code}","Spark 2.0.2
Spark Streaming Kafka 010
Mesos 0.28.0 - client mode

spark.executor.cores 1
spark.mesos.extra.cores 1",apachespark,ayushChauhan,darose,gsomogyi,helena_e,jincheng,kalvinnchau,kaushik_srinivas,koeninger,levg,prasincs,priyank.rastogi,rajesh.balamohan,sandeep.katta2007,sb58,tuckerpm,umesh9794@gmail.com,uncleGen,utkarshkajaria,vanzin,yuzhihong@gmail.com,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20911,SPARK-22606,SPARK-23663,,,,,,,SPARK-27720,SPARK-19888,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 04 13:20:48 UTC 2019,,,,,,,,,,"0|i38lbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/17 15:17;koeninger;This is a good error report, sorry it's taken me a while to get back to you on this.

My immediate suggestions to you as a workaround would be
- Try persist before windowing, so that batches of offsets from Kafka are only fetched once, rather than repeatedly and possibly simultaneously for a given kafka partition.  I'm assuming that's the underlying issue, but could be wrong.
- Failing that, KafkaRDD's constructor takes a boolean parameter indicating whether to use the consumer cache.  You can straightforwardly modify DirectKafkaInputDStream.compute to pass false.  This will require rebuilding only the kafka consumer jar, not redeploying all of spark.  This will be a performance hit, especially if you're using SSL, but is better than nothing.

Fixing this in the Spark master branch (either by allowing configuration of whether to use the consumer cache, or replacing the consumer cache with a pool of consumers with different group ids for the same topicpartition) is going to require getting the attention of a committer.  I don't really have the time to mess with that right now (happy to do the work, but zero interest in tracking down committers and arguing design decisions).

That being said, if one of the workarounds suggested above doesn't help you, let me know.
;;;","17/Jan/17 09:22;uncleGen;Any updates about this issue? One of the workarounds suggested above can help you?  [~kalvinnchau];;;","17/Jan/17 17:46;kalvinnchau;[~cody@koeninger.org]  [~uncleGen]


Thanks, I haven't had the opportunity to test either of those options yet. 
* But unless the persisting somehow prevents two Task threads within the executor from trying to read from the same partition, then I don't see how that would prevent the underlying issue. But I'm not 100% sure how that affects the scheduling of tasks. 
* I'll keep this in mind as well, seems straight forward.

Our current workaround is just making sure we have 1 task thread per executor, which is a performance hit especially for some of our higher velocity streams. To get around that we've increased the number of partitions on the topic, and upped the number of executors to match.

I think having consumer cache be configurable is a good idea, can't we technically already do that by setting the size of the cache to 0? (using ""spark.sql.kafkaConsumerCache.capacity"") Or will that fail? I haven't had a chance to test that either.
I think having a pool of consumers with N group ids for a topicPartition (N being the number task threads within an executor) seems like good idea.

If there were a solid direction going forward wouldn't mind tackling this issue either to contribute back!;;;","17/Jan/17 18:09;koeninger;I'd expect setting cache capacity to zero to cause failures, but it's probably (slightly) faster to try than just patching the lines of code I pointed out.

In general, a single application using kafka consumers should not be reading from different places in the same topicpartition in different threads, because it breaks ordering guarantees.  That's an implication of Kafka semantics, not Spark semantics.  That's why the consumer cache exists the way it does.

Changing that behavior on a widespread basis is going to break ordering guarantees, which will break some people's existing jobs.  Hence my comments about arguing design decisions with committers.;;;","18/Jan/17 01:34;helena_e;I've seen this as well, the exceptions, as expected, are never raised if not using the cache. 
Spark 2.1.

The exception is raised in the seek function. I added an opt-in config for that temporarily but will work on a better solution. Perf hits aren't something I can do :);;;","18/Jan/17 02:25;uncleGen;IMHO, persisting can not cover all scene, just like executor failed or cached data loss. Maybe, we can expose much clearer hint, like ""KafkaConsumer is not safe for multi-threaded access, you may set 'useConsumerCache' as false, or do not run multi-tasks in single executor"" or something else. Besides, we may expose useConsumerCache with a configuration (true as default). This will not chang the behavior on a widespread basis.;;;","18/Jan/17 03:20;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16629;;;","18/Jan/17 03:24;uncleGen;[~cody@koeninger.org] I provide a PR to give more clear hint for users when encounter this problem.;;;","07/Jun/17 17:34;apachespark;User 'markgrover' has created a pull request for this issue:
https://github.com/apache/spark/pull/18234;;;","08/Jun/17 16:56;vanzin;I merged Mark's patch above to master and branch-2.2, but it's just a work-around, not a fix, so I'll leave the bug open (and with no ""fix version"").;;;","01/Mar/18 14:08;gsomogyi;Same problem exists in structured streaming. Creating a new PR with similar work-around.;;;","01/Mar/18 15:04;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20703;;;","20/Mar/18 06:54;kaushik_srinivas;same issue found with kafka spark streaming 010.

With increased batch window, this issue seems to be more frequently appearing.

SPARK-23663 created for reference. Is disabling window the workaround as of now ?

We see in spark 2.2.0;;;","06/Apr/18 13:27;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/20997;;;","22/May/18 20:44;vanzin;Issue resolved by pull request 20997
[https://github.com/apache/spark/pull/20997];;;","12/Dec/18 06:32;ayushChauhan;Is this issue resolved because SPARK-22606 is still in progress? I am getting this error in spark 2.3.2 and spark-streaming-kafka-0-10. 

So this workaround is the solution to this problem? Is there any downside of setting this property?
{code:java}
spark.streaming.kafka.consumer.cache.enabled=false 
{code};;;","12/Dec/18 08:26;gsomogyi;There are many related issues open but this should solve the issue. As you see it's resolved on 2.4.0. The workaround is to turn cache off (what you've written).

I'll take a look at all the jiras and comment on which looks like relevant and ask for re-test.

 

GENERAL WARNING FOR THIS FEATURE: Kafka keeps order within one partition, but reading the same partition from multiple threads may change the order!

 ;;;","04/Mar/19 13:20;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/23959;;;",,,,,,,,,,,,,,,,,,,,,
SparkListenerSuite.local metrics fails when average executorDeserializeTime is too short.,SPARK-19181,13033966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,attilapiros,jsoltren,jsoltren,11/Jan/17 21:01,10/May/18 21:27,14/Jul/23 06:30,10/May/18 21:27,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.3.1,2.4.0,,,,Tests,,,,,0,,,,,,,,,"https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/scheduler/SparkListenerSuite.scala#L249

The ""local metrics"" test asserts that tasks should take more than 1ms on average to complete, even though a code comment notes that this is a small test and tasks may finish faster. I've been seeing some ""failures"" here on fast systems that finish these tasks quite quickly.

There are a few ways forward here:
1. Disable this test.
2. Relax this check.
3. Implement sub-millisecond granularity for task times throughout Spark.
4. (Imran Rashid's suggestion) Add buffer time by, say, having the task reference a partition that implements a custom Externalizable.readExternal, which always waits 1ms before returning.",,apachespark,attilapiros,jsoltren,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 10 21:27:09 UTC 2018,,,,,,,,,,"0|i38krj:",9223372036854775807,,,,,irashid,,,,,,,,,,,,,,,,,,,"11/Jan/17 21:13;jsoltren;SPARK-2208 disabled a similar metric previously.;;;","02/Feb/17 20:10;jsoltren;https://github.com/apache/spark/pull/16586 made some changes to create more workers and push the working time for this test slightly longer.

This solution will only work for so long and does not address the fundamental issue in the description.;;;","02/Mar/18 01:21;vanzin;Another failure (after quite some time):
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/87855/testReport/junit/org.apache.spark.scheduler/SparkListenerSuite/local_metrics/;;;","08/May/18 09:21;attilapiros;I am working on this.;;;","09/May/18 13:22;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/21280;;;","10/May/18 21:27;vanzin;Issue resolved by pull request 21280
[https://github.com/apache/spark/pull/21280];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the offset of short is 4 in OffHeapColumnVector's putShorts,SPARK-19180,13033939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yucai,yucai,yucai,11/Jan/17 20:11,18/Jan/17 22:49,14/Jul/23 06:30,13/Jan/17 21:41,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,"the offset of short is 4 in OffHeapColumnVector's putShorts, actually it should be 2.",,apachespark,davies,yucai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 13 21:41:32 UTC 2017,,,,,,,,,,"0|i38klj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/17 20:28;srowen;Are you sure? most stuff is int aligned in the JVM. You might be right but just making sure it is not merely because a short is 2 bytes;;;","11/Jan/17 22:34;yucai;Hi Owen,

Thanks a lot for comments, it is using unsafe API for OffHeapColumn, which should have no align.

See codes:
{code}
   @Override
   public void putShorts(int rowId, int count, short value) {
     long offset = data + 2 * rowId;
-    for (int i = 0; i < count; ++i, offset += 4) {
+    for (int i = 0; i < count; ++i, offset += 2) {
       Platform.putShort(null, offset, value);
     }
   }
{code}

And also, my testing:
{code}
scala> val column = ColumnVector.allocate(1024, ShortType, MemoryMode.OFF_HEAP)
column: org.apache.spark.sql.execution.vectorized.ColumnVector = org.apache.spark.sql.execution.vectorized.OffHeapColumnVector@56fc2cea

scala> column.putShorts(0, 4, 8.toShort)

scala> column.getShort(1)
res5: Short = 18432

scala> 

scala> val column = ColumnVector.allocate(1024, ShortType, MemoryMode.ON_HEAP)
column: org.apache.spark.sql.execution.vectorized.ColumnVector = org.apache.spark.sql.execution.vectorized.OnHeapColumnVector@7fb8d720

scala> column.putShorts(0, 4, 8.toShort)

scala> column.getShort(1)
res7: Short = 8
{code};;;","12/Jan/17 00:14;apachespark;User 'yucai' has created a pull request for this issue:
https://github.com/apache/spark/pull/16555;;;","13/Jan/17 21:41;davies;Issue resolved by pull request 16555
[https://github.com/apache/spark/pull/16555];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.yarn.access.namenodes description is wrong,SPARK-19179,13033759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,tgraves,tgraves,11/Jan/17 15:37,17/May/20 18:13,14/Jul/23 06:30,17/Jan/17 15:32,2.0.2,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,YARN,,,,0,,,,,,,,,"The description and name of spark.yarn.access.namenodes	 is off.  It says this is for HDFS namenodes when really this is to specify any hadoop filesystems.  It gets the credentials for those filesystems.

We should at least update the description on it to be more generic.  We could change the name on it but we would have to deprecated it and keep around current name as many people use it.",,apachespark,jerryshao,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 12 09:27:04 UTC 2017,,,,,,,,,,"0|i38k8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/17 07:29;jerryshao;Thanks [~tgraves] to point out the left thing, let me handle it.;;;","12/Jan/17 09:27;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/16560;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convert string of large numbers to int should return null,SPARK-19178,13033734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jan/17 14:21,14/Jan/17 03:47,14/Jul/23 06:30,13/Jan/17 17:24,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 14:30:04 UTC 2017,,,,,,,,,,"0|i38k33:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"11/Jan/17 14:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ml.R example fails in yarn-cluster mode due to lacks of e1071 package,SPARK-19158,13033495,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yeshavora,yeshavora,10/Jan/17 19:55,12/Jan/17 09:06,14/Jul/23 06:30,12/Jan/17 09:06,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Examples,,,,,0,,,,,,,,,"ml.R application fails in spark2 with yarn-cluster mode.
{code}
spark-submit --master yarn-cluster examples/src/main/r/ml/ml.R {code}

{code:title=application log}
17/01/03 04:35:30 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 6.8 KB, free 407.6 MB)
17/01/03 04:35:30 INFO BufferedStreamThread: Error : requireNamespace(""e1071"", quietly = TRUE) is not TRUE
17/01/03 04:35:30 ERROR Executor: Exception in task 0.0 in stage 65.0 (TID 65)
org.apache.spark.SparkException: R computation failed with
 Error : requireNamespace(""e1071"", quietly = TRUE) is not TRUE
	at org.apache.spark.api.r.RRunner.compute(RRunner.scala:108)
	at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
17/01/03 04:35:30 INFO CoarseGrainedExecutorBackend: Got assigned task 68
17/01/03 04:35:30 INFO Executor: Running task 3.0 in stage 65.0 (TID 68)
17/01/03 04:35:30 INFO BufferedStreamThread: Error : requireNamespace(""e1071"", quietly = TRUE) is not TRUE
17/01/03 04:35:30 ERROR Executor: Exception in task 3.0 in stage 65.0 (TID 68)
org.apache.spark.SparkException: R computation failed with
 Error : requireNamespace(""e1071"", quietly = TRUE) is not TRUE
Error : requireNamespace(""e1071"", quietly = TRUE) is not TRUE
	at org.apache.spark.api.r.RRunner.compute(RRunner.scala:108)
	at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:50)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
17/01/03 04:35:30 INFO CoarseGrainedExecutorBackend: Got assigned task 70
{code}

",,apachespark,yeshavora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 13:16:04 UTC 2017,,,,,,,,,,"0|i38im7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/17 13:16;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should be able to change spark.sql.runSQLOnFiles at runtime,SPARK-19157,13033442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Jan/17 16:49,11/Jan/17 05:34,14/Jul/23 06:30,11/Jan/17 05:34,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 05:34:09 UTC 2017,,,,,,,,,,"0|i38iaf:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"10/Jan/17 16:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16531;;;","11/Jan/17 05:34;smilegator;Issue resolved by pull request 16531
[https://github.com/apache/spark/pull/16531];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MLlib GeneralizedLinearRegression family and link should case insensitive ,SPARK-19155,13033406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,10/Jan/17 14:53,23/Jan/17 09:07,14/Jul/23 06:30,22/Jan/17 05:20,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,ML,,,,,0,,,,,,,,,"ML {{GeneralizedLinearRegression}} only support lowercase input for {{family}} and {{link}} currently. For example, the following code will throw exception:
{code}
val trainer = new GeneralizedLinearRegression().setFamily(""Gamma"")
{code}
However, R glm only accepts families: {{gaussian, binomial, poisson, Gamma}}. We should make {{family}} and {{link}} case insensitive. ",,apachespark,felixcheung,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 23 03:13:06 UTC 2017,,,,,,,,,,"0|i38i2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/17 14:55;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16516;;;","23/Jan/17 03:13;apachespark;User 'actuaryzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/16675;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"spark.kmeans should take seed, initSteps, and tol as parameters",SPARK-19142,13033225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,09/Jan/17 23:50,13/Jan/17 17:25,14/Jul/23 06:30,13/Jan/17 17:25,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SparkR,,,,,0,,,,,,,,,"spark.kmeans doesn't have interface to set initSteps, seed and tol. As Spark Kmeans algorithm doesn't take the same set of parameters as R kmeans, we should maintain a different interface in spark.kmeans.",,apachespark,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 09 23:53:05 UTC 2017,,,,,,,,,,"0|i38gyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 23:53;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16523;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Garbage left in source tree after SQL tests are run,SPARK-19137,13033180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,vanzin,vanzin,09/Jan/17 21:09,10/Jan/17 18:50,14/Jul/23 06:30,10/Jan/17 18:50,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,Structured Streaming,Tests,,,1,,,,,,,,,"Don't know when this started, but after I run tests in sbt I'm left with garbage files in my source repo:

{noformat}
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)

        sql/core/%253Cundefined%253E/
        sql/core/%3Cundefined%3E/
{noformat}

Tests should always write things under the ""target"" directory so that it gets automatically ignored by git and cleaned up by ""sbt/mvn clean.""",,apachespark,dongjoon,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 09 23:41:04 UTC 2017,,,,,,,,,,"0|i38gof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 21:35;dongjoon;+1;;;","09/Jan/17 23:25;dongjoon;Hi, [~vanzin].
I'll make a PR for this.;;;","09/Jan/17 23:41;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/16522;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix several sql, mllib and status api examples not working",SPARK-19134,13033045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,09/Jan/17 11:29,12/Dec/22 18:10,14/Jul/23 06:30,20/Jan/17 13:19,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,MLlib,PySpark,SQL,,,0,,,,,,,,,"*binary_classification_metrics_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/binary_classification_metrics_example.py
{code}

{code}
  File "".../spark/examples/src/main/python/mllib/binary_classification_metrics_example.py"", line 39, in <lambda>
    .rdd.map(lambda row: LabeledPoint(row[0], row[1]))
  File "".../spark/python/pyspark/mllib/regression.py"", line 54, in __init__
    self.features = _convert_to_vector(features)
  File "".../spark/python/pyspark/mllib/linalg/__init__.py"", line 80, in _convert_to_vector
    raise TypeError(""Cannot convert type %s into Vector"" % type(l))
TypeError: Cannot convert type <class 'pyspark.ml.linalg.SparseVector'> into Vector
{code}

*status_api_demo.py*

{code}
PYSPARK_PYTHON=python3 ./bin/spark-submit examples/src/main/python/status_api_demo.py
{code}

{code}
Traceback (most recent call last):
  File "".../spark/examples/src/main/python/status_api_demo.py"", line 22, in <module>
    import Queue
ImportError: No module named 'Queue'
{code}

*bisecting_k_means_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/bisecting_k_means_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/bisecting_k_means_example.py"", line 46, in <module>
    model.save(sc, path)
AttributeError: 'BisectingKMeansModel' object has no attribute 'save'
{code}

*elementwise_product_example.py*

{code}
./bin/spark-submit examples/src/main/python/mllib/elementwise_product_example.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/mllib/elementwise_product_example.py"", line 48, in <module>
    for each in transformedData2.collect():
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/pyspark/mllib/linalg/__init__.py"", line 478, in __getattr__
    return getattr(self.array, item)
AttributeError: 'numpy.ndarray' object has no attribute 'collect'
{code}

*hive.py*
{code}
./bin/spark-submit examples/src/main/python/sql/hive.py
{code}

{code}
Traceback (most recent call last):
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/examples/src/main/python/sql/hive.py"", line 47, in <module>
    spark.sql(""CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive"")
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/session.py"", line 541, in sql
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__
  File ""/Users/hyukjinkwon/Desktop/workspace/repos/forked/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 69, in deco
pyspark.sql.utils.AnalysisException: 'org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse);'
{code}


*SparkHiveExample*

{code}
./bin/run-example sql.hive.SparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:283)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:230)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:272)
	at org.apache.spark.sql.hive.client.HiveClientImpl.loadTable(HiveClientImpl.scala:685)
{code}


*JavaSparkHiveExample*

{code}
./bin/run-example sql.hive.JavaSparkHiveExample
{code}

{code}
Exception in thread ""main"" org.apache.hadoop.hive.ql.metadata.HiveException: Unable to alter table. java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./spark-warehouse
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:498)
	at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:484)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1668)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_14.loadTable(HiveShim.scala:722)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply$mcV$sp(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$loadTable$1.apply(HiveClientImpl.scala:686)
{code}",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 13:20:50 UTC 2017,,,,,,,,,,"0|i38fuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 11:33;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16515;;;","20/Jan/17 12:45;gurwls223;Oh [~yanboliang], it seems mistakenly not resolved.. :);;;","20/Jan/17 13:20;yanboliang;[~hyukjin.kwon] Thanks for kindly reminding, done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR glm Gamma family results in error,SPARK-19133,13032974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,09/Jan/17 06:07,12/Jan/17 04:04,14/Jul/23 06:30,10/Jan/17 19:49,2.0.0,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,ML,SparkR,,,,0,,,,,,,,,"> glm(y~1,family=Gamma, data = dy)
17/01/09 06:10:47 ERROR RBackendHandler: fit on org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper failed
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:167)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:108)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:40)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:346)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:652)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: glm_e3483764cdf9 parameter family given invalid value Gamma.
	at org.apache.spark.ml.param.Param.validate(params.scala:77)
	at org.apache.spark.ml.param.ParamPair.<init>(params.scala:528)
	at org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)
	at org.apache.spark.ml.param.Params$class.set(params.scala:609)
	at org.apache.spark.ml.PipelineStage.set(Pipeline.scala:42)
	at org.apache.spark.ml.regression.GeneralizedLinearRegression.setFamily(GeneralizedLinearRegression.scala:157)
	at org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper$.fit(GeneralizedLinearRegressionWrapper.scala:85)
	at org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper.fit(GeneralizedLinearRegressionWrapper.scala)
	... 36 more
Error in handleErrors(returnStatus, conn) :
  java.lang.IllegalArgumentException: glm_e3483764cdf9 parameter family given invalid value Gamma.
	at org.apache.spark.ml.param.Param.validate(params.scala:77)
	at org.apache.spark.ml.param.ParamPair.<init>(params.scala:528)
	at org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)
	at org.apache.spark.ml.param.Params$class.set(params.scala:609)
	at org.apache.spark.ml.PipelineStage.set(Pipeline.scala:42)
	at org.apache.spark.ml.regression.GeneralizedLinearRegression.setFamily(GeneralizedLinearRegression.scala:157)
	at org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper$.fit(GeneralizedLinearRegressionWrapper.scala:85)
	at org.apache.spark.ml.r.GeneralizedLinearRegressionWrapper.fit(GeneralizedLinearRegressionWrapper.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",,apachespark,felixcheung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 06:05:03 UTC 2017,,,,,,,,,,"0|i38fen:",9223372036854775807,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"09/Jan/17 06:58;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16511;;;","10/Jan/17 19:50;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16532;;;","11/Jan/17 06:05;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16543;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR should support setting and adding new column with singular value implicitly,SPARK-19130,13032951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,felixcheung,felixcheung,felixcheung,09/Jan/17 04:21,11/Jan/17 21:42,14/Jul/23 06:30,11/Jan/17 21:42,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,for parity with framework like dplyr,,apachespark,felixcheung,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-18823,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 21:42:25 UTC 2017,,,,,,,,,,"0|i38f9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 04:24;apachespark;User 'felixcheung' has created a pull request for this issue:
https://github.com/apache/spark/pull/16510;;;","11/Jan/17 21:42;shivaram;Resolved by https://github.com/apache/spark/pull/16510;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
alter table table_name drop partition with a empty string will drop the whole table,SPARK-19129,13032934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,licl,licl,09/Jan/17 02:22,17/Jan/17 18:03,14/Jul/23 06:30,17/Jan/17 18:03,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,correctness,,,,,,,,"{code}
    val spark = SparkSession
      .builder
      .appName(""PartitionDropTest"")
      .master(""local[2]"").enableHiveSupport()
      .getOrCreate()
    val sentenceData = spark.createDataFrame(Seq(
      (0, ""a""),
      (1, ""b""),
      (2, ""c"")))
      .toDF(""id"", ""name"")
    spark.sql(""drop table if exists licllocal.partition_table"")
    sentenceData.write.mode(SaveMode.Overwrite).partitionBy(""id"").saveAsTable(""licllocal.partition_table"")
    spark.sql(""alter table licllocal.partition_table drop partition(id='')"")
    spark.table(""licllocal.partition_table"").show()
{code}
the result is 
{code}
|name| id|
+----+---+
+----+---+
{code}

Maybe the partition match have something wrong when the partition value is set to empty string",,apachespark,cloud_fan,licl,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 18:03:54 UTC 2017,,,,,,,,,,"0|i38f5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/17 03:18;smilegator;Let me take a look. ;;;","13/Jan/17 20:50;smilegator;This is a bug we need to fix. Let me try it. Thanks!;;;","13/Jan/17 22:25;smilegator;This is actually a bug in Hive. Anyway, Spark can detect it and block it. Thanks for reporting it! ;;;","14/Jan/17 00:43;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16583;;;","17/Jan/17 18:03;cloud_fan;Issue resolved by pull request 16583
[https://github.com/apache/spark/pull/16583];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unnecessary shuffle+sort added if join predicates ordering differ from bucketing and sorting order,SPARK-19122,13032842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tejasp,tejasp,tejasp,08/Jan/17 01:06,11/Aug/17 22:14,14/Jul/23 06:30,11/Aug/17 22:14,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,0,,,,,,,,,"`table1` and `table2` are sorted and bucketed on columns `j` and `k` (in respective order)

This is how they are generated:
{code}
val df = (0 until 16).map(i => (i % 8, i * 2, i.toString)).toDF(""i"", ""j"", ""k"").coalesce(1)
df.write.format(""org.apache.spark.sql.hive.orc.OrcFileFormat"").bucketBy(8, ""j"", ""k"").sortBy(""j"", ""k"").saveAsTable(""table1"")
df.write.format(""org.apache.spark.sql.hive.orc.OrcFileFormat"").bucketBy(8, ""j"", ""k"").sortBy(""j"", ""k"").saveAsTable(""table2"")
{code}

Now, if join predicates are specified in query in *same* order as bucketing and sort order, there is no shuffle and sort.

{code}
scala> hc.sql(""SET spark.sql.autoBroadcastJoinThreshold=1"")
scala> hc.sql(""SELECT * FROM table1 a JOIN table2 b ON a.j=b.j AND a.k=b.k"").explain(true)

== Physical Plan ==
*SortMergeJoin [j#61, k#62], [j#100, k#101], Inner
:- *Project [i#60, j#61, k#62]
:  +- *Filter (isnotnull(k#62) && isnotnull(j#61))
:     +- *FileScan orc default.table1[i#60,j#61,k#62] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/table1], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct<i:int,j:int,k:string>
+- *Project [i#99, j#100, k#101]
   +- *Filter (isnotnull(j#100) && isnotnull(k#101))
      +- *FileScan orc default.table2[i#99,j#100,k#101] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct<i:int,j:int,k:string>
{code}


The same query with join predicates in *different* order from bucketing and sort order leads to extra shuffle and sort being introduced

{code}
scala> hc.sql(""SET spark.sql.autoBroadcastJoinThreshold=1"")
scala> hc.sql(""SELECT * FROM table1 a JOIN table2 b ON a.k=b.k AND a.j=b.j "").explain(true)

== Physical Plan ==
*SortMergeJoin [k#62, j#61], [k#101, j#100], Inner
:- *Sort [k#62 ASC NULLS FIRST, j#61 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(k#62, j#61, 200)
:     +- *Project [i#60, j#61, k#62]
:        +- *Filter (isnotnull(k#62) && isnotnull(j#61))
:           +- *FileScan orc default.table1[i#60,j#61,k#62] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/table1], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct<i:int,j:int,k:string>
+- *Sort [k#101 ASC NULLS FIRST, j#100 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(k#101, j#100, 200)
      +- *Project [i#99, j#100, k#101]
         +- *Filter (isnotnull(j#100) && isnotnull(k#101))
            +- *FileScan orc default.table2[i#99,j#100,k#101] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct<i:int,j:int,k:string>
{code}",,apachespark,cloud_fan,DjvuLee,hvanhovell,jacshen,maropu,rajeshhadoop,tejasp,vish741,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 13 06:22:23 UTC 2017,,,,,,,,,,"0|i38elb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/17 01:38;tejasp;When a `SortMergeJoinExec` node is created, the join keys in both `left` and `right` relations are extracted in their order of appearance in the query (see 0). Later, the list of keys are used as-is to define the required distribution (see 1) and ordering (see 2) for the sort merge join node. Since the ordering matters here (ie. `ClusteredDistribution(a,b) != ClusteredDistribution(b,a)`), this mismatches with the distribution and ordering of the children... thus `EnsureRequirements` ends up adding shuffle + sort.

0 : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/planning/patterns.scala#L103
1 : https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L80
2 : https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L85

;;;","08/Jan/17 01:43;tejasp;[~hvanhovell] : In a broader level, I see that when nodes for physical operators are created, the `requiredChildDistribution` and `requiredChildOrdering` is pretty much fixed at that point. While the planning is done and the child nodes are inspected, there is no way to change it.

For fixing this, my take would be :
* Allow the `requiredChildDistribution` and `requiredChildOrdering` to decided based on its children. ie. change `requiredChildOrdering()` to `requiredChildOrdering(childOrderings)`. Default behavior would be to ignore the input `childOrderings`. 
* In case of operators like `SortMergeJoinExec`, where the distribution and ordering requirement is way stricter, the operator could itself take a decision what ordering needs to be used.

||child output ordering||ordering of join keys in query||shuffle+sort needed||
| a, b | a, b | No |
| a, b | b, a | No |
| a, b, c, d | a, b | No |
| a, b, c, d | b, c | Yes |
| a, b | a, b, c, d | Yes |
| b, c | a, b, c, d | Yes |

Even SPARK-18067 will benefit from this change. Let me know if you have any opinion about this approach OR have something better in mind.
;;;","25/Jan/17 06:57;tejasp;[~hvanhovell] : ping !! If you are busy, can you suggest someone with whom I can discuss this ? I want to fix this but before sending a PR it will be good to agree on a proper design.;;;","26/Jan/17 15:17;hvanhovell;[~tejasp] We should fix this.

The rules make sense to me. I only want to add that {{EnsureRequirements}} fixes the plan bottom up. This means that we can use {{outputPartitioning}} and the {{outputOrdering}} of a plan's children to compute the plan's {{requiredChildDistribution}} and {{requiredChildOrdering}}. So we don't have to modify these methods.
;;;","18/Feb/17 18:36;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/16985;;;","12/May/17 08:58;cloud_fan;I tried the example but can't reproduce this issue, there is no shuffle in the second query. I checked with `HashPartitioning.satisfies`, it doesn't consider the expressions order.;;;","12/May/17 15:11;tejasp;[~cloud_fan]: 
- The test case in the [associated PR (#16985)|https://github.com/apache/spark/pull/16985] fails without the fix 
- I am able to repro this issue over master branch. Can you share exact steps that you used to repro ? I am guessing that `spark.sql.autoBroadcastJoinThreshold` needs to be overridden otherwise it wont pick sort merge join. Here are my exact steps to repro the example in the jira description:

{noformat}
$ git log
commit 92ea7fd7b6cd4641b2f02b97105835029ddadc5f
Author: Takeshi Yamamuro <yamamuro@apache.org>
Date:   Fri May 12 20:48:30 2017 +0800

build/sbt -Pyarn -Phadoop-2.4 -Phive package assembly/package
export SPARK_PREPEND_CLASSES=true
SPARK_LOCAL_IP=127.0.0.1 ./bin/spark-shell
{noformat}

In spark shell:
{noformat}
import org.apache.spark.sql._
val hc = SparkSession.builder.master(""local"").enableHiveSupport.getOrCreate()
hc.sql("" DROP TABLE table1 "")
hc.sql("" DROP TABLE table2 "")
val df = (0 until 16).map(i => (i % 8, i * 2, i.toString)).toDF(""i"", ""j"", ""k"").coalesce(1)

hc.sql(""SET spark.sql.autoBroadcastJoinThreshold=1"")
df.write.format(""org.apache.spark.sql.hive.orc.OrcFileFormat"").bucketBy(8, ""j"", ""k"").sortBy(""j"", ""k"").saveAsTable(""table1"")
df.write.format(""org.apache.spark.sql.hive.orc.OrcFileFormat"").bucketBy(8, ""j"", ""k"").sortBy(""j"", ""k"").saveAsTable(""table2"")


scala> hc.sql(""SELECT * FROM table1 a JOIN table2 b ON a.j=b.j AND a.k=b.k"").explain(true)
== Parsed Logical Plan ==
'Project [*]
+- 'Join Inner, (('a.j = 'b.j) && ('a.k = 'b.k))
   :- 'SubqueryAlias a
   :  +- 'UnresolvedRelation `table1`
   +- 'SubqueryAlias b
      +- 'UnresolvedRelation `table2`

== Analyzed Logical Plan ==
i: int, j: int, k: string, i: int, j: int, k: string
Project [i#86, j#87, k#88, i#89, j#90, k#91]
+- Join Inner, ((j#87 = j#90) && (k#88 = k#91))
   :- SubqueryAlias a
   :  +- SubqueryAlias table1
   :     +- Relation[i#86,j#87,k#88] orc
   +- SubqueryAlias b
      +- SubqueryAlias table2
         +- Relation[i#89,j#90,k#91] orc

== Optimized Logical Plan ==
Join Inner, ((j#87 = j#90) && (k#88 = k#91))
:- Filter (isnotnull(j#87) && isnotnull(k#88))
:  +- Relation[i#86,j#87,k#88] orc
+- Filter (isnotnull(j#90) && isnotnull(k#91))
   +- Relation[i#89,j#90,k#91] orc

== Physical Plan ==
*SortMergeJoin [j#87, k#88], [j#90, k#91], Inner
:- *Project [i#86, j#87, k#88]
:  +- *Filter (isnotnull(j#87) && isnotnull(k#88))
:     +- *FileScan orc default.table1[i#86,j#87,k#88] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table1], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct<i:int,j:int,k:string>
+- *Project [i#89, j#90, k#91]
   +- *Filter (isnotnull(j#90) && isnotnull(k#91))
      +- *FileScan orc default.table2[i#89,j#90,k#91] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table2], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct<i:int,j:int,k:string>



scala> hc.sql(""SELECT * FROM table1 a JOIN table2 b ON a.k=b.k AND a.j=b.j"").explain(true)
== Parsed Logical Plan ==
'Project [*]
+- 'Join Inner, (('a.k = 'b.k) && ('a.j = 'b.j))
   :- 'SubqueryAlias a
   :  +- 'UnresolvedRelation `table1`
   +- 'SubqueryAlias b
      +- 'UnresolvedRelation `table2`

== Analyzed Logical Plan ==
i: int, j: int, k: string, i: int, j: int, k: string
Project [i#106, j#107, k#108, i#109, j#110, k#111]
+- Join Inner, ((k#108 = k#111) && (j#107 = j#110))
   :- SubqueryAlias a
   :  +- SubqueryAlias table1
   :     +- Relation[i#106,j#107,k#108] orc
   +- SubqueryAlias b
      +- SubqueryAlias table2
         +- Relation[i#109,j#110,k#111] orc

== Optimized Logical Plan ==
Join Inner, ((k#108 = k#111) && (j#107 = j#110))
:- Filter (isnotnull(j#107) && isnotnull(k#108))
:  +- Relation[i#106,j#107,k#108] orc
+- Filter (isnotnull(k#111) && isnotnull(j#110))
   +- Relation[i#109,j#110,k#111] orc

== Physical Plan ==
*SortMergeJoin [k#108, j#107], [k#111, j#110], Inner
:- *Sort [k#108 ASC NULLS FIRST, j#107 ASC NULLS FIRST], false, 0
:  +- Exchange hashpartitioning(k#108, j#107, 200)
:     +- *Project [i#106, j#107, k#108]
:        +- *Filter (isnotnull(j#107) && isnotnull(k#108))
:           +- *FileScan orc default.table1[i#106,j#107,k#108] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table1], PartitionFilters: [], PushedFilters: [IsNotNull(j), IsNotNull(k)], ReadSchema: struct<i:int,j:int,k:string>
+- *Sort [k#111 ASC NULLS FIRST, j#110 ASC NULLS FIRST], false, 0
   +- Exchange hashpartitioning(k#111, j#110, 200)
      +- *Project [i#109, j#110, k#111]
         +- *Filter (isnotnull(k#111) && isnotnull(j#110))
            +- *FileScan orc default.table2[i#109,j#110,k#111] Batched: false, Format: ORC, Location: InMemoryFileIndex[file:/Users/tejasp/Desktop/dev/apache-hive-1.2.1-bin/warehouse/table2], PartitionFilters: [], PushedFilters: [IsNotNull(k), IsNotNull(j)], ReadSchema: struct<i:int,j:int,k:string>

{noformat};;;","13/May/17 05:36;cloud_fan;sorry I forgot to set the broadcast threshold, now I can reproduce this issue;;;","13/May/17 06:22;tejasp;Thanks for confirming. I have added it in the jira description in case someone comes across this in future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returned an Empty Result after Loading a Hive Table,SPARK-19120,13032829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smilegator,smilegator,smilegator,07/Jan/17 20:31,15/Jan/17 12:44,14/Jul/23 06:30,15/Jan/17 12:44,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,correctness,,,,,,,,"{noformat}
        sql(
          """"""
            |CREATE TABLE test (a STRING)
            |STORED AS PARQUET
          """""".stripMargin)

       spark.table(""test"").show()

        sql(
          s""""""
             |LOAD DATA LOCAL INPATH '$newPartitionDir' OVERWRITE
             |INTO TABLE test
           """""".stripMargin)

        spark.table(""test"").show()
{noformat}

The returned result is empty after table loading. We should refresh the metadata cache after loading the data to the table. ",,apachespark,cloud_fan,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 15 12:44:00 UTC 2017,,,,,,,,,,"0|i38eif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/17 20:33;smilegator;Will submit a fix soon.;;;","08/Jan/17 00:34;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16500;;;","15/Jan/17 12:44;cloud_fan;Issue resolved by pull request 16500
[https://github.com/apache/spark/pull/16500];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
script transformation does not work on Windows due to fixed bash executable location,SPARK-19117,13032810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,07/Jan/17 16:40,12/Dec/22 18:11,14/Jul/23 06:30,10/Jan/17 13:25,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"There are some tests failed on Windows via AppVeyor as below due to this problem :

{code}
 - script *** FAILED *** (553 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 54, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - Star Expansion - script transform *** FAILED *** (2 seconds, 375 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 389.0 failed 1 times, most recent failure: Lost task 0.0 in stage 389.0 (TID 725, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - test script transform for stdout *** FAILED *** (2 seconds, 813 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 391.0 failed 1 times, most recent failure: Lost task 0.0 in stage 391.0 (TID 726, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - test script transform for stderr *** FAILED *** (2 seconds, 407 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 393.0 failed 1 times, most recent failure: Lost task 0.0 in stage 393.0 (TID 727, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - test script transform data type *** FAILED *** (171 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 395.0 failed 1 times, most recent failure: Lost task 0.0 in stage 395.0 (TID 728, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - transform *** FAILED *** (359 milliseconds)
   Failed to execute query using catalyst:
   Error: Job aborted due to stage failure: Task 0 in stage 1347.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1347.0 (TID 2395, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified
  
 - schema-less transform *** FAILED *** (344 milliseconds)
   Failed to execute query using catalyst:
   Error: Job aborted due to stage failure: Task 0 in stage 1348.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1348.0 (TID 2396, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - transform with custom field delimiter *** FAILED *** (296 milliseconds)
   Failed to execute query using catalyst:
   Error: Job aborted due to stage failure: Task 0 in stage 1349.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1349.0 (TID 2397, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - transform with custom field delimiter2 *** FAILED *** (297 milliseconds)
   Failed to execute query using catalyst:
   Error: Job aborted due to stage failure: Task 0 in stage 1350.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1350.0 (TID 2398, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - transform with custom field delimiter3 *** FAILED *** (312 milliseconds)
   Failed to execute query using catalyst:
   Error: Job aborted due to stage failure: Task 0 in stage 1351.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1351.0 (TID 2399, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - transform with SerDe2 *** FAILED *** (437 milliseconds)
   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1355.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1355.0 (TID 2403, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - schemaless *** FAILED *** (78 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1968.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1968.0 (TID 3932, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified
  - script transformation - alias list *** FAILED *** (94 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1969.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1969.0 (TID 3933, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - alias list with type *** FAILED *** (93 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1970.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1970.0 (TID 3934, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - row format delimited clause with only one format property *** FAILED *** (78 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1971.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1971.0 (TID 3935, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - row format delimited clause with multiple format properties *** FAILED *** (94 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1972.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1972.0 (TID 3936, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - row format serde clauses with SERDEPROPERTIES *** FAILED *** (78 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1973.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1973.0 (TID 3937, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation - row format serde clauses without SERDEPROPERTIES *** FAILED *** (78 milliseconds)
   ...
   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1974.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1974.0 (TID 3938, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - cat without SerDe *** FAILED *** (156 milliseconds)
   ...
   Caused by: java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - cat with LazySimpleSerDe *** FAILED *** (63 milliseconds)
    ...
    org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2383.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2383.0 (TID 4819, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation should not swallow errors from upstream operators (no serde) *** FAILED *** (78 milliseconds)
    ...
    org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2384.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2384.0 (TID 4820, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - script transformation should not swallow errors from upstream operators (with serde) *** FAILED *** (47 milliseconds)
    ...
    org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2385.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2385.0 (TID 4821, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified

 - SPARK-14400 script transformation should fail for bad script command *** FAILED *** (47 milliseconds)
   ""Job aborted due to stage failure: Task 0 in stage 2386.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2386.0 (TID 4822, localhost, executor driver): java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, The system cannot find the file specified
{code}

The problem is in this line - https://github.com/apache/spark/blob/21c7539a5274a7e77686d17a6261d56592b85c2d/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/ScriptTransformation.scala#L70

We always assume {{bash}} is located in {{/bin/bash}}. In some cases such as installing Cygwin and use bash in cmd or using bash in Windows 10, they are not located there.
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 15 14:28:02 UTC 2017,,,,,,,,,,"0|i38ee7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/17 16:42;gurwls223;We can skip all the tests above if we are not going to support these bash in other systems. If we want to support these, we could introduce a hidden option for the bash location.;;;","07/Jan/17 16:44;gurwls223;I will submit a PR as soon as any committer decides this please.;;;","07/Jan/17 16:52;gurwls223;Ah, [~srowen], I just noticed some latest PRs dedicated to this functionality were reviewed by you. Could I please ask what you think about this?;;;","07/Jan/17 17:24;srowen;I think you could skip these tests in Windows. I think the 'assume' method may be good for this? or we've done the same elsewhere.;;;","08/Jan/17 11:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16501;;;","10/Jan/17 13:25;srowen;Issue resolved by pull request 16501
[https://github.com/apache/spark/pull/16501];;;","15/Jan/17 14:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/16586;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkSQL unsupports the command "" create external table if not exist  new_tbl like old_tbl location '/warehouse/new_tbl' """,SPARK-19115,13032773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ouyangxc.zte,ouyangxc.zte,ouyangxc.zte,07/Jan/17 06:45,12/Dec/22 18:10,14/Jul/23 06:30,14/Feb/17 03:44,2.0.1,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,"spark2.0.1 unsupports the command "" create external table if not exist  new_tbl like old_tbl location '/warehouse/new_tbl' ""
we tried to modify the  sqlbase.g4 file,change
""    | CREATE TABLE (IF NOT EXISTS)? target=tableIdentifier
        LIKE source=tableIdentifier                                    #createTableLike""
to
""    | CREATE EXTERNAL? TABLE (IF NOT EXISTS)? target=tableIdentifier
        LIKE source=tableIdentifier locationSpec?                                 #createTableLike""
modify method 'visitCreateTableLike' in scala file  'SparkSqlParser.scala' and  update case class CreateTableLikeCommand in 'tables.scala' file
finally  we compiled spark and replaced the jars as follow: 'spark-catalyst-2.0.1.jar','spark-sql_2.11-2.0.1.jar', and run  the command 'create external table if not exist  new_tbl like old_tbl location '/warehouse/new_tbl' successfully . ",spark2.0.1 hive1.2.1,apachespark,ouyangxc.zte,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Feb 09 08:38:03 UTC 2017,,,,,,,,,,"0|i38e5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/17 20:26;smilegator;When the location of the table is not user provided, it will be a managed table. It is not a bug, but by design.;;;","09/Jan/17 02:19;ouyangxc.zte;May I ask you whether Spark2.0.1 supports the following comman or not:create external table if not exists gen_tbl  like src_tbl?  
;;;","09/Jan/17 03:21;smilegator;Nope. When you using CREATE TABLE LIKE to create a table, it will be a managed table. ;;;","11/Jan/17 00:39;gurwls223;Hi all, I just wonder if it should be resolved as {{Won't Fix}}.;;;","12/Jan/17 00:51;ouyangxc.zte;May I ask you whether Spark supports the following comman or not:create external table if not exists gen_tbl like src_tbl location '/warehouse/data/gen_tbl' later version?
Do you have a plan to support this command in the future?;;;","18/Jan/17 07:25;ouyangxc.zte;spark2.x unsupports the sql command: create external table if not exists gen_tbl like src_tbl location '/warehouse/gen_tbl';;;","18/Jan/17 07:37;smilegator;Sorry for the late reply. This sounds reasonable. Does Hive support such a query?;;;","18/Jan/17 08:03;ouyangxc.zte;Hive has always supported the command,sparksql also supports the command  before spark2.0.
we have some spark applications  by using this command before upgrating to spark2.0 ,so I think spark2.x later should support the command.
thanks [~smilegator];;;","18/Jan/17 08:05;smilegator;Sure, I will do it. ;;;","18/Jan/17 08:35;ouyangxc.zte;thanks,I will create PR and try to resolve this issue.;;;","18/Jan/17 14:55;smilegator;You can make a try, but I am not sure it is straightforward to you. ;;;","19/Jan/17 03:21;apachespark;User 'ouyangxiaochen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16638;;;","09/Feb/17 08:38;apachespark;User 'ouyangxiaochen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16868;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
DistributedLDAModel returns different logPrior for original and loaded model,SPARK-19110,13032667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,06/Jan/17 20:38,10/Jan/17 00:14,14/Jul/23 06:30,07/Jan/17 19:42,1.3.1,1.4.1,1.5.2,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,ML,MLlib,,,,0,,,,,,,,,"While adding DistributedLDAModel training summary for SparkR, I found that the logPrior for original and loaded model is different.
For example, in the test(""read/write DistributedLDAModel""), I add the test:
val logPrior = model.asInstanceOf[DistributedLDAModel].logPrior
      val logPrior2 = model2.asInstanceOf[DistributedLDAModel].logPrior
      assert(logPrior === logPrior2)
The test fails:
-4.394180878889078 did not equal -4.294290536919573


",,apachespark,josephkb,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 10 00:14:04 UTC 2017,,,,,,,,,,"0|i38dif:",9223372036854775807,,,,,josephkb,,,,,,,,2.0.3,2.1.1,2.2.0,,,,,,,,,"06/Jan/17 20:45;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16491;;;","07/Jan/17 19:42;josephkb;Issue resolved by pull request 16491
[https://github.com/apache/spark/pull/16491];;;","10/Jan/17 00:14;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16524;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORC metadata section can sometimes exceed protobuf message size limit,SPARK-19109,13032650,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,nseggert,nseggert,06/Jan/17 20:04,12/Dec/22 18:10,14/Jul/23 06:30,17/Jan/18 06:28,1.6.3,2.0.2,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,2.3.0,,,,,SQL,,,,,1,,,,,,,,,"Basically, Spark inherits HIVE-11592 from its Hive dependency. From that issue:

If there are too many small stripes and with many columns, the overhead for storing metadata (column stats) can exceed the default protobuf message size of 64MB. Reading such files will throw the following exception
{code}
Exception in thread ""main"" com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.
        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)
        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)
        at com.google.protobuf.CodedInputStream.readRawBytes(CodedInputStream.java:811)
        at com.google.protobuf.CodedInputStream.readBytes(CodedInputStream.java:329)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics.<init>(OrcProto.java:1331)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics.<init>(OrcProto.java:1281)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics$1.parsePartialFrom(OrcProto.java:1374)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics$1.parsePartialFrom(OrcProto.java:1369)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics.<init>(OrcProto.java:4887)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics.<init>(OrcProto.java:4803)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics$1.parsePartialFrom(OrcProto.java:4990)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics$1.parsePartialFrom(OrcProto.java:4985)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics.<init>(OrcProto.java:12925)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics.<init>(OrcProto.java:12872)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics$1.parsePartialFrom(OrcProto.java:12961)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics$1.parsePartialFrom(OrcProto.java:12956)
        at com.google.protobuf.CodedInputStream.readMessage(CodedInputStream.java:309)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.<init>(OrcProto.java:13599)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.<init>(OrcProto.java:13546)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata$1.parsePartialFrom(OrcProto.java:13635)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata$1.parsePartialFrom(OrcProto.java:13630)
        at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)
        at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)
        at org.apache.hadoop.hive.ql.io.orc.OrcProto$Metadata.parseFrom(OrcProto.java:13746)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl$MetaInfoObjExtractor.<init>(ReaderImpl.java:468)
        at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:314)
        at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:228)
        at org.apache.hadoop.hive.ql.io.orc.FileDump.main(FileDump.java:67)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{code}

This is fixed in Hive 1.3, so it should be fairly straightforward to pick up the patch.

As a side note: Spark's management of its Hive fork/dependency seems incredibly arcane to me. Surely there's a better way than publishing to central from developers' personal repos.",,dongjoon,dougb,hexiaoqiao,nseggert,wangch@chinatelecom.cn,wangchao2017,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20901,,,,,,,,,,,,,,,,SPARK-19076,HIVE-11592,,,,,,,,SPARK-20682,SPARK-20728,SPARK-22279,,,"21/Aug/17 07:11;wangchao2017;InsertPic_.png;https://issues.apache.org/jira/secure/attachment/12882828/InsertPic_.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 17 06:46:00 UTC 2018,,,,,,,,,,"0|i38den:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/17 03:40;gurwls223;It seems this JIRA describes upgrading the version of Hive dependency which is currently 1.2.1 up to my knowledge. 
The stacktrace seems related with {{OrcFileOperator.getFileReader}} in Spark side which uses {{OrcFile.createReader}} to infer the schema.
Let me leave a link related with this.;;;","23/Jan/17 16:24;nseggert;That's one option, but we could also just backport the patch in HIVE-11592 to our Hive fork.;;;","09/Aug/17 08:04;wangchao2017;I meet this problem and resolved .I re-complie source code of hive-exec-1.2.1-spark2.jar of spark-2.1.0.
First download sourcecode of hive-exec-1.2.1-spark2.jar and the website is:
https://github.com/JoshRosen
Second: download the patch and put into ReaderImpl.java
https://issues.apache.org/jira/secure/attachment/12750949/HIVE-11592.1.patch 
Third：recompile and package the hive-exec-1.2.1-spark2.jar 
Last replace origin jar in spark-2.1.0/jars
;;;","18/Aug/17 07:56;dongjoon;Hi, [~nseggert] and [~wangchao2017].
Could you give us a way to reproduce this?;;;","21/Aug/17 07:11;wangchao2017;hi, I meet this problem and resolved by recompile  source code of hive-exec-1.2.1-spark2.jar of spark-2.1.0/jars
The source code website: https://github.com/JoshRosen 
Second: download the patch and put into ReaderImpl.java
https://issues.apache.org/jira/secure/attachment/12750949/HIVE-11592.1.patch 
then put this patch into ReaderImpl.java in Intellij IDE.
then, you can recompile and package the source code;
replace the origin jar in spark/jars



sydt2011@126.com
 
From: Dongjoon Hyun (JIRA)
Date: 2017-08-18 15:57
To: sydt2011
Subject: [jira] [Commented] (SPARK-19109) ORC metadata section can sometimes exceed protobuf message size limit
 
    [ https://issues.apache.org/jira/browse/SPARK-19109?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16131877#comment-16131877 ] 
 
Dongjoon Hyun commented on SPARK-19109:
---------------------------------------
 
Hi, [~nseggert] and [~wangchao2017].
Could you give us a way to reproduce this?
 
 
 
 
--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
;;;","21/Aug/17 14:42;nseggert;[~dongjoon] I don't have the ability to reproduce it on my end anymore. We did the same thing as [~wangchao2017] and re-built Spark against a version of Hive that contains the fix. If I remember correctly, the file in question as a little under a TB and had a few hundred columns.;;;","21/Aug/17 16:49;dongjoon;Thanks, Nic Eggert and sydt. I see.
I just wanted to resolve this officially in Apache Spark 2.3, but it's difficult to prove it without a valid test case.
I'll try to find another way. Thank you all again.;;;","22/Aug/17 02:13;wangchao2017;Thanks for your reply. I have compiled hive-exec-1.2.1-spark2.jar and it is about 10M ，it cannot be transfored to you because of your email size limit!
you can download the comiled jar in https://github.com/sydt2014/spark-hive     
       



sydt2011@126.com
 
From: Dongjoon Hyun (JIRA)
Date: 2017-08-22 00:50
To: sydt2011
Subject: [jira] [Commented] (SPARK-19109) ORC metadata section can sometimes exceed protobuf message size limit
 
    [ https://issues.apache.org/jira/browse/SPARK-19109?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16135422#comment-16135422 ] 
 
Dongjoon Hyun commented on SPARK-19109:
---------------------------------------
 
Thanks, Nic Eggert and sydt. I see.
I just wanted to resolve this officially in Apache Spark 2.3, but it's difficult to prove it without a valid test case.
I'll try to find another way. Thank you all again.
 
 
 
 
--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
;;;","22/Aug/17 05:11;dongjoon;Thanks, [~wangchao2017], but I think there are misunderstanding between you and me. :)
I already knew the Hive patch because [~nseggert] told that here.

The approach I want to do is to use new Apache ORC 1.4.0 directly. That's the reason why I ask the way of reproduce only here. I don't want to update the Hive fork of Spark. 

However, I really thank you for your attention and all your help!;;;","17/Jan/18 06:27;dongjoon;HIVE-11592 is fixed in Hive 1.3.0 and ORC 1.4.1 library has the patch. Since SPARK-20682 / SPARK-20728 / SPARK-22279, we are using native ORC implementation based on ORC 1.4.1. This issue is fixed in Apache Spark default configuration.
{code}
public static final int PROTOBUF_MESSAGE_MAX_LIMIT = 1024 << 20; // 1GB
{code};;;","17/Jan/18 06:46;wangch@chinatelecom.cn;thanks for your attention! I  am appreciated for your help !




中国电信集团公司 王超
部门中心：企业信息化事业部IT研发中心
移动电话：18916929162
工作邮箱：wangch@chinatelecom.cn 
通讯地址：上海市浦东新区秀沿西路189中国电信B23
邮政编码：201315

 
From: Dongjoon Hyun (JIRA)
Date: 2018-01-17 14:29
To: sydt2011
Subject: [jira] [Resolved] (SPARK-19109) ORC metadata section can sometimes exceed protobuf message size limit
 
     [ https://issues.apache.org/jira/browse/SPARK-19109?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]
 
Dongjoon Hyun resolved SPARK-19109.
-----------------------------------
       Resolution: Fixed
    Fix Version/s: 2.3.0
 
 
 
 
--
This message was sent by Atlassian JIRA
(v7.6.3#76005)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Styling for the configuration docs is broken,SPARK-19106,13032578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,nchammas,nchammas,06/Jan/17 15:21,28/Nov/17 15:13,14/Jul/23 06:30,07/Jan/17 19:19,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.1,,,,Documentation,,,,,0,,,,,,,,,"There are several styling problems with the configuration docs, starting roughly from the Scheduling section on down.

http://spark.apache.org/docs/latest/configuration.html

",,apachespark,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21889,SPARK-22627,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 15:21;nchammas;Screen Shot 2017-01-06 at 10.20.52 AM.png;https://issues.apache.org/jira/secure/attachment/12846026/Screen+Shot+2017-01-06+at+10.20.52+AM.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 07 19:19:47 UTC 2017,,,,,,,,,,"0|i38cyn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 17:17;srowen;Yeah, the section headings aren't rendering as section titles. Not a big deal but should be fixed. PR coming.;;;","06/Jan/17 17:19;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/16490;;;","07/Jan/17 19:19;srowen;Issue resolved by pull request 16490
[https://github.com/apache/spark/pull/16490];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 CompileException with Map and Case Class in Spark 2.1.0,SPARK-19104,13032517,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,nils.grabbert,nils.grabbert,06/Jan/17 10:18,18/Jul/17 01:39,14/Jul/23 06:30,27/Jun/17 17:00,2.1.0,2.2.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.2,2.2.0,,,,Optimizer,SQL,,,,6,,,,,,,,,"The following code will run with Spark 2.0.2 but not with Spark 2.1.0:

{code}
case class InnerData(name: String, value: Int)
case class Data(id: Int, param: Map[String, InnerData])

val data = Seq.tabulate(10)(i => Data(1, Map(""key"" -> InnerData(""name"", i + 100))))
val ds   = spark.createDataset(data)
{code}

Exception:
{code}
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 63, Column 46: Expression ""ExternalMapToCatalyst_value_isNull1"" is not an rvalue 
  at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:11004) 
  at org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:6639) 
  at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:5001) 
  at org.codehaus.janino.UnitCompiler.access$10500(UnitCompiler.java:206) 
  at org.codehaus.janino.UnitCompiler$13.visitAmbiguousName(UnitCompiler.java:4984) 
  at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3633) 
  at org.codehaus.janino.Java$Lvalue.accept(Java.java:3563) 
  at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4956) 
  at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4925) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3189) 
  at org.codehaus.janino.UnitCompiler.access$5100(UnitCompiler.java:206) 
  at org.codehaus.janino.UnitCompiler$9.visitAssignment(UnitCompiler.java:3143) 
  at org.codehaus.janino.UnitCompiler$9.visitAssignment(UnitCompiler.java:3139) 
  at org.codehaus.janino.Java$Assignment.accept(Java.java:3847) 
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3139) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2112) 
  at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:206) 
  at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1377) 
  at org.codehaus.janino.UnitCompiler$6.visitExpressionStatement(UnitCompiler.java:1370) 
  at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2558) 
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1370) 
  at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1450) 
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2811) 
  at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1262) 
  at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1234) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:538) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:890) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:894) 
  at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:206) 
  at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:377) 
  at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:369) 
  at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1128) 
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369) 
  at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1209) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:564) 
  at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:420) 
  at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:206) 
  at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:374) 
  at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:369) 
  at org.codehaus.janino.Java$AbstractPackageMemberClassDeclaration.accept(Java.java:1309) 
  at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:369) 
  at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:345) 
  at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:396) 
  at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:311) 
  at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:229) 
  at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:196) 
  at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:91) 
  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:935) 
  ... 77 more 
{code}

",,apachespark,cloud_fan,jse,marmbrus,maropu,mylesbaker,nathanwilliamgrand@gmail.com,nils.grabbert,praetp,spiricalsalsaz,steven.aerts,viirya,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-21391,,,SPARK-18891,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 13 18:04:03 UTC 2017,,,,,,,,,,"0|i38cl3:",9223372036854775807,,,,,,,,,,,,,2.3.0,,,,,,,,,,,"28/Jan/17 03:40;maropu;I think this issue is related to SPARK-18891;;;","03/May/17 09:04;nils.grabbert;[~maropu] I think this issue is not related to SPARK-18891. The example is still not working on 2.2 branch.;;;","11/May/17 12:03;steven.aerts;We bumped into this issue also in java and we reduced this to the following spark code:

{code:java}
case class Value (id: Int, value: String)
case class RowValue (c1: Map[Int, Value])
spark.createDataset(List(RowValue(Map(1 -> Value(2, ""bar"")))))
{code}

We confirm that it is still reproducible in spark 2.1.1.;;;","01/Jun/17 10:04;nathanwilliamgrand@gmail.com;We have also just come across this (on 2.1.0). Have tested and it is still an issue in the latest 2.3 nightly build. Is there a work-around for this issue? For us, this bug is a real pain as we use such case classes extensively in our libraries.;;;","02/Jun/17 06:39;nils.grabbert;[~marmbrus] Why are you moving this major bug to 2.3.0? As [~nathanwilliamgrand@gmail.com] has already mentioned, it is now almost impossible to work with case classes.;;;","02/Jun/17 16:38;marmbrus;I'm about to cut RC3 of 2.2 and there is no pull request to fix this.  Unfortunately that means it's not going to be fixed in 2.2.0;;;","26/Jun/17 04:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/18418;;;","26/Jun/17 04:37;viirya;Just found this issue. I proposed a PR to fix it. There's RC5 of 2.2, so I'm not sure if this can be in 2.2.0.;;;","27/Jun/17 17:00;cloud_fan;Issue resolved by pull request 18418
[https://github.com/apache/spark/pull/18418];;;","13/Jul/17 16:17;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18626;;;","13/Jul/17 18:04;apachespark;User 'kiszk' has created a pull request for this issue:
https://github.com/apache/spark/pull/18627;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cached tables are not used in SubqueryExpression,SPARK-19093,13032427,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dkbiswal,joshrosen,joshrosen,06/Jan/17 00:42,17/Mar/17 02:59,14/Jul/23 06:30,08/Jan/17 22:10,2.0.2,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"See reproduction at https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1903098128019500/2699761537338853/1395282846718893/latest.html

Consider the following:

{code}
Seq((""a"", ""b""), (""c"", ""d""))
  .toDS
  .write
  .parquet(""/tmp/rows"")

val df = spark.read.parquet(""/tmp/rows"")
df.cache()
df.count()
df.createOrReplaceTempView(""rows"")

spark.sql(""""""
  select * from rows cross join rows
"""""").explain(true)

spark.sql(""""""
  select * from rows where not exists (select * from rows)
"""""").explain(true)
{code}

In both plans, I'd expect that both sides of the joins would read from the cached table for both the cross join and anti join, but the left anti join produces the following plan which only reads the left side from cache and reads the right side via a regular non-cahced scan:

{code}
== Parsed Logical Plan ==
'Project [*]
+- 'Filter NOT exists#3994
   :  +- 'Project [*]
   :     +- 'UnresolvedRelation `rows`
   +- 'UnresolvedRelation `rows`

== Analyzed Logical Plan ==
_1: string, _2: string
Project [_1#3775, _2#3776]
+- Filter NOT predicate-subquery#3994 []
   :  +- Project [_1#3775 AS _1#3775#4001, _2#3776 AS _2#3776#4002]
   :     +- Project [_1#3775, _2#3776]
   :        +- SubqueryAlias rows
   :           +- Relation[_1#3775,_2#3776] parquet
   +- SubqueryAlias rows
      +- Relation[_1#3775,_2#3776] parquet

== Optimized Logical Plan ==
Join LeftAnti
:- InMemoryRelation [_1#3775, _2#3776], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
:     +- *FileScan parquet [_1#3775,_2#3776] Batched: true, Format: Parquet, Location: InMemoryFileIndex[dbfs:/tmp/rows], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string>
+- Project [_1#3775 AS _1#3775#4001, _2#3776 AS _2#3776#4002]
   +- Relation[_1#3775,_2#3776] parquet

== Physical Plan ==
BroadcastNestedLoopJoin BuildRight, LeftAnti
:- InMemoryTableScan [_1#3775, _2#3776]
:     +- InMemoryRelation [_1#3775, _2#3776], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)
:           +- *FileScan parquet [_1#3775,_2#3776] Batched: true, Format: Parquet, Location: InMemoryFileIndex[dbfs:/tmp/rows], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string>
+- BroadcastExchange IdentityBroadcastMode
   +- *Project [_1#3775 AS _1#3775#4001, _2#3776 AS _2#3776#4002]
      +- *FileScan parquet [_1#3775,_2#3776] Batched: true, Format: Parquet, Location: InMemoryFileIndex[dbfs:/tmp/rows], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string>
{code}",,apachespark,joshrosen,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 07 00:34:05 UTC 2017,,,,,,,,,,"0|i38c13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 03:18;smilegator;uh, this might be related to the subquery resolution.;;;","06/Jan/17 05:32;joshrosen;I'm not sure whether that's the case because I seemed to observe the cache resolving when using subqueries for regular joins. We should play around with it and see, though.;;;","06/Jan/17 05:45;smilegator;Let me try to do more investigation in this. ;;;","06/Jan/17 06:16;smilegator;Yes. The relation in `PlanExpression` is not replaced by the cached relation. Do you want to fix it by yourself?;;;","06/Jan/17 06:19;joshrosen;I'm a bit too busy with other work to tackle this right now, so this is up for grabs in case you'd like to work on it.

To be clear, I guess the bug here impacts SubqueryExpression rather than LeftAntiJoin specifically? If so, I should go ahead and update the JIRA description to be a bit clearer.;;;","06/Jan/17 06:23;smilegator;Yes. It is not related to LeftAntiJoin. It is in SubqueryExpression.

My teammate or I will take it. Thanks!;;;","06/Jan/17 06:30;smilegator;{noformat}
  /** Replaces segments of the given logical plan with cached versions where possible. */
  def useCachedData(plan: LogicalPlan): LogicalPlan = {
    plan transformDown {
      case currentFragment =>
        lookupCachedData(currentFragment)
          .map(_.cachedRepresentation.withOutput(currentFragment.output))
          .getOrElse(currentFragment)
    }
  }
{noformat}

The above function needs to consider the {{subqueryExpression}}. ;;;","07/Jan/17 00:34;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/16493;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save() API of DataFrameWriter should not scan all the saved files,SPARK-19092,13032422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,smilegator,smilegator,06/Jan/17 00:06,16/Jan/17 02:58,14/Jul/23 06:30,13/Jan/17 17:27,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"`DataFrameWriter`'s save() API is performing a unnecessary full filesystem scan for the saved files. The save() API is the most basic/core API in `DataFrameWriter`. We should avoid these unnecessary file scan. 

",,apachespark,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 15 19:33:02 UTC 2017,,,,,,,,,,"0|i38bzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 00:08;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16481;;;","15/Jan/17 19:33;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/16588;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sbin/start-history-server.sh scripts use of $@ without """"",SPARK-19083,13032213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zuo.tingbing9,zuo.tingbing9,zuo.tingbing9,05/Jan/17 09:47,06/Jan/17 17:59,14/Jul/23 06:30,06/Jan/17 17:59,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,,,,,,0,,,,,,,,,"sbin/start-history-server.sh script use of $@ without """" , this will affect the length of args which used in HistoryServerArguments::parse(args: List[String])

should write as follows:
exec ... org.apache.spark.deploy.history.HistoryServer 1 ""$@""",linux,apachespark,dongjoon,zuo.tingbing9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 06 07:24:03 UTC 2017,,,,,,,,,,"0|i38apj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/17 05:33;dongjoon;Hi, [~zuo.tingbing9].
Could you create a PR on GitHub? Apache Spark community uses GitHub.
Also, I removed the fix versions. The committers set them when they merge the pull request.;;;","06/Jan/17 05:59;zuo.tingbing9;Hi Dongjoon Hyun.
OK,I will create a PR. Thank you.
;;;","06/Jan/17 07:24;apachespark;User 'zuotingbing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16484;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The config ignoreCorruptFiles doesn't work for Parquet,SPARK-19082,13032163,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,05/Jan/17 04:47,09/Mar/17 18:49,14/Jul/23 06:30,16/Jan/17 07:28,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SQL,,,,,0,,,,,,,,,"
We have a config {{spark.sql.files.ignoreCorruptFiles}} which can be used to ignore corrupt files when reading files in SQL. Currently the {{ignoreCorruptFiles}} config has two issues and can't work for Parquet:

1. We only ignore corrupt files in {{FileScanRDD}} . Actually, we begin to read those files as early as inferring data schema from the files. For corrupt files, we can't read the schema and fail the program. A related issue reported at http://apache-spark-developers-list.1001551.n3.nabble.com/Skip-Corrupted-Parquet-blocks-footer-tc20418.html

2. In {{FileScanRDD}}, we assume that we only begin to read the files when starting to consume the iterator. However, it is possibly the files are read before that. In this case, {{ignoreCorruptFiles}} config doesn't work too.
",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19885,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 05 05:06:04 UTC 2017,,,,,,,,,,"0|i38aef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/17 05:06;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16474;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LauncherState should be only set to SUBMITTED after the application is submitted,SPARK-19073,13031827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shimingfei,shimingfei,shimingfei,04/Jan/17 01:46,04/Jan/17 10:44,14/Jul/23 06:30,04/Jan/17 10:28,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Deploy,,,,,0,,,,,,,,,"LauncherState should be only set to SUBMITTED after the application is submitted.
Currently the state is set before the application is actually submitted.",,apachespark,shimingfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 04 10:28:03 UTC 2017,,,,,,,,,,"0|i388br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/17 01:47;apachespark;User 'shimingfei' has created a pull request for this issue:
https://github.com/apache/spark/pull/16459;;;","04/Jan/17 10:28;srowen;Issue resolved by pull request 16459
[https://github.com/apache/spark/pull/16459];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catalyst's IN always returns false for infinity,SPARK-19072,13031822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,kayousterhout,kayousterhout,04/Jan/17 00:48,04/Jan/17 06:40,14/Jul/23 06:30,04/Jan/17 06:40,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,Tests,,,,0,,,,,,,,,"This bug was caused by the fix for SPARK-18999 (https://github.com/apache/spark/pull/16402)

This can be reproduced by adding the following test to PredicateSuite.scala (which will consistently fail):

    val value = NonFoldableLiteral(Double.PositiveInfinity, DoubleType)
    checkEvaluation(In(value, List(value)), true)

This bug is causing org.apache.spark.sql.catalyst.expressions.PredicateSuite.IN to fail approximately 10% of the time (it fails anytime the value is Infinity or -Infinity and the correct answer is True -- e.g., https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70826/testReport/org.apache.spark.sql.catalyst.expressions/PredicateSuite/IN/, https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/70830/console).",,apachespark,kayousterhout,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 04 06:40:32 UTC 2017,,,,,,,,,,"0|i388an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/17 03:41;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16469;;;","04/Jan/17 06:40;yhuai;Issue resolved by pull request 16469
[https://github.com/apache/spark/pull/16469];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose task 'status' and 'duration' in spark history server REST API.,SPARK-19069,13031818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,paragpc,paragpc,paragpc,04/Jan/17 00:14,20/Jan/17 16:50,14/Jul/23 06:30,20/Jan/17 16:49,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"Although Spark history server UI shows task ‘status’ and ‘duration’ fields, it does not expose these fields in the REST API response. For the Spark history server API users, it is not possible to determine task status and duration. Spark history server has access to task status and duration from event log, but it is not exposing these in API. This patch is proposed to expose task ‘status’ and ‘duration’ fields in Spark history server REST API.

e.g. Spark history server UI: PFA

e.g. Spark history sever REST API response with no ‘status’ and ‘duration’:
{noformat}
{
      ""taskId"" : 7,
      ""index"" : 0,
      ""attempt"" : 0,
      ""launchTime"" : ""2017-01-02T17:32:43.037GMT"",
      ""executorId"" : ""2"",
      ""host"" : ""ip-10-171-154-17.ec2.internal"",
      ""taskLocality"" : ""NODE_LOCAL"",
      ""speculative"" : false,
      ""accumulatorUpdates"" : [ ],
      ""taskMetrics"" : {
        ""executorDeserializeTime"" : 138,
        ""executorRunTime"" : 10524,
        ""resultSize"" : 2078,
        ""jvmGcTime"" : 240,
        ""resultSerializationTime"" : 0,
        ""memoryBytesSpilled"" : 0,
        ""diskBytesSpilled"" : 0,
        ""inputMetrics"" : {
          ""bytesRead"" : 0,
          ""recordsRead"" : 0
        },
        ""outputMetrics"" : {
          ""bytesWritten"" : 7474953,
          ""recordsWritten"" : 287254
        },
        ""shuffleReadMetrics"" : {
          ""remoteBlocksFetched"" : 4,
          ""localBlocksFetched"" : 3,
          ""fetchWaitTime"" : 203,
          ""remoteBytesRead"" : 4740801,
          ""localBytesRead"" : 2011044,
          ""recordsRead"" : 134
        },
        ""shuffleWriteMetrics"" : {
          ""bytesWritten"" : 0,
          ""writeTime"" : 0,
          ""recordsWritten"" : 0
        }
      }
    }
{noformat}

",,apachespark,irashid,jonathak,paragpc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/17 00:22;paragpc;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12845456/screenshot-1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 16:49:32 UTC 2017,,,,,,,,,,"0|i3889r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/17 22:30;apachespark;User 'paragpc' has created a pull request for this issue:
https://github.com/apache/spark/pull/16473;;;","20/Jan/17 16:49;irashid;Issue resolved by pull request 16473
[https://github.com/apache/spark/pull/16473];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR LDA doesn't set optimizer correctly,SPARK-19066,13031808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wm624,wm624,wm624,03/Jan/17 23:26,18/Jan/17 05:25,14/Jul/23 06:30,16/Jan/17 14:11,,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,SparkR,,,,,0,,,,,,,,,"spark.lda pass the optimizer ""em"" or ""online"" to the backend. However, LDAWrapper doesn't set optimizer based on the value from R. Therefore, for optimizer ""em"", the `isDistributed` field is FALSE, which should be TRUE.

In addition, the `summary` method should bring back the results related to `DistributedLDAModel`. ",,apachespark,wm624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 19:55:03 UTC 2017,,,,,,,,,,"0|i3887j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/17 23:31;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16464;;;","17/Jan/17 19:55;apachespark;User 'wangmiao1981' has created a pull request for this issue:
https://github.com/apache/spark/pull/16623;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropDuplicates uses the same expression id for Alias and Attribute and breaks attribute replacement,SPARK-19065,13031804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,marmbrus,marmbrus,03/Jan/17 23:09,17/Jan/17 18:43,14/Jul/23 06:30,17/Jan/17 17:58,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,,,,,0,,,,,,,,,"Right now if you use .dropDuplicates in a stream you get an exception because attribute replacement is broken

Here is an example:
{code}
org.apache.spark.sql.AnalysisException: resolved attribute(s) accountName#34351,eventSource#34331,resources#34339,eventType#34333,readOnly#34335,date#34350,errorCode#34327,errorMessage#34328,userAgent#34344,eventVersion#34334,eventTime#34332,recipientAccountId#34336,sharedEventID#34341,timing#34349,apiVersion#34325,additionalEventData#34324,requestParameters#34338,sourceIPAddress#34342,serviceEventDetails#34343,timestamp#34323,awsRegion#34326,eventName#34330,responseElements#34340,filename#34347,requestID#34337,vpcEndpointId#34346,line#34348,userIdentity#34345 missing from requestID#34119,eventSource#34113,serviceEventDetails#34125,eventVersion#34116,userIdentity#34127,requestParameters#34120,accountName#34133,apiVersion#34107,eventTime#34114,additionalEventData#34106,line#34130,readOnly#34117,sourceIPAddress#34124,eventID#34329,errorCode#34109,resources#34121,timing#34131,userAgent#34126,eventType#34115,recipientAccountId#34118,errorMessage#34110,vpcEndpointId#34128,sharedEventID#34123,filename#34129,awsRegion#34108,responseElements#34122,date#34132,timestamp#34105,eventName#34112 in operator !Project [timestamp#34323, additionalEventData#34324, apiVersion#34325, awsRegion#34326, errorCode#34327, errorMessage#34328, eventID#34329, eventName#34330, eventSource#34331, eventTime#34332, eventType#34333, eventVersion#34334, readOnly#34335, recipientAccountId#34336, requestID#34337, requestParameters#34338, resources#34339, responseElements#34340, sharedEventID#34341, sourceIPAddress#34342, serviceEventDetails#34343, userAgent#34344, userIdentity#34345, vpcEndpointId#34346, ... 5 more fields];;

!Project [timestamp#34323, additionalEventData#34324, apiVersion#34325, awsRegion#34326, errorCode#34327, errorMessage#34328, eventID#34329, eventName#34330, eventSource#34331, eventTime#34332, eventType#34333, eventVersion#34334, readOnly#34335, recipientAccountId#34336, requestID#34337, requestParameters#34338, resources#34339, responseElements#34340, sharedEventID#34341, sourceIPAddress#34342, serviceEventDetails#34343, userAgent#34344, userIdentity#34345, vpcEndpointId#34346, ... 5 more fields]
+- Aggregate [eventID#34329], [first(timestamp#34323, false) AS timestamp#34105, first(additionalEventData#34324, false) AS additionalEventData#34106, first(apiVersion#34325, false) AS apiVersion#34107, first(awsRegion#34326, false) AS awsRegion#34108, first(errorCode#34327, false) AS errorCode#34109, first(errorMessage#34328, false) AS errorMessage#34110, eventID#34329, first(eventName#34330, false) AS eventName#34112, first(eventSource#34331, false) AS eventSource#34113, first(eventTime#34332, false) AS eventTime#34114, first(eventType#34333, false) AS eventType#34115, first(eventVersion#34334, false) AS eventVersion#34116, first(readOnly#34335, false) AS readOnly#34117, first(recipientAccountId#34336, false) AS recipientAccountId#34118, first(requestID#34337, false) AS requestID#34119, first(requestParameters#34338, false) AS requestParameters#34120, first(resources#34339, false) AS resources#34121, first(responseElements#34340, false) AS responseElements#34122, first(sharedEventID#34341, false) AS sharedEventID#34123, first(sourceIPAddress#34342, false) AS sourceIPAddress#34124, first(serviceEventDetails#34343, false) AS serviceEventDetails#34125, first(userAgent#34344, false) AS userAgent#34126, first(userIdentity#34345, false) AS userIdentity#34127, first(vpcEndpointId#34346, false) AS vpcEndpointId#34128, ... 5 more fields]
   +- Project [timestamp#34323, additionalEventData#34324, apiVersion#34325, awsRegion#34326, errorCode#34327, errorMessage#34328, eventID#34329, eventName#34330, eventSource#34331, eventTime#34332, eventType#34333, eventVersion#34334, readOnly#34335, recipientAccountId#34336, requestID#34337, requestParameters#34338, resources#34339, responseElements#34340, sharedEventID#34341, sourceIPAddress#34342, serviceEventDetails#34343, userAgent#34344, userIdentity#34345, vpcEndpointId#34346, ... 5 more fields]
      +- Relation[timestamp#34323,additionalEventData#34324,apiVersion#34325,awsRegion#34326,errorCode#34327,errorMessage#34328,eventID#34329,eventName#34330,eventSource#34331,eventTime#34332,eventType#34333,eventVersion#34334,readOnly#34335,recipientAccountId#34336,requestID#34337,requestParameters#34338,resources#34339,responseElements#34340,sharedEventID#34341,sourceIPAddress#34342,serviceEventDetails#34343,userAgent#34344,userIdentity#34345,vpcEndpointId#34346,... 5 more fields] parquet

	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:337)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:67)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:67)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:68)
	at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.IncrementalExecution.optimizedPlan$lzycompute(IncrementalExecution.scala:60)
	at org.apache.spark.sql.execution.streaming.IncrementalExecution.optimizedPlan(IncrementalExecution.scala:60)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3.apply(StreamExecution.scala:516)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3.apply(StreamExecution.scala:508)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:265)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:45)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(StreamExecution.scala:508)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcV$sp(StreamExecution.scala:267)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:256)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:256)
	at org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:265)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:45)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:256)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:251)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:186)
{code}",,apachespark,cloud_fan,marmbrus,ozawa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-17866,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 17:58:14 UTC 2017,,,,,,,,,,"0|i3886n:",9223372036854775807,,,,,,,,,,,,,2.1.1,,,,,,,,,,,"13/Jan/17 11:28;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16564;;;","17/Jan/17 17:58;cloud_fan;Issue resolved by pull request 16564
[https://github.com/apache/spark/pull/16564];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix pip install issue with ml sub components,SPARK-19064,13031801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,holden,holden,holden,03/Jan/17 22:53,16/Apr/17 11:58,14/Jul/23 06:30,06/Mar/17 16:17,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,PySpark,,,,,0,,,,,,,,,"The ML sub components aren't currently published, add them and add a test they can be imported so it doesn't happen again.",,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19608,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 04 01:10:03 UTC 2017,,,,,,,,,,"0|i3885z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/17 01:10;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/16465;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.writeByteBuffer should not modify buffer position,SPARK-19062,13031747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,03/Jan/17 20:11,04/Jan/17 19:24,14/Jul/23 06:30,04/Jan/17 19:24,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"[~mridulm80] pointed out that Utils.writeByteBuffer may change the position of the underlying byte buffer, which could potentially lead to subtle bugs for callers of that function.  We should change this so Utils.writeByteBuffer doesn't change the buffer position.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 03 20:15:04 UTC 2017,,,,,,,,,,"0|i387u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/17 20:15;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/16462;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to retrieve data from a parquet table whose name starts with underscore,SPARK-19059,13031670,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jayadevan.m,gbloisi,gbloisi,03/Jan/17 14:34,19/Jan/17 12:21,14/Jul/23 06:30,19/Jan/17 12:21,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,Spark Core,,,,,0,,,,,,,,,"It looks like there is some bug introduced in Spark 2.1.0 preventing to read data from a parquet table (hive support is enabled) whose name starts with underscore. CREATE and INSERT statements on the same table instead seems to work as expected.

The problem can be reproduced from spark-shell through the following steps:
1) Create a table with some values
scala> spark.sql(""CREATE TABLE `_a`(i INT) USING parquet"").show
scala> spark.sql(""INSERT INTO `_a` VALUES (1), (2), (3)"").show

2) Select data from the just created and filled table --> no results
scala> spark.sql(""SELECT * FROM `_a`"").show
+---+
|  i|
+---+
+---+

3) rename the table so that the prefixing underscore disappears
scala> spark.sql(""ALTER TABLE `_a` RENAME TO `a`"").show

4) select data from the just renamed table --> results are shown
scala> spark.sql(""SELECT * FROM `a`"").show
+---+
|  i|
+---+
|  1|
|  2|
|  3|
+---+



",,apachespark,gbloisi,jayadevan.m,remonk,thomastechs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 18 19:12:30 UTC 2017,,,,,,,,,,"0|i387db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/17 21:20;remonk;I was not able to reproduce it
{code}
spark-2.1.0-bin-hadoop2.7$ bin/spark-shell 
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).
17/01/03 13:10:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/01/03 13:10:46 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1483477846511).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.0
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_66)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.sql(""CREATE TABLE `_a`(i INT) USING parquet"").show
17/01/03 13:11:20 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/01/03 13:11:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/01/03 13:11:23 WARN HiveMetaStore: Location: file:/$HOME/Downloads/spark-2.1.0-bin-hadoop2.7/spark-warehouse/_a specified for non-external table:_a
++
||
++
++


scala> spark.sql(""INSERT INTO `_a` VALUES (1), (2), (3)"").show
SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
++
||
++
++


scala> spark.sql(""SELECT * FROM `_a`"").show
+---+
|  i|
+---+
|  1|
|  2|
|  3|
+—+

{code};;;","03/Jan/17 21:33;gbloisi;I'm using version 2.1.0 and I suspect it is a regression introduced with the changes in metadata caching.;;;","03/Jan/17 21:35;gbloisi;Please note your environment is printing version 2.0.0 even though the command line prompt seems located in 2.1.0 folder ;;;","03/Jan/17 22:27;remonk;My apologies, yes, I had my SPARK_HOME set to a different path/version when I ran that test.  I’m able to reproduce that issue.;;;","18/Jan/17 18:57;thomastechs;Me and [~jayadevan.m] working on it.;;;","18/Jan/17 19:03;apachespark;User 'jayadevanmurali' has created a pull request for this issue:
https://github.com/apache/spark/pull/16635;;;","18/Jan/17 19:12;jayadevan.m;@uncleGen @Eric Liang @cloud-fan

Could you please review this PR;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix partition related behaviors with DataFrameWriter.saveAsTable,SPARK-19058,13031656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,03/Jan/17 13:07,05/Jan/17 06:11,14/Jul/23 06:30,05/Jan/17 06:11,,,,,,,,,,,,,,,,,,,,,,,,,,,2.2.0,,,,,SQL,,,,,0,,,,,,,,,,,apachespark,cloud_fan,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 05 06:11:59 UTC 2017,,,,,,,,,,"0|i387a7:",9223372036854775807,,,,,,,,,,,,,2.2.0,,,,,,,,,,,"03/Jan/17 13:13;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/16460;;;","05/Jan/17 06:11;cloud_fan;Issue resolved by pull request 16460
[https://github.com/apache/spark/pull/16460];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSession initialization will be associated with invalid SparkContext when new SparkContext is created to replace stopped SparkContext,SPARK-19055,13031570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,03/Jan/17 03:14,12/Jan/17 12:55,14/Jul/23 06:30,12/Jan/17 12:55,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0.3,2.1.1,2.2.0,,,PySpark,SQL,,,,0,,,,,,,,,"In SparkSession initialization, we store created the instance of SparkSession into a class variable _instantiatedContext. Next time we can use SparkSession.builder.getOrCreate() to retrieve the existing SparkSession instance.

However, when the active SparkContext is stopped and we create another new SparkContext to use, the existing SparkSession is still associated with the stopped SparkContext. So the operations with this existing SparkSession will be failed.

We need to detect such case in SparkSession and renew the class variable _instantiatedContext if needed.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19138,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 03 03:36:05 UTC 2017,,,,,,,,,,"0|i386r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/17 03:36;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/16454;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix EventTimeWatermarkSuite 'delay in months and years handled correctly',SPARK-19050,13031433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,01/Jan/17 19:57,01/Jan/17 21:26,14/Jul/23 06:30,01/Jan/17 21:26,2.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,2.1.1,2.2.0,,,,Structured Streaming,Tests,,,,0,,,,,,,,,,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19049,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 01 20:00:07 UTC 2017,,,,,,,,,,"0|i385wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/17 19:59;zsxwing;monthsSinceEpoch in this test is like math.floor(num), so monthDiff has two possible values.;;;","01/Jan/17 20:00;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/16449;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
